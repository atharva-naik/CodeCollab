{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.REASON.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f08a1",
   "metadata": {},
   "source": [
    "## Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.BAD.value_counts()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af315d9",
   "metadata": {},
   "source": [
    "- The class is composed of 80% majority (negative) class and 20% minority (positive) class. This is an example of a fairly imbalanced class\n",
    "\n",
    "\n",
    "- The objective of this project is to maximize the prediction on the minority (positive) class, i.e sensitivity or recall of the applicant defaulted on loan or seriously delinquent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3350c23",
   "metadata": {},
   "source": [
    "## Statistical description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68cebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c4f82",
   "metadata": {},
   "source": [
    "- The average loan is about 11207 dollars and the average collateral is about 57,386 dollars "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f1947",
   "metadata": {},
   "source": [
    "## Correlation coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b37616",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df4a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.style.background_gradient().set_precision(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfadbfd",
   "metadata": {},
   "source": [
    "- We can see that value and amount due on existing mortgage have a strong correlation of 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44c979",
   "metadata": {},
   "source": [
    "## Exploratory data analysis\n",
    "\n",
    "Next, let's look at the scatter plot for proper visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45382c1",
   "metadata": {},
   "source": [
    "Let's check for the class distribution after splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea55d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Class training distribution:\\n', (y_train.value_counts()/X_train.shape[0])*100)\n",
    "print('\\n')\n",
    "print('Class test distribution:\\n', (y_test.value_counts()/X_test.shape[0])*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395245d",
   "metadata": {},
   "source": [
    "- Indeed, the training and test class distributions are a representative of the original class distribution. This is called stratified sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4bc95",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "As we saw above, there quite a substantial amount of missing values. Before we build a machine learning model, we will first fix the missing values for a better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867d9db",
   "metadata": {},
   "source": [
    "## DataFrame Imputer\n",
    "\n",
    "We now create a dataframe imputer to impute missing categorical and numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is based on some nice code by 'sveitser' at http://stackoverflow.com/a/25562948\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "    \n",
    "    def __init_(self):\n",
    "        \n",
    "        \"\"\" Impute missing categorical and numerical  values.\n",
    "        Columns of dtype object are imputed with the most frequent value in column.\n",
    "        Columns of other types are imputed with median of column.\"\"\"   \n",
    "        \n",
    "    def fit(self, X, y= None):\n",
    "        \n",
    "        self.impute = pd.Series([X[col].value_counts().index[0] if X[col].dtype == np.dtype('O') \n",
    "                                else X[col].median() for col in X.columns], index = X.columns)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y= None):\n",
    "        \n",
    "        return X.fillna(self.impute)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb2ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the DataFrame Imputer class\n",
    "imputer = DataFrameImputer()\n",
    "\n",
    "# Fit transform the training set\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "\n",
    "# Only transform the training set\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f04548",
   "metadata": {},
   "source": [
    "Now we can cross-check for missing values after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1d10b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b37b8",
   "metadata": {},
   "source": [
    "## Feature scaling and handling categorical attributes\n",
    "\n",
    "In this section, we will scale the features and convert the categorical attributes to numerics. This process is standard for machine learning algorithms to work efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e82716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical attributes\n",
    "cat_subset = list(X.select_dtypes('O'))\n",
    "\n",
    "# List of numerical attributes\n",
    "num_subset = list(X.select_dtypes('number'))\n",
    "\n",
    "# Binarize the categorical attributes\n",
    "cat_attribs = [([cat], LabelBinarizer()) for cat in cat_subset]\n",
    "\n",
    "# Scale the numerical attributes\n",
    "num_attribs = [([num], StandardScaler()) for num in num_subset]\n",
    "\n",
    "# Build a dataframe mapper pipeline\n",
    "mapper = DataFrameMapper(cat_attribs + num_attribs)\n",
    "\n",
    "# Fit transform the training set\n",
    "X_train_prepared = mapper.fit_transform(X_train_imputed)\n",
    "\n",
    "# Only transform the training set\n",
    "X_test_prepared = mapper.transform(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1dab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the data after pre-processing\n",
    "print('Training set size after pre-processing:', X_train_prepared.shape)\n",
    "print('Test set size after pre-processing:', X_test_prepared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60849aae",
   "metadata": {},
   "source": [
    "## Class variable to numpy array\n",
    "\n",
    "In this section, we convert the class variable to numpy array, which is what machine learning algorithms expect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969cc4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_label = y_train.values\n",
    "y_test_label = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee120de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the the target variable\n",
    "print(y_train_label.shape)\n",
    "print(y_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb225752",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set class distribution:', np.bincount(y_train_label))\n",
    "print('Test set class distribution:',  np.bincount(y_test_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce2fe9",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eefb1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore deprecated warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# Model performance metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, auc\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, average_precision_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b5df06",
   "metadata": {},
   "source": [
    "Let's define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99373694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection by cross-validation\n",
    "def model_selection_cv(model, n_training_samples, n_training_labels, cv_fold, scoring = None):\n",
    "    \n",
    "        # Fit the imbalanced training set\n",
    "        model.fit(n_training_samples, n_training_labels)\n",
    "        \n",
    "        # Compute accuracy on 10-fold cross validation\n",
    "        score = cross_val_score(model, n_training_samples, n_training_labels, cv = cv_fold, scoring = scoring)\n",
    "\n",
    "        # Make prediction on 10-fold cross validation\n",
    "        y_val_pred = cross_val_predict(model, n_training_samples, n_training_labels, cv = cv_fold)\n",
    "\n",
    "        # Make probability prediction on 10-fold cross validation\n",
    "        y_pred_proba = cross_val_predict(model,n_training_samples, n_training_labels, \n",
    "                                         cv = cv_fold, method = 'predict_proba')[:,1]\n",
    "\n",
    "        # Print results\n",
    "        print('CV score: %f (%f)'%(score.mean(), score.std()))\n",
    "        print('AUROC: %f'%(roc_auc_score(n_training_labels, y_pred_proba)))\n",
    "        print('Predicted classes:', np.unique(y_val_pred))\n",
    "        print('Confusion matrix:\\n', confusion_matrix(n_training_labels, y_val_pred))\n",
    "        print('Classification report:\\n', classification_report(n_training_labels, y_val_pred))\n",
    "        print('#####################################################################')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction on the test set\n",
    "def model_prediction(model, n_training_samples, n_training_labels, n_test_samples, n_test_labels):\n",
    "   \n",
    "        # Fit the training set\n",
    "        model.fit(n_training_samples, n_training_labels)\n",
    "\n",
    "        # Make prediction on the test set\n",
    "        y_predict = model.predict(n_test_samples)\n",
    "\n",
    "        # Compute the accuracy of the model\n",
    "        accuracy = accuracy_score(n_test_labels, y_predict)\n",
    "\n",
    "        # Predict probability\n",
    "        y_predict_proba = model.predict_proba(n_test_samples)[:,1]\n",
    "\n",
    "        print('Test accuracy:  %f'%(accuracy))\n",
    "        print('AUROC: %f'%(roc_auc_score(n_test_labels, y_predict_proba)))\n",
    "        print('AUPRC: %f'%(average_precision_score(n_test_labels, y_predict_proba)))\n",
    "        print('Predicted classes:', np.unique(y_predict))\n",
    "        print('Confusion matrix:\\n', confusion_matrix(n_test_labels, y_predict))\n",
    "        print('Classification report:\\n', classification_report(n_test_labels, y_predict))\n",
    "        print('#####################################################################') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR Curves\n",
    "def Plot_ROC_Curve_and_PRC(model, n_training_samples, n_training_labels, n_test_samples, n_test_labels,\n",
    "                         color= None, label =None): \n",
    "                        \n",
    "    model.fit(n_training_samples, n_training_labels)\n",
    "\n",
    "    y_pred_proba = model.predict_proba(n_test_samples)[:, 1]\n",
    "\n",
    "    # Compute the fpr and tpr for each classifier\n",
    "    fpr, tpr, thresholds = roc_curve(n_test_labels, y_pred_proba)\n",
    "    \n",
    "    # Compute the precisions and recalls for the classifier\n",
    "    precisions, recalls, thresholds = precision_recall_curve(n_test_labels, y_pred_proba)\n",
    "    \n",
    "    # Compute the area under the ROC curve for each classifier\n",
    "    area_auc =roc_auc_score(n_test_labels, y_pred_proba)\n",
    "    \n",
    "    # Compute the area under the PR curve for the classifier\n",
    "    area_prc = auc(recalls, precisions)\n",
    "    \n",
    "    # ROC Curve\n",
    "    plt.subplot(121)\n",
    "    plt.plot(fpr, tpr, color = color, label = (label) %area_auc)\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.xlabel('False positive rate (FPR)')\n",
    "    plt.ylabel('True positive rate (TPR)')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc = 'best')\n",
    "    \n",
    "    # PR Curve\n",
    "    plt.subplot(122)\n",
    "    plt.plot(recalls, precisions, color = color, label = (label) %area_prc)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df36c17a",
   "metadata": {},
   "source": [
    "# 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2242e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation prediction\n",
    "model_selection_cv(RandomForestClassifier(), X_train_prepared, y_train_label, cv_fold = 10, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ed900",
   "metadata": {},
   "source": [
    "## A. Hyperparameter tuning to optimize AUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfcc527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "params = {'n_estimators': [100, 300, 500],'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "          'max_depth': range(2,10,1),'min_samples_split': range(2,10,1),'min_samples_leaf': range(1,10,2),\n",
    "          'bootstrap': [True, False],'class_weight': ['balanced', 'balanced_subsample',{0:1, 1:4}, {0:1, 1:10}]}\n",
    "          \n",
    "rf =  RandomForestClassifier()\n",
    "                         \n",
    "rsearch_rf = RandomizedSearchCV(estimator = rf, param_distributions = params, n_iter = 100,\n",
    "                                  scoring = 'average_precision', cv = 10, n_jobs = -1, \n",
    "                                  random_state = 42, verbose = 1)  \n",
    "                                                        \n",
    "rsearch_rf.fit(X_train_prepared, y_train_label)\n",
    "\n",
    "print('Best score:', rsearch_rf.best_score_) \n",
    "print('Best hyperparameters:', rsearch_rf.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31efc7",
   "metadata": {},
   "source": [
    "## B. Prediction on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d5dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction(rsearch_rf.best_estimator_, X_train_prepared, y_train_label, X_test_prepared, y_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae624ad",
   "metadata": {},
   "source": [
    "# 2. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# Cross-validation prediction\n",
    "model_selection_cv(xgb.XGBClassifier(), X_train_prepared, y_train_label, cv_fold = 10, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653aae5",
   "metadata": {},
   "source": [
    "## A. Hyperparameter tuning  to optimize AUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train test split library\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify sampling with 20% test set and 80% training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f355dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set size:', X_train.shape)\n",
    "print('Class training set size:', y_train.shape)\n",
    "print('Test set size:', X_test.shape)\n",
    "print('Class test set size:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = sum(y_train_label == 0)/sum(y_train_label ==1)\n",
    "ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde86e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb =  xgb.XGBClassifier(objective = 'binary:logistic', scale_pos_weight = ratio, n_estimators = 5000)\n",
    "                         \n",
    "params = {'max_depth': range(1,10,2), 'min_child_weight': range(1,6,2),'gamma':[i/100 for i in range(0,6)],\n",
    "         'subsample':[i/10 for i in range(5,9)],'colsample_bytree': [i/10 for i in range(5,9)],\n",
    "         'learning_rate': [0.01, 0.02, 0.05, 0.1]}\n",
    "              \n",
    "rsearch_xgb = RandomizedSearchCV(estimator = xgb, param_distributions = params, n_iter = 200,  \n",
    "                                  scoring = 'average_precision', cv = 10, n_jobs = -1, \n",
    "                                  random_state = 42, verbose = 1)  \n",
    "                                                        \n",
    "rsearch_xgb.fit(X_train_prepared, y_train_label)\n",
    "\n",
    "print('Best score:', rsearch_xgb.best_score_) \n",
    "print('Best hyperparameters:', rsearch_xgb.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c28724",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction(rsearch_xgb.best_estimator_, X_train_prepared, y_train_label, X_test_prepared, y_test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388fb4dd",
   "metadata": {},
   "source": [
    "## ROC and PR Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b81f3",
   "metadata": {},
   "source": [
    "# Home Equity  (HMEQ) Loan\n",
    "The data set Home Equity Loan (HMEQ) reports characteristics and delinquency information for 5,960 home equity loans. A home equity loan is a loan where the obligor uses the equity of his or her home as the underlying collateral. \n",
    "\n",
    "\n",
    "# Attribute Information\n",
    "\n",
    "The data set has the following characteristics:\n",
    "\n",
    "1). BAD: 1 = applicant defaulted on loan or seriously delinquent; 0 = applicant paid loan\n",
    "\n",
    "2). LOAN: Amount of the loan request\n",
    "\n",
    "3). MORTDUE: Amount due on existing mortgage\n",
    "\n",
    "4). VALUE: Value of current property\n",
    "\n",
    "5). REASON: DebtCon = debt consolidation; HomeImp = home improvement\n",
    "\n",
    "6). JOB: Occupational categories\n",
    "\n",
    "7). YOJ: Years at present job\n",
    "\n",
    "8). DEROG: Number of major derogatory reports\n",
    "\n",
    "9). DELINQ: Number of delinquent credit lines\n",
    "\n",
    "10).  CLAGE: Age of oldest credit line in months\n",
    "\n",
    "11). NINQ: Number of recent credit inquiries\n",
    "\n",
    "12). CLNO: Number of credit lines\n",
    "\n",
    "13). DEBTINC: Debt-to-income ratio\n",
    "\n",
    "# Data Set Information\n",
    "\n",
    "[CREDIT RISK ANALYTICS](http://www.creditriskanalytics.net/citation-formats.html)\n",
    "\n",
    "\n",
    "# Objective\n",
    "\n",
    "The object of this project is to maximize the prediction  on the applicant defaulted on loan or seriously delinquent. In other words, we will select models based on high sensitivity.\n",
    "\n",
    "# Practice Skills\n",
    "Listed below are the practice skills we will learn in this project\n",
    "\n",
    "- Data cleaning\n",
    "\n",
    "- Advanced classification techniques like random forest and xgboost\n",
    "\n",
    "- Imbalanced learning\n",
    "\n",
    "- Receiver operating Characteristics (ROC) and Precision Recall Curves (PRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894dd24",
   "metadata": {},
   "source": [
    "## Data visualization and manupulation library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Ignore deprecated warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Pandas and numpy for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib  for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seaborn for data visualization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c9fa09",
   "metadata": {},
   "source": [
    "## Load and examine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hmeq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27845cbc",
   "metadata": {},
   "source": [
    "## Data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ed1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af8dea",
   "metadata": {},
   "source": [
    "## Data type and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb10469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9086cdd0",
   "metadata": {},
   "source": [
    "- There are two categorical attributes and 11 numerical attributes\n",
    "\n",
    "\n",
    "- There are also missing values in many attributes. We will come back to them later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710dd9d5",
   "metadata": {},
   "source": [
    "## Categorical count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cce6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set font scale and style\n",
    "sns.set(font_scale = 1)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# List few attributes\n",
    "attrib_list = ['LOAN','MORTDUE', 'VALUE', 'CLAGE']\n",
    "\n",
    "# make pairplot\n",
    "sns.pairplot(data = df.dropna(), vars = attrib_list, palette=\"hls\", hue = 'BAD', height = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d2118",
   "metadata": {},
   "source": [
    "Next, let's look at the boxplot which is useful for visualizing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0f276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set font scale and style\n",
    "sns.set(font_scale = 1.5)\n",
    "sns.set_style('ticks')\n",
    "\n",
    "fig, axes = plt.subplots(2,2, figsize = (20,10))\n",
    "\n",
    "# make pairplot\n",
    "sns.boxplot(data = df.dropna(), x ='BAD',  y= 'MORTDUE', hue = 'JOB', palette=\"husl\", ax = axes[0,0])\n",
    "sns.boxplot(data = df.dropna(), x ='BAD',  y= 'LOAN', hue = 'JOB', palette=\"husl\", ax = axes[0,1])\n",
    "sns.boxplot(data = df.dropna(), x ='BAD',  y= 'VALUE', hue = 'JOB', palette=\"husl\", ax = axes[1,0])\n",
    "sns.boxplot(data = df.dropna(), x ='BAD',  y= 'DEBTINC', hue = 'JOB', palette=\"husl\", ax = axes[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d32cbf",
   "metadata": {},
   "source": [
    "- A thorough check of the numerical attributes shows that there are no extreme outliers\n",
    "\n",
    "- The box plot also shows  that self employed are likely to default on their loan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70539571",
   "metadata": {},
   "source": [
    "Let's look at the relationship of loan and the class using the bar chart"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
