{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff48f9a",
   "metadata": {},
   "source": [
    "# Stochastic Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadc6275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2] \n",
    "y = iris.target\n",
    "svc1 = svm.SVC(C=1.0, gamma='auto', kernel='linear')\n",
    "svc2 = svm.SVC(C=1.0, gamma='auto', kernel='rbf')\n",
    "svc1.fit(X, y)\n",
    "svc2.fit(X, y)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "h = (x_max / x_min)/100\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "plt.subplot(1, 1, 1)\n",
    "\n",
    "Z1 = svc1.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z1 = Z1.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z1, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.title('Sklearn SVC')\n",
    "plt.show()\n",
    "\n",
    "Z2 = svc2.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z2 = Z2.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z2, cmap=plt.cm.Paired, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.title('Sklearn RBF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08ad99",
   "metadata": {},
   "source": [
    "### Python - Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc5c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark\n",
    "# From Spark documnetation: http://spark.apache.org/docs/2.2.0/mllib-linear-methods.html#linear-support-vector-machines-svms\n",
    "\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"data/mllib/sample_svm_data.txt\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = SVMWithSGD.train(parsedData, iterations=100)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/pythonSVMWithSGDModel\")\n",
    "sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e90fa4",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990339aa",
   "metadata": {},
   "source": [
    "* Implementing in R: https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4042c",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3350cb55",
   "metadata": {},
   "source": [
    "![450px-Gradient_ascent_%28surface%29.png](attachment:450px-Gradient_ascent_%28surface%29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5109cd47",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e3eb2",
   "metadata": {},
   "source": [
    "Wait, we've already seen this graphic before! It has come back to help us build our intuition of how stochastic gradient descent works. As we explorerd earlier, we can imagine the objective function output we are looking to optimize is on the $Z$ axis, and that the input variables our objective function lie on the $X$ and $Y$ axes. \n",
    "\n",
    "We'll remember that in traditional gradient descent we calculate the gradient (the vector of all the partial derivatives for a given point), and move in the opposite direction of the steepest differential (aka - we descend). So how does *stochastic* gradient descent differ?\n",
    "\n",
    "The \"stochastic part\" is a random shuffling of the order of training data to be serached over in the iterative gradient updating. \n",
    "\n",
    "Do we know if we're ever going to get there? From Wikipeida \"when the learning rates decrease with an appropriate rate, and subject to relatively mild assumptions, stochastic gradient descent converges almost surely to a global minimum when the objective function is convex or pseudoconvex, and otherwise converges almost surely to a local minimum\". For those of you interested in why this is so, I encourage you to check out: Robbins-Siegmund paper from 1971. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677b777b",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cd3ab6",
   "metadata": {},
   "source": [
    "Choose an initial vector of parameters {\\displaystyle w} w and learning rate {\\displaystyle \\eta } \\eta \n",
    "\n",
    "Repeat until an approximate minimum is obtained:\n",
    "* Randomly shuffle examples in the training set.\n",
    "\n",
    "For ${\\displaystyle i=1,2,...,n} {\\displaystyle i=1,2,...,n}$, do:\n",
    "* ${\\displaystyle \\!w:=w-\\eta \\nabla Q_{i}(w).} \\!w:=w-\\eta \\nabla Q_{i}(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f33c51",
   "metadata": {},
   "source": [
    "### Python - Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "Y = np.array([1, 1, 2, 2])\n",
    "clf = linear_model.SGDClassifier()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc42d83",
   "metadata": {},
   "source": [
    "## Simulated Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10406d5f",
   "metadata": {},
   "source": [
    "Let's explore another method for optimizing SVMs that also involves a random variable. This technique attempts to find a global optimum, and was inspired by temperature regulation in metullurgy. \n",
    "\n",
    "\n",
    "* More useful in discrete data contexts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11bb2c",
   "metadata": {},
   "source": [
    "### Pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5defaa3b",
   "metadata": {},
   "source": [
    "Let s = s0\n",
    "\n",
    "For k = 0 through kmax (exclusive):\n",
    "* T ← temperature(k ∕ kmax)\n",
    "* Pick a random neighbour, snew ← neighbour(s)\n",
    "\n",
    "If P(E(s), E(snew), T) ≥ random(0, 1):\n",
    "* s ← snew\n",
    "\n",
    "Output: the final state s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70150a",
   "metadata": {},
   "source": [
    "### CODE: Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4268f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.anneal.html\n",
    "\n",
    "from scipy import optimize\n",
    "np.random.seed(777)  # Seeded to allow replication.\n",
    "x0 = np.array([2., 2.]) #Seeds a guess\n",
    "params = (2, 3, 7, 8, 9, 10, 44, -1, 2, 26, 1, -2, 0.5)\n",
    "\n",
    "myopts = {'C'     : 'boltzmann',   # Non-default value.\n",
    "          'gamma' : None,  # Default, formerly `maxeval`.\n",
    "         }\n",
    "          \n",
    "res2 = optimize.minimize(f, x0, args=params, method='Anneal', options=myopts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae286e7",
   "metadata": {},
   "source": [
    "### CODE: Skyler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7216b0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/skylergrammer/SimulatedAnnealing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da08410",
   "metadata": {},
   "source": [
    "## Particle Swarm Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52f138",
   "metadata": {},
   "source": [
    "### Psuedocode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48223c",
   "metadata": {},
   "source": [
    " for each particle i = 1, ..., S do:\n",
    "   * Initialize the particle's position with a uniformly distributed random vector: xi ~ U(blo, bup)\n",
    "   * Initialize the particle's best known position to its initial position: pi ← xi\n",
    "   \n",
    "   if f(pi) < f(g) then\n",
    "       update the swarm's best known  position: g ← pi\n",
    "   Initialize the particle's velocity: vi ~ U(-|bup-blo|, |bup-blo|)\n",
    "   \n",
    "while a termination criterion is not met do:\n",
    "   * for each particle i = 1, ..., S do\n",
    "   \n",
    "      for each dimension d = 1, ..., n do\n",
    "         Pick random numbers: rp, rg ~ U(0,1)\n",
    "         Update the particle's velocity: vi,d ← ω vi,d + φp rp (pi,d-xi,d) + φg rg (gd-xi,d)\n",
    "      Update the particle's position: xi ← xi + vi\n",
    "      if f(xi) < f(pi) then\n",
    "         Update the particle's best known position: pi ← xi\n",
    "         if f(pi) < f(g) then\n",
    "            Update the swarm's best known position: g ← pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c9207d",
   "metadata": {},
   "source": [
    "### CODE: Pyswarms"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
