{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a12e9e",
   "metadata": {},
   "source": [
    "# Learning with Keras \n",
    "----------------------------------------------------\n",
    "This is a demo of a Binary classification, I apply all encoding techniches que you already know, we are no looking to do a bag of words instead we are trying to figureout the sequence of words, order matter in a sentences and in particular in Sentiment analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51217680",
   "metadata": {},
   "source": [
    "#########################################################################\n",
    "######## @angelo337 Angelo Rodriguez\n",
    "######## @version: 0.1 \n",
    "######## @ https://bigdatacolombia.slack.com/\n",
    "######## @todo: enhance the code with a embeding could be Glove or Word2Vec\n",
    "######## @see: check http://keras.io for mor references\n",
    "######## ################################################################"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a157549",
   "metadata": {},
   "source": [
    "#Copyright 2015 Creangel Ltda. All Rights Reserved.\n",
    "#Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#you may not use this file except in compliance with the License.\n",
    "#You may obtain a copy of the License at\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#Unless required by applicable law or agreed to in writing, software\n",
    "#distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#See the License for the specific language governing permissions and\n",
    "#limitations under the License.\n",
    "#=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf086b",
   "metadata": {},
   "source": [
    "# Loading tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e4a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to force to be compatible con Python 3.xx\n",
    "#from __future__ import absolute_import\n",
    "#from __future__ import print_function\n",
    "%matplotlib inline \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import unicodedata\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "import time, re\n",
    "from datetime import datetime\n",
    "import collections\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stemmer = SnowballStemmer(\"english\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3177c64",
   "metadata": {},
   "source": [
    "## Loading utilities tools to process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a71e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, classification_report\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52ebc8",
   "metadata": {},
   "source": [
    "## Loading actual Tools and Layers of Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization #https://github.com/fchollet/keras/blob/master/examples/kaggle_otto_nn.py\n",
    "from keras.layers.advanced_activations import PReLU, LeakyReLU, ELU, ParametricSoftplus, ThresholdedLinear, ThresholdedReLU\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import History, EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# this is to be sure that al results in development are comparable\n",
    "np.random.seed(1337)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb259d",
   "metadata": {},
   "source": [
    "## actual code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0defc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Tweets_airline.csv\")\n",
    "vocabulary_size = 0\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "#print (df.shape)\n",
    "#print (df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca77e4",
   "metadata": {},
   "source": [
    "## Some small utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanword(w):        \n",
    "    return re.sub('[^a-zA-Z0-9,]' , ' ' , w)  \n",
    "\n",
    "def cleantext(review):\n",
    "    review = BeautifulSoup(review ,\"lxml\").get_text()\n",
    "    review_words = cleanword(review.lower()).split()    \n",
    "#    stop = stopwords.words('english')\n",
    "#    stemmed_words = [stemmer.stem(w) for w in review_words if w not in stop]\n",
    "#    return \" \".join(stemmed_words)\n",
    "    return \" \".join(review_words)\n",
    "\n",
    "\n",
    "df[\"text\"]  = df[\"text\"].apply(cleantext)\n",
    "print \"An example of a single message:\", df.head(5)['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3f535",
   "metadata": {},
   "source": [
    "## Tokenize all sentences\n",
    "it is a good Idea to tokenize with Keras, instead of bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# ML ##############################################3\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split \n",
    "\n",
    "X_clean = df[\"text\"] \n",
    "\n",
    "sentences = X_clean\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print \"Found %d uniq words tokens.\" % len(word_freq.items())\n",
    "w_tokens = len(word_freq.items())\n",
    "vocabulary_size = w_tokens +1\n",
    "print \"Example of a tokenize sentence\", tokenized_sentences[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words= len(word_freq.items())+1\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([w,i] for i,w in enumerate(index_to_word))\n",
    "print \"using vocabulary size %d\" % vocabulary_size\n",
    "for i, sent in enumerate (tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "X_clean = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "X_clean = np.asarray(X_clean)\n",
    "print \"Example of a sequence sentence\", X_clean[33]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3708c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder is a utility class to help normalize labels such \n",
    "# that they contain only values between 0 and n_classes-1. \n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[\"airline_sentiment\"])\n",
    "print \"Example of a sentiment sentence: \", df[\"airline_sentiment\"][33]\n",
    "y = le.transform(df[\"airline_sentiment\"])\n",
    "y = np.asarray(y)\n",
    "print \"Example of a sentiment After Encoder: \",y[33]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y, test_size=0.2) #, random_state=42\n",
    "\n",
    "print(\"Convert class vector to binary class matrix (for use with categorical_crossentropy)\")\n",
    "y_train = np.array(y_train, dtype = int)\n",
    "y_test = np.array(y_test, dtype = int)\n",
    "nb_classes = np.max(y_train)+1\n",
    "nb_classes = int(nb_classes)\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "print \"Example of a sentiment After categorical Encoder: \",Y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4678ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vectorizing sequence data...\")\n",
    "tokenizer = Tokenizer(nb_words=max_words)\n",
    "X_train = tokenizer.sequences_to_matrix(X_train, mode=\"binary\")\n",
    "X_test = tokenizer.sequences_to_matrix(X_test, mode=\"binary\")\n",
    "\n",
    "#X_train = tokenizer.sequences_to_matrix(X_train, mode=\"count\")\n",
    "#X_test = tokenizer.sequences_to_matrix(X_test, mode=\"count\")\n",
    "print X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046a588",
   "metadata": {},
   "source": [
    "## Some parameters \n",
    "That make the model perform better or worse depending on the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69806b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 350\n",
    "maxlen = 350  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 64\n",
    "max_words = w_tokens+1 # you should run it for the first time anc copy the value of mismatch dimension [1] (32,15101)x(11501,64)->(32,64)\n",
    "nb_epoch = 300\n",
    "value_dense = 64\n",
    "drop_layer = 0.5\n",
    "##\n",
    "# BatchNormalization\n",
    "# Normalize the activations of the previous layer at each batch, \n",
    "# i.e. applies a transformation that maintains the mean activation \n",
    "# close to 0 and the activation standard deviation close to 1.\n",
    "#\n",
    "##\n",
    "b_norm = 1\n",
    "extra_layers = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4cb92",
   "metadata": {},
   "source": [
    "![alt_text](http://scs.ryerson.ca/~aharley/neural-networks/images/fcn_visualization7_big.png \"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e20d86",
   "metadata": {},
   "source": [
    "## Building the Actual Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb6d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building model...\")\n",
    "model = Sequential()\n",
    "model.add(Dense(value_dense, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "if(b_norm):\n",
    "    model.add(BatchNormalization())\n",
    "model.add(Dropout(drop_layer))\n",
    "    \n",
    "for i in range(extra_layers):\n",
    "    model.add(Dense((value_dense), input_shape=(max_words,)))\n",
    "    model.add(Activation('relu'))\n",
    "    if(b_norm):\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout((drop_layer)))\n",
    "    \n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax')) #softmax\n",
    "# this is to use Stochastic Gradiend Decendant\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.95, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adadelta') #Adadelta outperform SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3e1394",
   "metadata": {},
   "source": [
    "![alt text](http://scs.ryerson.ca/~aharley/neural-networks/images/one_target_graddescent.gif \"http://scs.ryerson.ca/~aharley/neural-networks/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d1153",
   "metadata": {},
   "source": [
    "## Generate image of the model that we are using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa8db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='model.png')\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import to_graph\n",
    "SVG(to_graph(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969901e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1, \n",
    "          show_accuracy=True, validation_split=0.1, callbacks=[history,early_stopping])\n",
    "score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1, show_accuracy=True)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy: %.2f' % (score[1]*100),\"%\")\n",
    "#print \"history.history\",history.totals\n",
    "classes = model.predict_classes(X_test, batch_size=32)\n",
    "proba = model.predict_proba(X_test, batch_size=32)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9040b",
   "metadata": {},
   "source": [
    "## Evaluate your Results"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
