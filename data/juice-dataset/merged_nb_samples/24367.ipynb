{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353fb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ffc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "from numpy import random as nprandom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1b8b6",
   "metadata": {},
   "source": [
    "Certain `nltk` components require a local cache of data, such as the word tokenizer, and the part of speech tagger. This tells the library to look for the files it needs in current working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90205e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path[0] = './nltk_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0262e00",
   "metadata": {},
   "source": [
    "When a library fails to load due to local files not being present, `nltk` throws a `LookupError`. The module also has a `download` method, which can be used to download specific data from a github repository, which can be used to recover from the error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29072600",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    english = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', './nltk_data')\n",
    "    english = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca55abae",
   "metadata": {},
   "source": [
    "Now on to the fun stuff! The file `adams.txt` is a plain-text copy of Douglas Adams' book, *So Long, and Thanks For All the Fish*. We can use the natural language toolkit to split the book into a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c158b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rsrc/adams.txt') as fp:\n",
    "    sentences = english.tokenize(fp.read())[1:]\n",
    "    tokenized = [nltk.word_tokenize(s) for s in sentences]\n",
    "    douglas_adams = nltk.Text(word.lower() for ws in tokenized for word in ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5070f88",
   "metadata": {},
   "source": [
    "NLTK also has a neural-network based part of speech tagger that works impressively well! We can use it to tag every word in a sentence with its part of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da5fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nprandom.choice(tokenized)\n",
    "\n",
    "try:\n",
    "    tagged = nltk.pos_tag(text)\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger', './nltk_data')\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    \n",
    "print(tagged)\n",
    "douglas_adams_tagged = nltk.pos_tag(douglas_adams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4363eb",
   "metadata": {},
   "source": [
    "There's a lot we can do with that information! For instance, we can create a catalog of words belonging to every part of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eee590",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "parts_of_speech = {}\n",
    "\n",
    "for word, pos in douglas_adams_tagged:\n",
    "    # words to parts of speech:\n",
    "    words.setdefault(word, [])\n",
    "    words[word].append(pos)\n",
    "    # parts of speech to words:\n",
    "    parts_of_speech.setdefault(pos, set())\n",
    "    parts_of_speech[pos].add(word)\n",
    "\n",
    "for k in words:\n",
    "    words[k] = sorted(words[k])\n",
    "\n",
    "for k in parts_of_speech:\n",
    "    parts_of_speech[k] = sorted(parts_of_speech[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b8dc2e",
   "metadata": {},
   "source": [
    "We can also do a simple statistical analysis on which words and parts of speech are most common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c024ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_stats = {}\n",
    "pos_stats = {}\n",
    "\n",
    "for word, pos in douglas_adams_tagged:\n",
    "    # count words:\n",
    "    word_stats.setdefault(word, 0)\n",
    "    word_stats[word] += 1\n",
    "    # count parts of speech:\n",
    "    pos_stats.setdefault(pos, 0)\n",
    "    pos_stats[pos] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1b21f",
   "metadata": {},
   "source": [
    "This turns out to be a lot cooler than you might think! Word frequency in natural language tends to follow a surprisingly neat power-law distribution. According to [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law), a plot should show the frequency of the most common words decreasing exponentially (or linearly, on a log-log plot):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked = sorted(word_stats.items(), key=lambda t: -1 * t[1])\n",
    "x = np.log([t[1] for t in ranked])\n",
    "y = np.log([i + 1 for i in range(len(ranked))])\n",
    "    \n",
    "pyplot.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc9e912",
   "metadata": {},
   "source": [
    "That's really cool! It means there's some constant K such that every word is about K times more common than the *next* most common word.\n",
    "\n",
    "I wonder if Zipf's law also applies to the most common parts of speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4572bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sorted(pos_stats.values(), key=lambda n: -1 * n)\n",
    "y = ([i + 1 for i in range(len(x))])\n",
    "\n",
    "pyplot.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d77945",
   "metadata": {},
   "source": [
    "It doesn't look like it, however that is still a neat totally-non-random distribution!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f2a45e",
   "metadata": {},
   "source": [
    "We can also use the natural language toolkit to lookup the *pronunciation* of a given word. The [The CMU Pronouncing Dictionary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict) provides precomputed transcriptions for most words in the English language, and is made available as a module in the `nltk` library:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
