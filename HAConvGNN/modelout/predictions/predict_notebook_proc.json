{
    "3448": "One hot encoding",
    "28220": "Feature Selection",
    "29402": "Feature selection",
    "4319": "We can also take a look at the distribution of Fare and class",
    "16242": "The below diagram was looking at the most important features",
    "25823": "We need to create Field objects to process the text dataframe before using this method",
    "28605": "We can use a better",
    "1387": "Train the model",
    "15914": "Ticket",
    "22081": "Import Libraries",
    "37507": "Model Architecture",
    "8335": "Feature Selection",
    "4425": "The distribution is interesting",
    "19902": "Bottom 10 Sales by Shop",
    "10259": "We don t forget in test set",
    "38514": "The below diagram illustrates kernel used only on the whole image",
    "19923": "Number of distinct questions in a question",
    "8826": "Cabin",
    "6914": "Model Building",
    "15145": "One hot encoding",
    "34682": "The original train set and test data are not in the same dataframe",
    "8871": "We have now we have now we can drop it",
    "13551": "Embarked",
    "24834": "Create the model",
    "7410": "Alley",
    "37029": "Observation",
    "38086": "One hot encoding",
    "20237": "Cabin",
    "8934": "Create the Family Size column",
    "4802": "We have 929 unique prices of unique items mean mean so we drop it",
    "31045": "Removing links",
    "11483": "Imputing missing values",
    "29334": "Trying the model with the whole dataset",
    "20723": "Once",
    "14779": "SibSp",
    "25020": "The residual interesting in the bottom right the range of the range",
    "32685": "Setup 1 complete with high",
    "36056": "Grouping by Time",
    "42821": "The mutivariate kde was far",
    "21404": "One Hot Encoding",
    "31661": "We can also take a categorical variable but them all the data analysis with the same and use the same categorical data",
    "34092": "We have now possible loading outliers in our dataset",
    "15873": "Random Forest",
    "1909": "The target variable is right skewed",
    "1938": "Days surpassed",
    "42573": "Stacking",
    "26850": "An even interesting",
    "16988": "Gradient Boosting",
    "16393": "The number of parents and the data present on the original data",
    "876": "The properties",
    "18134": "Feature Tuning",
    "37459": "The words dog and SNAP days is a high only",
    "10755": "We have 3 missing values in Embarked",
    "28822": "The number of products by age",
    "6813": "Train Predict",
    "7634": "Blend Ridge",
    "37041": "Compiling the model",
    "5566": "Statistical Description",
    "14915": "We can convert the categorical titles to ordinal",
    "41417": "Feature Scaling",
    "36390": "Feature Engineering",
    "2322": "Feature Selection",
    "8825": "SibSp Parch",
    "19866": "Reading the tweets",
    "37160": "Submission",
    "32949": "We can use the target variable",
    "12284": "The most Covid 19 most frequent is actually us",
    "38765": "We can use the model on the training data itself",
    "3712": "Checking for missing values",
    "36920": "Correlation",
    "14470": "Distribution of target variable",
    "24166": "Data builder",
    "29322": "We can now create a new data frame and can be used for learning",
    "20440": "bureau",
    "24466": "We have 3 example let s check how many rows in our training data set",
    "26929": "We have problem to build our transforms we need to have",
    "21854": "Model",
    "3340": "Missing values",
    "37213": "We have got the activation and 8 layer",
    "9810": "We have 38 special feature is around of the house prices is not more than a more than or more than than the feature",
    "16439": "Missing values Age",
    "28878": "We need to standardize the shape of our testing dataset",
    "20828": "We have created balanced and found now",
    "1350": "Fare",
    "3429": "Embarked",
    "24035": "Display interactive filter based on click",
    "7267": "We have missing data in train and test data",
    "13532": "We finally repeat this process using a loop for loop as an array",
    "24519": "We can drop the info of missing values",
    "22658": "The accuracy function with the loss function",
    "9040": "Feature Engineering",
    "32864": "Lag Features",
    "27456": "The Goal",
    "41992": "We improved the difference between the year",
    "23630": "TTA",
    "20630": "We have text and simple functions to create a few dataframe in a few outliers in the test set",
    "31404": "Loading the data",
    "37674": "We can pick a list of the top counts of the features are more than the image",
    "2100": "You can use the most important variables have an important numeric variables with the target column",
    "16995": "LightGBM",
    "21774": "One hot encoding",
    "25887": "Display heatmap by count",
    "20497": "We need to know how much below it means that is actually how we have to use our CNN CNN which is use to use to a much much",
    "26176": "Importing the packages",
    "28502": "Reshape the data",
    "24427": "We can then evaluate our data into inputs and then learn our model",
    "29981": "Train Test Split",
    "34105": "The distribution of classes 0 passengers have higher than children have better than others",
    "8159": "We have 3 example let s plot this feature is just a plot",
    "41199": "Modeling",
    "10347": "Scaling",
    "18726": "The number of passengers with the maximum than the maximum",
    "32382": "Sex",
    "35520": "Correlation",
    "27208": "The plot makes the most frequent words that is important in the train set e",
    "38824": "References",
    "29928": "Feature Importance",
    "35750": "We have 3 example let s create a sample in sample in sample and training set",
    "6762": "we have to have a look at the test data",
    "20906": "Defining the architecture",
    "27890": "Confusion matrix",
    "1605": "Fare",
    "4254": "Checking for missing values",
    "3691": "Submission",
    "41786": "We can optimize the epoch because the best fit the learning rate",
    "41259": "This model accuracy was a model",
    "25862": "Model Evaluation",
    "9883": "Embarked Sex Fare Survived",
    "14903": "We can visualize this dataframe with a histogram of Survival",
    "35634": "We can use a transformer called",
    "23846": "Checking out the shape of train and test train",
    "38043": "Observations of sale price are important in the target variable",
    "27499": "XGBOOST",
    "14434": "Plot the number of classes vs 0",
    "473": "We have a few columns with the dataset",
    "40445": "Numerical variables",
    "23616": "We have numerical features due to numerical categorical variables",
    "8307": "XGBoost",
    "32322": "Test Set",
    "1616": "Random Forest",
    "39312": "Train Predict",
    "2944": "Correlation matrix",
    "33088": "We have missed the data in the training and validation set you can take a new method on the test data",
    "8533": "Plot the evaluation metrics",
    "27828": "We have successfully addressed the engineered we need to work with our models",
    "22752": "Evaluation",
    "13855": "We have 3 categories if we have to get a new column and take a look at the features",
    "27331": "Checking for train and test",
    "17949": "Name Title",
    "5038": "ps ind 03",
    "23087": "Submission",
    "10673": "Feature Scaling",
    "24047": "Univariate Analysis",
    "32300": "We have a simple simple more complex for now",
    "42882": "We have 3 example in this notebook using the specified method",
    "17338": "Hyperparameter Tuning",
    "305": "XGBoost",
    "33295": "One Hot Encoding",
    "21117": "Save Leader",
    "27541": "Display interactive filter based on click",
    "15408": "The first thing that is how good passengers had a good idea to know how good passengers when we do not do not do not do not not do",
    "10079": "Correlation matrix",
    "4562": "Numerical variables",
    "22532": "Parch vs survived",
    "35932": "Fare",
    "21516": "Train Test Split",
    "32224": "Nice The metrics is just a sequence of the sequence point",
    "36978": "Submission",
    "30318": "We can then train the training process as they were now since as well as well as all of any as any with any of each feature",
    "35689": "Hyperparameter Tuning",
    "37346": "Building the model",
    "3174": "We can also display the corresponding fare as one more top the feature name means the mean of the categorical variables",
    "20172": "Best Model",
    "15295": "Logistic Regression",
    "37231": "We have text and extra prices to investigate the effect of categorical variables",
    "13842": "Embarked Sex",
    "31907": "Predict test set",
    "38745": "Sex",
    "1936": "Sale Price",
    "13756": "Survival by Class and Gender",
    "22333": "Text Analysis",
    "33581": "Library Settings",
    "8881": "Create the vocabulary for getting the feature",
    "10309": "Close",
    "12663": "Split the data into train and test sets",
    "31433": "We use nn using Keras model",
    "1095": "Embarked",
    "16933": "The public leaderboard looks like it is still like the three line is not linear when the submission like the following called like or not",
    "26990": "Submission",
    "4122": "Modeling",
    "29789": "We have 3 example is important to the predictions",
    "36347": "Train Test Split",
    "13542": "The number of passengers with had a better job is not",
    "34020": "Correlation matrix",
    "34512": "We have 3 6 affect the available dataset in the data in object data",
    "1254": "Fixing LotFrontage",
    "24349": "This is the myterious image code to make use in the image space",
    "26254": "In order to make the original images of the code",
    "16716": "Decision Tree",
    "16904": "Ticket",
    "12033": "Correlation",
    "4110": "SalePrice is not normal",
    "6788": "Fare",
    "32745": "Create submission file",
    "8075": "We fill the missing values in the median of the most common values",
    "13622": "One hot encoding",
    "21589": "The main reason why I ll assign this feature",
    "3017": "Missing values are in range line of missing values are in the mean mean",
    "12603": "Correlation Between Sibsp Target aboard",
    "4993": "Model Evaluation",
    "939": "Feature Engineering",
    "34064": "Parch",
    "25941": "XGBoost",
    "2330": "Decision Tree",
    "38211": "Reshape",
    "3731": "Drop columns",
    "36351": "We can also take a look at the images",
    "33703": "Display interactive filter based on continuous variable",
    "3578": "Data Cleaning",
    "4451": "Train Test Split",
    "41471": "The number of passengers with two below",
    "16256": "Change Datatypes",
    "36192": "We can also store the pixel array using the method but the method method of the image",
    "40449": "There are no visible pockets in the sales in the training data",
    "18165": "Feature Engineering",
    "24325": "Feature Scaling",
    "27957": "We can then check the training dataset to get a good then the training set and the train set",
    "7726": "The distribution is right skewed",
    "18953": "Display distribution of a continous variable",
    "28595": "LotArea",
    "15849": "Family",
    "5916": "We have now let s take a look at the variables",
    "28410": "Predict test set",
    "23888": "There are two types of seniority under 6 skewed data with such as many variables",
    "20454": "Bivariate Analysis",
    "20621": "Random Forest",
    "15821": "One Hot Encoding",
    "25294": "The Flesch Kincaid Grade Level",
    "4723": "Loading the data",
    "41870": "We can use a custom library for one of these examples by visualising the image and looking at the label",
    "3276": "Fill missing values",
    "19578": "Item lags",
    "18336": "We can check if the correlation matrix is anything and then then the target feature",
    "26636": "How to rank correct array",
    "31854": "Trend",
    "32150": "We need to create Field objects for each methods up to transform output",
    "21578": "We can then combine out two counts where the end of the order into a order by a score of missing values",
    "4007": "Appendix",
    "38668": "Decision Tree",
    "32639": "The following text is right skewed",
    "28189": "Model Evaluation",
    "22826": "working with total datasets",
    "28139": "Prediction",
    "38779": "Save submission",
    "23567": "Model Building",
    "30988": "Plot the metrics",
    "25456": "Model Building",
    "13279": "Feature Selection",
    "1923": "Sale Price",
    "32410": "Logistic Regression",
    "16031": "Embarked",
    "1900": "Decision Tree",
    "7452": "Checking for missing values",
    "3666": "Missing Data",
    "12115": "Elastic Net",
    "102": "Correlation",
    "15652": "Linear Regression",
    "15927": "SibSp",
    "24153": "Create a glance to extract certain where the corresponding name of the Name",
    "13675": "Checking the missing values",
    "26282": "Model Selection",
    "26951": "Model training",
    "36380": "Split the data into train and validation",
    "30367": "CatBoost",
    "897": "Random Forest",
    "14560": "We can check the number of cabin feature is higher than the first cabin of the missing values",
    "8668": "Data loader processing",
    "27462": "we implement a DataFrameDataset t vector space so we can do the text",
    "17871": "Age",
    "25847": "Cleaning text",
    "4886": "We have 38 special Cabin and Embarked",
    "30682": "Make Submission",
    "54": "We must make our pipeline for some features using the Keras model using the method",
    "12103": "One Hot Encoding",
    "6928": "We have 59 year column",
    "38670": "Model and Accuracy",
    "16263": "We have 3 example in the whole dataset images so we need to train a new method that images with the images",
    "38501": "The distributions are almost done",
    "9474": "Train the model",
    "6380": "We can then check the number of examples are not the feature of the linear regression of the kernel",
    "13477": "ROC Curve",
    "38658": "We can also make the histogram of the code to get the average of the feature",
    "18962": "The number of parents children is more than the house is more than",
    "20751": "identifying the missing value in installments payments",
    "29883": "Plot the distribution of a word",
    "38968": "Demonstration Average",
    "35128": "datetime",
    "21844": "We define the weights of the input and network we can just use it as it can just use as well",
    "1048": "We don t have any outliers because we have just two two features as it may not using them",
    "4940": "Missing values",
    "21655": "We can then combine our embeddings in the embeddings then then then the order in order to get a list of the range of the columns",
    "27216": "TTA",
    "40714": "We can say that the most important variables have a look at the same itself",
    "8094": "Analyze by pivoting features",
    "42919": "Analysis of Temperature is suitable",
    "25938": "The missing values using a categorical function for a column",
    "24739": "We have 3 predictions and create XGBoost to the competition dataframe",
    "20589": "Random Forest",
    "34222": "The obtained function is moderately around around and the average point of the most",
    "17610": "Model Evaluation",
    "9612": "Display death by count",
    "32143": "How to compute the min of numpy array",
    "27762": "Preparing data",
    "18162": "The following",
    "33691": "The column is almost far",
    "12216": "Displaying nodes",
    "11101": "Removing Outiliers",
    "6141": "The author full ROOT properties",
    "26944": "Missing values",
    "15032": "Embarked",
    "42565": "CatBoost",
    "26865": "We can then connect the maximum as they are not by the feature importances but all the most important feature by the feature of the learning of the missing",
    "37500": "Look at the corrolation",
    "28600": "One Hot Encoding",
    "30382": "we have 49 train and test datasets",
    "33813": "Random Forest",
    "35346": "Splitting the dataset raw",
    "37951": "Predict test data",
    "29718": "LightGBM",
    "21573": "We have now possible loading out of our 7 Price",
    "8528": "Missing Data",
    "10977": "Printing the pretrained of the metrics",
    "12953": "Analyze by pivoting features",
    "33676": "Number of days in the days of strings",
    "11015": "Sex",
    "37655": "Save model",
    "7710": "SalePrice vs SalePrice",
    "35530": "Blending",
    "20256": "We can now retrieve the Python weights",
    "12371": "Replace Missing Values in the missing values in the data",
    "8078": "Categorical Features",
    "294": "Groupby Count",
    "3596": "Elastic Net",
    "18948": "Display distribution of a continous variable",
    "37138": "Model Architecture",
    "32570": "Clustering",
    "34044": "Feature Engineering",
    "26476": "We have now predicted prices of probability of 0 and we have to predict we got",
    "33287": "The code may be a little overwhelming but you look at its time to also it if you can use if you can can also if the way if",
    "11659": "The accuracy of the model is much better than the accuracy",
    "13233": "Confirm the data",
    "20060": "Submission",
    "34271": "Appendix",
    "12918": "The most important feature created maximum is better",
    "9937": "Fare",
    "24672": "Feature Engineering",
    "18297": "We can then perform several library a custom library with reduce information",
    "1171": "There are two types of outliers in train data and test datasets with several class",
    "42059": "We have 3 groups years to correct the groups from their and then we can build a new column with the test data as we can fill in the",
    "28489": "Categorical Variables",
    "15648": "Decision Tree",
    "17255": "Reading the data",
    "6499": "We can then train the model comes and then do the same as the one more categorical features that have one value",
    "8017": "Pclass",
    "1161": "Predicting",
    "6011": "XGBoost",
    "17942": "Sex",
    "24752": "We don t forget exists in the normality of the numerical data when we just just a numerical using training and test data",
    "34661": "Sale Price",
    "245": "Lasso",
    "1884": "One Hot Encoding",
    "22680": "As xgboost of logistic regression is good but we have to train using this function",
    "40377": "Reading the data",
    "30884": "Model Evaluation",
    "16701": "Family",
    "13454": "The Age distribution is slightly skewed",
    "1674": "We have now predicted prices of the various layer",
    "32845": "Model 4",
    "28087": "We can use the following code to save our algorithm",
    "6588": "GridSearchCV",
    "24991": "Use the data matrix in train data",
    "13277": "We have 3 outliers now we have to have a random forest classifier and make features",
    "32137": "How to rank the positions in 2D Data",
    "32869": "Add previous feature features",
    "8293": "Linear SVM",
    "2257": "Embarked Sex Fare Survived",
    "11689": "Model Evaluation",
    "10160": "We can also print the Python image using the method",
    "31643": "Feature Selection",
    "5456": "We have 38 special sale price",
    "37118": "The general zoning classification",
    "14277": "Decision Tree",
    "15540": "Random Forest",
    "30180": "USING",
    "2935": "Data Visualization",
    "164": "Checking for missing values",
    "8479": "Train the Model",
    "16498": "K Nearest Neighbor",
    "23023": "we have a trimmed with mean for all across all the outliers",
    "37017": "Observation",
    "8782": "One hot encoding for categorical variables",
    "13711": "Missing values are missing",
    "36875": "Data Transformation",
    "43247": "We can use a custom cross validation is useful as if if you take a look at two within with the model",
    "10276": "We begin by using keras library using best folds model performance",
    "15842": "Parch",
    "38704": "Ticket Number",
    "7307": "Outlier detection",
    "29419": "Removing Uncleaned",
    "39946": "Ridge regression",
    "11048": "Reading the data",
    "42403": "Submission",
    "20755": "MasVnrType column",
    "5806": "Hyperparameter Tuning",
    "30941": "TASK EXPLORE SALES TRAINING",
    "15629": "Checking for null values",
    "4561": "Feature scaling",
    "11973": "Categorical Features replace missing values",
    "21588": "We can pick the same process by combining the data",
    "20830": "Days and UNSTACKING DATAFRAME",
    "31685": "To evaluate the performance of the performance is the performance is the score",
    "18310": "item",
    "149": "XGBoost",
    "12036": "we have a lot of missing values in the data",
    "9428": "BoxPlot",
    "2675": "Feature Selection",
    "21198": "Clustering",
    "15611": "We can keep convert to category object",
    "22927": "The factorplot suggests that is the most important feature",
    "10036": "Name",
    "14795": "K Means",
    "5430": "Fireplace quality mode",
    "5468": "Feature Importance",
    "11046": "Importing the packages",
    "24310": "we compute the output of 0",
    "18324": "Feature selection",
    "32515": "We can use the parameters for our task by the method",
    "34013": "We can then increase the training process by cleaning the training sample images",
    "31686": "We can then precision recall with the learning rate by using the model",
    "12512": "Ridge Regression",
    "665": "Decision Tree",
    "9745": "Parch",
    "2936": "SalePrice is the variable I need to predict let s do some analysis on this variable first",
    "16463": "Convert the categorical column into ordinal numbers",
    "24061": "Cumulative font",
    "11951": "Modeling",
    "3220": "We can use our pipeline as a 2D array using the end function as well as a regression problem with the better",
    "8139": "We need to align the dataframes so it means that we can use to have the same process",
    "14844": "We can then combine the provided columns for now",
    "15350": "Confusion matrix",
    "27836": "Importing Libraries",
    "15876": "Parameters of the best parameters",
    "9007": "Checking for missing values",
    "15874": "Random Forest",
    "4891": "The main reason for the most common titles",
    "40151": "We have 3 example where the unicode we need to have a look at the spatiality of the number of features",
    "32507": "Compiling the model",
    "23815": "Feature Correlation",
    "11313": "Correlation",
    "32980": "Label Encoding",
    "1282": "Fare",
    "34906": "Checking for missing values",
    "23425": "This means that I have just found interesting",
    "1862": "Hyperparameter Tuning",
    "956": "Random Forest",
    "27098": "We have 3 optimal parameters for this model using the default point",
    "20523": "Correlation",
    "2451": "Data preprocessing",
    "37783": "Feature Selection",
    "31712": "We have 3 example in the tweets so we need to convert it to it back to it",
    "32768": "One Hot Encoding",
    "25420": "Analysis of punctuation marks repetition",
    "36672": "Splitting the Training Data",
    "21071": "Text",
    "19045": "Loading the data",
    "34418": "Display world and dense examples",
    "2793": "Model",
    "35608": "We can also take a look at the summary shape",
    "15954": "Loading the files",
    "8514": "Categorical variables",
    "27209": "We have created possible affect simple classes",
    "32498": "Train the model",
    "627": "Ticket Number Remap",
    "37195": "Feature Engineering",
    "1753": "Age",
    "30397": "Model",
    "4293": "we have to all the Store dataset faster",
    "2918": "The next step is to work with the overall cell object when you are correct to predict a much plot",
    "36998": "we have a trimmed with an important part and store it is important than these which is an important for any that the column is for that our our",
    "28133": "Splitting the Training Data",
    "40269": "Feature Transformations",
    "41058": "PCA",
    "2136": "Data Preprocessing",
    "24813": "There are two total missing values in the dataset",
    "20795": "Feature Engineering",
    "14339": "We can keep use as if ones may be useful to check if more if you need a useful or not on their or their data and a prediction",
    "36855": "We use the model works using the image form as before",
    "36290": "Logistic Regression",
    "20525": "SalePrice is not normal",
    "9124": "Type of features",
    "17362": "Random Forest",
    "5146": "There are no NAs in the data type in the object ones I have no missing values",
    "27337": "Reshape",
    "42985": "TF IDF VECTORIZATION ON QUORA QUESTION PAIR SIMILARITY",
    "37707": "LSTM",
    "6139": "Missing values",
    "5135": "Numerical variables",
    "10959": "There are no visible pockets the variance correlation with SalePrice",
    "3463": "We have to create a new column so let s use a category for the prediction",
    "6113": "Missing values",
    "4715": "There are two types of customers with 15 missing values in the dataset as well",
    "24696": "Keras example",
    "29463": "The main reason is a sparse cross validation was so that are present in the counts where is the same of the same",
    "23251": "Model Building",
    "1534": "Ticket Feature",
    "7430": "Random Forest Regressor",
    "2215": "Plotting the metrics Model",
    "14424": "Creating functions for new feature",
    "31580": "Correlation matrix",
    "3725": "XGBoost",
    "16676": "Basic Chart",
    "35252": "Distribution of product",
    "18157": "We have been transformed in the effect models",
    "34232": "We need to implement our base library using the library library",
    "2875": "Feature importance",
    "1074": "SalePrice is not normal",
    "31913": "Reshape",
    "11757": "XGBoost",
    "4147": "Missing values",
    "20448": "bureau balance",
    "34273": "Importing the dataset",
    "5852": "Correlation",
    "20719": "There are some categorical variables in this information",
    "8024": "Cabin Missing Values",
    "28190": "Stage",
    "4701": "Modeling",
    "12396": "Feature selection",
    "9807": "Missing Data",
    "8434": "PoolQC data description says NA means No Pool That make sense given the huge ratio of missing value 99 and majority of houses have no Pool at all in",
    "30824": "Loading the data",
    "19882": "Feature Scaling",
    "41535": "PCA",
    "4038": "Label Encoding",
    "19324": "We can then execute the training data as well as a function of the dataset as a function of the same using the first and test of our test",
    "16405": "Sex",
    "9905": "Logistic Regression",
    "3861": "Splitting the Training Data",
    "26297": "Training",
    "5353": "Display distribution of a continous variable",
    "18575": "Fare vs Survived",
    "8370": "K Nearest Neighbor",
    "26412": "We can also use a custom class on fare ticket ticket on the same dataset",
    "24400": "Submission",
    "15232": "We can drop the unwanted columns",
    "31008": "Interpreting Model",
    "38101": "The Chart of a continous is a list",
    "552": "We can use our net",
    "8368": "We can also look into the data into Pclass and Pclass",
    "15076": "Survived",
    "15595": "Checking for null values",
    "9871": "Feature Engineering",
    "11784": "Cabin",
    "40001": "Import Libraries",
    "39024": "Split the data into train and test sets",
    "15271": "Decision Tree",
    "20157": "Data Preparation",
    "34287": "Feature Engineering",
    "9423": "We have 3 predictions and Religious events",
    "42628": "The most important errors are in the most important features",
    "31116": "Correlation matrix",
    "14619": "Fare",
    "23040": "Discount",
    "11032": "Random Forest",
    "27064": "The plot confirms 1st class was sold",
    "43246": "CatBoost",
    "22124": "Model Evaluation",
    "23513": "We can then combine the outliers",
    "31681": "Logistic Regression",
    "15730": "Random Forest",
    "15328": "We can use the alphabets SibSp column",
    "35073": "Submission",
    "18661": "We use StratifiedKFold parameter to the optimal parameters to make our model using the parameters",
    "41928": "Display time series",
    "36801": "Just a look at the data",
    "4960": "Random Forest Regressor",
    "27377": "The most frequent class is always the most intrigous",
    "38936": "Submission",
    "42616": "We have 3 example let s plot this point is set and it s time to get a much using",
    "22477": "Educational",
    "34703": "Number of products by month from the beginning of all the month from the month",
    "10861": "Analyze Garage Area",
    "21611": "Days way of verifying NA 3 1 etc have multiple missing values or 1",
    "30000": "Reshape",
    "37176": "we compute the model on the testing dataset",
    "40058": "Model CatBoost",
    "18129": "Random Forest Regressor",
    "27032": "The best score is moderately better than the score",
    "29925": "Distribution of Search w r t gender",
    "16533": "Age",
    "43136": "Defining the architecture",
    "16650": "Missing Data",
    "5154": "Feature Selection",
    "23883": "I believe going to know what how they were how to understand",
    "12893": "go to top of section eda",
    "34753": "We have text our embeddings and then work for our train and test data",
    "8548": "Missing values",
    "10108": "Predict on test set",
    "28325": "Analysis with the number of customers becoming given the data",
    "12130": "One hot encoding",
    "15011": "We can say that the Passenger classes survived is not the same",
    "27280": "The residual returns the full underscore is unbalanced strongly the following",
    "19304": "We can use the sklearn lasso regression algorithm with the following code",
    "30265": "We can also evaluate the classifier but the predicted order but depending on the training images",
    "20362": "We implement our Selected the data frame",
    "24749": "Imputing Missing Values",
    "35557": "Gradient Boosting",
    "15984": "Feature Selection",
    "7453": "We can fill missing values based on Age and Pclass",
    "13893": "We can safely says as if ones may be useful to start with the end of people and better",
    "5904": "Random Forest",
    "40412": "I am also interested affect affect variable analysis",
    "16378": "Embarked",
    "23036": "The distribution is right skewed",
    "20396": "Model Building",
    "16367": "There are only 2 NaN values in the median of the median",
    "19582": "We have now 3 functions in the test dataset",
    "34015": "Days distribution",
    "31544": "There are 2 missing values in Embarked",
    "5193": "Takeaways",
    "30532": "Reading",
    "19614": "The following text is right skewed",
    "14100": "Random Forest",
    "21525": "The most important part 1 the maximum is almost 3 only 3 object data so that are not used for classification is a is to a better as as",
    "39776": "Model and Accuracy",
    "22273": "We fill the missing values in the Cabin Age with the median",
    "5163": "Gradient Boosting",
    "30972": "To Know the parameters I tried that the curve I have random random performance",
    "32030": "Age",
    "22110": "Feature word Common",
    "26013": "The target variable is right skewed",
    "32820": "We need to create Field values",
    "8090": "Reading the data",
    "33608": "Model",
    "19529": "We have 3 6 functions to store your dataset",
    "28130": "The original text and 75 functions",
    "9235": "TFIDF W2V Clustering",
    "29570": "Modelling",
    "25675": "Comparing the word length of words in selected text and train x text",
    "26804": "The main text of the leaderboard it is creating a sample sample",
    "1879": "Survival by Age and Pclass",
    "11755": "Model Parameters",
    "6631": "Sex",
    "7087": "We can also use a general method that if you have a look at the same",
    "24018": "The accuracy is the accuracy is the accuracy is an ensemble in the validation data when the most important feature by the class is the right be be be",
    "6257": "Parch",
    "23620": "Train Test Split",
    "29814": "We can optimize using a maximum search CV",
    "6052": "Days way of verifying NA",
    "41635": "The number of passengers are more likely to survive is the number of passengers",
    "13469": "Parch",
    "43365": "Save Leader",
    "37093": "Marginal Boxplot with different feature",
    "37891": "we have to scale our off possible accuracy",
    "38479": "Display distribution of a continous variable",
    "17752": "We can then combine the maximum search but well as a random search method as a better than how the correlation is not in the training dataset and then",
    "27369": "we have to transform the Name feature",
    "5975": "Train Test Split",
    "7957": "Model Evaluation",
    "8393": "We can then connect numpy array",
    "27147": "Change with 80 distribution",
    "13087": "We use the net",
    "16437": "Fare",
    "7414": "Statistical Signifiance",
    "18426": "XGBoost",
    "30770": "We can use callbacks parameter to the performance of our training data using validate our model by using validation and use",
    "27137": "Display heatmap of quantitative variables with a gradient",
    "7362": "Data preparation",
    "9217": "Pclass",
    "29103": "Model",
    "13974": "Sex",
    "31740": "We can then combine our embeddings in order to use a K cross kernel",
    "43039": "Train the model",
    "23424": "This means that I have just found interesting",
    "40841": "I m gonna just two state names from both two datasets based on their name range",
    "41668": "Main part load train pred and blend",
    "4343": "We can also visualize the titles",
    "22974": "The next thing is a categorical titles",
    "15385": "We can replace Quorans with Quora contributors",
    "24907": "Confirmed COVID 19 cases Per day in India",
    "720": "There are some examples values in the lines",
    "20956": "Model Evaluation",
    "10965": "We have numerical variables so we need to convert categorical features into a dataframe",
    "38103": "Reshape the data",
    "29727": "Statistical Signifiance",
    "10720": "Sex",
    "30091": "Data augmentation",
    "7122": "Pclass vs Survived",
    "33690": "We can then able the same order as whether the training set and it means as well as a submission as the data as we have a much to",
    "4852": "XGBoost",
    "16766": "Feature Selection",
    "35354": "Loading the data",
    "19836": "Age Young have significant chance of the outliers",
    "42313": "The below model building is maximum to predict the score",
    "10351": "PCA Encoder",
    "30901": "Missing values are frequently in the train data",
    "22053": "The dirty fix I adopted before it looks like it is a good idea to look at",
    "1718": "View Columns",
    "2659": "We have to train the model",
    "38029": "Decision Tree",
    "17696": "Decision Tree",
    "29466": "Masonry veneer type",
    "21146": "The most important feature is not good for the model",
    "32210": "Add lag values for item cnt month for month city item",
    "33889": "We now repeat the data using previous weights",
    "5536": "We can drop the Ticket feature in the Ticket feature",
    "9672": "We can then execute the training and test data are in the train and test datasets",
    "42006": "Ticket",
    "29140": "We have 3 outliers individually",
    "15057": "Model Evaluation",
    "25453": "The number of images is much than the following",
    "18007": "Correlation Heat Map",
    "15724": "Decision Tree",
    "18475": "Promo2SinceWeek Promo2SinceYear and PromoInterval",
    "28222": "Predict",
    "23600": "We can then evaluate the training data and apply the same table as the same task",
    "2951": "XGBoost",
    "33032": "There are no missing values in the dataset span",
    "28696": "We have far skewness of 784 having pre GarageYrBlt the previous year so we can use a plot and most important to a right of a model",
    "43000": "The next 100 is suitable to find the number of object type are in the data",
    "27642": "Splitting X y",
    "36364": "Model Evaluation",
    "27647": "Submission",
    "16385": "Sex",
    "42540": "We have already balanced the important features perform in our dataset now we need to use our classifier",
    "32818": "Go to Contents Menu",
    "13664": "Library and Data",
    "42053": "This bar plot distribution of the missing values in the numerical column",
    "4919": "We have applied rows in the data so we need to convert it to it to a numerical data into a data set",
    "16005": "We can check out the percentage of Age age and Passenger class are not important to not not not create the model is not to the first not the",
    "38310": "Decision Tree",
    "2445": "We can visualize the feature importance transformation",
    "19899": "Correlation Analysis",
    "9767": "We can now remove the pixel columns that we can be using pandas",
    "28166": "we have a balanced model is required",
    "20508": "Data Augmentation",
    "22646": "Imputing the missing values with mode",
    "26661": "Numerical",
    "12463": "We have missed the remaining categorical variables in our dataset now let us train the dataset now",
    "14249": "Embarked",
    "42836": "K Nearest Neighbors",
    "10821": "Decision Tree",
    "22650": "The below is interesting",
    "32082": "The number of parents uncomment are presented isn t a sample",
    "23937": "Display distribution of a continous variable",
    "16606": "We can use this into a feature using two map of different feature",
    "21393": "Submission",
    "32865": "Item lags",
    "25728": "We use the first layer to check for the training data",
    "15781": "Random Forest",
    "1516": "Correlation",
    "41300": "We can also evaluate the best classifier model but the scores then the best method on the best accuracy",
    "11404": "Exploring the data",
    "23745": "Logistic Regression Model",
    "17935": "Embarked",
    "29422": "We got the point with the point vs which is to predict the right performance",
    "22328": "Removing links",
    "37916": "We implement our LGB formula were found during the boosting table",
    "8303": "Submission",
    "41227": "Logistic Regression",
    "24694": "The learning rate is learning rate",
    "15530": "Fare",
    "10635": "Taking helper functions to combine clean data analysis into this feature we can make the same process work and we need to also it again again again to it",
    "37833": "Grid Search Implementation",
    "41970": "perform CV using Google FE",
    "23528": "Feature Selection",
    "37662": "Train Valid split",
    "41846": "XGBoost",
    "38957": "Data Overview",
    "43350": "The overall way I would assign it looks like it is a high only only the high",
    "7640": "SalePrice vs GarageArea",
    "40720": "Train and predict",
    "1684": "The distribution is right skewed",
    "42325": "The following model building now work to be object as the training data is not the training set",
    "3162": "We can keep convert to category object type",
    "14284": "Decision Tree",
    "5885": "We implement our grid search using random search regressor",
    "6042": "LightGBM",
    "23295": "Categorical Features",
    "20771": "PCA",
    "7354": "Filling",
    "19196": "The char 38 special last index 1st class",
    "20081": "look at sales day of item store",
    "29112": "We use the model using rmsprop of input layer and then t the other model using the model",
    "7709": "SalePrice is not normal",
    "13500": "Title",
    "20643": "Applying Neural Networks",
    "32139": "How do sales differ in a category",
    "32239": "Displaying the chosen image metrics",
    "16226": "Dropping Name Ticket category because it is no useful",
    "28078": "As expected the distribution of survival is high according to the survival rate",
    "2983": "Random Forest",
    "42781": "Build CNN Model",
    "37019": "Name",
    "43397": "Printing the version of the generator",
    "23606": "The most important part and accuracy with the range",
    "29331": "Basic Ensemble Modelling",
    "11116": "Train Test Split",
    "43323": "Reshape the Data",
    "15754": "Cross Validation",
    "7557": "Random Forest",
    "12090": "Applying Linear Regression",
    "35321": "Model training",
    "30284": "Just like ai probability about the 1st score",
    "22828": "The following text and assign the metrics on the test data",
    "14825": "Embarked",
    "40743": "Test Time Augmentation",
    "9087": "Data Visualization",
    "24912": "Display interactive filter based on click over legend",
    "31096": "Feature Engineering",
    "20125": "Model Evaluation",
    "42417": "Histogram plots of features in shop",
    "18914": "Correlation matrix",
    "37746": "Loading the dataset",
    "30860": "Importing Libraries",
    "15681": "Model 3 4",
    "25479": "Callback",
    "22046": "This looks rough",
    "38953": "We can also make the optimizer and optimizer with filters to the model",
    "22407": "We have got a few more missing values are to have to have to have",
    "229": "Stage",
    "27152": "Any looks halfway decent",
    "43124": "SVM",
    "28110": "Model Building",
    "43063": "Distribution of target variable",
    "32691": "Train the model",
    "35502": "Training the model",
    "29362": "We split the data into train and test images",
    "5045": "Correlation matrix heatmap",
    "43022": "Check the locations",
    "42455": "We have 3 outliers affect the total variable",
    "35463": "Visualiza the skin cancer",
    "41909": "We don t have any outliers in our dataset",
    "30520": "We can then check how they affect the number of samples is a high number of the number of number of the time to the test set",
    "31680": "Compiling the model",
    "4608": "SUMMARY",
    "29780": "We can also print the pixel matrix where we can use the first cross validation",
    "31618": "Score for GridSearchCV",
    "22606": "We use the previous difference let s make the last below of the variables",
    "34022": "The target column is interesting",
    "41162": "There are no missing data in the training data set is in",
    "27458": "Text",
    "15155": "Converting the Categorical Features",
    "5398": "Embarked",
    "8080": "We inspect first level columns that are relevant in the following",
    "36939": "Fare",
    "38293": "We can then connect the model as follows together",
    "16913": "One hot encoding creates 3 family",
    "34240": "Imports",
    "29930": "Plot on word vs x",
    "20408": "Display distribution of a continous variable",
    "816": "We log the Saleprice with the data analysis",
    "33894": "There are no visible patterns now we don t have only one missing values so I use only only one missing values in the dataset",
    "35769": "We can then evaluate the leaderboard then we check the default precision by the default predictions",
    "2530": "AdaBoost",
    "1632": "The info method is applied to work with the label",
    "12778": "Model Building",
    "27832": "We got the effect of the effect of the most common values",
    "2321": "Model Evaluation",
    "2421": "Feature Engineering",
    "39423": "Sex",
    "5254": "Feature Selection",
    "42642": "Remove punctuations special characters numbers",
    "20035": "We have 3 predicted 8 library technique on 8 label",
    "13229": "Predicting",
    "37911": "We save the outputs is the accuracy of the way",
    "31349": "Submission",
    "8088": "Submission",
    "13503": "Correlation",
    "22276": "Embarked",
    "29017": "Age",
    "18433": "We can also evaluate the network classifier",
    "14713": "K Nearest Neighbor",
    "3033": "Predict",
    "40688": "We can then combine our cabin and then combine the two datasets and then use a number as well as the same submission",
    "9245": "Correlation matrix",
    "15720": "We learn for numerical columns with the data analysis",
    "530": "Missing values in Embarked",
    "13607": "We have successfully addressed the maximum object values in string now",
    "42413": "Exmaine the data",
    "32033": "The optimal parameters I would change the training loop as a score was using the parameters that was a 2 model is not that the training set that that",
    "32039": "We can then precision recall f1 using f1",
    "40853": "Observation",
    "430": "MiscFeature the missing amount",
    "42237": "Observation",
    "33446": "Hyperparameter Tuning",
    "938": "We have only 3 LotFrontage columns that may have just some values in our data",
    "30202": "And now you want to find the parameters",
    "3516": "identifying the data",
    "37888": "Linear regression elastic net",
    "21501": "We have already transformed images so let s take a look into our train data into train and test data to train with a test set",
    "27310": "Split train and validation set",
    "39439": "Submission",
    "2300": "Feature engineering",
    "8382": "Fence",
    "28367": "The most important part have more than missing values",
    "25815": "Model",
    "34048": "We use the Saleprice per possible below",
    "28463": "LightGBM",
    "3671": "There are no missing values in numerical columns",
    "33251": "Missing Data",
    "41361": "Sale Price IR3 IR2 IR1 Reg",
    "19300": "Several of all questions in the dataset",
    "10685": "Decision Tree",
    "24255": "Embarked",
    "307": "Random Forest",
    "19896": "item cnt",
    "7108": "Instancia um objeto chamado dt with classification technique",
    "18560": "The general zoning classification",
    "27323": "Model",
    "14531": "Cabin",
    "27157": "Display distribution of a continous variable",
    "35369": "We use a multinomial function to make the pipeline",
    "20826": "We can extract average of family members as a few more important feature",
    "12357": "MasVnrArea",
    "21322": "We can also estimate the PDF smoothly but the target variable then check the kernel is not linear",
    "40669": "Bivariate Analysis",
    "23676": "We can get the training process by training the model",
    "78": "Cabin",
    "38013": "Number of total of month in a month",
    "7942": "Model Building Process",
    "16042": "Train vs Test",
    "37639": "Display distribution of a continous variable",
    "2560": "The accuracy of model present on leaderboard images",
    "39958": "Gradient Boosting",
    "2917": "There are no missing values in Embarked",
    "2908": "Age",
    "17652": "Model 3 times with hyperparameter tuning",
    "21581": "We can then able to remove our data using our machine learning method by where the data where we need to be our data",
    "39788": "The most important part of the most important variables are between 0 and 1",
    "21487": "Preparing Function",
    "15463": "Feature Importance",
    "18204": "Model 4 Input ReLu 512 512",
    "24883": "Exmaine the installments payments dataset",
    "26012": "TotalBsmtSF",
    "36818": "Reading of all the data types",
    "19569": "Dataset",
    "15723": "Logistic Regression",
    "27151": "To check if there is a friend approach in the following type is not type on the following",
    "14803": "Observation",
    "40969": "The number of set and the same looks like",
    "31366": "Train Test Split",
    "16870": "Feature Selection",
    "25186": "Loading the data",
    "28504": "CNN model",
    "22468": "Ordered Bar Chart",
    "37300": "Removing links",
    "9788": "Correlation matrix",
    "42774": "Data with better",
    "20304": "The last thing that we need to do some of the categorical features before do not do it",
    "4463": "Sex",
    "7598": "The target variable Distribution",
    "547": "SVM",
    "34488": "We begin by importing the pixel matrix",
    "40770": "We can also evaluate the image along with the digit",
    "31236": "Correlation",
    "2120": "The Hyperparameters is interesting It is almost taking the average value in the accuracy",
    "23293": "Numerical Features",
    "26181": "One Hot Encoding",
    "21771": "We have 3 missing values in dataset",
    "7393": "we can drop the combined dataset to select preprocessing",
    "25673": "Gradient Boosting Regressor",
    "16345": "Submission",
    "15528": "We don t have any missing values",
    "6451": "Predict test data",
    "9858": "Sex",
    "29713": "Count of target variable",
    "24447": "Text",
    "38132": "Correlation Between Sibsp Survived",
    "24982": "Categorical Features",
    "38944": "Confirmed Cases",
    "3930": "Stacking Prediction",
    "747": "GridSearchCV for Hyperparameters",
    "17460": "We don t need Id so we need to fill with the mean mean of the mean value and categorical data with the null values with with with with",
    "1640": "We have now the Output in the original training data and the model",
    "43024": "Several shops",
    "32568": "Save Cleansed",
    "40676": "The below diagram was another appear of how good can be used as a kernel",
    "26568": "The 25 images is moderately out of the 75 and then are pretty good but this can then create an the same with the same performance",
    "43306": "There are no missing values in the test data",
    "32526": "Train model",
    "22286": "We re going to use the minority point layer to compare the model",
    "30867": "We can get a better sense for one of these examples by visualising the image",
    "32730": "Showing best parameters",
    "15456": "The plot is interesting",
    "11760": "Gradient Boosting",
    "40480": "Random Forest",
    "21127": "Age distribution of numerical attributes",
    "1103": "The dependent variable is right skewed",
    "40241": "Importing the packages",
    "31830": "Feature ranking",
    "41400": "Change",
    "17468": "Convert columns where the type names",
    "41525": "We can clearly survive of age on board",
    "40170": "TASK IMPORT SALES TRAINING",
    "16660": "Selecting best Sex Embarked to numerical data",
    "14826": "Ticket",
    "443": "Missing Data",
    "27517": "Model 4",
    "3160": "The main reason is full around yielded",
    "26450": "We have got around towards the correct end of the whole training dataset as well",
    "2007": "This looks rough",
    "8100": "Exploratory Data Analysis",
    "28212": "Model",
    "13124": "Sex",
    "975": "Model",
    "16004": "We have now simple the maximum of all the categorical columns and so let s look at the values of this is in this is with the test",
    "36589": "Model Building",
    "42072": "XGBoost",
    "41442": "We have successfully 784 disaster tweets",
    "17999": "We can then combine out the counts of two learning to validate our train data using a most using a model called called or a training data and a",
    "7581": "We should impute mising MSZoning with 0",
    "29835": "Loading the saved weights",
    "3443": "Age vs Survived",
    "30964": "We can use a boosting type to the best parameter of the feature importances",
    "12789": "We can also print further the effect model but it should be object as a parameter convert to the data",
    "20350": "Model",
    "23527": "Callback",
    "17449": "Train Test Split",
    "24666": "Submission",
    "12979": "Create the data into getting category",
    "40775": "Confusion Matrix",
    "30425": "Data visualization",
    "41322": "The smaller the prob1 is unbalance the curve we are not the score",
    "13283": "Model and Accuracy",
    "41063": "One Hot Encoding",
    "34484": "Split the data into train and validation sets",
    "11875": "Fixing Skewness",
    "15001": "Checking for null values",
    "14813": "Pclass vs Survived",
    "15238": "Checking for missing values",
    "17802": "Pclass vs Survived",
    "43150": "We need to know how much if they are not overfitting so if they means with the first so if we use the first words to our our our",
    "20446": "Missing Data",
    "38427": "Model Building",
    "22096": "Model",
    "7518": "We can also evaluate the optimal parameters that are provided by the cross validation",
    "41949": "We need to make sure that our classifiers are overfitting to the top set of top by the top set of train and test set",
    "17359": "Model Selection",
    "1734": "Sex",
    "26080": "Training",
    "38719": "We need a few functions to the fully",
    "1428": "LOGISTIC REGRESSION",
    "20122": "Feature Importance",
    "27250": "We use the same process as our neural network",
    "8932": "Data Preprocessing",
    "9182": "I am also interested how the most important variables affect 0",
    "6272": "Embarked",
    "36820": "Importing the Libraries",
    "2805": "Feature Engineering",
    "9162": "ps ind 14",
    "17547": "We notice there is several 200 order as follows",
    "6829": "Distribution of target variable",
    "4881": "About Customer",
    "40931": "Model Evaluation",
    "21937": "We got information about tweets",
    "540": "Title",
    "42722": "the correlation coefficient is the most important features",
    "15869": "Predictions",
    "8843": "Categorical Variables",
    "26558": "The accuracy of the model is trained for the model",
    "29044": "We can also take a look at the images shape",
    "8778": "Embarked",
    "13721": "We can also be a look at the age of the graph",
    "17358": "Tune Models",
    "28995": "Data Visualization",
    "5142": "The object thing that we need to do some categorical features",
    "27093": "Model",
    "14885": "The general zoning classification",
    "32877": "Feature Selection",
    "15015": "Pclass vs survived",
    "3214": "Univariate Analysis",
    "18207": "Save the model",
    "27329": "Save the submission file",
    "10592": "XGBoost",
    "663": "Logistic Regression",
    "40945": "Gradient Boosting Regressor",
    "32356": "we have to create a new data frame",
    "9678": "XGBoost",
    "18192": "The number of products have more than the total number of month and the data is not survived",
    "8314": "Label encoding",
    "13577": "This could be a continuous feature but it contains only unique missing continuous variables",
    "20083": "Sales",
    "21811": "Price Segments",
    "25204": "Generating submission",
    "3593": "Ridge regression",
    "41023": "Submission",
    "28155": "We have 3 example object so we need to transform our models and make our dataset",
    "22791": "Importing Libraries",
    "4386": "check the distribution of train set",
    "27303": "The number of products present in the ship",
    "40309": "Model Architecture",
    "3667": "Categorical Features",
    "9688": "Correlation",
    "28158": "The default tree be a bit that is very low so I predict the performance of the model",
    "26240": "The Discriminator ids function comes the generator days on the Titanic x precision",
    "18326": "Ensemble Model",
    "41263": "The plot",
    "32095": "How to limit the second chapter",
    "24719": "K Nearest Neigbhbors",
    "4388": "Removing overfit of null values",
    "39179": "The cross validation score is used for the corresponding model is a good model",
    "39193": "Feature Selection",
    "6018": "We need to create some helper functions to use our neural network so let s use to use to train our data set to set set set to set",
    "1080": "Importing the packages",
    "17888": "Name Title",
    "18194": "This indicates was a list of missing values",
    "1894": "Support Vector Machine SVM",
    "2762": "Missing values in categorical column",
    "39738": "Ticket",
    "12215": "Lasso",
    "39877": "The target variable is right skewed",
    "22464": "Plotting Curves of model accuracy",
    "5115": "The most important variables have more than 20 missing values",
    "13978": "Name",
    "41339": "One hot encoding",
    "10628": "Random Forest",
    "3442": "The distribution of classes passengers with a little using the common classes was as a m",
    "29715": "Correlation matrix",
    "16575": "Ticket Number Cluster",
    "22962": "We can also evaluate the image along with the digit",
    "21353": "We can then combine our embeddings we need to reduce our models",
    "23318": "Add previous shop item id",
    "39228": "Random Forest",
    "2455": "Feature Scaling",
    "29906": "Model",
    "33250": "Describing encoding",
    "24118": "Model Building Evaluation",
    "21792": "Decision Tree",
    "16922": "Random Forest",
    "5895": "Feature Selection",
    "10335": "Missing Values",
    "14672": "Random Forest Regressor",
    "15299": "Support Vector Machine SVM",
    "19661": "Prophet",
    "20605": "Type of the missing values",
    "10127": "Modeling",
    "22355": "Stacking",
    "14311": "Blend Ridge",
    "6151": "Model and Accuracy",
    "10785": "The number of images is much better than the data",
    "26905": "One Hot Encoding",
    "33693": "We can then check the number of examples values are not in the training set and the most important feature",
    "14127": "Embarked",
    "26511": "We have 3 8 activation function to work with the whole dataset",
    "40447": "Drop columns with more than 50 of data",
    "21897": "Train 15 Models",
    "205": "Feature Engineering",
    "23451": "The plot makes is a large trend that it work with more than it",
    "16950": "we have to submit the classifier",
    "21262": "Quick Number of Number of characters in train data",
    "567": "XGBOOST",
    "41491": "Decision Tree",
    "10729": "Logistic Regression",
    "21328": "The core of actual the corresponding values is very high but we might want to plot the feature",
    "20136": "Normalization",
    "25672": "We can use the k Nearest Neighbors where we use a sample using the method",
    "30663": "The following text is full probability of cat is an color",
    "3412": "Parch vs survived",
    "40958": "Missing values",
    "9287": "Ticket",
    "35588": "We have already balanced rows so we need to transform it to transform our models and prediction",
    "37632": "Model configuration",
    "1624": "CatBoost",
    "25882": "Distribution of product among continuous variables",
    "13506": "Applying Random Foresting",
    "37217": "We need to know how many top values are not so we have to use text so we need to remove the text which have to use to the",
    "12099": "Functional mode for continuous variable",
    "4421": "Random Forest Regressor",
    "11171": "We can then perform box cox transformation but we simply remove the feature since are highly correlated to the same using the same analysis",
    "10935": "Missing values",
    "30138": "Library and Data",
    "32051": "We can also estimate the top losses goods",
    "10307": "Displaying nodes",
    "35632": "K nearest neighbors",
    "40882": "We have now simple the optimal parameters for now",
    "17528": "Name",
    "6054": "We have 878049 nulls and Religious events",
    "31648": "MLP",
    "17671": "Survival by Age and Port of Embarkation",
    "35378": "Modelling",
    "38552": "We can also estimate the PDF smoothly and confirm can be found by later to the feature",
    "28272": "We can then perform the maximum as well",
    "23480": "The next step is the score in the correct dataset",
    "12378": "We need to create an example of process text based on the text into text",
    "43198": "Submission",
    "36974": "Feature Selection",
    "2750": "MasVnrArea and MasVnrType have NaN values have missing values",
    "2385": "XGBoost",
    "32635": "Save Cleansed",
    "24986": "Correlation matrix heatmap",
    "28095": "Model",
    "10812": "We can use the alphabets method to scale out the training data in our model and take a look into our model as validation set",
    "4417": "Ridge Regression",
    "41042": "We can then connect the grayscale of the images images We define a dataframe using the method since then t have a code to the first first but this",
    "22926": "We can also look at the coefficient of our word around a text names",
    "33771": "Normalize the data",
    "21759": "We have 3 outliers affect the year",
    "9126": "Missing values",
    "35415": "Make Predictions",
    "9586": "Display boundaries",
    "4442": "The target variable is right skewed",
    "19125": "We can then evaluate our grid search using the search method called method by the best method",
    "10556": "Feature ranking",
    "37204": "LightGBM",
    "6407": "We don t forget affect 0 where the others according to the target",
    "16251": "Train model",
    "19548": "The text model needs to be using performance",
    "14742": "We have to sort the graph of each graph",
    "23652": "the target column is a general trend to use it to us it into a text time",
    "8018": "Survived",
    "35187": "The Challenges and selected variables",
    "15111": "Cabin",
    "15074": "Parch",
    "36671": "Text",
    "7139": "Family Size",
    "31077": "Days Missing Values",
    "17703": "Reading the test dataset",
    "28593": "Interior",
    "8414": "This is an important feature because there are no missing values",
    "12909": "check the number of test set",
    "11141": "Look at the neighbourhood spreads InterOut array",
    "24299": "Reshape the Data",
    "13748": "Create TotalBath Feature",
    "15739": "Title Title",
    "43210": "Train model",
    "302": "Feature Engineering",
    "13158": "Feature Scaling",
    "7891": "How to rank items in a multidimensional variable",
    "12013": "LightGBM",
    "4679": "There are three positively obvious negative classes 0",
    "20680": "Model Evaluation",
    "6755": "Statistical Significance",
    "24147": "We can then perform the total number of examples in the order",
    "502": "Family",
    "38470": "Load Data",
    "2260": "Family Size",
    "19831": "Feature Engineering",
    "10767": "Feature ranking",
    "24011": "We use the model works using gradient boosting but used as the following level but the final process s get the same as below",
    "38472": "Train model",
    "22765": "we have a look at the data",
    "36746": "Feature Scaling",
    "7286": "Logistic Regression",
    "18768": "Model Selection",
    "38023": "We begin by plotting the network itself",
    "1050": "Number of products by activity index",
    "42561": "This says specify the range of the first layer",
    "15235": "Submission",
    "26988": "Training",
    "7864": "Checking for null values",
    "1532": "LotFrontage",
    "7676": "The columns with high correlation between 0",
    "21536": "XGBoost",
    "27977": "Pair looks better",
    "19911": "Add lag values of item cnt month and month item cnt month and month for which are for a for for",
    "34176": "We can also evaluate the network of the parameters that confirm you can take a look at the parameters of our model",
    "9260": "There are no missing values in the dataset",
    "29904": "Train Test Split",
    "38987": "the model of train data",
    "31713": "Predicting",
    "22608": "We have 3 predicted prices of the data types we need to fill in this dataset",
    "25590": "Random Forest Regressor",
    "32542": "Missing Values",
    "6673": "Random Forest",
    "30389": "Splitting the Training Data",
    "4693": "We can then check the number of highly correlated variables using our analysis",
    "14290": "Feature Importance",
    "6789": "Cabin",
    "17899": "The ROC metrics is the most interesting of the coefficients",
    "7849": "Model 1 Baseline LGB",
    "32787": "Feature word",
    "13873": "We can use the model to label out the difference method on the training data into training and validation set",
    "12786": "We use the TanH activation function Leaky",
    "20099": "We can then use the maximum feature importances across all the fields fields as one as the feature importance as well as the learning rate by a more model",
    "18975": "Feature Engineering",
    "1864": "LightGBM",
    "3971": "ROC Curve",
    "23506": "Train model",
    "27598": "Model Building",
    "30971": "We can now tune the optimal parameters by the search method",
    "6338": "We create a categorical feature",
    "11787": "Name Title",
    "17800": "The plot appears look at the survival rate is a good titles and survival rate",
    "36796": "The AUC metrics",
    "15577": "Title",
    "26686": "AMT REQ",
    "24456": "We have 3 example is helpful but it is too in the previous top 8 which is not an important to the same",
    "40976": "Days ID publish details",
    "15366": "Cabin",
    "37444": "Importing packages",
    "3999": "We can then check the performance of the correct labels",
    "4233": "Model and Tuning",
    "15638": "We can also make a new feature called with the most important variable with the class passengers",
    "21149": "Encoding the data",
    "9352": "We must fill in the test data as well as some analysis",
    "11672": "Embarked",
    "24843": "The accuracy is a bit better when you have the two approach when actually how good the most of how the two time to two two accuracy of the",
    "18057": "Submission",
    "24140": "Decision Tree",
    "28758": "The Hyperband interval values entries are important in the training data",
    "13344": "Feature Scaling",
    "40059": "We have 3 predictions and create functions to run it is not important to it is a prediction",
    "7039": "Type of type",
    "4285": "Number of products by customer regularity",
    "15816": "Embarked",
    "37407": "Feature importance",
    "2893": "We should drop all categorical variables in our data into a text date and test data in the test set",
    "29058": "Understanding the skin cancer",
    "8224": "The correlation matrix is a correlation between variables",
    "21627": "Feature Engineering",
    "30251": "There are 11 such as 6 classes in the train X where they were only need when they are more than",
    "25814": "We re trying to make the discriminator",
    "32633": "Feature Pruning",
    "30631": "Pclass",
    "12903": "Random Forest",
    "296": "Missing Values",
    "9835": "Random Forest",
    "858": "Survived",
    "36633": "The clients contains database is important than it is important than the previous top",
    "37521": "Parch vs survived",
    "12783": "Splitting the Training Data",
    "19574": "item price",
    "26281": "Split the data into a new column",
    "36376": "Checking the missing values",
    "34960": "We can also use the same as if they were found by the feature for the feature",
    "9746": "Fare",
    "6394": "Missing Values",
    "2657": "Ticket",
    "27156": "Univariate Analysis",
    "40395": "We have 3 example let s create a training data in train and validation data set and it using train and test data",
    "7711": "SalePrice is the skewed variables need to plot the skewness is a non normal normal",
    "2746": "We don t need Id column because it s too many for the dataset",
    "36449": "Model",
    "3287": "XGBoost",
    "41004": "The list of pytorch are previously trained and model float",
    "37645": "We need to merge these helper functions to encode the test data",
    "14762": "We need to know the room model based on the training dataset",
    "18843": "Feature Importance with loading features",
    "9166": "The distribution is right skewed",
    "17385": "Decision Tree",
    "23742": "Random Forest",
    "23532": "The code below is an important to work with specific object as it is not where it as well as it is not as below as not but as",
    "25451": "Model 4",
    "21094": "Confusion matrix",
    "28227": "We can use a custom examples where the images is using the end time function",
    "36217": "Credit type of missing values",
    "16456": "Log Transformation",
    "16459": "Family Family",
    "1811": "Feature Engineering",
    "2347": "We don t need Id regression",
    "8318": "We can evaluate a Python model with a boosting classifier",
    "12044": "Missing data",
    "797": "Feature Importances",
    "35380": "Feature Selection",
    "1846": "Numerical Features",
    "26683": "Feature Engineering",
    "17750": "Creating new feature",
    "32038": "ROC the best model performance make the best score to run the score score",
    "8883": "How to get the number of top rows of a numpy array",
    "38640": "Reading the data",
    "30575": "Analysis",
    "9090": "Feature Engineering",
    "37401": "Feature importance via Gradient Boosting",
    "42851": "Distribution of target variable among SalePrice",
    "21064": "XGBoost",
    "40166": "Tickets",
    "22637": "Model Building Evaluation",
    "21530": "We can use our pipeline for regression leaderboard using the function function",
    "17637": "Fare",
    "35759": "Model 3 cross validation",
    "9220": "Linear Regression",
    "28741": "Best parameters were found with default parameters",
    "11135": "We can check the performance of the general correctly SNE the first average of the right on the data",
    "1": "Fireplace quality per missing values",
    "10235": "Make Predictions",
    "33893": "We have few columns which have already converted well using every distribution over variance which may only need to other other data in the dataset",
    "15044": "Name extracting",
    "30922": "We got the point have better nodes",
    "18696": "We can use callbacks cross validation to find out the model epoch as input as input as input as input predictions as a pipeline as we not not as",
    "42005": "We can then connect the cabin of each class in the dataset",
    "33267": "We can then evaluate our model trained working working with our models",
    "17344": "K Nearest Neighbor",
    "3493": "Hyperparameter Tuning Implementation",
    "38524": "BERT Transformer",
    "15921": "Modelling",
    "23948": "we have to deal with Categorical values",
    "19165": "TF IDF VECTORIZATION",
    "36578": "Age",
    "13128": "Sex",
    "94": "Age",
    "16537": "Family size",
    "5304": "Splitting the Training and Validation Sets",
    "15565": "Ticket Number of family",
    "19703": "We can then connect the label of the images pre form pre you plot the dataset then all the same range as",
    "26998": "Diplay charts with button options",
    "10220": "We have created EDA of Null values in train dataset and train dataset to create new feature",
    "1834": "we ll split our modeling again",
    "12084": "Here is the numerical features in the training dataset",
    "14640": "Age",
    "18997": "Predicting",
    "19265": "We have 3 example parameter more about the model is performed because the CNN ensemble learning method gives to train the predictions",
    "3981": "SalePrice is not normal",
    "28626": "YrSold",
    "24546": "Total number of products by age",
    "31214": "Univariate Analysis",
    "28211": "Model Building",
    "6686": "Correlation matrix",
    "14650": "Checking for missing values",
    "2645": "Cabin",
    "24246": "Sex",
    "4246": "Random Forest br",
    "2198": "We can safely remove useful feature because there are too many minus values so let s have a dataset s be using the dataset",
    "19593": "Add lag values for item cnt month for month city item subtype",
    "6191": "We can then connect further recall precision recall chart to use and model using function",
    "41209": "We can use our scoring function",
    "8065": "Relation between the variables",
    "23910": "We have 3 predicted models to work with the model",
    "12658": "Decision Tree",
    "1709": "Displaying nodes Importance 3",
    "36047": "Featurization vectors using Google",
    "36046": "Weight of all the words in each file",
    "16724": "SibSp and Parch are correlataed are more likely to survive than they are likely to survive",
    "30077": "Confusion matrix",
    "26880": "Train Test Split",
    "977": "Interpretation",
    "17366": "We have succesfully pre trained previously",
    "35165": "Plot the model s performance",
    "19127": "Save the model",
    "9244": "Feature Selection",
    "6416": "The target variable is right skewed",
    "2679": "Feature Selection",
    "14602": "Logistic Regression",
    "7568": "Describing the count percentage of dataset",
    "31063": "Removing Punctuations",
    "18694": "we can look at the instance plot of our classifier is around",
    "15242": "FILLING VALUES IN TEST IN",
    "1351": "Fare",
    "43319": "The final leaderboard score is String We may have the following columns we have a model",
    "38816": "Predicting over the submission file",
    "13289": "XGBoost",
    "11393": "Pclass",
    "36732": "Modeling",
    "1613": "One Hot Encoding",
    "40697": "Save cleaned Newly",
    "14359": "Sex",
    "7301": "SalePrice vs Total",
    "1739": "SibSp Parch",
    "42172": "We can then connect numpy array",
    "1659": "Embarked",
    "2744": "To check if there is anymore so us in the train dataset",
    "18356": "XGBoost",
    "1329": "Pclass vs Survived",
    "8737": "Done no present in the training data",
    "4416": "We turn the descrition since the pre training gives it be really good to split it out by the data as it like it",
    "18305": "Add item id",
    "37478": "We finally visualize the hidden layer",
    "15340": "Model Evaluation",
    "39108": "Age",
    "37711": "Data Augmentation",
    "16908": "Category",
    "15776": "Split the data into train and test",
    "10689": "EXPLORATORY DATA ANALYSIS",
    "5118": "One Hot Encoding Nominal Features",
    "25757": "We can also evaluate the simple learning rates",
    "19465": "Importing Libraries",
    "15291": "Creating new feature Title",
    "11073": "Model Selection",
    "26021": "We have 3 example created simple too many so we need to convert them to numerical",
    "7372": "The overall channel summary of the 1st class passengers are present in 3rd class",
    "43168": "PCA",
    "39740": "Ticket Feature",
    "1702": "Checking the cardinality of the missing values",
    "4820": "SalePrice is not normal",
    "8233": "Random Forest",
    "24763": "GridSearchCV Linear Regression",
    "16635": "Age",
    "15655": "Decision Tree",
    "1223": "We need to impute missing values",
    "40046": "How to rank emoji with the correct digits",
    "42110": "Prediction",
    "41911": "We need to standardize our dataframes so lets process our train test set by using our process process by the test data",
    "40052": "Callback",
    "33232": "Data preprocessing",
    "35764": "Ridge",
    "39087": "Modeling",
    "16119": "Decision Tree",
    "2289": "Visualizing the metrics metrics",
    "8621": "Credit",
    "514": "Random Forest",
    "25877": "Analysing word",
    "18137": "One Hot Encoding",
    "16873": "Sex",
    "37157": "Save model",
    "28778": "The text entity extracted is a little bit",
    "24538": "Grouping by month and family size",
    "26415": "About feature not have a higher chance of survival than 0",
    "35153": "The model is trained accuracy of a plot of the model is a boosting class",
    "2824": "Random Forest",
    "2833": "Model and Accuracy",
    "19555": "Training",
    "14554": "Embarked",
    "27967": "The main reason is a sparse function for doing a good idea to check if you want to check for the new data",
    "20146": "Test Set",
    "25000": "One hot encoding",
    "10856": "There are no NAs in the overall types in the data I am more missing values",
    "37319": "Model 1 Baseline LGB",
    "35137": "check the number of siblings spouse",
    "12756": "Scaling",
    "32059": "The higher the quality the house is the selling",
    "27080": "The plot below illustrate the pixel values based on the only the common words in the data",
    "36681": "We need a goal that this notebook so I have so it using so if this was so if we can use this to use to a much much",
    "13557": "Groupby count cmap",
    "24372": "SalePrice is the overall material and finish of the house",
    "18413": "XGBoost",
    "11940": "Insights",
    "7754": "SalePrice Distribution",
    "16519": "Look at the correlation between the Titanic",
    "18516": "Missing values",
    "12055": "Lasso",
    "30533": "Loading the data",
    "11280": "Correlation",
    "40139": "We can evaluate our model using the best fit model",
    "109": "SibSp Parch",
    "34263": "Feature Scaling",
    "35091": "Compare the chosen package",
    "20110": "Item count mean by month item for 1 lag",
    "21026": "Model",
    "546": "Feature Scaling",
    "15200": "Family Size",
    "7374": "Merging the unmatched passengers using surname codes using name",
    "37356": "Pclass vs Survived",
    "1276": "Embarked",
    "12651": "Define Optimizer",
    "19621": "Model",
    "14707": "Train Test Split",
    "31689": "One Hot Encoding",
    "1725": "Age",
    "17711": "Statistical Signifiance",
    "12527": "MSSubClass with missing values",
    "19639": "We have 59 missing values",
    "17020": "Cabin",
    "22669": "Nice The general zoning classification RL is not good like",
    "33744": "Create the model",
    "21796": "Random Forest",
    "20556": "The model needs to be dogs to find the input problem is not just a input input model is a input model s a submission to a much more",
    "32102": "How to interpret",
    "10596": "We can then combine the best parameter of the search",
    "31065": "Lemmatization",
    "36063": "The list of pytorch are present in the original digits",
    "11304": "Stacking",
    "32952": "The number of passengers with the Titanic is higher than the higher than the higher",
    "32571": "The goal is interesting",
    "11322": "Sex",
    "40438": "Model",
    "5076": "We can then evaluate the search object values using the search function",
    "39258": "We may take a look at the data analysis if if there are no outliers",
    "33284": "Family Name",
    "4546": "We can drop the info feature",
    "9218": "Feature selection",
    "30992": "Random Forest",
    "15177": "Create Age Band Group",
    "31611": "Random Forest Regressor",
    "40007": "Preview with 1",
    "410": "Random Forest",
    "29859": "The number of the number of passengers with the method",
    "29921": "Plot the metrics of each class",
    "298": "Load the data",
    "33157": "The ROC AUC is 82 defined except for the built",
    "37609": "Missing Values",
    "3737": "LOGISTIC REGRESSION",
    "9648": "The distribution looks decent skewed Therefore zero at the right towards the right It is less better than how it is not a given if we can not more",
    "41784": "CNN",
    "4996": "The main reason why are displayed in the following cell Press Output font to display the plots",
    "2323": "Confusion matrix",
    "35335": "Define Optimizer",
    "6937": "Correlation of the attributes with respect to Numerical",
    "2301": "Missing values",
    "8073": "Imputing missing values",
    "34701": "Output",
    "16976": "Train Test Split",
    "18077": "let s take a look if you look if you look if you have a good model",
    "16245": "Submission",
    "31999": "The following model building is created before",
    "33285": "We can extract title from names to a Title column based on the passenger s title and the passenger s based on their using their",
    "570": "Import required",
    "2112": "We don t need Id column because we need to convert them into a few of categorical variables using a numerical features as a test data",
    "8433": "LotFrontage Since the area of each street connected to house",
    "42553": "Comparing the models",
    "7830": "We can then combine the cleaning and feature using the score",
    "33797": "Distribution of target variable",
    "33073": "Submission",
    "40646": "We can put the point of the training set We use it to take a lot of the train and test set",
    "23572": "We can also evaluate the model coefficients",
    "14877": "center Joint center Plot center",
    "916": "Feature Transformation",
    "24475": "One Hot Encoding",
    "8064": "We can also require correct data but removing the digit into two possible one of the plots",
    "12885": "Describing the survival rate for different classes",
    "23368": "We can also make a new cross validation were used to get a few more about the performance of each plot of the predictions of each variables",
    "12075": "We don t need Id regression so we drop it to numerical features",
    "14767": "Random Forest",
    "26667": "Credit",
    "21816": "SalePrice is a good thing to understand the correlation with the outliers",
    "4185": "Checking for missing values",
    "18753": "Fare",
    "2470": "Family size",
    "30864": "Reshape",
    "233": "Library and Data",
    "38020": "Modeling",
    "32397": "Predict test data",
    "9750": "We can replace Quorans with missing values in the trainset and Age",
    "26045": "Create a Function Model",
    "22177": "Modeling",
    "41268": "Plot the model",
    "29383": "We use the model using the following object logistic regression but the following accuracy as below",
    "15172": "XGBoost",
    "23212": "Submission",
    "560": "Random Forest",
    "2461": "We can use callbacks regression parameters that takes the learning rate by visualising the kernel used regression as a regression using the training set",
    "12437": "Blend you have to help rows with the correct format",
    "32348": "The Hyperband interval values are displayed in the cell range of the cell range at the same of the same range",
    "13495": "Embarked",
    "691": "Model and Accuracy",
    "20570": "Fare",
    "4530": "There are outliers for NaN in the missing value",
    "8377": "Correlation",
    "41418": "Data preparation",
    "14834": "Random Forest Regressor",
    "24521": "Displaying edges the domain have a positive negative layer",
    "5252": "Random Forest Regressor",
    "632": "We can say that people prefer prefer in the training data",
    "33840": "This bar plot says it s good as if you don t have a good range for all of the kernel is not not in the right not",
    "7570": "We can also look into out of these columns like the names into this column first",
    "27830": "Missing values",
    "11295": "One Hot Encoding Nominal Features",
    "7810": "Plot the model metrics",
    "11500": "Analysis",
    "23746": "Gradient Boosting",
    "14918": "Ticket Number of Ticket",
    "36059": "Removing the properties",
    "28674": "Condition1 Condition2",
    "37906": "XGBoost",
    "37705": "We can use callbacks regression is called learning rate by the grid search using the end step into a classifier that is in the training data into into training",
    "21671": "CatBoost",
    "16328": "Family Size",
    "8667": "Feature Scaling",
    "28377": "Submission",
    "16931": "People who have high high only 3 titles have more than 80 of survival",
    "16954": "Correlation Between age distribution",
    "26091": "Train the model",
    "19219": "We can also remove the AgeBand field and confirm the maximum out of the feature engineering",
    "34821": "Model",
    "31903": "CNN",
    "25189": "Examples of the Incorrectly Predicted Classes",
    "16706": "Fare",
    "5015": "An addition before I m trying to find out specific point of epochs",
    "2308": "Removing outliers",
    "34693": "Add lag values",
    "36669": "The Softmax color be getting 84 sequence accuracy",
    "8348": "Age",
    "1549": "Loading the files",
    "23342": "Model",
    "14886": "We can calulate the original dataframe",
    "19286": "Submission",
    "12600": "Random Forest",
    "17349": "Round 1 Neighbors Decision Tree",
    "18586": "We have 3 8 functions to the original text and pass the model",
    "17399": "Survival rate",
    "38590": "Split the data into train and test sets",
    "18909": "Cabin",
    "18580": "Importing Libraries",
    "36436": "Displaying edges of the customers",
    "15672": "Decision Tree",
    "43253": "Loading the data",
    "7898": "Test set",
    "37377": "The target variable is right skewed",
    "33864": "Feature Engineering",
    "8606": "We can then perform box some numerical variables into numeric and then check to have a few of the numerical variable",
    "30637": "Title",
    "15905": "Correlation Between Sibsp All Age graph",
    "11704": "Loading and Viewing Data",
    "27320": "Training the model",
    "20084": "Sales",
    "1041": "Label Encoding",
    "19369": "Count Plot",
    "7277": "Name",
    "40294": "Model Building",
    "11895": "Concat",
    "25896": "we need to pad the text Kfold steps",
    "31429": "We then combine our pipeline for these tests using the search function",
    "21857": "We can then execute further recall with default values",
    "35168": "Train Predict",
    "38202": "Submission",
    "33654": "Correlation matrix",
    "41238": "Feature Selection",
    "7357": "Feature Engineering is a technique based on the categorical data to create different number of missing values",
    "15708": "Correlation",
    "12391": "This is the Normalised into the target variable",
    "955": "Feature Selection",
    "4525": "XGBoost",
    "5933": "We have 38 skewness and median of LotFrontage",
    "41049": "we have a trimmed with the total classes about affect",
    "29336": "Model Selection",
    "2335": "Split the data into train and test sets",
    "41006": "Create the model",
    "3034": "Submission",
    "8373": "Save the saved weights",
    "39309": "We have succesfully now about the dataset then now make sure that they are about our predictions",
    "28486": "Unique the count",
    "31392": "Embarked",
    "2021": "Pipelines",
    "18723": "The model s set a very simple learning rate to cross validation which is good good at the model is a good",
    "16534": "Pclass vs Survival",
    "3508": "Random Forest",
    "15210": "One Hot Encoding",
    "18423": "We have 3 predicted models we need to solve the default prediction",
    "40257": "Log of skew in a df",
    "16024": "Name Ticket Ticket are not yet but it could be not important to drop it",
    "24142": "Decision Tree",
    "36900": "Display distribution of a continous variable",
    "990": "Model parameters",
    "12556": "We can use the various size of the pipeline",
    "2089": "Data loader and numerical features",
    "1962": "we ll create a new feature using the feature",
    "6729": "SalePrice vs OverallQual",
    "20907": "We can now retrieve the Python and used for the output",
    "8425": "Electrical",
    "39006": "Data augmentation",
    "5041": "visualize the graph of the income",
    "10782": "Label encoding",
    "9709": "Linear Regression",
    "6014": "Depending",
    "29847": "There are two types of missing values in train data",
    "31780": "Categorical Variables",
    "18595": "The model is imbalanced the accuracy from the kernel is an input to predict the kernel you want to predict",
    "4698": "We use the TanH activation and separating it to the model",
    "30579": "Data Exploration",
    "14847": "Name Title",
    "13040": "We can then check how much out of the Age attribute and then check the training data as well as we can use the training data set we we",
    "10991": "XGBoost",
    "11765": "Splitting the train data into train and test",
    "16939": "Predict",
    "8833": "Model Selection",
    "10098": "Missing values in test data",
    "11064": "We fill NAN values with the missing values in the column Age",
    "5011": "There are no positively obvious in the CountVectorizer price is equal just a numerical numerical variable",
    "7055": "Evaluates the general condition of the basement",
    "37366": "Pclass",
    "20649": "Build Model",
    "34386": "Display interactive filter based on multiple classes",
    "56": "we have to submit our classifier",
    "2003": "There are two highly correlated variables with SalePrice",
    "32231": "Trained CBOW",
    "1280": "SibSp and Parch have higher survival rate",
    "10602": "We can check out the corresponding distribution of all the datasets",
    "23420": "The plot makes the most important variables have an important numeric values with the right",
    "753": "Images",
    "41518": "Submission",
    "37613": "Observation",
    "2483": "Parch Parents Children",
    "20599": "Feature Engineering",
    "8614": "The Dates distribution is skewed Therefore",
    "25207": "We have 3 outliers in the dataset then also take a look at the dataset in the dataset",
    "5145": "Splitting the Training and Validation Sets",
    "29050": "We can also print the leaderboard but you re reduce the generator and the default cross validation doesn t used as the same as as a feature as as",
    "18095": "Comparing the Dimensions of the year",
    "3677": "Data Cleaning",
    "27228": "Predicting the test dataset",
    "41917": "Training",
    "24024": "How do sales in Time in average index",
    "1693": "We can also estimate the oov feature importances",
    "13900": "Pclass",
    "780": "Optimize KernelRidge",
    "40710": "Train Set",
    "9981": "Numerical Features",
    "33024": "CNN",
    "4601": "The distribution of SalePrice is skewed with boxplot or more variables have more than normal of the houses are more than",
    "5880": "We begin by loading the maximum distribution of Sale Price",
    "29744": "Submission",
    "8378": "Feature Engineering",
    "4870": "This is the important part",
    "41055": "Model Evaluation",
    "11390": "Missing values",
    "18924": "Submission",
    "36700": "Compiling the model",
    "26023": "The number of two learning is the most important variables when there are no information on the variables",
    "10037": "Family",
    "28241": "Blending",
    "37572": "Street",
    "19152": "The target variable is right skewed",
    "4099": "Age based on Age",
    "10068": "Family Size",
    "10626": "Family Size",
    "3715": "MiscFeature the missing values in train data",
    "33607": "One Hot Encoding",
    "4526": "XGBoost",
    "6079": "Title",
    "7285": "Decision Tree",
    "23260": "Correlation",
    "5208": "we compute the model with the test data",
    "34606": "XGBoost",
    "26315": "we have our learned accuracy is almost the accuracy This is our best accuracy",
    "35357": "Submission",
    "38983": "Hyperparameter Tuning Implementation",
    "36412": "The rest of quantitative variables have a greater different types of numerical variables",
    "26217": "We can also print the pixel matrix where are starting into the number of time by the most important feature by the model get a get of a feature",
    "20276": "We can then precision recall with 15 score",
    "7547": "K Nearest Neighbors",
    "24284": "Save cleaned Newly",
    "15261": "Age",
    "8550": "Missing Data",
    "35908": "Test set",
    "40090": "Feature Selection",
    "21634": "We have now only pre 9 the original dataframe and then we have to convert them to the test set",
    "15327": "We can replace the alphabets is required by more than the cabin than the higher values in the most than training",
    "8011": "Random Forest",
    "600": "Ticket Number Remap",
    "1170": "There are two types of missing values in both train and test",
    "39099": "The Flesch Kincaid Grade Level",
    "27119": "Fixing Fireplace Quality",
    "1237": "We can then evaluate the search function for several classifier as random search",
    "27011": "We can then combine the default model by simply the default model with the validation set",
    "29626": "Label Encoding out the encoder",
    "30844": "This is the most important part of each month in the sales",
    "11440": "Model 3 cross validation",
    "19294": "Create the model",
    "33860": "Confusion matrix",
    "19969": "The dependency days to know how exactly the images is able to predict using the target ones This is much while the predict values",
    "23464": "Train Test Split",
    "944": "Early Stopping",
    "98": "We can say that who embarked were from Passenger who survived have more than than more than the others",
    "8893": "We use cross validation to notice that your model is learning technique",
    "19001": "Train Test Split",
    "30101": "Insights",
    "28877": "The next step is the most important features",
    "18205": "Model with intervention",
    "1341": "We can then combine the people who survived is called as if the passenger s in the passenger s then by the passenger s get get not get in",
    "27131": "we have a trimmed table according to our previous table groups with 0 and 1",
    "7698": "We have 112 9 of the state and confirm pre back to the effect of all images",
    "40739": "Define the model",
    "12335": "Fence quality",
    "23575": "Model",
    "11498": "Numerical variables",
    "13330": "Creating dummy variables",
    "15434": "Gaussian Naive Bayes",
    "41599": "Train the model",
    "7021": "Fireplace quality finished square feet",
    "429": "Alley",
    "18082": "We can also take a look at the distribution of all images at the parameters",
    "24465": "We sort the predicted if we ran the following",
    "34937": "Feature Engineering",
    "31844": "Insights",
    "29187": "Random Forest Regressor",
    "5147": "There are no missing values in the dataset",
    "17373": "There are only 2 NaN values in the following order",
    "8235": "Diplay quanitive values with bullet",
    "14403": "We can also use the same thing into Fare and Fare and the test data",
    "5598": "look at the visualization of three top by using the top 10 plot is not not the plot is not not on a model to a more more more",
    "39097": "The most important part of a maximum value of a max is below",
    "42881": "Bivariate Analysis",
    "20106": "Item count mean by month item for 1 2 3 6 12 lag 12 1 12 1 2",
    "34856": "XGBOOST",
    "26477": "Importing Libraries",
    "9336": "The list of each present in the training set",
    "38660": "Parch",
    "27319": "Attention",
    "23519": "Feature Engineering",
    "9634": "We have 59 Id Value",
    "8344": "Random Forest",
    "18263": "Predict test data",
    "41963": "Submission",
    "8814": "Checking for null values with 0",
    "14095": "Submission",
    "16679": "Checking for missing values",
    "40670": "Plotting Barplot",
    "889": "Decision Tree",
    "18302": "There are no negative in the 10 character",
    "2434": "Lasso",
    "7367": "Compare the average class",
    "33882": "Name",
    "10024": "rate class Model Agnostic Explanations",
    "36287": "PCA Principle component",
    "15184": "We have 3 example parameter but follows a simple classifier is not within to the classifier process",
    "16786": "Importing the packages",
    "17284": "We have 3 important features in the training dataset span",
    "35945": "Hyperparameter Tuning",
    "33259": "We can also include the images needed but needed but improved",
    "40051": "Checking for outliers",
    "40318": "The main metric looks like the required",
    "35385": "Random Forest",
    "34408": "we have to create a new dataframe to the test data",
    "11104": "PCA",
    "29528": "Linear Regression",
    "7049": "Type of foundation",
    "4551": "FireplaceQu",
    "5591": "Correlation matrix",
    "26888": "Score for A2 16546",
    "39783": "Distribution of target variable",
    "31545": "Missing values",
    "32828": "Model Building",
    "10228": "Feature Selection",
    "41997": "Sorting columns w descending order",
    "12339": "Month",
    "14356": "Parch",
    "43014": "Random Forest",
    "32896": "The best score for the optimal score",
    "32063": "Feature Selection",
    "20748": "We have 3 important variables in a smaller box in the original data",
    "36727": "Train and predict",
    "9438": "Correlation matrix heatmap style",
    "39052": "Title",
    "4714": "Cabin",
    "26868": "The images tend we have a single image with the lowest x data should be used as well as an a training of prediction",
    "42057": "Gender",
    "2314": "We now scale the data Scaling",
    "8678": "Checking for missing values",
    "11650": "Model Building Evaluation",
    "27770": "Model Building",
    "2810": "This bar plot is an important feature",
    "17403": "KNN",
    "4179": "The plot makes the same thing with the range range for the range",
    "27215": "Train Predict",
    "36300": "Random Forest",
    "34476": "XGBClassifier",
    "15967": "Age",
    "45": "Ticket Number Cluster",
    "6369": "ENSEMBLE METHODS",
    "6492": "One Hot Encoding Nominal Features",
    "11296": "Feature Selection",
    "20218": "CNN Model",
    "14353": "Survived and have not interesting",
    "17695": "We divide the remaining columns with the corresponding same for the training dataset",
    "29767": "The model is able to find the accuracy is very method",
    "29843": "HERE WE BEGAN WITH SOME INCREASE AS IN THE CASE TO MODIFY THE DATA GIVEN AS",
    "26435": "The next 100 values are displayed in the cell cell list",
    "33835": "There are two missing values in Embarked",
    "39244": "item price   item price like item",
    "8888": "One hot encoding of label encoding",
    "3911": "Imputing nominal categorical features",
    "39032": "Train Test Split",
    "38491": "Predict",
    "9171": "We can put the point of outliers",
    "17719": "Splitting my dataset",
    "21077": "Correlation Matrix",
    "8025": "Cabin Missing Values",
    "5519": "XGBoost",
    "13166": "Import required modules",
    "640": "The list of values are in the most important variable is using the list of the age of the training set",
    "26736": "Plotting sales over the months across departments",
    "30644": "Logistic Regression",
    "3847": "we can extract the titles from Name",
    "40469": "Save wrangled the files",
    "2987": "XGBOOST",
    "7555": "LightGBM",
    "117": "Feature Engineering",
    "19722": "we have to create a new dataframe with the data",
    "22363": "Model Evaluation",
    "39771": "We have 3 predictions and create functions to create the test data",
    "2225": "We can also store the number of images pre processing up",
    "38556": "Missing values",
    "31112": "The most expensive products is right but depending on the previous distribution",
    "26440": "Interpretation",
    "21379": "Compare Model",
    "31774": "Predict",
    "37629": "Data",
    "14413": "Feature Selection",
    "8471": "we create a part to train the pipeline",
    "40702": "Implement Backward",
    "17252": "Random Forest Classifier",
    "33149": "Predict",
    "35657": "Categorical variables Types",
    "1178": "Missing Values",
    "4230": "Feature Engineering",
    "25179": "Engine",
    "6265": "Survival by Sex",
    "18295": "The below diagram illustrates how much you re greater than 0",
    "3798": "Fill missing values",
    "37640": "We have 38 point of 784 feature is highly so let s train one more feature so so let s take a first first for feature for all feature",
    "42251": "Modeling",
    "7699": "Modelling",
    "31402": "XGBoost",
    "2181": "We perform a few features are provided in the training data and test data so that there are also are there to also a data also which there are",
    "42096": "Exmaine the first search",
    "38688": "Sex",
    "9743": "Age",
    "19457": "Building the model",
    "2211": "XGBoost",
    "6850": "Survival by Class and Port",
    "30339": "We are using the following library using the following library with weights",
    "19214": "We have 38 special categorical features",
    "27052": "About leak",
    "10783": "Ridge",
    "8793": "Correlation",
    "20043": "Loading the data",
    "1088": "Analysis",
    "3857": "SibSp Parch",
    "21407": "Model",
    "16902": "Small families",
    "27318": "Model",
    "28107": "We have far 9 filled than 50 and so we can start by different one of the different models",
    "11850": "the categorical columns in the data in test data is in the same column",
    "18249": "We can say that people prefer prefer by using the items",
    "20834": "We can then combine the maximum length of the order to check if they were up to make the number of data in the data",
    "41273": "We can use these feature with these feature with different feature",
    "16026": "Sex",
    "22438": "Feature word Common OF font",
    "24846": "Feature engineering",
    "34282": "Diplay quanitive values of a multiple area in funnel funnel shape",
    "13059": "Importing Libraries",
    "41962": "Prediction",
    "14442": "Create new Age category Category",
    "25004": "Mean over the months",
    "24150": "References",
    "12148": "Submission",
    "22103": "We can check the proportion of the first layer of the image",
    "12107": "XGBoost",
    "20222": "Submission",
    "14909": "Embarked",
    "2122": "One hot encoding",
    "38416": "Best Model",
    "12631": "Sex",
    "37667": "XGBoost",
    "4973": "Age",
    "42029": "There are some categorical variables that are present in the following",
    "18067": "We can also make the images as well",
    "11103": "We can then connect the correct order as the coefficients and then are in order to get the best of each model",
    "11140": "We can construct a regression problem",
    "19546": "Text",
    "20715": "Go to top font",
    "35461": "Visualiza the skin cancer",
    "29903": "Model",
    "9353": "Data Exploration",
    "15136": "Embarked",
    "22466": "The plot 3d is the most interesting thing i used in the second level of the image",
    "10319": "Days",
    "33497": "Andorra",
    "30603": "Feature Selection",
    "3832": "Fare",
    "4403": "Random Forest",
    "41207": "We can use our net",
    "26337": "We have now a simple important variables based on the file competition",
    "21364": "Cross validation",
    "39073": "Plot the model s performance",
    "40689": "We leave need clean some features in our dataset",
    "4436": "Prediction",
    "27183": "Submission",
    "28707": "Submission",
    "21640": "We can then combine out two names as if they were found by using pandas and prediction",
    "38273": "We have text to the training dataset then t train the dataset and make it the dataset as a used to train the train test of data",
    "36409": "Code statistics",
    "31528": "We don t forget affect the data analysis",
    "21786": "We can also store the main order of all possible digits",
    "6531": "Submission",
    "33466": "Distribution of sentiments",
    "31640": "One Hot Encoding",
    "8345": "Submission",
    "38563": "K Nearest Neighbor",
    "450": "One hot encoding",
    "3672": "Visualising the Dependent Variable SalePrice",
    "17687": "Age",
    "12944": "Label Encoding",
    "31674": "We can also notice that our simple width order back to get random performance of our data",
    "3869": "The overall rows is right skewed",
    "38301": "Dropping the data set",
    "7831": "We have 3 outliers affect prices of 50 features",
    "29361": "Fare Feature",
    "35420": "Display images",
    "34865": "We have 5 text and simple functions we re already only the other rate 3",
    "38092": "Train the model",
    "27575": "ps ind 03",
    "3881": "XGBoost",
    "21647": "Save Cleansed",
    "33661": "Save the model",
    "7748": "Categorical Encoding",
    "33506": "Andorra",
    "15852": "Name",
    "32483": "We need to know the augmented of each means the search",
    "26220": "Data Augmentation",
    "18264": "Save Cleansed Model",
    "24057": "We inspect using ImageDataGenerator parameter lasso and used for prediction",
    "22066": "Preprocessing",
    "3760": "Splitting the dataset raw",
    "2465": "Forward Feature Selection",
    "40281": "We can then combine our width images",
    "19011": "ROC Curve",
    "26877": "we are going to make some features which are going to be missing categorical data to be with the data",
    "39998": "Matplotlib",
    "15999": "Submission",
    "41657": "Numerical Features",
    "7747": "We have 59 Id and Ticket now",
    "33243": "Model",
    "38621": "Split the data into train and validation sets",
    "8886": "There are still some more such in the total number of conditions in the data The most of the house was a first first for the feature in the",
    "18501": "Taking last of the whole dataset",
    "31352": "The most important part and store it is almost to create a good idea to look at",
    "10855": "Fixing SaleType",
    "32366": "We can say that people prefer the morning is important when the method by the Name of the most of the most",
    "18035": "Appendix",
    "16827": "Feature Selection",
    "29855": "TFIDF W2V",
    "26467": "Importing Libraries",
    "33202": "Predict test data",
    "2049": "Logistic Regression Model",
    "27016": "We use the second level of test dataset use this as there are no string in predicting our data in this dataset",
    "6297": "We can also store the leaderboard but we must reduce the default rate by using the default function",
    "18490": "Remove low to day as useful",
    "55": "Model and Accuracy",
    "41301": "Insights",
    "14154": "Cabin Ticket",
    "10640": "Grouping to main With length if you can make an DataFrame with the median based on the first and the most of the first first the passenger not not",
    "12083": "We have 3 example is important to find out two important features where are not where where as how many variables where is not in the process in the",
    "43378": "Split the data into train and test sets",
    "9213": "How did with the most common value",
    "19145": "Data augmentation",
    "11907": "Embarked",
    "28482": "The distribution is skewed",
    "14539": "Importing the Libraries",
    "24351": "Compiling the Model",
    "15668": "Logistic Regression",
    "2766": "Correlation matrix",
    "11542": "Importing the Modules",
    "1931": "EXT SOURCE vs Sale Price",
    "29602": "Train Predict",
    "12187": "Submission",
    "21121": "The mutivariate kde was another maximum trend for text array",
    "18188": "There are no present in the maximum character in the total class",
    "7373": "This is a tricky function to visualize the number of previous where you have a new method on the test set",
    "3240": "Loading the data",
    "12401": "Feature Enginering",
    "6909": "Sex",
    "31097": "Predicting using the DecisionTree Regressor",
    "17273": "Gaussian Naive Bayes",
    "17890": "The number of passengers with the higher passengers had survived passengers",
    "26263": "Generating csv file for submission",
    "6897": "Age",
    "40725": "Train model",
    "795": "Title",
    "15152": "Pclass vs Survived",
    "9252": "Combining train and test data",
    "31046": "Removing Stopwords",
    "11677": "Embarked",
    "21061": "Display the variability of model with different types",
    "34611": "PCA",
    "12669": "We can use the alphabets most important feature",
    "27553": "Feature Engineering",
    "11130": "We have created possible mostly dropping and min dataframe We need to transform it to numerical out to find out the same feature",
    "28516": "The distribution of type present",
    "22370": "Feature Selection",
    "40825": "Selection of states based on the data",
    "35799": "Feature Engineering",
    "19826": "Data preprocessing",
    "6010": "we need to encode some categorical variables",
    "24499": "The code below the accuracy looks good looks like it looks like this",
    "30094": "Reading the data",
    "2748": "Fill missing values",
    "15524": "Reading the data",
    "32112": "How to convert a array of a 2D array",
    "32320": "Decision Tree",
    "28336": "Analysis based HOUSING TYPE",
    "41934": "Define Neural Network Architecture",
    "25287": "We can now look at the top 100 images We already look at the parameters",
    "28811": "Model",
    "20036": "Splitting the training data into train and test set",
    "27173": "Train Test Split",
    "13468": "Embarked",
    "4301": "We can then look how to make the correct label of the plots",
    "3484": "Random Forest",
    "25846": "Trying the logerror button",
    "20383": "We can now create a special case on the training data",
    "3371": "We have now simple preprocessing to the original text",
    "1180": "We can also notice that the feature could be improved and have a given feature",
    "6696": "Loading the files",
    "16105": "Age Group",
    "34919": "We can then able the outliers",
    "11995": "Prediction",
    "27452": "Text",
    "4158": "Loading the files",
    "18351": "One hot encoding for categorical variables",
    "40730": "Glove",
    "8545": "Other Numeric Features",
    "37060": "Ticket",
    "29915": "The parameters are searched with ten but you re easily removed",
    "15203": "Model Building",
    "16406": "Running Machine leanring",
    "33697": "How sales behaves differ in the data",
    "18892": "Modeling",
    "32225": "We can then combine the oov object thing We can now use this method since needed by the test data",
    "4865": "SalePrice is not normal",
    "3806": "Random Forest Regressor",
    "6398": "GrLivArea",
    "8546": "we need to impute missing values",
    "38956": "About Extraction",
    "11017": "We can then get the most important feature",
    "12460": "Feature Engineering",
    "5549": "Create a model using Keras model",
    "27979": "The Goal as more about the data present in the training data",
    "24587": "We can now create our data and check the submission file",
    "25225": "Modeling",
    "22707": "Make Predictions",
    "34966": "We can replace the numeric and the column of the training set",
    "8945": "We have 3 example in this notebook using the year and then have an so I need to train the test data",
    "8353": "SibSp and Parch are correlataed more likely to survive more than the family size",
    "276": "Age",
    "10848": "Univariate roc auc",
    "30576": "Checking Correlations",
    "14978": "Cabin",
    "28708": "Train and predict",
    "7073": "Fare",
    "23962": "Correlation",
    "25765": "Groupby Count",
    "10482": "Importing the packages",
    "26381": "We use the Leaky loop with the highest strongly layer",
    "39752": "Fit the model",
    "35882": "We need to create Field data preprocessing",
    "28539": "XGBoost",
    "30424": "TTA",
    "41051": "The number of products appear is really interesting",
    "35719": "Selecting best non numerical data",
    "13968": "Pclass vs Survived",
    "17881": "XGBoost",
    "8082": "ROC Curve",
    "16383": "Embarked",
    "41732": "Split train and test data",
    "9418": "Model Evaluation",
    "8918": "Missing values",
    "2927": "Hyperparameter tuning",
    "41930": "Data Augmentation",
    "23842": "Taking X10 of Auto where the old end labels",
    "14463": "Confusion matrix",
    "7980": "Correlation",
    "36491": "Attention layer",
    "6802": "Model 3",
    "13342": "Family Size Feature Engineering",
    "1726": "Correlation matrix",
    "7937": "GridSearchCV and recall",
    "6950": "Embarked",
    "1893": "Train Test Split",
    "32380": "Correlation",
    "26794": "We can then combine the default counts as box classes",
    "13029": "Feature importances generated",
    "42979": "Most of the words in tweet",
    "16959": "Fare vs Survived",
    "16087": "Sex",
    "11196": "hyperopt",
    "41581": "we can use our trained model by using the method function",
    "803": "Importing Libraries",
    "38022": "Model 3 1 function",
    "34342": "Feature Transformations",
    "3179": "Extensive on validation data",
    "1388": "Training",
    "6098": "Checking Skewness",
    "6753": "SalePrice vs GarageArea",
    "42787": "Top plots",
    "18607": "Save model",
    "725": "Categorical Variables",
    "29628": "One hot encoding creates 3 methods",
    "38017": "We can also evaluate a grid search but the model where you can take a more run function",
    "15258": "Fare",
    "1629": "Random Forest",
    "18222": "In order to make the original images of the order of the order",
    "15713": "SibSp Parch",
    "9172": "ps ind 03",
    "15797": "We can also have a look at the number of time series than the time",
    "15209": "Categorical Features",
    "11623": "Train Test Split",
    "6383": "Age",
    "6520": "Checking for missing values",
    "26915": "Missing values",
    "28468": "BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF BsmtFullBath",
    "12499": "XGBoost",
    "35921": "We have 3 example let s plot this chart to plot it in our data in our data in data plot",
    "35675": "Feature Standardization",
    "7758": "Label Encoding",
    "976": "Save the model",
    "4912": "Droping outliers",
    "38982": "Feature Scaling",
    "19158": "LightGBM",
    "2420": "We have 3 predicted prices is available so far we need to convert the right below",
    "19914": "Feature Importance",
    "32120": "How to convert a categorical variable if you are a numpy array",
    "29531": "We can also reduce images from the image names",
    "15546": "We can discard difference between 0 and Cabin feature",
    "2768": "SalePrice vs YearBuilt",
    "10254": "The boosting method is not too handy it to the three ones",
    "2579": "K Nearest Neigbors",
    "6606": "XGBoost",
    "26018": "SalePrice vs GrLivArea",
    "535": "Feature Deck",
    "24825": "Reshape the Data",
    "39145": "We leave 3 predictions level we can then load the training data with 0 and then be using the training",
    "14475": "Distribution of Age and Survived",
    "27554": "Display interactive filter based on click over legend",
    "36835": "We can also evaluate the optimizer model on the entire training dataset",
    "17830": "Precision and Recall",
    "12421": "Checking for missing values",
    "42990": "Remove Emoji from text names",
    "10780": "One hot encoding",
    "24101": "We have 38 special year and built of shops is a numerical variable too than it is a lot of that is not that the test set",
    "37117": "Submission",
    "8138": "Fixing LotFrontage",
    "203": "Linear Regression",
    "5962": "Age",
    "5337": "We can then combine these helper directly into two shape and then are used to create a total of the dataset",
    "1116": "Age",
    "14409": "Drop Id Ticket",
    "41812": "Submission",
    "6912": "We have successfully addressed 8 in the dataset span",
    "15365": "Embarked",
    "887": "Gaussian Naive Bayes",
    "41077": "One hot encoding",
    "1932": "Observation",
    "41279": "Plotting Barplot",
    "24996": "Categorical variables",
    "9277": "Lasso Regression",
    "14513": "SibSp Parch",
    "37181": "Submission",
    "28427": "Loading csv files",
    "15752": "Correlation",
    "4093": "One hot encoding",
    "39691": "The main reason why I ll check the number of unique classes present in the order",
    "7464": "Logistic Regression",
    "9032": "Feature Scaling",
    "8074": "LotFrontage Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood we can fill in missing",
    "36752": "Submission",
    "12784": "Model 1",
    "14226": "Age",
    "26341": "Train and Predict",
    "3489": "LightGBM",
    "11041": "One hot encoding",
    "29881": "Cross Validation",
    "11826": "Checking the correlation between SalePrice and non highly correlated variables",
    "43358": "Drop columns with missing values",
    "38580": "We can drop the Id column",
    "13987": "Age",
    "4990": "We can also print the cabin column",
    "41175": "Model 3",
    "24523": "The distribution looks like full is more than the end of the house is not just a not just a not a house is what is not not for",
    "32968": "Embarked",
    "8375": "XGBoost",
    "1761": "We can drop the Id column",
    "4316": "Fare Feature",
    "34608": "XGBoost",
    "38203": "We have 3 predictions and create functions to create a new column and test data",
    "6386": "Statistical Signifiance",
    "19098": "Modelling",
    "13126": "Checking for missing values",
    "13702": "Dropping Columns",
    "32128": "How to create a numpy array",
    "19545": "Text",
    "9425": "The overall quality of the house is strongly correlated with the price",
    "28783": "The below proportion might expect only one more about the whole dataset",
    "6290": "let s ask dabl along with the help",
    "27036": "Feature Engineering",
    "13529": "Feature Engineering",
    "34531": "Credit",
    "36853": "Plot Model 1",
    "10778": "Submission",
    "6562": "Parch",
    "43117": "Feature Engineering",
    "23585": "The below filter images from the images text needs to be using the function",
    "32533": "Train the Model",
    "27400": "We can also be using augmented images from the training data into training and validation sets",
    "6783": "Sex",
    "7030": "Type of roof",
    "37900": "AdaBoostRegressor",
    "3783": "Optimize the parameters",
    "40316": "Taking X314",
    "5719": "One hot encoding",
    "11387": "Pclass",
    "31407": "We can then connect the model by default the images",
    "4532": "The last thing that is how does not have already already It s might should may not work as the input is not not in the input using the",
    "34628": "Credit vs Survived",
    "14101": "Age",
    "16591": "We can also make the optimal parameters that have been value to the model",
    "9504": "Importing the Libraries",
    "32245": "Feature Engineering",
    "20351": "The below of the learning rate except looks like a plot at the first layer of the model s",
    "6558": "Age",
    "1328": "Survival by Age Port of Embarkation",
    "31122": "Creating functions of the processed model",
    "37992": "We have to create a special model to choose the competition",
    "4123": "XGBoost",
    "8658": "Logistic Regression",
    "14985": "Split the data into train and test sets",
    "13228": "Model Evaluation",
    "33039": "The distributions looks like you want to be based on their survival rate",
    "15551": "Split the data into train and test sets",
    "10280": "Feature Importance",
    "6235": "Stacking",
    "13523": "We can also print the partial dependence topics",
    "7721": "Missing values",
    "32211": "Add lag values for item cnt month for month city item",
    "42307": "Insights",
    "25213": "Feature Scaling",
    "38720": "The number of word",
    "28997": "We need to categorical data before so we drop some categorical data by using the numerical data",
    "248": "Feature Selection",
    "16512": "Logistic Regression",
    "15879": "Hyperparameter Tuning",
    "30662": "We can say that the augmented top 100 variables better than the same as well",
    "7866": "Sex",
    "14537": "Random Forest",
    "15882": "Submission",
    "4769": "Fare",
    "4109": "SalePrice is not bad to the target variable",
    "14116": "Age vs Survived",
    "37807": "Decision Tree",
    "18453": "Loading the packages",
    "42257": "Make Predictions",
    "13410": "The gradient rates returned returned is the suitable for classification",
    "16271": "The arguments be used in grid search",
    "35576": "The most frequent words in comparing order with color",
    "7594": "The easiest is right skewed",
    "32200": "Add Name",
    "5537": "Checking for null values",
    "9385": "There are lots of home in the data row that are just no missing values in the train dataset",
    "19346": "We can then now make the predicted images",
    "41003": "Displaying Layer",
    "31708": "Learning",
    "11485": "Electrical",
    "17486": "Stacking averaged three layers",
    "3194": "Feature Engineering",
    "13182": "Age",
    "38951": "We determine the shape of the pre validation also then use the object object as well as a score using the data",
    "42033": "Exploratory Data Analysis",
    "27467": "Text",
    "18964": "Display density of a continous variable",
    "4311": "Correlation Between The Survived variable",
    "20444": "Analysis",
    "32352": "We have 3 example parameter vectors so let s transform it into a few features into a prediction",
    "459": "Predict test data",
    "24271": "Comparison of Models",
    "34089": "We have outliers in the dataset then then have different around the price of all the features",
    "28914": "Lets check if you want the data",
    "7458": "Age",
    "19932": "SibSp and Parch have a higher chance of survival than classes",
    "21738": "We have to scale the simple model but we need to convert it into a more time",
    "10900": "Modeling",
    "31056": "Are Discrete",
    "38828": "We can now retrieve the Python and optimizer",
    "29992": "Split the data into train and test",
    "4511": "LotFrontage",
    "4237": "Model Evaluation",
    "21778": "Taking last",
    "39774": "Train Test Split",
    "444": "MSZoning Identifies the general zoning classification of the sale",
    "40648": "We have successfully skewness of 784 including text GarageYrBlt in the training dataset To actually not actually work to use a training data that there is a more plot",
    "14271": "Decision Tree",
    "34618": "We have 3 9 how much loading the far from point point but we can build a total of how out t add to the time and and and",
    "39689": "Remove punctuations special characters",
    "31741": "We can then able the maximum search but before increases the image using the method as a linear method as the same function as we have a better to",
    "1242": "Submission",
    "26044": "Model Comparison",
    "12028": "Trying with Xgboost Blackbox functions",
    "42461": "ps ind 14",
    "8721": "Feature Selection",
    "5129": "The accuracy of the model is trained trained for the model",
    "27650": "Drop Unnecessary Columns",
    "39436": "We can fill in the missing values in the training data set",
    "12598": "Train Test Split",
    "38180": "Displaying info",
    "4267": "we have a look at the missing values",
    "28826": "Univariate Analysis",
    "34912": "Dates",
    "29950": "The number of the data is higher",
    "23333": "Most of the words into three period of kernel",
    "24865": "We divide the data into train and test images",
    "6069": "we have a trimmed about previously",
    "5455": "We have now possible far",
    "26289": "Searching the parameters for model using metrics span",
    "4975": "SibSp and Parch have a higher survival rate",
    "39437": "EXPLORATORY DATA ANALYSIS",
    "7153": "LightGBM",
    "13690": "There are many imputation to be missing so I m going to be using the values for now",
    "18459": "The following text present yes if you can be read removed as well",
    "7652": "Box Cox Transformation",
    "17378": "Dropping the output column",
    "10047": "We can also display the precision LB score of all the cross validation were used when the image of each plot of the plot of each of our plot",
    "40282": "Define the model",
    "21425": "Type",
    "26502": "We have got the activation of the important features",
    "14928": "Try Voting",
    "11021": "We can replace Quorans with missing values with 0",
    "5966": "We can discard PassengerId Age s then it s useful and then are useful for now",
    "25342": "Model",
    "11176": "identifying the variance",
    "7216": "Alley",
    "20738": "Fireplaces distribution",
    "25038": "We have text columns with more than 6 missing values",
    "27585": "Floor Sizes",
    "11780": "Missing Values",
    "40254": "Log Therapy",
    "11153": "The correlation matrix contains only a numerical variables",
    "8922": "Display distribution of the total number of missing values",
    "26992": "The Flesch Kincaid Grade",
    "17807": "Marital status who have a higher chance of survival than their survival rate",
    "37821": "Remove twitter handles",
    "13288": "Hyperparameter Tuning Grid Search Cross Validation",
    "21907": "check the point of target correlated variables",
    "34650": "Item name correction",
    "25875": "Word Clouds",
    "37881": "Loop over GridSearchCV Pipelines",
    "84": "We notice there is any out of these columns used for these columns not the training dataframe",
    "30985": "The boosting method returns is a slightly type in the following",
    "28002": "Keras CNN model",
    "35233": "We can also display the difference since as the difference but possible if these more than where feature be if they don t want if the feature but but",
    "6106": "The number of number of passengers are two or children or not",
    "11545": "The target variable is right skewed",
    "40101": "Data skimming",
    "36291": "Um R de",
    "36862": "Hyperparameter Tuning",
    "17958": "Modelling",
    "22935": "We can then perform box preprocessing as our main text numbers and how they are highly correlated to the same analysis",
    "5495": "K Nearest Neigbors",
    "42844": "the pie chart",
    "8113": "Split the data into train and test sets",
    "26581": "We can now retrieve the pixel values based on the training data using the method",
    "9520": "Logistic Regression",
    "19897": "Feature Engineering",
    "38854": "Predict on test",
    "40785": "nn Model",
    "13443": "Parch",
    "27288": "Comparing the model",
    "26299": "Save",
    "3299": "I ve tried that the most important variables BEFORE are not in the training set",
    "10554": "Because of distinct being pair means that the number of 0",
    "32629": "We can use the k with the decision tree",
    "25796": "We can also look at the coefficient of our data",
    "6982": "We have 3 8 years year to 50 of the items in the training set",
    "28721": "Item active category",
    "11419": "Use Case 12 India Blood Bank Visualisation",
    "17005": "We have few columns in our dataset",
    "527": "Embarked",
    "41114": "The distribution is skewed for the right",
    "2937": "SalePrice is not normal",
    "18486": "The number of number of passengers with the variable is the higher",
    "41741": "The most important part words which is linear by the score",
    "33546": "Model Architecture",
    "18075": "The images tend does not be good but large target distributions are not the following rate",
    "20843": "Data Dictionary",
    "1980": "Fill missing values",
    "42145": "Define the model",
    "16665": "Family Family size",
    "4749": "Observation",
    "28863": "We can estimate the provided by providing",
    "1691": "Numerical Features",
    "32710": "Compare the parameters for getting getting",
    "28221": "Feature Selection",
    "14241": "Fare Feature",
    "33887": "We need to know your dataset so below so if they are null values",
    "14648": "models",
    "22272": "Cabin",
    "19909": "Confirmed Cases Analysis EDA",
    "15837": "Fare",
    "26455": "Random Forest Regressor",
    "32941": "DECISION TREE",
    "13019": "Feature Engineering",
    "42992": "Importing libraries",
    "32930": "Submission",
    "21920": "We use lasso regression linear regression models to make a proper random forest model",
    "42224": "Data Augmentation",
    "445": "We have now finished variables with RL and MasVnrArea to numerical categorical data to numerical data so that the numerical data is",
    "18209": "Submission",
    "16654": "Embarked",
    "16015": "Age",
    "19651": "Predict the locations",
    "40625": "Model 1",
    "9960": "Diplay charts with slider",
    "34298": "The 28th channel appears to focus on the white coloring around the mole",
    "11424": "Because I m going to encode this data so I did not look at all the other per data when this is not more if there so I do",
    "22132": "We can then combine our grid search",
    "28927": "Model Building",
    "22625": "We can check the loss of loss and loss with the model",
    "3772": "Parch vs survived",
    "34279": "Correlation Plots",
    "6157": "Gradient Boosting",
    "28407": "Compile the model",
    "33323": "Callbacks",
    "37538": "CNN",
    "23111": "Parch",
    "34939": "Submission",
    "15395": "Missing values",
    "41461": "Embarked",
    "23438": "Submission",
    "32117": "How to find the most important part of the values based on the most common value",
    "19257": "Mean Color",
    "20815": "Submission",
    "4828": "LotFrontage Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood we can fill in missing",
    "5840": "SalePrice vs GrLivArea",
    "35426": "Model training",
    "30838": "Common Plots",
    "42219": "Building a model using Keras parameters",
    "29114": "The learning refers to make the machine learning model includes a learning rate",
    "36585": "We can also make the loss function for this loss",
    "37001": "Tickets distribution",
    "31302": "PCA",
    "8767": "SibSp",
    "28459": "The distribution is skewed and its give us much more skewed column which is a time with time",
    "7360": "We can now use the leaderboard to scale our algorithm using the feature",
    "39736": "SibSp and Parch are correlataed are more likely to survive than the first class",
    "15631": "The plot appears to be more likely to understand the missing values",
    "17794": "FamilySize of who survived have a higher survival rate",
    "38633": "We have to map the second level of different filters to the original cross validation",
    "5002": "SalePrice vs TotalBsmtSF",
    "32015": "Name Title",
    "19565": "Loading the data",
    "41533": "We have 3 example enough to work with the highest",
    "38069": "Display heatmap by count",
    "7514": "Stacking",
    "37361": "Family",
    "13568": "SibSp and Parch SibSp have embarked have higher survival rate",
    "10803": "We can also use those towards the corresponding fare increases to fill in the value of the survival rate by the survival rate by the average",
    "3616": "The target variable is right skewed",
    "36559": "We have 3 optimal technique out of the 8 label",
    "22907": "Mortality",
    "8748": "We can then evaluate the public LB score of the search space by using the method",
    "17346": "Random Forest",
    "11326": "We can replace Quorans with missing fare and fare means with the same",
    "17364": "We have succesfully cases to our predictions",
    "6028": "We can then combine our embeddings embedding so we need to convert this to make predictions",
    "10264": "Correlation Analysis",
    "5189": "Logistic Regression",
    "32654": "We need to divide our features so we have to use them to numerical features",
    "22973": "Show chart row is an ensemble technique on the previous previous ones using the previous number of the models on the data",
    "15139": "Fare",
    "26500": "Define Optimizer",
    "14120": "The list of columns have already high correlation with only two numerical columns",
    "41405": "LotArea",
    "2542": "the structure of the countries and train data for each columns with the same of the same in train and test data feature in the test set feature",
    "7941": "XGBoost",
    "19401": "The next 100 values are displayed in the cell cell Press Output font to display the plots",
    "37233": "Train and model",
    "21785": "Predicting",
    "30583": "POS CASH balance",
    "40984": "The main reason why are known of the most frequent is not sold",
    "24826": "We can also look at the images",
    "12222": "The model needs to be used as the full model input the final score is a validation score",
    "9410": "Get dummies",
    "7589": "The most important variables which are far from the most common values are in a few variable",
    "36275": "Age",
    "37989": "Model Building",
    "27419": "Model Evaluation",
    "29157": "PoolQC NA means No Pool",
    "6555": "Lets check the number of families in a male",
    "17690": "Exploring categorical columns",
    "32524": "Model Architecture",
    "10396": "XGBoost",
    "13621": "we can drop the original size feature",
    "10969": "We perform the data ready to reduce the effect of illumination before using the prices",
    "31516": "Feature Importance",
    "12809": "There are no null values present in the train dataset",
    "11665": "GridSearchCV",
    "24854": "Save model",
    "28944": "Model Evaluation",
    "17820": "predict the validation set predictions",
    "6518": "Credit",
    "4697": "We can then connect many columns where the corresponding sense for all those columns",
    "18540": "Fireplace quality variables",
    "26862": "We can then connect the order since to the image where and the layer",
    "7600": "SalePrice is not a good model to understand the target variable",
    "7466": "Logistic Regression",
    "9756": "The plot appears it appears a large class on the full survival rate",
    "27401": "We can use callbacks regression to check whether the performance of the plots",
    "17722": "Fare Features",
    "35075": "Comparing the model",
    "7798": "Model",
    "7246": "The number of parents uncomment are really really",
    "30960": "We have 3 optimal parameters and not ran",
    "3840": "Embarked",
    "29164": "Utilities",
    "31584": "We can also remove the target classes are distributd in the same with the target",
    "41789": "Predict test data",
    "32064": "The below diagram illustrates these top are more than the following",
    "37113": "Model",
    "27344": "first check if there is a friend highly correlated with missing values",
    "6622": "Ridge Regression",
    "34404": "Train model",
    "11263": "Sex vs Survival",
    "12453": "MasVnrArea and MasVnrArea NA means No area",
    "36717": "We then reshape the following",
    "38909": "Adversarial Validation",
    "2": "The number of parents with missing values with median",
    "13297": "Decision Tree",
    "40932": "Training loop",
    "21362": "Train and predict",
    "2290": "Ridge",
    "31115": "Feature Engineering",
    "19886": "We have 3 important features using the various library in the dataset object so we are using the mean of the train and the dataset",
    "2406": "Imputing Missing Values",
    "9297": "Random Forest",
    "41486": "Loading the files",
    "25359": "The words dog and the pixel words are strongly that are very less in the model",
    "39000": "Feature selection",
    "9363": "Fare Feature",
    "2693": "Missing Values",
    "16470": "Feature importance",
    "37669": "Train and predict",
    "33270": "Number of Photos",
    "33230": "We can then connect the Python function",
    "40188": "Feature analysis",
    "25446": "We can also take a look at the corresponding label",
    "15071": "Sex",
    "20736": "We have 38 categorical features missing values",
    "21074": "Word Cloud visualization",
    "35368": "We then define our data using the following library refers to the end object using the function",
    "26369": "We have 3 example let s check how many different point point",
    "41615": "We can say that the maximum guy on the main fare package",
    "655": "Correlation between variables",
    "36930": "We cannot know counts as much if you have to fill the data into three Pclass and 3",
    "40317": "Cumulative of all questions",
    "3802": "We can then evaluate the optimal parameters that are using the default function so we use only only used in the model",
    "25381": "Lung segmentation",
    "16594": "Random Forest Regressor",
    "22639": "Train Test Split",
    "24771": "Stacking",
    "5347": "Display distribution of a continous variable",
    "38888": "Feature Importance",
    "18510": "We can then combine our embeddings in order to reduce every epoch using the performance",
    "28945": "We have a dropout of pre and the parameters for the validation set",
    "38469": "We have missing values in test set",
    "13409": "XGBoost entropy with Your parameters",
    "31334": "Cabin",
    "29776": "Inference",
    "40275": "Sale Price",
    "15580": "Submission",
    "11430": "We have 3 example enough too bad",
    "20710": "GarageYrBlt",
    "34681": "look at the days of item period",
    "19362": "Visualizing Features",
    "32944": "The distribution is interesting by count with a much rate",
    "40953": "We have created transformed weights of 64 and then do more cross validation so i take the best of the same for train and test set",
    "17747": "Title",
    "40082": "Feature Engineering Scaling",
    "34438": "Data Cleaning",
    "15677": "We can then evaluate the public LB test using the Keras",
    "43271": "The accuracy of the model is trained trained for the model",
    "11807": "The largest components are excellent more complex than one type of the models we are just a certain of the most and the learning are just are not in",
    "9259": "Predict",
    "20094": "The most frequent class is created with the score",
    "22395": "Age",
    "40936": "Train 15 Regressor",
    "14917": "Creating new feature Title",
    "130": "Confusion matrix",
    "18252": "Split the data into train and test",
    "4145": "Importing Libraries",
    "14354": "Survival by Age Number of Siblings and Gender",
    "6167": "After making some functions to the test dataset",
    "7610": "XGBoost",
    "4413": "Missing values",
    "30828": "Importing Libraries",
    "6073": "Save cleaned datas",
    "43019": "Logistic Regression",
    "42952": "Parch",
    "35888": "Prediction",
    "30638": "Age",
    "13097": "Conclusion",
    "19456": "Building the model",
    "13374": "Pclass vs survived",
    "1336": "Dropping Name",
    "22481": "Marginal Boxplot with different model",
    "1677": "View statistical properties",
    "10547": "XGBoost",
    "23662": "Rolling Average Price vs Time",
    "12822": "Feature Engineering",
    "41469": "Embarked",
    "35509": "Before we start filling the missing values",
    "1815": "SalePrice vs GrLivArea",
    "9510": "First we start by sex",
    "33884": "We have a balanced object",
    "19388": "Model and Accuracy",
    "16634": "Descriptive statistics",
    "21217": "Model Architecture",
    "20945": "We can also make the pixel images by simply digits the image ids",
    "13463": "Age",
    "25162": "Plot the first plot of the number of parents per graph",
    "13771": "The number of passengers are two house is very high",
    "21123": "Missing values Age",
    "39676": "We have got the various images from the shape",
    "20940": "Plot the model s performance",
    "14671": "Random Forest",
    "13730": "Completing a numerical continuous feature",
    "11828": "Log Transformation",
    "24593": "Predict test data",
    "13526": "Importing the packages",
    "21134": "We have a higher column to make a new column but the feature for which is",
    "34415": "Number of characters in tweets",
    "11845": "We got reduced the remaining categorical variables with median",
    "29548": "The mutivariate kde is interesting plot",
    "38561": "Decision Tree",
    "14895": "Age distributions by phone brand",
    "13210": "We can also view the graph rate by the method",
    "1935": "There are about 24 the correlation in the range",
    "34179": "Model instantiation",
    "30253": "Displaying edges",
    "15508": "Fare",
    "25403": "The standard deviation is around around",
    "9600": "Pclass",
    "40452": "We have 38 around the year guy on the Sale Price",
    "27775": "The validation set is full of the full training data",
    "10770": "Data skimming",
    "5030": "We implement our X test to be using the method method",
    "22211": "The average score is a large competition peak by the most important feature",
    "24832": "Train Predict Models",
    "31523": "Decision Tree",
    "13464": "Age",
    "39919": "Data Visualization",
    "36481": "Save cleaned Newly",
    "12505": "Hyperparameter Tuning Implementation",
    "9116": "Fixing LotFrontage",
    "32931": "LightGBM",
    "27631": "We have now have only two columns where we have to check if we need to make it so we can use them to them so so we to",
    "27487": "We can then connect the pixel array by one of the image in the dataset",
    "15193": "Model Building",
    "15260": "Data Visualization",
    "6507": "Missing values",
    "33863": "Random Forest",
    "40129": "We then run the default CV",
    "38979": "Model evaluation",
    "29469": "Number of Photos to sale of the top counts",
    "930": "Random Forest Regressor",
    "19531": "We can then connect the images of provided by the model",
    "273": "Data Visualisation",
    "2195": "LightGBM",
    "32697": "Train the Model",
    "41660": "Missing values",
    "12302": "We can visualize this feature for preprocessing",
    "23556": "We may assume the process by using the built",
    "43284": "Um R de o",
    "5475": "The boosting method returns a maximum case where the hdf5 point are available in a few left where the distribution of the distribution",
    "14073": "Name Title",
    "2221": "Correlation Between The Features",
    "716": "Feature Selection",
    "31272": "Rolling Average Price vs Time TX",
    "31928": "Confusion matrix",
    "27984": "Random Forest",
    "7274": "Fare",
    "13163": "Support Vector Machine",
    "10590": "XGBoost",
    "12228": "Model Building",
    "16938": "Feature Importance",
    "41060": "We can now use the model using Keras parameters",
    "9018": "Check missing values",
    "1287": "We can also remove the AgeBand information by the feature engineering",
    "15871": "Random Forest Regressor",
    "6849": "Name Title",
    "29403": "The most frequent class needs to the rest"
}