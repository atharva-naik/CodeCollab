{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50cf0b06",
   "metadata": {},
   "source": [
    "# Random Forest classifiers\n",
    "\n",
    "In this section of the tutorial, we will investigate the use of Random Forest classifiers in `sklearn`. As for all models in the `sklearn` framework, Random Forests mainly rely on `fit(X, y)` and `predict(X)` methods. Once fitted, relative importance of the features can be accessed _via_ the `feature_importances_` property.\n",
    "\n",
    "More information about the use of Random Forests for Classification in `sklearn` can be found at: <http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>.\n",
    "\n",
    "To begin with, let us import libraries we need and define a function to plot a fitted classifier (this function will not be specific to Random Forests) in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc803e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_circles\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_decision(clf, X, y):\n",
    "    # Build a 2D grid and perform classification using clf on this grid\n",
    "    xx, yy = np.meshgrid(np.arange(X[:,0].min() - .5, X[:,0].max() + .5, .01),\n",
    "                         np.arange(X[:,1].min() - .5, X[:,1].max() + .5, .01))\n",
    "    zz = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, zz, alpha=.2)\n",
    "    # Plot data\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40)\n",
    "    # Set figure coordinate limits\n",
    "    plt.xlim(X[:,0].min() - .5, X[:,0].max() + .5)\n",
    "    plt.ylim(X[:,1].min() - .5, X[:,1].max() + .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3ae1a9",
   "metadata": {},
   "source": [
    "Then, we load some data and train a forest made of a single tree (`n_estimators=1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=100, random_state=0, noise=.1, factor=.6)\n",
    "clf = RandomForestClassifier(n_estimators=1)\n",
    "clf.fit(X, y)\n",
    "plot_decision(clf, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa15f921",
   "metadata": {},
   "source": [
    "Now, if we vary the number of trees in the model, things can change a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for i, n_trees in enumerate([1, 10, 100]):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    clf = RandomForestClassifier(n_estimators=n_trees)\n",
    "    clf.fit(X, y)\n",
    "    plot_decision(clf, X, y)\n",
    "    plt.title(\"%d tree(s)\" % n_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20ff3d",
   "metadata": {},
   "source": [
    "Once a model fitted, we can have a look at relative importance of the different features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22351c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b8c5a8",
   "metadata": {},
   "source": [
    "To get an idea, we can add random components and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=100, random_state=0, noise=.1, factor=.8)\n",
    "X = np.hstack((X, np.random.randn(100, 10)))\n",
    "clf = RandomForestClassifier(n_estimators=1)\n",
    "clf.fit(X, y)\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8cf844",
   "metadata": {},
   "source": [
    "Surprisingly enough, first 2 dimensions do not seem to be the most informative. This is because we do not have sufficient amount of data to assess feature importance. If we add some:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
