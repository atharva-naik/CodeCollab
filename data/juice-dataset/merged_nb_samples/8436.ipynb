{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c423739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir( os.path.join('..', 'notebook_format') )\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. magic to print version\n",
    "# 2. magic so that the notebook will reload external python modules\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965374e",
   "metadata": {},
   "source": [
    "# Chi-Square Feature Selection\n",
    "\n",
    "Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested. The benefits of performing feature selection before modeling your data are:\n",
    "\n",
    "- Avoid Overfitting: Less redundant data gives performance boost to the model and results in less opportunity to make decisions based on noise\n",
    "- Reduces Training Time: Less data means that algorithms train faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a56372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose we have the following toy text data\n",
    "X = np.array(['call you tonight', 'Call me a cab', 'please call me... PLEASE!', 'he will call me'])\n",
    "y = [1, 1, 2, 0]\n",
    "\n",
    "# we'll convert it to a dense document-term matrix,\n",
    "# so we can print a more readable output\n",
    "vect = CountVectorizer()\n",
    "X_dtm = vect.fit_transform(X)\n",
    "X_dtm = X_dtm.toarray()\n",
    "pd.DataFrame(X_dtm, columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb163eea",
   "metadata": {},
   "source": [
    "One common feature selection method that is used with text data is the Chi-Square feature selection. The $\\chi^2$ test is used in statistics to test the independence of two events. More specifically in feature selection we use it to test whether the occurrence of a specific term and the occurrence of a specific class are independent. More formally, given a document $D$, we estimate the following quantity for each term and rank them by their score:\n",
    "\n",
    "$$\n",
    "\\chi^2(D, t, c) = \\sum_{e_t \\in \\{0, 1\\}} \\sum_{e_c \\in \\{0, 1\\}} \n",
    "\\frac{ ( N_{e_te_c} - E_{e_te_c} )^2 }{ E_{e_te_c} }$$\n",
    "\n",
    "Where\n",
    "\n",
    "- $N$ is the observed frequency in and $E$ the expected frequency\n",
    "- $e_t$ takes the value 1 if the document contains term $t$ and 0 otherwise\n",
    "- $e_c$ takes the value 1 if the document is in class $c$ and 0 otherwise\n",
    "\n",
    "For each feature (term), a corresponding high $\\chi^2$ score indicates that the null hypothesis $H_0$ of independence (meaning the document class has no influence over the term's frequency) should be rejected and the occurrence of the term and class are dependent. In this case, we should select the feature for the text classification.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We first compute the observed count for each class. This is done by building a contingency table from an input $X$ (feature values) and $y$ (class labels). Each entry $i$, $j$ corresponds to some feature $i$ and some class $j$, and holds the sum of the $i_{th}$ feature's values across all samples belonging to the class $j$.\n",
    "\n",
    "Note that although the feature values here are represented as frequencies, this method also works quite well in practice when the values are tf-idf values, since those are just weighted/scaled frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b5a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize the output column,\n",
    "# this makes computing the observed value a \n",
    "# simple dot product\n",
    "y_binarized = LabelBinarizer().fit_transform(y)\n",
    "print(y_binarized)\n",
    "print()\n",
    "\n",
    "# our observed count for each class (the row)\n",
    "# and each feature (the column)\n",
    "observed = np.dot(y_binarized.T, X_dtm)\n",
    "print(observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1aaef",
   "metadata": {},
   "source": [
    "e.g. the second row of the observed array refers to the total count of the terms that belongs to class 1. Then we compute the expected frequencies of each term for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e506e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the probability of each class and the feature count; \n",
    "# keep both as a 2 dimension array using reshape\n",
    "class_prob = y_binarized.mean(axis = 0).reshape(1, -1)\n",
    "feature_count = X_dtm.sum(axis = 0).reshape(1, -1)\n",
    "expected = np.dot(class_prob.T, feature_count)\n",
    "print(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40077448",
   "metadata": {},
   "outputs": [],
   "source": [
    "chisq = (observed - expected) ** 2 / expected\n",
    "chisq_score = chisq.sum(axis = 0)\n",
    "print(chisq_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d549c",
   "metadata": {},
   "source": [
    "We can confirm our result with the scikit-learn library using the `chi2` function. The following code chunk  computes chi-square value for each feature. For the returned tuple, the first element is the chi-square scores, the scores are better if greater. The second element is the p-values, they are better if smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f85aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2score = chi2(X_dtm, y)\n",
    "chi2score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56bbbf7",
   "metadata": {},
   "source": [
    "The scikit-learn library provides the `SelectKBest` class that can be used with a suite of different statistical tests. It will rank the features with the statistical test that we've specified and select the top k performing ones (meaning that these terms is considered to be more relevant to the task at hand than the others), where k is also a number that we can specify."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
