{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c55c825",
   "metadata": {},
   "source": [
    "# Finding Enron persons of interest from email and financial data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a444e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library imports\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "# package imports\n",
    "from ggplot import *\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# project imports\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef06878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load date and transform to dataframe \n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# As soon as you print the data frame you see the'Total' \n",
    "# row is present from the spreadsheet and needs removed\n",
    "data_dict.pop('TOTAL',0)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict, orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d505d3",
   "metadata": {},
   "source": [
    "## Understanding the Dataset and Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"In our dataset we have\"\n",
    "print 'Observations(Enron employees): %s' % df.shape[0]\n",
    "print 'Features: %s' % df.shape[1]\n",
    "poi_count = sum(df['poi'])\n",
    "poi_percent = round(poi_count/float(df.shape[0]),4) * 100\n",
    "print \"With %s of them (%s%%) being persons of interest\" % (poi_count, poi_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bdeab",
   "metadata": {},
   "source": [
    "When we preview our dataset, it looks like there will be a lot of data missing throughout it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2decad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5cb6a4",
   "metadata": {},
   "source": [
    " Let's get a handle on how many missing observations there are for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed924d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_missing(df, cutoff_percent = 0.):\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            missing_percent = sum(df[col] == 'NaN')/float(df.shape[0])\n",
    "            if missing_percent >= cutoff_percent:\n",
    "                print col, round(missing_percent, 2)\n",
    "        except:\n",
    "            # This is just poi bool\n",
    "            pass\n",
    "\n",
    "percent_missing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0dd79",
   "metadata": {},
   "source": [
    "So it looks like we are missing some values in every field we have (besides poi which is not listed here) but we kind of expected as much with messy data. Let's focus on the most worysome ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd23ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing(df, .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e5dce",
   "metadata": {},
   "source": [
    "With these values missing in over half of observations, we will want to be especially cautious about how we use them. Althrough since we are hunting for very specific people that only make up 12% of the observations, it is possible some of them could be very useful.\n",
    "\n",
    "With loan_advances only appearing in 2% I'll be removing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22bbfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('loan_advances',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19834ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a version of the dataframe with missing values filled in\n",
    "df_filled = df.replace('NaN',0)\n",
    "df_filled['email_address'] = df_filled['email_address'].replace(0, 'None')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f99bb",
   "metadata": {},
   "source": [
    "Outliers are going to be tricky with this data set. I wanted to find the most agredious outliers but with so much missing data as 0s and the insanely high financial benifits some of these people were recieving, I had to go all the way up to features that have a max value seven standard deviations above the mean to focus on half the features.\n",
    "\n",
    "It looks like most of the features with big outliers are compensation items besides normal salary and bonuses. This kind of makes sense though with there be hevily compensated execs and normal employees so we don't know that any of these are actually problematic and will have to look into them more individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13081ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting to not use scientific notation since that made this harder to read\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "df_description = df_filled.describe()\n",
    "\n",
    "largest_outliers = [x for x in df_description if \n",
    "                    df_description[x]['max'] > df_description[x]['mean'] + df_description[x]['std'] * 7]\n",
    "\n",
    "df_description[largest_outliers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_description:\n",
    "    print ggplot(aes('poi', col), data = df_filled) +\\\n",
    "            geom_boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9381ad5",
   "metadata": {},
   "source": [
    "I looked through the box-plots of the features with the largest outliers and made a list of those where the outlier seems especially far away which might be problematic. It turns out though that most of them are Kenneth Lay whom we know made out like a bandit from the fraud so for now I will be leaving all the outliers alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggressive_outliers = ['total_payments', 'restricted_stock', \n",
    "                        'shared_receipt_with_poi', 'restricted_stock_deferred',\n",
    "                        'other']\n",
    "outlier_list = []\n",
    "for outlier in aggressive_outliers:\n",
    "    outlier_row = df_filled[df_filled[outlier] == max(df_filled[outlier])]\n",
    "    outlier_list.append({'feature' : outlier, 'person' : outlier_row.index.values[0], 'poi': outlier_row['poi'][0]})\n",
    "\n",
    "pd.DataFrame(outlier_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d7fbd",
   "metadata": {},
   "source": [
    "## Optimize Feature Selection/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d16e62",
   "metadata": {},
   "source": [
    "People that recieve a director fee seem to be different from regular company employees, they don't draw a salary or bonuses and instead are compensated in stocks and director fees. I would like to see if using director as a boolian is more informative that specific values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89132394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['director'] = df['director_fees'] != 'NaN'\n",
    "df[df['director_fees'] != 'NaN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5bd95",
   "metadata": {},
   "source": [
    "I am also making features of what percent of emails were to/from pois instead of the straight counts that we have in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_percents(poi_emails, all_emails):\n",
    "    percent_emails_to_poi = []\n",
    "    for from_poi, all_from in zip(df[poi_emails], df[all_emails]):\n",
    "        try:\n",
    "            perc_to_poi = float(from_poi)/all_from\n",
    "            percent_emails_to_poi.append(round(perc_to_poi,4))\n",
    "        except:\n",
    "            percent_emails_to_poi.append('NaN')\n",
    "    return percent_emails_to_poi\n",
    "\n",
    "df['percent_from_emails_to_poi'] = email_percents('from_this_person_to_poi', 'from_messages')\n",
    "df['percent_emails_from_poi'] = email_percents('from_poi_to_this_person', 'to_messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8430a4",
   "metadata": {},
   "source": [
    "From the plots below, it looks like the percents will be a more helpful parameter since it at least gives us some decent sized areas with only non pois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5fb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(aes(x = 'from_this_person_to_poi', y = 'from_poi_to_this_person',\n",
    "           color = 'poi' ), data = df ) +\\\n",
    "    geom_point() +\\\n",
    "    labs(title = \"Count of emails to/from poi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e6be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(aes(x = 'percent_from_emails_to_poi', y = 'percent_emails_from_poi',\n",
    "           color = 'poi' ), data = df ) +\\\n",
    "    geom_point()+\\\n",
    "    labs(title = \"Percent of emails to/from poi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958c066",
   "metadata": {},
   "source": [
    "I was initially concerned about the person who sent 100% of their emails to a poi, but Gene Humphrey seems to be accurate, so we won't toss it as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099fc185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop one of the featrures from all sets with over .75 correlation\n",
    "features_less_corr_train = features_train.drop(['director_fees','to_messages',\n",
    "                                                'total_payments', 'total_stock_value'], axis =1)\n",
    "features_less_corr_test = features_test.drop(['director_fees','to_messages',\n",
    "                                                'total_payments', 'total_stock_value'], axis =1)\n",
    "\n",
    "# trean and set top import features for the less correlated dataframe\n",
    "dt_lc_clf = DecisionTreeClassifier(random_state=1809)\n",
    "dt_lc_clf.fit(features_less_corr_train, labels_train)\n",
    "importance_lc_df = pd.DataFrame({'features':features_less_corr_train.columns,\n",
    "                                 'importance':dt_lc_clf.feature_importances_})\n",
    "dt_lc_important_features = importance_lc_df.sort_values('importance', ascending= False)['features'][0:6]\n",
    "importance_lc_df.sort_values('importance', ascending= False)\n",
    "\n",
    "# Scale the less correlated features\n",
    "scaler = StandardScaler().fit(features_less_corr_train)\n",
    "scaled_less_corr_features_train = scaler.transform(features_less_corr_train)\n",
    "scaled_less_corr_features_test = scaler.transform(features_less_corr_test)\n",
    "\n",
    "# Do logistic regression RFE on the less correlated features\n",
    "lc_model = LogisticRegression()\n",
    "lc_rfe = RFE(lc_model, 5)\n",
    "lc_rfe.fit(scaled_less_corr_features_train, labels_train)\n",
    "\n",
    "# set the features selected based on less correleated features\n",
    "lc_rfe_selected = features_less_corr_train.columns[lc_rfe.support_]\n",
    "\n",
    "importance_lc_df['lc_rfe_selected'] = lc_rfe.support_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd72704",
   "metadata": {},
   "source": [
    "Since we removed two of the features selected previously by decision trees, there are obviously a couple new ones in the most important list, interestingly thought the logistic regression rfe is the same features and I would have expected it to be the most affected by the correlation change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190c17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if RFE is the same in both feature sets\n",
    "print lc_rfe_selected == rfe_selected\n",
    "\n",
    "importance_lc_df.sort_values('importance', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make scaled featues DataFrames for consistency and feature selection below\n",
    "scaled_less_corr_features_train = pd.DataFrame(scaled_less_corr_features_train,\n",
    "                                               columns=features_less_corr_train.columns)\n",
    "scaled_less_corr_features_test = pd.DataFrame(scaled_less_corr_features_test, \n",
    "                                              columns=features_less_corr_test.columns)\n",
    "scaled_features_train = pd.DataFrame(scaled_features_train, columns=features_train.columns)\n",
    "scaled_features_test = pd.DataFrame(scaled_features_test, columns=features_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28b8de6",
   "metadata": {},
   "source": [
    "### Pick and Tune an Algorithm\n",
    "\n",
    "We have some parameters to work with now so lets see how they perform with different Algorithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28116510",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos_tried = []\n",
    "\n",
    "def add_results_to_dict(algo_name, data_used, pred, less_corr, scaled):\n",
    "    '''Add the information about what algo was tried, which data \n",
    "    and its performance to algos_tried list'''\n",
    "    algo_tried = {'algo': algo_name,\n",
    "                  'data_used': data_used,\n",
    "                  'accuracy' : accuracy_score(labels_test, pred),\n",
    "                  'precision' : precision_score(labels_test, pred),\n",
    "                  'recall': recall_score(labels_test, pred),\n",
    "                  'less_corr': less_corr,\n",
    "                  'scaled': scaled}\n",
    "    algos_tried.append(algo_tried)\n",
    "\n",
    "def test_all_datasets(classifier, algo, scaled = False):\n",
    "    '''Try an algorithm with all of the datasets that have been prepared\n",
    "    and the results to algos_tried'''\n",
    "    if scaled:\n",
    "        train_features = scaled_features_train\n",
    "        test_features = scaled_features_test\n",
    "        train_less_corr = scaled_less_corr_features_train\n",
    "        test_less_corr = scaled_less_corr_features_test\n",
    "    else:\n",
    "        train_features = features_train\n",
    "        test_features = features_test\n",
    "        train_less_corr = features_less_corr_train\n",
    "        test_less_corr = features_less_corr_test\n",
    "    classifier(train_features, features_test, algo, 'All', False, scaled) \n",
    "    classifier(train_less_corr, test_less_corr,\n",
    "               algo, 'All Less Corr', True, scaled) \n",
    "    classifier(train_features[dt_important_features], \n",
    "                 features_test[dt_important_features],\n",
    "                 algo, 'DT Important', False, scaled) \n",
    "    classifier(train_less_corr[dt_lc_important_features], \n",
    "                 test_less_corr[dt_lc_important_features],\n",
    "                 algo, 'DT Important', True, scaled)\n",
    "    classifier(train_features[rfe_selected], \n",
    "                 features_test[rfe_selected],\n",
    "                 algo, 'RFE Selected', False, scaled)\n",
    "# this is pointless since they are the same values in both datasets\n",
    "#     classifier(train_less_corr[lc_rfe_selected], \n",
    "#                  test_less_corr[lc_rfe_selected],\n",
    "#                  algo, 'RFE Selected', True, scaled)\n",
    "    classifier(features_train_pca, features_test_pca,\n",
    "                 algo, 'PCA features', False, True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1176ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_classify(train_features, test_features, algo_name, data_used, less_corr, scaled):\n",
    "    '''Train and predict passed features with Gaussian nieve bays'''\n",
    "    nb_clf = GaussianNB()\n",
    "    nb_clf.fit(train_features, labels_train)\n",
    "    nb_pred = nb_clf.predict(test_features)\n",
    "    add_results_to_dict(algo_name, data_used, nb_pred, less_corr, scaled)\n",
    "\n",
    "test_all_datasets(nb_classify, 'Naive Bayes', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classify(train_features, test_features, algo_name, data_used, less_corr, scaled):\n",
    "    '''Train and predict passed features with Support Vector Machine'''\n",
    "    svm_clf = SVC()\n",
    "    svm_clf.fit(train_features, labels_train)\n",
    "    svm_pred = svm_clf.predict(test_features)\n",
    "    add_results_to_dict(algo_name, data_used, svm_pred, less_corr, scaled)\n",
    "    \n",
    "test_all_datasets(svm_classify, 'SVM', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05c787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_classify(train_features, test_features, algo_name, data_used, less_corr, scaled):\n",
    "    dt_clf = DecisionTreeClassifier(random_state=1809)\n",
    "    dt_clf.fit(train_features, labels_train)\n",
    "    dt_pred = dt_clf.predict(test_features)\n",
    "    add_results_to_dict(algo_name, data_used, dt_pred, less_corr, scaled)\n",
    "\n",
    "test_all_datasets(dt_classify, 'Decision Tree', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8e217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_classify(train_features, test_features, algo_name, data_used, less_corr, scaled):\n",
    "    rf_clf = RandomForestClassifier()\n",
    "    rf_clf.fit(train_features, labels_train)\n",
    "    rf_pred = rf_clf.predict(test_features)\n",
    "    add_results_to_dict(algo_name, data_used, rf_pred, less_corr, scaled)\n",
    "\n",
    "test_all_datasets(rf_classify, 'Random Forest', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b28d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_classify(train_features, test_features, algo_name, data_used, less_corr, scaled):\n",
    "    ada_clf = AdaBoostClassifier()\n",
    "    ada_clf.fit(train_features, labels_train)\n",
    "    ada_pred = ada_clf.predict(test_features)\n",
    "    add_results_to_dict(algo_name, data_used, ada_pred, less_corr, scaled)\n",
    "\n",
    "test_all_datasets(ada_classify, 'AdaBoost', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6393f3c",
   "metadata": {},
   "source": [
    "After running all of the algorithms I selected there is a lot to look at! Unfortunately our SVM's best fit with all data sets was to return false for all predictions, this gave it decent accuracy since people of interest are fairly rare... but it's completely useless to us. Let's focus on the best performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos_tried_df = pd.DataFrame.from_dict(algos_tried)\n",
    "algos_tried_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e703a",
   "metadata": {},
   "source": [
    "Interestingly the AdaBoost had the same accuracy (tied for the highest), precision, and recall wit all of the data, less correlated decision tree importance data and all features decision tree importance data. I was surprised it achieved the same results with 3 different sets of data but maybe this shows how powerful of a tool Adaboost can be even with fairly small datasets. \n",
    "\n",
    "The random forest model with all data is also tied for highest accuracy, but was right 100% of the time it predicted the person to be a poi, it also made incorrect predictions more often though. This is a good contender for best algorithm if we are planning to just look closer at the people it predicts to be a poi. We may waste a lot of time researching bad leads though since it incorrectly predicts poi so often.\n",
    "\n",
    "The Naive Bayes results from decision tree important features are good accuracy with somewhat balanced precision and recall. We have a pretty small dataset so far, so we can go with one of the more computationally intense algorithms if we think it serves us better. But this could be a good pick if all the sudden we were trying looking for these traits across an entire industry or some other massive dataset.\n",
    "\n",
    "I really like the decision tree for this situation. It doesn't have quite as hight of accuracy or precision as the rest, but unlike the others we can easily understand why it is making the decisions it is making. This could be invaluable in an investigation because we could look at why people are being picked as a poi and decide if it makes sense to investigate them further. For example, we might see that Kenneth Lay's executive assistant gets tagged as a poi because of a high percent of emails to them from other pois but we would expect them to receive a lot of emails from Kenneth Lay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668eb0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['percent_from_emails_to_poi']==1][\n",
    "    ['from_this_person_to_poi', 'from_messages', 'percent_from_emails_to_poi']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef9e8e",
   "metadata": {},
   "source": [
    "Now that we have created our new features lets split the test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split poi from the features\n",
    "labels = df_filled['poi']\n",
    "# I have relized I am not doing anything interesting with email addresses and they cause type issues\n",
    "features = df_filled.drop(['poi', 'email_address'], axis=1)\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size=0.25, random_state=1809)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646bc41f",
   "metadata": {},
   "source": [
    "I am really interested in learning about feature selection and seeing what different results we get from different \n",
    "strategies. I started with feature importance from random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96840d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=1809)\n",
    "dt_clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cfa47",
   "metadata": {},
   "source": [
    "It looks like a LOT of our features are useless, but we can see that 3 features are really useful for this approach and the importance plateaus around the 5th or 6th most important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({'features':features_train.columns, 'importance':dt_clf.feature_importances_})\n",
    "dt_important_features = importance_df.sort_values('importance', ascending= False)['features'][0:6]\n",
    "importance_df.sort_values('importance', ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54358087",
   "metadata": {},
   "source": [
    "Next I want to fit a logistic regreassion and do a recusrive feature elimination. For this we need to first scale the data since we are using regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b654eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the training and testing features in a consistant way\n",
    "scaler = StandardScaler().fit(features_train)\n",
    "scaled_features_train = scaler.transform(features_train)\n",
    "scaled_features_test = scaler.transform(features_test)\n",
    "\n",
    "# Set up a logistic regression model and run RFE with\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 5)\n",
    "rfe.fit(scaled_features_train, labels_train)\n",
    "\n",
    "# The features that RFE has selected, to be used with mosdels later\n",
    "rfe_selected = features_train.columns[rfe.support_]\n",
    "\n",
    "# Checking the order is consistant for my own sanity\n",
    "if (rfe.support_ == [x in rfe_selected for x in importance_df['features'].values]).all():\n",
    "    importance_df['rfe_selected'] = rfe.support_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097102d",
   "metadata": {},
   "source": [
    "The recursive feature elimination has very little overlap with the top features from decision trees. Which is surprising to me, but likely has to do with the very different ways in which regression and decision trees use features. It will be interesting to see how this affects their performance with different algorithms."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
