{
    "750332": {
        "jupyter_code_cell": "bx = b.plot(kind='bar',figsize=(7,5))\nbx.set_ylabel(\"Project Duration\")",
        "matched_tutorial_code_inds": [
            3799,
            3857,
            6311,
            6303,
            6264
        ],
        "matched_tutorial_codes": [
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical."
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ]
        ]
    },
    "114564": {
        "jupyter_code_cell": "imr = Imputer(missing_values='NaN', strategy='mean', axis=1)\nimr = imr.fit(df)\nimputed_data = imr.transform(df.values)\nimputed_data",
        "matched_tutorial_code_inds": [
            6166,
            6765,
            5789,
            6519,
            5454
        ],
        "matched_tutorial_codes": [
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "dta = pd.concat((indprod, income, sales, emp), axis=1)\ndta.columns = ['indprod', 'income', 'sales', 'emp']\ndta.index.freq = dta.index.inferred_freq",
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Macroeconomic data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ]
        ]
    },
    "1438958": {
        "jupyter_code_cell": "os.system(\"{0} freyberg.pst\".format(ppp))",
        "matched_tutorial_code_inds": [
            4076,
            4065,
            4744,
            5292,
            4067
        ],
        "matched_tutorial_codes": [
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"ecdf\")\n",
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\")\n",
            "ani.save(filename=\"/tmp/pillow_example.gif\", writer=\"pillow\")\nani.save(filename=\"/tmp/pillow_example.apng\", writer=\"pillow\")",
            "print(res.cusum)\nfig = res.plot_cusum()",
            "sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\", bw_adjust=2)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Empirical cumulative distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Empirical cumulative distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation"
            ],
            [
                "matplotlib->Tutorials->Introductory->Animations using Matplotlib->Animation Writers->Saving Animations"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Choosing the smoothing bandwidth",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Choosing the smoothing bandwidth"
            ]
        ]
    },
    "826529": {
        "jupyter_code_cell": "flights_context_info['oridst'] = flights_context_info.city_origin + '-' + flights_context_info.city_destination\nflights_context_info.groupby('oridst').is_delayed.sum().sort_values(ascending = False).head(10)",
        "matched_tutorial_code_inds": [
            3644,
            3979,
            3779,
            5789,
            5445
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "margeff = logit_res.get_margeff()\nprint(margeff.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Logit Model"
            ]
        ]
    },
    "859230": {
        "jupyter_code_cell": "category = processCategorial(e['category'])\nids = processText(e['id'],200)\ndata = processData(e)",
        "matched_tutorial_code_inds": [
            4435,
            5586,
            3684,
            3638,
            5445
        ],
        "matched_tutorial_codes": [
            "word_list = open('/usr/share/dict/words').readlines()\n word_list = map(str.strip, word_list)",
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "margeff = logit_res.get_margeff()\nprint(margeff.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Logit Model"
            ]
        ]
    },
    "67925": {
        "jupyter_code_cell": "#Creating groups in each of the column based on importance with Decision Tree splits \n#First we'll apply this to hour variable in training set with target registered users\ndtree=DecisionTreeRegressor()\nx = train['hour']\ny = train['registered']\nx = x.reshape(-1, 1)\ndtree.fit(x,y)\n\ndot_data = StringIO()\nexport_graphviz(dtree, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())",
        "matched_tutorial_code_inds": [
            473,
            383,
            6437,
            370,
            5118
        ],
        "matched_tutorial_codes": [
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "# Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "# Load the data from Spector and Mazzeo (1980)\nIn [1]: import statsmodels.api as sm\n\nIn [2]: spector_data = sm.datasets.spector.load_pandas()\n\nIn [3]: spector_data.exog = sm.add_constant(spector_data.exog)\n\n# Logit Model\nIn [4]: logit_mod = sm.Logit(spector_data.endog, spector_data.exog)\n\nIn [5]: logit_res = logit_mod.fit()\nOptimization terminated successfully.\n         Current function value: 0.402801\n         Iterations 7\n\nIn [6]: print(logit_res.summary())\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  GRADE   No. Observations:                   32\nModel:                          Logit   Df Residuals:                       28\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.3740\nTime:                        17:12:32   Log-Likelihood:                -12.890\nconverged:                       True   LL-Null:                       -20.592\nCovariance Type:            nonrobust   LLR p-value:                  0.001502\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -13.0213      4.931     -2.641      0.008     -22.687      -3.356\nGPA            2.8261      1.263      2.238      0.025       0.351       5.301\nTUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\nPSI            2.3787      1.065      2.234      0.025       0.292       4.465\n=============================================================================="
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard"
            ],
            [
                "statsmodels->User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Examples"
            ]
        ]
    },
    "1118014": {
        "jupyter_code_cell": "print(train_df[train_df.open == 0]['sales'].sum())\nprint(train_df[train_df.open == 0]['customers'].sum())",
        "matched_tutorial_code_inds": [
            5533,
            162,
            128,
            5782,
            5452
        ],
        "matched_tutorial_codes": [
            "means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "print(delta.transform(extract_fn_name).filter(lambda fn: \"TensorIterator\" in fn))\n\n\n\n<strong>Instruction count delta (filter)</strong>",
            "print(res_f2.summary())\nprint(res_f.summary())",
            "print(anes_data.exog.head())\nprint(anes_data.endog.head())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "torch->PyTorch Recipes->Timer quick start->6. A/B testing with Callgrind"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ]
        ]
    },
    "138265": {
        "jupyter_code_cell": "# change the sheduled_day column & appt_day columns to pandas datetime objects\n\ndf.scheduled_day = pd.to_datetime(df.scheduled_day)\ndf.appt_day = pd.to_datetime(df.appt_day)\n\n# check that this worked by returning the weekday using dt from the datetime module\n\n#df.scheduled_day.dt.weekday\ndf.appt_day.dt.weekday[0:5]",
        "matched_tutorial_code_inds": [
            5133,
            6944,
            6936,
            6754,
            2762
        ],
        "matched_tutorial_codes": [
            "# some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)",
            "# Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.extend(updated_endog)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "# Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.append(updated_endog, refit=False)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "# fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ]
        ]
    },
    "1127152": {
        "jupyter_code_cell": "pct_change_df['2010'].std()",
        "matched_tutorial_code_inds": [
            6813,
            3734,
            2592,
            5490,
            3647
        ],
        "matched_tutorial_codes": [
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "delays.nsmallest(5).sort_values()",
            "co2_data.index.min(), co2_data.index.max()",
            "affair_mod.fittedvalues[1000]",
            "hdf.loc[pd.IndexSlice[:, ['ORD', 'DSM']], ['dep_time', 'dep_delay']]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ]
        ]
    },
    "1002849": {
        "jupyter_code_cell": "# read world_bank_projects into dataframe with only columns related to countries\nworld_bank_projects = pd.read_json('data/world_bank_projects.json')[['countrycode', 'countryshortname']]\n# Sort countries by number of projects\nworld_bank_projects = world_bank_projects.groupby('countryshortname').count().sort_values('countrycode', ascending=False)\n# Print countries by descending order (top 10 only)\nworld_bank_projects.columns = ['Total Projects']\nworld_bank_projects[:10]",
        "matched_tutorial_code_inds": [
            6684,
            5128,
            3091,
            6314,
            3679
        ],
        "matched_tutorial_codes": [
            "# Compute the impact of the news on the four periods that we previously\n# forecasted: 2008Q3 through 2009Q2\nnews = res_pre.news(res_post, start='2008Q3', end='2009Q2')\n# Note: one alternative way to specify these impact dates is\n# `start='2008Q3', periods=4`",
            "# Load in the example macroeconomic dataset\ndta = sm.datasets.macrodata.load_pandas().data\n# Make sure we have an index with an associated frequency, so that\n# we can refer to time periods with date strings or timestamps\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')\n\n# Step 1: construct an SARIMAX model for US inflation data\nmodel = sm.tsa.SARIMAX(dta.infl, order=(4, 0, 0), trend='c')\n\n# Step 2: fit the model's parameters by maximum likelihood\nresults = model.fit()\n\n# Step 3: explore / use results\n\n# - Print a table summarizing estimation results\nprint(results.summary())\n\n# - Print only the estimated parameters\nprint(results.params)\n\n# - Create diagnostic figures based on standardized residuals:\n#   (1) time series graph\n#   (2) histogram\n#   (3) Q-Q plot\n#   (4) correlogram\nresults.plot_diagnostics()\n\n# - Examine diagnostic hypothesis tests\n# Jarque-Bera: [test_statistic, pvalue, skewness, kurtosis]\nprint(results.test_normality(method='jarquebera'))\n# Goldfeld-Quandt type test: [test_statistic, pvalue]\nprint(results.test_heteroskedasticity(method='breakvar'))\n# Ljung-Box test: [test_statistic, pvalue] for each lag\nprint(results.test_serial_correlation(method='ljungbox'))\n\n# - Forecast the next 4 values\nprint(results.forecast(4))\n\n# - Forecast until 2020Q4\nprint(results.forecast('2020Q4'))\n\n# - Plot in-sample dynamic prediction starting in 2005Q1\n#   and out-of-sample forecasts until 2010Q4 along with\n#   90% confidence intervals\npredict_results = results.get_prediction(start='2005Q1', end='2010Q4', dynamic=True)\npredict_df = predict_results.summary_frame(alpha=0.10)\nfig, ax = plt.subplots()\npredict_df['mean'].plot(ax=ax)\nax.fill_between(predict_df.index, predict_df['mean_ci_lower'],\n                predict_df['mean_ci_upper'], alpha=0.2)\n\n# - Simulate two years of new data after the end of the sample\nprint(results.simulate(8, anchor='end'))\n\n# - Impulse responses for two years\nprint(results.impulse_responses(8))",
            "# visualize the decision surface, projected down to the first\n# two principal components of the dataset\npca = (n_components=8).fit(data_train)\n\nX = pca.transform(data_train)\n\n# Generate grid along first two principal components\nmultiples = (-2, 2, 0.1)\n# steps along first component\nfirst = multiples[:, ] * pca.components_[0, :]\n# steps along second component\nsecond = multiples[:, ] * pca.components_[1, :]\n# combine\ngrid = first[, :, :] + second[:, , :]\nflat_grid = grid.reshape(-1, data.shape[1])\n\n# title for the plots\ntitles = [\n    \"SVC with rbf kernel\",\n    \"SVC (linear kernel)\\n with Fourier rbf feature map\\nn_components=100\",\n    \"SVC (linear kernel)\\n with Nystroem rbf feature map\\nn_components=100\",\n]\n\n(figsize=(18, 7.5))\nplt.rcParams.update({\"font.size\": 14})\n# predict and plot\nfor i, clf in enumerate((kernel_svm, nystroem_approx_svm, fourier_approx_svm)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    (1, 3, i + 1)\n    Z = clf.predict(flat_grid)\n\n    # Put the result into a color plot\n    Z = Z.reshape(grid.shape[:-1])\n    levels = (10)\n    lv_eps = 0.01  # Adjust a mapping from calculated contour levels to color.\n    (\n        multiples,\n        multiples,\n        Z,\n        levels=levels - lv_eps,\n        cmap=plt.cm.tab10,\n        vmin=0,\n        vmax=10,\n        alpha=0.7,\n    )\n    (\"off\")\n\n    # Plot also the training points\n    (\n        X[:, 0],\n        X[:, 1],\n        c=targets_train,\n        cmap=plt.cm.tab10,\n        edgecolors=(0, 0, 0),\n        vmin=0,\n        vmax=10,\n    )\n\n    (titles[i])\n()\n()\n\n\n<img alt=\"SVC with rbf kernel, SVC (linear kernel)  with Fourier rbf feature map n_components=100, SVC (linear kernel)  with Nystroem rbf feature map n_components=100\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\" srcset=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\"/>",
            "# Get the federal funds rate data\nfrom statsmodels.tsa.regime_switching.tests.test_markov_regression import fedfunds\n\ndta_fedfunds = pd.Series(\n    fedfunds, index=pd.date_range(\"1954-07-01\", \"2010-10-01\", freq=\"QS\")\n)\n\n# Plot the data\ndta_fedfunds.plot(title=\"Federal funds rate\", figsize=(12, 3))\n\n# Fit the model\n# (a switching mean is the default of the MarkovRegession model)\nmod_fedfunds = sm.tsa.MarkovRegression(dta_fedfunds, k_regimes=2)\nres_fedfunds = mod_fedfunds.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\"/>",
            "# Bring in the flights data\n\nflights = pd.read_hdf('flights.h5', 'flights')\n\nweather_locs = weather.index.levels[0]\n# The `categories` attribute of a Categorical is an Index\norigin_locs = flights.origin.cat.categories\ndest_locs = flights.dest.cat.categories\n\nairports = weather_locs &amp; origin_locs &amp; dest_locs\nairports"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 2: computing the \u201cnews\u201d from a new observation"
            ],
            [
                "statsmodels->User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Overview of usage"
            ],
            [
                "sklearn->Examples->Miscellaneous->Explicit feature map approximation for RBF kernels->Decision Surfaces of RBF Kernel SVM and Linear SVM"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept"
            ],
            [
                "pandas_toms_blog->Indexes->Set Operations"
            ]
        ]
    },
    "345352": {
        "jupyter_code_cell": "Z = linkage(pca_ap, 'ward')\nc, coph_dists = cophenet(Z, pdist(pca_ap))\nplt.title('Dendrogram')\nplt.xlabel('Index Numbers')\nplt.ylabel('Distance')\ndendrogram(\n    Z,\n    leaf_rotation=90.,  \n    leaf_font_size=8.,)\nplt.show()",
        "matched_tutorial_code_inds": [
            3314,
            6016,
            2974,
            6902,
            5719
        ],
        "matched_tutorial_codes": [
            "X, y = (\n    n_samples=9,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_classes=3,\n    n_clusters_per_class=1,\n    class_sep=1.0,\n    random_state=0,\n)\n\n(1)\nax = ()\nfor i in range(X.shape[0]):\n    ax.text(X[i, 0], X[i, 1], str(i), va=\"center\", ha=\"center\")\n    ax.scatter(X[i, 0], X[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax.set_title(\"Original points\")\nax.axes.get_xaxis().set_visible(False)\nax.axes.get_yaxis().set_visible(False)\nax.axis(\"equal\")  # so that boundaries are displayed correctly as circles\n\n\ndef link_thickness_i(X, i):\n    diff_embedded = X[i] - X\n    dist_embedded = (\"ij,ij-i\", diff_embedded, diff_embedded)\n    dist_embedded[i] = \n\n    # compute exponentiated distances (use the log-sum-exp trick to\n    # avoid numerical instabilities\n    exp_dist_embedded = (-dist_embedded - (-dist_embedded))\n    return exp_dist_embedded\n\n\ndef relate_point(X, i, ax):\n    pt_i = X[i]\n    for j, pt_j in enumerate(X):\n        thickness = link_thickness_i(X, i)\n        if i != j:\n            line = ([pt_i[0], pt_j[0]], [pt_i[1], pt_j[1]])\n            ax.plot(*line, c=cm.Set1(y[j]), linewidth=5 * thickness[j])\n\n\ni = 3\nrelate_point(X, i, ax)\n()\n\n\n<img alt=\"Original points\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nca_illustration_001.png\" srcset=\"../../_images/sphx_glr_plot_nca_illustration_001.png\"/>",
            "infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration->Original points"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ]
        ]
    },
    "213412": {
        "jupyter_code_cell": "df_profitability = df_profitability.append(df_failed_midwifing_txns, sort=True)\nlen(df_profitability)",
        "matched_tutorial_code_inds": [
            5789,
            5948,
            5401,
            5504,
            5289
        ],
        "matched_tutorial_codes": [
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "data = sm.datasets.stackloss.load()\ndata.exog = sm.add_constant(data.exog)",
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "dta = sm.datasets.star98.load_pandas().data\nprint(dta.columns)",
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Estimation"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ]
        ]
    },
    "678314": {
        "jupyter_code_cell": "# convert to a pandas object and then to a dataframe\ndf = ds[variable].to_pandas().to_frame(name=variable)\n\n# get a summary of the data including the percentiles listed\ndf.describe(percentiles=[.1,.25,.5,.75,.9])",
        "matched_tutorial_code_inds": [
            6938,
            6703,
            6949,
            6953,
            6662
        ],
        "matched_tutorial_codes": [
            "# Construct the forecast errors\nforecast_errors = forecasts.apply(lambda column: endog - column).reindex(forecasts.index)\n\nprint(forecast_errors.iloc[:5, :5])",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# Annual frequency, using a PeriodIndex\nindex = pd.period_range(start='2000', periods=4, freq='A')\nendog1 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog1.index)",
            "# Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "# Construct the model instance\nmod_uc = sm.tsa.UnobservedComponents(inf, \"rwalk\", autoregressive=1)\n\n# Fit the model via maximum likelihood\nres_uc_mle = mod_uc.fit()\nprint(res_uc_mle.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example",
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ]
        ]
    },
    "1061813": {
        "jupyter_code_cell": "plt.bar([0,1,2,3], [10,11,12,13], yerr=[1,2,1,3])",
        "matched_tutorial_code_inds": [
            4692,
            4684,
            4679,
            4715,
            5034
        ],
        "matched_tutorial_codes": [
            "plt.title(r'$\\sigma_i=15$')",
            "plt.plot(x, y, linewidth=2.0)",
            "plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_002.png\" srcset=\"../../_images/sphx_glr_pyplot_002.png, ../../_images/sphx_glr_pyplot_002_2_0x.png 2.0x\"/>",
            "plt.style.use('fivethirtyeight')",
            "plt.savefig('figure.pdf', backend='pgf')"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with text->Using mathematical expressions in text"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Controlling line properties"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Controlling the style"
            ],
            [
                "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend"
            ]
        ]
    },
    "132599": {
        "jupyter_code_cell": "out_sample = customers.xs(2015,level='year').copy()\nout_sample['prob_predicted'] = pd.Series(model_prob.predict(out_sample),index=out_sample.index)\nout_sample.drop(['revenue','next_revenue','next_is_active'],axis=1,inplace=True)\nout_sample.head()",
        "matched_tutorial_code_inds": [
            6022,
            4682,
            2466,
            2817,
            2314
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "res = []\nfor subset_label, X, df in [\n    (\"train\", X_train, df_train),\n    (\"test\", X_test, df_test),\n]:\n    exposure = df[\"Exposure\"].values\n    res.append(\n        {\n            \"subset\": subset_label,\n            \"observed\": df[\"ClaimAmount\"].values.sum(),\n            \"predicted, frequency*severity model\": (\n                exposure * glm_freq.predict(X) * glm_sev.predict(X)\n            ),\n            \"predicted, tweedie, power=%.2f\"\n            % glm_pure_premium.power: (exposure * glm_pure_premium.predict(X)),\n        }\n    )\n\nprint((res).set_index(\"subset\").T)",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Pure Premium Modeling via a Product Model vs single TweedieRegressor"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ]
        ]
    },
    "1005762": {
        "jupyter_code_cell": "formattedRows = [[dt.datetime.fromtimestamp(int(list[0])), list[1], list[2], float(list[3])]  for list in rows]\ndf = pd.DataFrame(formattedRows, columns=['datetime','highway','direction','simpleMPH'])",
        "matched_tutorial_code_inds": [
            5716,
            3859,
            3979,
            4462,
            5990
        ],
        "matched_tutorial_codes": [
            "pd.DataFrame(\n    np.column_stack([[r.llf, r.deviance, r.pearson_chi2] for r in results_all]),\n    columns=names,\n    index=[\"llf\", \"deviance\", \"pearson chi2\"],\n)",
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "from scipy.spatial import Voronoi\n vor = Voronoi(points)\n vor.vertices\narray([[0.5, 0.5],\n       [0.5, 1.5],\n       [1.5, 0.5],\n       [1.5, 1.5]])",
            "_ = plt.boxplot([scales[0][0], scales[0][1], scales[1][0], scales[1][1]])\nplt.ylabel(\"Estimated scale\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ]
        ]
    },
    "1055213": {
        "jupyter_code_cell": "# -*- coding: utf-8 -*-\n# Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import Imputer",
        "matched_tutorial_code_inds": [
            5357,
            6376,
            6781,
            6808,
            7
        ],
        "matched_tutorial_codes": [
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom pandas_datareader.data import DataReader",
            "%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.multivariate.pca import PCA\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Linear Mixed-Effects"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: Chandrasekhar recursions"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "torch->PyTorch Recipes->Loading data in PyTorch->5. [Optional] Visualize the data"
            ]
        ]
    },
    "312353": {
        "jupyter_code_cell": "sns.countplot(y='Tenure', data=mcq)",
        "matched_tutorial_code_inds": [
            4055,
            3963,
            3966,
            4071,
            4056
        ],
        "matched_tutorial_codes": [
            "sns.displot(tips, x=\"size\", discrete=True)\n",
            "sns.relplot(data=flights_wide, kind=\"line\")\n",
            "sns.catplot(data=flights_wide, kind=\"box\")\n",
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n",
            "sns.displot(tips, x=\"day\", shrink=.8)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ]
        ]
    },
    "1391789": {
        "jupyter_code_cell": "genreList = ['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', \\\n 'Documentary', 'Drama', 'Family', 'Fantasy', 'History', 'Horror', \\\n 'Musical', 'Mystery' ,'Romance', 'Sci-Fi', 'Sport', 'Thriller', 'War', 'Western']",
        "matched_tutorial_code_inds": [
            6015,
            6558,
            3857,
            403,
            175
        ],
        "matched_tutorial_codes": [
            "['Intercept',\n 'C(E)[T.2]',\n 'C(E)[T.3]',\n 'C(M)[T.1]',\n 'C(E)[T.2]:C(M)[T.1]',\n 'C(E)[T.3]:C(M)[T.1]',\n 'X']",
            "# Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "# Download training data from open datasets.\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=(),\n)\n\n# Download test data from open datasets.\n = (\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=(),\n)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Working with data"
            ]
        ]
    },
    "17393": {
        "jupyter_code_cell": "rd = random_layout(     cnodes, cedges)\nfd = forceatlas2_layout(cnodes, cedges)\n\n\ntf.Images(rd_d,fd_d,rd_b,fd_b).cols(2)",
        "matched_tutorial_code_inds": [
            2611,
            4128,
            5470,
            4031,
            1673
        ],
        "matched_tutorial_codes": [
            "X = (0, 5, num=30).reshape(-1, 1)\ny = target_generator(X, add_noise=False)",
            "x = np.random.normal(0, 1, 50)\ny = x * 2 + np.random.normal(0, 2, size=x.size)\nsns.regplot(x=x, y=y)\n",
            "dta[\"affair\"] = (dta[\"affairs\"]  0).astype(float)\nprint(dta.head(10))",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "x, y = np.meshgrid(np.linspace(-1, 1, 400), np.linspace(-1, 1, 400))\nsmall_mesh = x + (1j * y)\n\nx, y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nmesh = x + (1j * y)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Data generation"
            ],
            [
                "seaborn->User guide and tutorial->Statistical estimation and error bars->Error bars on regression fits",
                "seaborn->Statistical operations->Statistical estimation and error bars->Error bars on regression fits"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Julia set"
            ]
        ]
    },
    "414066": {
        "jupyter_code_cell": "keep_vars2=['PRCNT_PV_CUST',\n#'TTL_IBO_DOWN',\n'DEPTH_WIDTH',\n'PRNCT_ENGAGED_FRONTLN',\n'PRNCT_RPT_BUYER',\n'NUMBER_LEGS_BONUS',      \n'PERS_TO_GRP_PV_RATIO',\n'BNS_DIF_TO_PERS',         \n'BNS_DIF_USD_AMT',\n'CNTRY_KEY_NO',        \n'TARGET_RATE_DIA',\n'MONTH_SILVER'  ,     \n'PERS_GRP_PV_RATIO_IBO_LEVEL_1',\n\n'PRIOR_SILVER_6',\n'PRIOR_BONUSAMT_12_CV',\n'PRCNT_PV_FROM_RPT_ABO',\n'PRCNT_PV_LEVEL_1',\n'LEGS_12_PLUS_BNS',\n'IMC_MONTHS_AFTER_SIGNUP',\n'PRIOR_PERS_GRP_PV_RT_6_CV',\n'RATIO_RPT_BUYERS_6MONTH',\n\n'RATIO_LEGS_6MONTH',\n'RATIO_GROUPPV_6MONTH',        \n'IMC_GROUP_PV',\n          \n'PRCNT_PVIBO_MB3',\n'IMC_QUAL_LEGS_QTY',\n'BNS_USD_AMT_IBO_LEVEL_4', \n'IMC_KEY_NO',\n'MO_YR_KEY_NO',\n\n    'STILL_TIME',\n    'RATIO_APPS_3_14',\n            \n       #'PROP_BL_OTHER_ORD_CNT_3', \n       #     'PROP_BL_DURABLES_ORD_CNT_3',\n       #'PROP_BL_HOMECARE_ORD_CNT_3', \n       'PROP_LAST_3DAY_PV_LC_AMT_3', \n        'PROP_RTURN_PV_LC_AMT_3',\n       'AVG_DMD_ORD_CNT_3',\n       #     'PROP_RTURN_PV_LC_AMT_0',\n            'APP_1_AGE',\n            'APP_2_POP', \n            'LPY_FLAGS', \n            'CONTRIBUTORS'\n           ]             ",
        "matched_tutorial_code_inds": [
            6137,
            3504,
            190,
            965,
            6598
        ],
        "matched_tutorial_codes": [
            "res_mb_hk = metabin(e2i, nei, c2i, nci, data=dat2, sm=\"OR\", Q.Cochrane=FALSE, method=\"MH\", method.tau=\"DL\", hakn=FALSE, backtransf=FALSE)\n res_mb_hk\n     logOR            95%-CI %W(fixed) %W(random)\n1   2.7081 [ 0.5265; 4.8896]       0.3        0.7\n2   1.2567 [ 0.2658; 2.2476]       2.1        3.2\n3   0.3749 [-0.3911; 1.1410]       5.4        5.4\n4   1.6582 [ 0.3245; 2.9920]       0.9        1.8\n5   0.7850 [-0.0673; 1.6372]       3.5        4.4\n6   0.3617 [-0.1528; 0.8762]      12.1       11.8\n7   0.5754 [-0.3861; 1.5368]       3.0        3.4\n8   0.2505 [-0.4881; 0.9892]       6.1        5.8\n9   0.6506 [-0.3877; 1.6889]       2.5        3.0\n10  0.0918 [-0.8067; 0.9903]       4.5        3.9\n11  0.2739 [-0.1047; 0.6525]      23.1       21.4\n12  0.4858 [ 0.0804; 0.8911]      18.6       18.8\n13  0.1823 [-0.6830; 1.0476]       4.6        4.2\n14  0.9808 [-0.4178; 2.3795]       1.3        1.6\n15  1.3122 [-1.0055; 3.6299]       0.4        0.6\n16 -0.2595 [-1.4450; 0.9260]       3.1        2.3\n17  0.1384 [-0.5076; 0.7844]       8.5        7.6\n\nNumber of studies combined: k = 17\n\n                      logOR           95%-CI    z  p-value\nFixed effect model   0.4428 [0.2678; 0.6178] 4.96 &lt; 0.0001\nRandom effects model 0.4295 [0.2504; 0.6086] 4.70 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0017 [0.0000; 0.4589]; tau = 0.0410 [0.0000; 0.6774];\n I^2 = 1.1% [0.0%; 51.6%]; H = 1.01 [1.00; 1.44]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 16.18   16  0.4404\n\nDetails on meta-analytical method:\n- Mantel-Haenszel method\n- DerSimonian-Laird estimator for tau^2\n- Jackson method for confidence interval of tau^2 and tau\n\n res_mb_hk$TE.fixed\n[1] 0.4428186730553189\n res_mb_hk$seTE.fixed\n[1] 0.08928560091027186\n c(res_mb_hk$lower.fixed, res_mb_hk$upper.fixed)\n[1] 0.2678221109331694 0.6178152351774684",
            "lw = 2\n\nsvrs = [svr_rbf, svr_lin, svr_poly]\nkernel_label = [\"RBF\", \"Linear\", \"Polynomial\"]\nmodel_color = [\"m\", \"c\", \"g\"]\n\nfig, axes = (nrows=1, ncols=3, figsize=(15, 10), sharey=True)\nfor ix, svr in enumerate(svrs):\n    axes[ix].plot(\n        X,\n        svr.fit(X, y).predict(X),\n        color=model_color[ix],\n        lw=lw,\n        label=\"{} model\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[svr.support_],\n        y[svr.support_],\n        facecolor=\"none\",\n        edgecolor=model_color[ix],\n        s=50,\n        label=\"{} support vectors\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[((len(X)), svr.support_)],\n        y[((len(X)), svr.support_)],\n        facecolor=\"none\",\n        edgecolor=\"k\",\n        s=50,\n        label=\"other training data\",\n    )\n    axes[ix].legend(\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, 1.1),\n        ncol=1,\n        fancybox=True,\n        shadow=True,\n    )\n\nfig.text(0.5, 0.04, \"data\", ha=\"center\", va=\"center\")\nfig.text(0.06, 0.5, \"target\", ha=\"center\", va=\"center\", rotation=\"vertical\")\nfig.suptitle(\"Support Vector Regression\", fontsize=14)\n()\n\n\n<img alt=\"Support Vector Regression\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_regression_001.png\"/>",
            "classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\n()\n, y = [0][0], [0][1]\nwith ():\n     = model()\n    predicted, actual = classes[[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')",
            "transform = (\n    [(224),\n     (),\n     ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntrain_set = (root='./data', train=True, download=True, transform=transform)\ntrain_loader = (train_set, batch_size=32, shuffle=True)",
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables"
            ],
            [
                "sklearn->Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels->Look at the results"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Loading Models"
            ],
            [
                "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps->1. Prepare the data and model"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ]
        ]
    },
    "606145": {
        "jupyter_code_cell": "by_team17.plot.scatter(x = 'Votes', y = 'Avg_Viewers', title = 'All Star Votes vs Average Team Viewership')",
        "matched_tutorial_code_inds": [
            3798,
            3962,
            3973,
            3638,
            5586
        ],
        "matched_tutorial_codes": [
            "team\nAtlanta Hawks        0.585366\nBoston Celtics       0.585366\nBrooklyn Nets        0.256098\nCharlotte Hornets    0.585366\nChicago Bulls        0.512195\ndtype: float64",
            "flights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n",
            "year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing long-form data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ]
        ]
    },
    "492406": {
        "jupyter_code_cell": "y = jobs.salarylevel.values\nX = jobs.iloc[:, 5:17]\n\nX_train, X_test, y_train, y_test = train_test_split(jobs.title, jobs.salarylevel, train_size = .5, random_state=90)\n\nmodel = make_pipeline(CountVectorizer(stop_words='english'),\n                      LogisticRegression())\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nyhat_pp = model.predict_proba(X_test)",
        "matched_tutorial_code_inds": [
            2080,
            3449,
            2955,
            5411,
            2966
        ],
        "matched_tutorial_codes": [
            "y = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\n\nfig, axes = (1, 2, figsize=(10, 3))\n\naxes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\naxes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\naxes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\naxes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n()\n()\n\n\n<img alt=\"plot pcr vs pls\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\"/>",
            "X = digits.data[indices[:340]]\ny = digits.target[indices[:340]]\nimages = digits.images[indices[:340]]\n\nn_total_samples = len(y)\nn_labeled_points = 40\n\nindices = (n_total_samples)\n\nunlabeled_set = indices[n_labeled_points:]",
            "result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>",
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->The data"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "996843": {
        "jupyter_code_cell": "def dateformat(earliest_cr_line_date):\n    date_split = earliest_cr_line_date.split('-')\n    if int(date_split[1]) > 18:\n        date_split[1] = '19' +  date_split[1]\n    else:\n        date_split[1] = '20' +  date_split[1]\n    return '-'.join(date_split)\nloans_frame['earliest_cr_line_mod'] = loans_frame['earliest_cr_line'].apply(dateformat)",
        "matched_tutorial_code_inds": [
            357,
            2396,
            1715,
            350,
            3703
        ],
        "matched_tutorial_codes": [
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(\" \", 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(\" \", 1)[-1]\n    return \"predicted: %s\\ntrue:      %s\" % (pred_name, true_name)\n\n\nprediction_titles = [\n    title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])\n]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n\n<img alt=\"predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Blair true:      Blair, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Schroeder true:      Schroeder, predicted: Powell true:      Powell, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_002.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_002.png\"/>",
            "def t_test(x, y):\n    diff = y - x\n    var = np.var(diff, ddof=1)\n    num = np.mean(diff)\n    denom = np.sqrt(var / len(x))\n    return np.divide(num, denom)\n\nt_value = t_test(before_sample, after_sample)",
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def mode(x):\n    '''\n    Arbitrarily break ties.\n    '''\n    return x.value_counts().index[0]\n\naggfuncs = {'tmpf': 'mean', 'relh': 'mean',\n            'sped': 'mean', 'mslp': 'mean',\n            'p01i': 'mean', 'vsby': 'mean',\n            'gust_mph': 'mean', 'skyc1': mode,\n            'skyc2': mode, 'skyc3': mode}\n# TimeGrouper works on a DatetimeIndex, so we move `station` to the\n# columns and then groupby it as well.\ndaily = (weather.reset_index(level=\"station\")\n                .groupby([pd.TimeGrouper('1d'), \"station\"])\n                .agg(aggfuncs))\n\ndaily.head()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Learning PyTorch->What is torch.nn really?->Using your GPU"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Calculating the test statistics"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Wrapping DataLoader"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->Merge Version"
            ]
        ]
    },
    "967051": {
        "jupyter_code_cell": "data = data[data['3 Year Annualised Return'] != '-']",
        "matched_tutorial_code_inds": [
            5681,
            6911,
            5712,
            1495,
            2822
        ],
        "matched_tutorial_codes": [
            "data[:3]",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "1124099": {
        "jupyter_code_cell": "pd.options.display.max_rows = 200\ndf[discount_percentage>50]",
        "matched_tutorial_code_inds": [
            6834,
            4061,
            3965,
            4093,
            6813
        ],
        "matched_tutorial_codes": [
            "pd.get_dummies(hsb2.race.values, drop_first=False)",
            "sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n",
            "sns.relplot(data=flights_wide.transpose(), kind=\"line\")\n",
            "sns.pairplot(penguins)\n",
            "pca_model = PCA(dta.T, standardize=False, demean=True)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "1416954": {
        "jupyter_code_cell": "#pandas is a powerful Python data analysis toolkit\n#fast, flexible, and expressive data structures designed to make \n#working with relational or labeled data\ntrain = pd.read_csv(\"data/train.csv\")\ntrain.head()",
        "matched_tutorial_code_inds": [
            2504,
            6703,
            6670,
            4894,
            3776
        ],
        "matched_tutorial_codes": [
            "from sklearn.datasets import \n\ndiabetes = ()\nX, y = diabetes.data, diabetes.target\nprint(diabetes.DESCR)",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# Retrieve the posterior means\nparams = pm.summary(trace_uc)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_uc_bayes = mod_uc.smooth(params)",
            "# histogram our data with numpy\ndata = np.random.randn(1000)\nn, bins = np.histogram(data, 100)",
            "tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Feature Selection->Model-based and sequential feature selection->Loading the data"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "matplotlib->Tutorials->Advanced->Path Tutorial->Compound paths"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ]
        ]
    },
    "473630": {
        "jupyter_code_cell": "msno.bar(df, color=\"blue\", figsize=(30,18))",
        "matched_tutorial_code_inds": [
            4071,
            4056,
            4072,
            3963,
            4055
        ],
        "matched_tutorial_codes": [
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n",
            "sns.displot(tips, x=\"day\", shrink=.8)\n",
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "sns.relplot(data=flights_wide, kind=\"line\")\n",
            "sns.displot(tips, x=\"size\", discrete=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ]
        ]
    },
    "1425124": {
        "jupyter_code_cell": "df_oz = pd.read_csv('D:\\\\TeachingMaterials\\\\BusinessAnalytics\\\\Visualization\\\\VizData\\\\ozone.csv')\ndf_con = pd.read_csv('D:\\\\TeachingMaterials\\\\BusinessAnalytics\\\\Visualization\\\\VizData\\\\construction.csv')\ndf_test = pd.read_csv('D:\\\\TeachingMaterials\\\\BusinessAnalytics\\\\Visualization\\\\VizData\\\\test.csv')",
        "matched_tutorial_code_inds": [
            1762,
            5456,
            3730,
            6811,
            3731
        ],
        "matched_tutorial_codes": [
            "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ]
        ]
    },
    "311291": {
        "jupyter_code_cell": "dup2.drop_duplicates(subset=index)",
        "matched_tutorial_code_inds": [
            6969,
            6228,
            3720,
            6001,
            5999
        ],
        "matched_tutorial_codes": [
            "exog = sm.add_constant(exog, prepend=True)",
            "dta.plot(figsize=(12, 8))",
            "flights.dep_time.unique()",
            "lm.model.data.orig_exog[:5]",
            "lm.model.exog[:5]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "1061434": {
        "jupyter_code_cell": "###### random UPSAMPLING OF THE UPSAMPLING OF MINORITY CLASS I.E., 'SUICIDES' \n\n#!pip install --user imblearn\nsys.path.append(\"/users/bhavani/appdata/roaming/python/python36/site-packages\")\nfrom imblearn.over_sampling import SMOTE\nsmt = SMOTE()\nX_up_train, y_up_train = smt.fit_sample(X_train, y_train)",
        "matched_tutorial_code_inds": [
            4740,
            6636,
            5639,
            368,
            6437
        ],
        "matched_tutorial_codes": [
            "#### MATPLOTLIBRC FORMAT\n\n## NOTE FOR END USERS: DO NOT EDIT THIS FILE!\n##\n## This is a sample Matplotlib configuration file - you can find a copy\n## of it on your system in site-packages/matplotlib/mpl-data/matplotlibrc\n## (relative to your Python installation location).\n## DO NOT EDIT IT!\n##\n## If you wish to change your default style, copy this file to one of the\n## following locations:\n##     Unix/Linux:\n##         $HOME/.config/matplotlib/matplotlibrc OR\n##         $XDG_CONFIG_HOME/matplotlib/matplotlibrc (if $XDG_CONFIG_HOME is set)\n##     Other platforms:\n##         $HOME/.matplotlib/matplotlibrc\n## and edit that copy.\n##\n## See https://matplotlib.org/stable/tutorials/introductory/customizing.html#customizing-with-matplotlibrc-files\n## for more details on the paths which are checked for the configuration file.\n##\n## Blank lines, or lines starting with a comment symbol, are ignored, as are\n## trailing comments.  Other lines must have the format:\n##     key: val  # optional comment\n##\n## Formatting: Use PEP8-like style (as enforced in the rest of the codebase).\n## All lines start with an additional '#', so that removing all leading '#'s\n## yields a valid style file.\n##\n## Colors: for the color values below, you can either use\n##     - a Matplotlib color string, such as r, k, or b\n##     - an RGB tuple, such as (1.0, 0.5, 0.0)\n##     - a double-quoted hex string, such as \"#ff00ff\".\n##       The unquoted string ff00ff is also supported for backward\n##       compatibility, but is discouraged.\n##     - a scalar grayscale intensity such as 0.75\n##     - a legal html color name, e.g., red, blue, darkslategray\n##\n## String values may optionally be enclosed in double quotes, which allows\n## using the comment character # in the string.\n##\n## This file (and other style files) must be encoded as utf-8.\n##\n## Matplotlib configuration are currently divided into following parts:\n##     - BACKENDS\n##     - LINES\n##     - PATCHES\n##     - HATCHES\n##     - BOXPLOT\n##     - FONT\n##     - TEXT\n##     - LaTeX\n##     - AXES\n##     - DATES\n##     - TICKS\n##     - GRIDS\n##     - LEGEND\n##     - FIGURE\n##     - IMAGES\n##     - CONTOUR PLOTS\n##     - ERRORBAR PLOTS\n##     - HISTOGRAM PLOTS\n##     - SCATTER PLOTS\n##     - AGG RENDERING\n##     - PATHS\n##     - SAVING FIGURES\n##     - INTERACTIVE KEYMAPS\n##     - ANIMATION\n\n##### CONFIGURATION BEGINS HERE\n\n\n## ***************************************************************************\n## * BACKENDS                                                                *\n## ***************************************************************************\n## The default backend.  If you omit this parameter, the first working\n## backend from the following list is used:\n##     MacOSX QtAgg Gtk4Agg Gtk3Agg TkAgg WxAgg Agg\n## Other choices include:\n##     QtCairo GTK4Cairo GTK3Cairo TkCairo WxCairo Cairo\n##     Qt5Agg Qt5Cairo Wx  # deprecated.\n##     PS PDF SVG Template\n## You can also deploy your own backend outside of Matplotlib by referring to\n## the module name (which must be in the PYTHONPATH) as 'module://my_backend'.\n##backend: Agg\n\n## The port to use for the web server in the WebAgg backend.\n#webagg.port: 8988\n\n## The address on which the WebAgg web server should be reachable\n#webagg.address: 127.0.0.1\n\n## If webagg.port is unavailable, a number of other random ports will\n## be tried until one that is available is found.\n#webagg.port_retries: 50\n\n## When True, open the web browser to the plot that is shown\n#webagg.open_in_browser: True\n\n## If you are running pyplot inside a GUI and your backend choice\n## conflicts, we will automatically try to find a compatible one for\n## you if backend_fallback is True\n#backend_fallback: True\n\n#interactive: False\n#figure.hooks:          # list of dotted.module.name:dotted.callable.name\n#toolbar:     toolbar2  # {None, toolbar2, toolmanager}\n#timezone:    UTC       # a pytz timezone string, e.g., US/Central or Europe/Paris\n\n\n## ***************************************************************************\n## * LINES                                                                   *\n## ***************************************************************************\n## See https://matplotlib.org/stable/api/artist_api.html#module-matplotlib.lines\n## for more information on line properties.\n#lines.linewidth: 1.5               # line width in points\n#lines.linestyle: -                 # solid line\n#lines.color:     C0                # has no affect on plot(); see axes.prop_cycle\n#lines.marker:          None        # the default marker\n#lines.markerfacecolor: auto        # the default marker face color\n#lines.markeredgecolor: auto        # the default marker edge color\n#lines.markeredgewidth: 1.0         # the line width around the marker symbol\n#lines.markersize:      6           # marker size, in points\n#lines.dash_joinstyle:  round       # {miter, round, bevel}\n#lines.dash_capstyle:   butt        # {butt, round, projecting}\n#lines.solid_joinstyle: round       # {miter, round, bevel}\n#lines.solid_capstyle:  projecting  # {butt, round, projecting}\n#lines.antialiased: True            # render lines in antialiased (no jaggies)\n\n## The three standard dash patterns.  These are scaled by the linewidth.\n#lines.dashed_pattern: 3.7, 1.6\n#lines.dashdot_pattern: 6.4, 1.6, 1, 1.6\n#lines.dotted_pattern: 1, 1.65\n#lines.scale_dashes: True\n\n#markers.fillstyle: full  # {full, left, right, bottom, top, none}\n\n#pcolor.shading: auto\n#pcolormesh.snap: True  # Whether to snap the mesh to pixel boundaries. This is\n                        # provided solely to allow old test images to remain\n                        # unchanged. Set to False to obtain the previous behavior.\n\n## ***************************************************************************\n## * PATCHES                                                                 *\n## ***************************************************************************\n## Patches are graphical objects that fill 2D space, like polygons or circles.\n## See https://matplotlib.org/stable/api/artist_api.html#module-matplotlib.patches\n## for more information on patch properties.\n#patch.linewidth:       1.0    # edge width in points.\n#patch.facecolor:       C0\n#patch.edgecolor:       black  # if forced, or patch is not filled\n#patch.force_edgecolor: False  # True to always use edgecolor\n#patch.antialiased:     True   # render patches in antialiased (no jaggies)\n\n\n## ***************************************************************************\n## * HATCHES                                                                 *\n## ***************************************************************************\n#hatch.color:     black\n#hatch.linewidth: 1.0\n\n\n## ***************************************************************************\n## * BOXPLOT                                                                 *\n## ***************************************************************************\n#boxplot.notch:       False\n#boxplot.vertical:    True\n#boxplot.whiskers:    1.5\n#boxplot.bootstrap:   None\n#boxplot.patchartist: False\n#boxplot.showmeans:   False\n#boxplot.showcaps:    True\n#boxplot.showbox:     True\n#boxplot.showfliers:  True\n#boxplot.meanline:    False\n\n#boxplot.flierprops.color:           black\n#boxplot.flierprops.marker:          o\n#boxplot.flierprops.markerfacecolor: none\n#boxplot.flierprops.markeredgecolor: black\n#boxplot.flierprops.markeredgewidth: 1.0\n#boxplot.flierprops.markersize:      6\n#boxplot.flierprops.linestyle:       none\n#boxplot.flierprops.linewidth:       1.0\n\n#boxplot.boxprops.color:     black\n#boxplot.boxprops.linewidth: 1.0\n#boxplot.boxprops.linestyle: -\n\n#boxplot.whiskerprops.color:     black\n#boxplot.whiskerprops.linewidth: 1.0\n#boxplot.whiskerprops.linestyle: -\n\n#boxplot.capprops.color:     black\n#boxplot.capprops.linewidth: 1.0\n#boxplot.capprops.linestyle: -\n\n#boxplot.medianprops.color:     C1\n#boxplot.medianprops.linewidth: 1.0\n#boxplot.medianprops.linestyle: -\n\n#boxplot.meanprops.color:           C2\n#boxplot.meanprops.marker:          ^\n#boxplot.meanprops.markerfacecolor: C2\n#boxplot.meanprops.markeredgecolor: C2\n#boxplot.meanprops.markersize:       6\n#boxplot.meanprops.linestyle:       --\n#boxplot.meanprops.linewidth:       1.0\n\n\n## ***************************************************************************\n## * FONT                                                                    *\n## ***************************************************************************\n## The font properties used by `text.Text`.\n## See https://matplotlib.org/stable/api/font_manager_api.html for more information\n## on font properties.  The 6 font properties used for font matching are\n## given below with their default values.\n##\n## The font.family property can take either a single or multiple entries of any\n## combination of concrete font names (not supported when rendering text with\n## usetex) or the following five generic values:\n##     - 'serif' (e.g., Times),\n##     - 'sans-serif' (e.g., Helvetica),\n##     - 'cursive' (e.g., Zapf-Chancery),\n##     - 'fantasy' (e.g., Western), and\n##     - 'monospace' (e.g., Courier).\n## Each of these values has a corresponding default list of font names\n## (font.serif, etc.); the first available font in the list is used.  Note that\n## for font.serif, font.sans-serif, and font.monospace, the first element of\n## the list (a DejaVu font) will always be used because DejaVu is shipped with\n## Matplotlib and is thus guaranteed to be available; the other entries are\n## left as examples of other possible values.\n##\n## The font.style property has three values: normal (or roman), italic\n## or oblique.  The oblique style will be used for italic, if it is not\n## present.\n##\n## The font.variant property has two values: normal or small-caps.  For\n## TrueType fonts, which are scalable fonts, small-caps is equivalent\n## to using a font size of 'smaller', or about 83 % of the current font\n## size.\n##\n## The font.weight property has effectively 13 values: normal, bold,\n## bolder, lighter, 100, 200, 300, ..., 900.  Normal is the same as\n## 400, and bold is 700.  bolder and lighter are relative values with\n## respect to the current weight.\n##\n## The font.stretch property has 11 values: ultra-condensed,\n## extra-condensed, condensed, semi-condensed, normal, semi-expanded,\n## expanded, extra-expanded, ultra-expanded, wider, and narrower.  This\n## property is not currently implemented.\n##\n## The font.size property is the default font size for text, given in points.\n## 10 pt is the standard value.\n##\n## Note that font.size controls default text sizes.  To configure\n## special text sizes tick labels, axes, labels, title, etc., see the rc\n## settings for axes and ticks.  Special text sizes can be defined\n## relative to font.size, using the following values: xx-small, x-small,\n## small, medium, large, x-large, xx-large, larger, or smaller\n\n#font.family:  sans-serif\n#font.style:   normal\n#font.variant: normal\n#font.weight:  normal\n#font.stretch: normal\n#font.size:    10.0\n\n#font.serif:      DejaVu Serif, Bitstream Vera Serif, Computer Modern Roman, New Century Schoolbook, Century Schoolbook L, Utopia, ITC Bookman, Bookman, Nimbus Roman No9 L, Times New Roman, Times, Palatino, Charter, serif\n#font.sans-serif: DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif\n#font.cursive:    Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n#font.fantasy:    Chicago, Charcoal, Impact, Western, Humor Sans, xkcd, fantasy\n#font.monospace:  DejaVu Sans Mono, Bitstream Vera Sans Mono, Computer Modern Typewriter, Andale Mono, Nimbus Mono L, Courier New, Courier, Fixed, Terminal, monospace\n\n\n## ***************************************************************************\n## * TEXT                                                                    *\n## ***************************************************************************\n## The text properties used by `text.Text`.\n## See https://matplotlib.org/stable/api/artist_api.html#module-matplotlib.text\n## for more information on text properties\n#text.color: black\n\n## FreeType hinting flag (\"foo\" corresponds to FT_LOAD_FOO); may be one of the\n## following (Proprietary Matplotlib-specific synonyms are given in parentheses,\n## but their use is discouraged):\n## - default: Use the font's native hinter if possible, else FreeType's auto-hinter.\n##            (\"either\" is a synonym).\n## - no_autohint: Use the font's native hinter if possible, else don't hint.\n##                (\"native\" is a synonym.)\n## - force_autohint: Use FreeType's auto-hinter.  (\"auto\" is a synonym.)\n## - no_hinting: Disable hinting.  (\"none\" is a synonym.)\n#text.hinting: force_autohint\n\n#text.hinting_factor: 8  # Specifies the amount of softness for hinting in the\n                         # horizontal direction.  A value of 1 will hint to full\n                         # pixels.  A value of 2 will hint to half pixels etc.\n#text.kerning_factor: 0  # Specifies the scaling factor for kerning values.  This\n                         # is provided solely to allow old test images to remain\n                         # unchanged.  Set to 6 to obtain previous behavior.\n                         # Values  other than 0 or 6 have no defined meaning.\n#text.antialiased: True  # If True (default), the text will be antialiased.\n                         # This only affects raster outputs.\n#text.parse_math: True  # Use mathtext if there is an even number of unescaped\n                        # dollar signs.\n\n\n## ***************************************************************************\n## * LaTeX                                                                   *\n## ***************************************************************************\n## For more information on LaTeX properties, see\n## https://matplotlib.org/stable/tutorials/text/usetex.html\n#text.usetex: False  # use latex for all text handling. The following fonts\n                     # are supported through the usual rc parameter settings:\n                     # new century schoolbook, bookman, times, palatino,\n                     # zapf chancery, charter, serif, sans-serif, helvetica,\n                     # avant garde, courier, monospace, computer modern roman,\n                     # computer modern sans serif, computer modern typewriter\n#text.latex.preamble:   # IMPROPER USE OF THIS FEATURE WILL LEAD TO LATEX FAILURES\n                        # AND IS THEREFORE UNSUPPORTED. PLEASE DO NOT ASK FOR HELP\n                        # IF THIS FEATURE DOES NOT DO WHAT YOU EXPECT IT TO.\n                        # text.latex.preamble is a single line of LaTeX code that\n                        # will be passed on to the LaTeX system. It may contain\n                        # any code that is valid for the LaTeX \"preamble\", i.e.\n                        # between the \"\\documentclass\" and \"\\begin{document}\"\n                        # statements.\n                        # Note that it has to be put on a single line, which may\n                        # become quite long.\n                        # The following packages are always loaded with usetex,\n                        # so beware of package collisions:\n                        #   geometry, inputenc, type1cm.\n                        # PostScript (PSNFSS) font packages may also be\n                        # loaded, depending on your font settings.\n\n## The following settings allow you to select the fonts in math mode.\n#mathtext.fontset: dejavusans  # Should be 'dejavusans' (default),\n                               # 'dejavuserif', 'cm' (Computer Modern), 'stix',\n                               # 'stixsans' or 'custom'\n## \"mathtext.fontset: custom\" is defined by the mathtext.bf, .cal, .it, ...\n## settings which map a TeX font name to a fontconfig font pattern.  (These\n## settings are not used for other font sets.)\n#mathtext.bf:  sans:bold\n#mathtext.cal: cursive\n#mathtext.it:  sans:italic\n#mathtext.rm:  sans\n#mathtext.sf:  sans\n#mathtext.tt:  monospace\n#mathtext.fallback: cm  # Select fallback font from ['cm' (Computer Modern), 'stix'\n                        # 'stixsans'] when a symbol can not be found in one of the\n                        # custom math fonts. Select 'None' to not perform fallback\n                        # and replace the missing character by a dummy symbol.\n#mathtext.default: it  # The default font to use for math.\n                       # Can be any of the LaTeX font names, including\n                       # the special name \"regular\" for the same font\n                       # used in regular text.\n\n\n## ***************************************************************************\n## * AXES                                                                    *\n## ***************************************************************************\n## Following are default face and edge colors, default tick sizes,\n## default font sizes for tick labels, and so on.  See\n## https://matplotlib.org/stable/api/axes_api.html#module-matplotlib.axes\n#axes.facecolor:     white   # axes background color\n#axes.edgecolor:     black   # axes edge color\n#axes.linewidth:     0.8     # edge line width\n#axes.grid:          False   # display grid or not\n#axes.grid.axis:     both    # which axis the grid should apply to\n#axes.grid.which:    major   # grid lines at {major, minor, both} ticks\n#axes.titlelocation: center  # alignment of the title: {left, right, center}\n#axes.titlesize:     large   # font size of the axes title\n#axes.titleweight:   normal  # font weight of title\n#axes.titlecolor:    auto    # color of the axes title, auto falls back to\n                             # text.color as default value\n#axes.titley:        None    # position title (axes relative units).  None implies auto\n#axes.titlepad:      6.0     # pad between axes and title in points\n#axes.labelsize:     medium  # font size of the x and y labels\n#axes.labelpad:      4.0     # space between label and axis\n#axes.labelweight:   normal  # weight of the x and y labels\n#axes.labelcolor:    black\n#axes.axisbelow:     line    # draw axis gridlines and ticks:\n                             #     - below patches (True)\n                             #     - above patches but below lines ('line')\n                             #     - above all (False)\n\n#axes.formatter.limits: -5, 6  # use scientific notation if log10\n                               # of the axis range is smaller than the\n                               # first or larger than the second\n#axes.formatter.use_locale: False  # When True, format tick labels\n                                   # according to the user's locale.\n                                   # For example, use ',' as a decimal\n                                   # separator in the fr_FR locale.\n#axes.formatter.use_mathtext: False  # When True, use mathtext for scientific\n                                     # notation.\n#axes.formatter.min_exponent: 0  # minimum exponent to format in scientific notation\n#axes.formatter.useoffset: True  # If True, the tick label formatter\n                                 # will default to labeling ticks relative\n                                 # to an offset when the data range is\n                                 # small compared to the minimum absolute\n                                 # value of the data.\n#axes.formatter.offset_threshold: 4  # When useoffset is True, the offset\n                                     # will be used when it can remove\n                                     # at least this number of significant\n                                     # digits from tick labels.\n\n#axes.spines.left:   True  # display axis spines\n#axes.spines.bottom: True\n#axes.spines.top:    True\n#axes.spines.right:  True\n\n#axes.unicode_minus: True  # use Unicode for the minus symbol rather than hyphen.  See\n                           # https://en.wikipedia.org/wiki/Plus_and_minus_signs#Character_codes\n#axes.prop_cycle: cycler('color', ['1f77b4', 'ff7f0e', '2ca02c', 'd62728', '9467bd', '8c564b', 'e377c2', '7f7f7f', 'bcbd22', '17becf'])\n                  # color cycle for plot lines as list of string color specs:\n                  # single letter, long name, or web-style hex\n                  # As opposed to all other parameters in this file, the color\n                  # values must be enclosed in quotes for this parameter,\n                  # e.g. '1f77b4', instead of 1f77b4.\n                  # See also https://matplotlib.org/stable/tutorials/intermediate/color_cycle.html\n                  # for more details on prop_cycle usage.\n#axes.xmargin:   .05  # x margin.  See `axes.Axes.margins`\n#axes.ymargin:   .05  # y margin.  See `axes.Axes.margins`\n#axes.zmargin:   .05  # z margin.  See `axes.Axes.margins`\n#axes.autolimit_mode: data  # If \"data\", use axes.xmargin and axes.ymargin as is.\n                            # If \"round_numbers\", after application of margins, axis\n                            # limits are further expanded to the nearest \"round\" number.\n#polaraxes.grid: True  # display grid on polar axes\n#axes3d.grid:    True  # display grid on 3D axes\n\n#axes3d.xaxis.panecolor:    (0.95, 0.95, 0.95, 0.5)  # background pane on 3D axes\n#axes3d.yaxis.panecolor:    (0.90, 0.90, 0.90, 0.5)  # background pane on 3D axes\n#axes3d.zaxis.panecolor:    (0.925, 0.925, 0.925, 0.5)  # background pane on 3D axes\n\n## ***************************************************************************\n## * AXIS                                                                    *\n## ***************************************************************************\n#xaxis.labellocation: center  # alignment of the xaxis label: {left, right, center}\n#yaxis.labellocation: center  # alignment of the yaxis label: {bottom, top, center}\n\n\n## ***************************************************************************\n## * DATES                                                                   *\n## ***************************************************************************\n## These control the default format strings used in AutoDateFormatter.\n## Any valid format datetime format string can be used (see the python\n## `datetime` for details).  For example, by using:\n##     - '%x' will use the locale date representation\n##     - '%X' will use the locale time representation\n##     - '%c' will use the full locale datetime representation\n## These values map to the scales:\n##     {'year': 365, 'month': 30, 'day': 1, 'hour': 1/24, 'minute': 1 / (24 * 60)}\n\n#date.autoformatter.year:        %Y\n#date.autoformatter.month:       %Y-%m\n#date.autoformatter.day:         %Y-%m-%d\n#date.autoformatter.hour:        %m-%d %H\n#date.autoformatter.minute:      %d %H:%M\n#date.autoformatter.second:      %H:%M:%S\n#date.autoformatter.microsecond: %M:%S.%f\n## The reference date for Matplotlib's internal date representation\n## See https://matplotlib.org/stable/gallery/ticks/date_precision_and_epochs.html\n#date.epoch: 1970-01-01T00:00:00\n## 'auto', 'concise':\n#date.converter:                  auto\n## For auto converter whether to use interval_multiples:\n#date.interval_multiples:         True\n\n## ***************************************************************************\n## * TICKS                                                                   *\n## ***************************************************************************\n## See https://matplotlib.org/stable/api/axis_api.html#matplotlib.axis.Tick\n#xtick.top:           False   # draw ticks on the top side\n#xtick.bottom:        True    # draw ticks on the bottom side\n#xtick.labeltop:      False   # draw label on the top\n#xtick.labelbottom:   True    # draw label on the bottom\n#xtick.major.size:    3.5     # major tick size in points\n#xtick.minor.size:    2       # minor tick size in points\n#xtick.major.width:   0.8     # major tick width in points\n#xtick.minor.width:   0.6     # minor tick width in points\n#xtick.major.pad:     3.5     # distance to major tick label in points\n#xtick.minor.pad:     3.4     # distance to the minor tick label in points\n#xtick.color:         black   # color of the ticks\n#xtick.labelcolor:    inherit # color of the tick labels or inherit from xtick.color\n#xtick.labelsize:     medium  # font size of the tick labels\n#xtick.direction:     out     # direction: {in, out, inout}\n#xtick.minor.visible: False   # visibility of minor ticks on x-axis\n#xtick.major.top:     True    # draw x axis top major ticks\n#xtick.major.bottom:  True    # draw x axis bottom major ticks\n#xtick.minor.top:     True    # draw x axis top minor ticks\n#xtick.minor.bottom:  True    # draw x axis bottom minor ticks\n#xtick.alignment:     center  # alignment of xticks\n\n#ytick.left:          True    # draw ticks on the left side\n#ytick.right:         False   # draw ticks on the right side\n#ytick.labelleft:     True    # draw tick labels on the left side\n#ytick.labelright:    False   # draw tick labels on the right side\n#ytick.major.size:    3.5     # major tick size in points\n#ytick.minor.size:    2       # minor tick size in points\n#ytick.major.width:   0.8     # major tick width in points\n#ytick.minor.width:   0.6     # minor tick width in points\n#ytick.major.pad:     3.5     # distance to major tick label in points\n#ytick.minor.pad:     3.4     # distance to the minor tick label in points\n#ytick.color:         black   # color of the ticks\n#ytick.labelcolor:    inherit # color of the tick labels or inherit from ytick.color\n#ytick.labelsize:     medium  # font size of the tick labels\n#ytick.direction:     out     # direction: {in, out, inout}\n#ytick.minor.visible: False   # visibility of minor ticks on y-axis\n#ytick.major.left:    True    # draw y axis left major ticks\n#ytick.major.right:   True    # draw y axis right major ticks\n#ytick.minor.left:    True    # draw y axis left minor ticks\n#ytick.minor.right:   True    # draw y axis right minor ticks\n#ytick.alignment:     center_baseline  # alignment of yticks\n\n\n## ***************************************************************************\n## * GRIDS                                                                   *\n## ***************************************************************************\n#grid.color:     \"#b0b0b0\"  # grid color\n#grid.linestyle: -          # solid\n#grid.linewidth: 0.8        # in points\n#grid.alpha:     1.0        # transparency, between 0.0 and 1.0\n\n\n## ***************************************************************************\n## * LEGEND                                                                  *\n## ***************************************************************************\n#legend.loc:           best\n#legend.frameon:       True     # if True, draw the legend on a background patch\n#legend.framealpha:    0.8      # legend patch transparency\n#legend.facecolor:     inherit  # inherit from axes.facecolor; or color spec\n#legend.edgecolor:     0.8      # background patch boundary color\n#legend.fancybox:      True     # if True, use a rounded box for the\n                                # legend background, else a rectangle\n#legend.shadow:        False    # if True, give background a shadow effect\n#legend.numpoints:     1        # the number of marker points in the legend line\n#legend.scatterpoints: 1        # number of scatter points\n#legend.markerscale:   1.0      # the relative size of legend markers vs. original\n#legend.fontsize:      medium\n#legend.labelcolor:    None\n#legend.title_fontsize: None    # None sets to the same as the default axes.\n\n## Dimensions as fraction of font size:\n#legend.borderpad:     0.4  # border whitespace\n#legend.labelspacing:  0.5  # the vertical space between the legend entries\n#legend.handlelength:  2.0  # the length of the legend lines\n#legend.handleheight:  0.7  # the height of the legend handle\n#legend.handletextpad: 0.8  # the space between the legend line and legend text\n#legend.borderaxespad: 0.5  # the border between the axes and legend edge\n#legend.columnspacing: 2.0  # column separation\n\n\n## ***************************************************************************\n## * FIGURE                                                                  *\n## ***************************************************************************\n## See https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure\n#figure.titlesize:   large     # size of the figure title (``Figure.suptitle()``)\n#figure.titleweight: normal    # weight of the figure title\n#figure.labelsize:   large     # size of the figure label (``Figure.sup[x|y]label()``)\n#figure.labelweight: normal    # weight of the figure label\n#figure.figsize:     6.4, 4.8  # figure size in inches\n#figure.dpi:         100       # figure dots per inch\n#figure.facecolor:   white     # figure face color\n#figure.edgecolor:   white     # figure edge color\n#figure.frameon:     True      # enable figure frame\n#figure.max_open_warning: 20   # The maximum number of figures to open through\n                               # the pyplot interface before emitting a warning.\n                               # If less than one this feature is disabled.\n#figure.raise_window : True    # Raise the GUI window to front when show() is called.\n\n## The figure subplot parameters.  All dimensions are a fraction of the figure width and height.\n#figure.subplot.left:   0.125  # the left side of the subplots of the figure\n#figure.subplot.right:  0.9    # the right side of the subplots of the figure\n#figure.subplot.bottom: 0.11   # the bottom of the subplots of the figure\n#figure.subplot.top:    0.88   # the top of the subplots of the figure\n#figure.subplot.wspace: 0.2    # the amount of width reserved for space between subplots,\n                               # expressed as a fraction of the average axis width\n#figure.subplot.hspace: 0.2    # the amount of height reserved for space between subplots,\n                               # expressed as a fraction of the average axis height\n\n## Figure layout\n#figure.autolayout: False  # When True, automatically adjust subplot\n                           # parameters to make the plot fit the figure\n                           # using `tight_layout`\n#figure.constrained_layout.use: False  # When True, automatically make plot\n                                       # elements fit on the figure. (Not\n                                       # compatible with `autolayout`, above).\n#figure.constrained_layout.h_pad:  0.04167  # Padding around axes objects. Float representing\n#figure.constrained_layout.w_pad:  0.04167  # inches. Default is 3/72 inches (3 points)\n#figure.constrained_layout.hspace: 0.02     # Space between subplot groups. Float representing\n#figure.constrained_layout.wspace: 0.02     # a fraction of the subplot widths being separated.\n\n\n## ***************************************************************************\n## * IMAGES                                                                  *\n## ***************************************************************************\n#image.aspect:          equal        # {equal, auto} or a number\n#image.interpolation:   antialiased  # see help(imshow) for options\n#image.cmap:            viridis      # A colormap name (plasma, magma, etc.)\n#image.lut:             256          # the size of the colormap lookup table\n#image.origin:          upper        # {lower, upper}\n#image.resample:        True\n#image.composite_image: True  # When True, all the images on a set of axes are\n                              # combined into a single composite image before\n                              # saving a figure as a vector graphics file,\n                              # such as a PDF.\n\n\n## ***************************************************************************\n## * CONTOUR PLOTS                                                           *\n## ***************************************************************************\n#contour.negative_linestyle: dashed  # string or on-off ink sequence\n#contour.corner_mask:        True    # {True, False}\n#contour.linewidth:          None    # {float, None} Size of the contour line\n                                     # widths. If set to None, it falls back to\n                                     # `line.linewidth`.\n#contour.algorithm:          mpl2014 # {mpl2005, mpl2014, serial, threaded}\n\n\n## ***************************************************************************\n## * ERRORBAR PLOTS                                                          *\n## ***************************************************************************\n#errorbar.capsize: 0  # length of end cap on error bars in pixels\n\n\n## ***************************************************************************\n## * HISTOGRAM PLOTS                                                         *\n## ***************************************************************************\n#hist.bins: 10  # The default number of histogram bins or 'auto'.\n\n\n## ***************************************************************************\n## * SCATTER PLOTS                                                           *\n## ***************************************************************************\n#scatter.marker: o         # The default marker type for scatter plots.\n#scatter.edgecolors: face  # The default edge colors for scatter plots.\n\n\n## ***************************************************************************\n## * AGG RENDERING                                                           *\n## ***************************************************************************\n## Warning: experimental, 2008/10/10\n#agg.path.chunksize: 0  # 0 to disable; values in the range\n                        # 10000 to 100000 can improve speed slightly\n                        # and prevent an Agg rendering failure\n                        # when plotting very large data sets,\n                        # especially if they are very gappy.\n                        # It may cause minor artifacts, though.\n                        # A value of 20000 is probably a good\n                        # starting point.\n\n\n## ***************************************************************************\n## * PATHS                                                                   *\n## ***************************************************************************\n#path.simplify: True  # When True, simplify paths by removing \"invisible\"\n                      # points to reduce file size and increase rendering\n                      # speed\n#path.simplify_threshold: 0.111111111111  # The threshold of similarity below\n                                          # which vertices will be removed in\n                                          # the simplification process.\n#path.snap: True  # When True, rectilinear axis-aligned paths will be snapped\n                  # to the nearest pixel when certain criteria are met.\n                  # When False, paths will never be snapped.\n#path.sketch: None  # May be None, or a 3-tuple of the form:\n                    # (scale, length, randomness).\n                    #     - *scale* is the amplitude of the wiggle\n                    #         perpendicular to the line (in pixels).\n                    #     - *length* is the length of the wiggle along the\n                    #         line (in pixels).\n                    #     - *randomness* is the factor by which the length is\n                    #         randomly scaled.\n#path.effects:\n\n\n## ***************************************************************************\n## * SAVING FIGURES                                                          *\n## ***************************************************************************\n## The default savefig parameters can be different from the display parameters\n## e.g., you may want a higher resolution, or to make the figure\n## background white\n#savefig.dpi:       figure      # figure dots per inch or 'figure'\n#savefig.facecolor: auto        # figure face color when saving\n#savefig.edgecolor: auto        # figure edge color when saving\n#savefig.format:    png         # {png, ps, pdf, svg}\n#savefig.bbox:      standard    # {tight, standard}\n                                # 'tight' is incompatible with pipe-based animation\n                                # backends (e.g. 'ffmpeg') but will work with those\n                                # based on temporary files (e.g. 'ffmpeg_file')\n#savefig.pad_inches:  0.1       # padding to be used, when bbox is set to 'tight'\n#savefig.directory:   ~         # default directory in savefig dialog, gets updated after\n                                # interactive saves, unless set to the empty string (i.e.\n                                # the current directory); use '.' to start at the current\n                                # directory but update after interactive saves\n#savefig.transparent: False     # whether figures are saved with a transparent\n                                # background by default\n#savefig.orientation: portrait  # orientation of saved figure, for PostScript output only\n\n### tk backend params\n#tk.window_focus:   False  # Maintain shell focus for TkAgg\n\n### ps backend params\n#ps.papersize:      letter  # {auto, letter, legal, ledger, A0-A10, B0-B10}\n#ps.useafm:         False   # use of AFM fonts, results in small files\n#ps.usedistiller:   False   # {ghostscript, xpdf, None}\n                            # Experimental: may produce smaller files.\n                            # xpdf intended for production of publication quality files,\n                            # but requires ghostscript, xpdf and ps2eps\n#ps.distiller.res:  6000    # dpi\n#ps.fonttype:       3       # Output Type 3 (Type3) or Type 42 (TrueType)\n\n### PDF backend params\n#pdf.compression:    6  # integer from 0 to 9\n                        # 0 disables compression (good for debugging)\n#pdf.fonttype:       3  # Output Type 3 (Type3) or Type 42 (TrueType)\n#pdf.use14corefonts: False\n#pdf.inheritcolor:   False\n\n### SVG backend params\n#svg.image_inline: True  # Write raster image data directly into the SVG file\n#svg.fonttype: path      # How to handle SVG fonts:\n                         #     path: Embed characters as paths -- supported\n                         #           by most SVG renderers\n                         #     None: Assume fonts are installed on the\n                         #           machine where the SVG will be viewed.\n#svg.hashsalt: None      # If not None, use this string as hash salt instead of uuid4\n\n### pgf parameter\n## See https://matplotlib.org/stable/tutorials/text/pgf.html for more information.\n#pgf.rcfonts: True\n#pgf.preamble:  # See text.latex.preamble for documentation\n#pgf.texsystem: xelatex\n\n### docstring params\n#docstring.hardcopy: False  # set this when you want to generate hardcopy docstring\n\n\n## ***************************************************************************\n## * INTERACTIVE KEYMAPS                                                     *\n## ***************************************************************************\n## Event keys to interact with figures/plots via keyboard.\n## See https://matplotlib.org/stable/users/explain/interactive.html for more\n## details on interactive navigation.  Customize these settings according to\n## your needs. Leave the field(s) empty if you don't need a key-map. (i.e.,\n## fullscreen : '')\n#keymap.fullscreen: f, ctrl+f   # toggling\n#keymap.home: h, r, home        # home or reset mnemonic\n#keymap.back: left, c, backspace, MouseButton.BACK  # forward / backward keys\n#keymap.forward: right, v, MouseButton.FORWARD      # for quick navigation\n#keymap.pan: p                  # pan mnemonic\n#keymap.zoom: o                 # zoom mnemonic\n#keymap.save: s, ctrl+s         # saving current figure\n#keymap.help: f1                # display help about active tools\n#keymap.quit: ctrl+w, cmd+w, q  # close the current figure\n#keymap.quit_all:               # close all figures\n#keymap.grid: g                 # switching on/off major grids in current axes\n#keymap.grid_minor: G           # switching on/off minor grids in current axes\n#keymap.yscale: l               # toggle scaling of y-axes ('log'/'linear')\n#keymap.xscale: k, L            # toggle scaling of x-axes ('log'/'linear')\n#keymap.copy: ctrl+c, cmd+c     # copy figure to clipboard\n\n\n## ***************************************************************************\n## * ANIMATION                                                               *\n## ***************************************************************************\n#animation.html: none  # How to display the animation as HTML in\n                       # the IPython notebook:\n                       #     - 'html5' uses HTML5 video tag\n                       #     - 'jshtml' creates a JavaScript animation\n#animation.writer:  ffmpeg        # MovieWriter 'backend' to use\n#animation.codec:   h264          # Codec to use for writing movie\n#animation.bitrate: -1            # Controls size/quality trade-off for movie.\n                                  # -1 implies let utility auto-determine\n#animation.frame_format: png      # Controls frame format used by temp files\n\n## Path to ffmpeg binary.  Unqualified paths are resolved by subprocess.Popen.\n#animation.ffmpeg_path:  ffmpeg\n## Additional arguments to pass to ffmpeg.\n#animation.ffmpeg_args:\n\n## Path to ImageMagick's convert binary.  Unqualified paths are resolved by\n## subprocess.Popen, except that on Windows, we look up an install of\n## ImageMagick in the registry (as convert is also the name of a system tool).\n#animation.convert_path: convert\n## Additional arguments to pass to convert.\n#animation.convert_args: -layers, OptimizePlus\n#\n#animation.embed_limit:  20.0     # Limit, in MB, of size of base64 encoded\n                                  # animation in HTML (i.e. IPython notebook)",
            "# Prior hyperparameters\n\n# Prior for obs. cov. is inverse-Wishart(v_1^0=k + 3, S10=I)\nv10 = mod.k_endog + 3\nS10 = np.eye(mod.k_endog)\n\n# Prior for state cov. variances is inverse-Gamma(v_{i2}^0 / 2 = 3, S+{i2}^0 / 2 = 0.005)\nvi20 = 6\nSi20 = 0.01",
            "# Now create a bootstrap confidence interval around the a LOWESS fit\n\n\ndef lowess_with_confidence_bounds(\n    x, y, eval_x, N=200, conf_interval=0.95, lowess_kw=None\n):\n    \"\"\"\n    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n    \"\"\"\n    # Lowess smoothing\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, xvals=eval_x, **lowess_kw)\n\n    # Perform bootstrap resamplings of the data\n    # and  evaluate the smoothing at a fixed set of points\n    smoothed_values = np.empty((N, len(eval_x)))\n    for i in range(N):\n        sample = np.random.choice(len(x), len(x), replace=True)\n        sampled_x = x[sample]\n        sampled_y = y[sample]\n\n        smoothed_values[i] = sm.nonparametric.lowess(\n            exog=sampled_x, endog=sampled_y, xvals=eval_x, **lowess_kw\n        )\n\n    # Get the confidence interval\n    sorted_values = np.sort(smoothed_values, axis=0)\n    bound = int(N * (1 - conf_interval) / 2)\n    bottom = sorted_values[bound - 1]\n    top = sorted_values[-bound]\n\n    return smoothed, bottom, top\n\n\n# Compute the 95% confidence interval\neval_x = np.linspace(0, 4 * np.pi, 31)\nsmoothed, bottom, top = lowess_with_confidence_bounds(\n    x, y, eval_x, lowess_kw={\"frac\": 0.1}\n)",
            "# helper functions\n\ndef images_to_probs(net, images):\n    '''\n    Generates predictions and corresponding probabilities from a trained\n    network and a list of images\n    '''\n    output = net(images)\n    # convert output probabilities to predicted class\n    _, preds_tensor = torch.max(output, 1)\n    preds = np.squeeze(preds_tensor.numpy())\n    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n\n\ndef plot_classes_preds(net, images, labels):\n    '''\n    Generates matplotlib Figure using a trained network, along with images\n    and labels from a batch, that shows the network's top prediction along\n    with its probability, alongside the actual label, coloring this\n    information based on whether the prediction was correct or not.\n    Uses the \"images_to_probs\" function.\n    '''\n    preds, probs = images_to_probs(net, images)\n    # plot the images in the batch, along with predicted and true labels\n    fig = plt.figure(figsize=(12, 48))\n    for idx in np.arange(4):\n        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n        matplotlib_imshow(images[idx], one_channel=True)\n        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n            classes[preds[idx]],\n            probs[idx] * 100.0,\n            classes[labels[idx]]),\n                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n    return fig",
            "# Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->The matplotlibrc file->The default matplotlibrc file"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->5. Tracking model training with TensorBoard"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ]
        ]
    },
    "292213": {
        "jupyter_code_cell": "list_of_countries = ['England', 'Italy', 'Spain', 'Germany', 'Brazil']\nn_yr = 15\n\nrating_combine = pd.DataFrame(index = range(0, n_yr), columns = list_of_countries)\n\nfor c in list_of_countries:\n    for n in range(0,n_yr):\n        rating_352_Overall_later, _ = get_best_squad_n_n_yr_later(n, squad_352_strict, c)\n        rating_combine[c].iloc[n] = rating_352_Overall_later\n        \nax = rating_combine.plot(kind = 'line', figsize = (15,10), title = 'Country 3-5-2 best 11 rating by time')\nax.set_xlabel(\"n years later\")\nax.set_ylabel(\"team rating\")\n    ",
        "matched_tutorial_code_inds": [
            3264,
            1486,
            1783,
            3281,
            6061
        ],
        "matched_tutorial_codes": [
            "cred_intervals = []\nintervals = [0.5, 0.75, 0.95]\n\nfor interval in intervals:\n    cred_interval = list(t_post.interval(interval))\n    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n\ncred_int_df = (\n    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\n).set_index(\"interval\")\ncred_int_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach->Region of Practical Equivalence"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ]
        ]
    },
    "779867": {
        "jupyter_code_cell": "_ = omni_data.fillna({col: omni_data[col].mean() for col in omni_data.columns}, inplace=True)",
        "matched_tutorial_code_inds": [
            5470,
            2585,
            5586,
            1944,
            3612
        ],
        "matched_tutorial_codes": [
            "dta[\"affair\"] = (dta[\"affairs\"]  0).astype(float)\nprint(dta.head(10))",
            "noise_std = 0.75\ny_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)",
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "centers = [[1, 1], [-1, -1], [1, -1]]\nX, _ = (n_samples=10000, centers=centers, cluster_std=0.6)",
            "tfidf_transformer = TfidfTransformer()\n X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n X_train_tfidf.shape\n(2257, 35788)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noisy targets"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ],
            [
                "sklearn->Examples->Clustering->A demo of the mean-shift clustering algorithm->Generate sample data"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies"
            ]
        ]
    },
    "1113305": {
        "jupyter_code_cell": "answers = np.array('Republican Party; Republican Party; Democratic Party; Representative; Representative; Representative; Senator; Senator; Judge'.split('; '))\n\npk_score = []\nfor row in dems.index:\n    response = np.array(dems.loc[row, 'pk_ideo_baseline':'pk_SCJ_baseline'])\n    pk_score.append(sum(response==answers))\n\nold_pk_list = [col for col in dems.columns if col.startswith('pk_')]\ndems.drop(old_pk_list, axis = 1, inplace = True)\ndems.drop('pol_knowledge', axis =1, inplace = True)\n\ndems['pk_score'] = pk_score\n",
        "matched_tutorial_code_inds": [
            1978,
            6020,
            4682,
            2773,
            2314
        ],
        "matched_tutorial_codes": [
            "unique_labels = set(labels)\ncore_samples_mask = (labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in (0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\n(f\"Estimated number of clusters: {n_clusters_}\")\n()\n\n\n<img alt=\"Estimated number of clusters: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_002.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_002.png\"/>",
            "resid = interM_lm32.get_influence().summary_frame()[\"standard_resid\"]\n\nplt.figure(figsize=(6, 6))\nresid = resid.reindex(X.index)\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X.loc[idx],\n        resid.loc[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X[~[32]]\")\nplt.ylabel(\"standardized resids\")",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Plot results"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ]
        ]
    },
    "1435816": {
        "jupyter_code_cell": "def weightedRandomHelper(pairs):  \n    total = sum(pair[0] for pair in pairs)\n    r = randint(1, total)\n    for (weight, value) in pairs:\n        r -= weight\n        if r <= 0: return value",
        "matched_tutorial_code_inds": [
            1768,
            1765,
            347,
            1163,
            3659
        ],
        "matched_tutorial_codes": [
            "def fp_fc_layer(last_hs, parameters):\n    z2 = (np.dot(parameters['W2'], last_hs)\n          + parameters['b2'])\n    a2 = sigmoid(z2)\n    return a2",
            "def fp_forget_gate(concat, parameters):\n    ft = sigmoid(np.dot(parameters['Wf'], concat)\n                 + parameters['bf'])\n    return ft",
            "class Lambda():\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 1, 28, 28)",
            "class MyModule():\n    def __init__(self):\n        super().__init__()\n        self.lin = (100, 10)\n\n    def forward(self, x):\n        return (self.lin(x))\n\nmod = ()\n = (mod)\nprint(((10, 100)))",
            "def dataframe_method(self, inplace=False):\n    data = self.copy()  # regardless of inplace\n    result = ...\n    if inplace:\n        self._update_inplace(data)\n    else:\n        return result"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->But how do you obtain sentiment from the LSTM\u2019s output?"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Forward Propagation"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->nn.Sequential"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Basic Usage"
            ],
            [
                "pandas_toms_blog->Method Chaining->Method Chaining->Inplace?"
            ]
        ]
    },
    "23260": {
        "jupyter_code_cell": "feature_matrix_encoded, features_encoded = ft.encode_features(feature_matrix, features)\n\npipeline_preprocessing = [(\"imputer\",\n                           SimpleImputer()),\n                          (\"scaler\", RobustScaler(with_centering=True))]\nfeature_matrix_encoded.tail()",
        "matched_tutorial_code_inds": [
            2314,
            2955,
            2112,
            3549,
            2454
        ],
        "matched_tutorial_codes": [
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\norder_centroids = original_space_centroids.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names_out()\n\nfor i in range(true_k):\n    print(f\"Cluster {i}: \", end=\"\")\n    for ind in order_centroids[i, :10]:\n        print(f\"{terms[ind]} \", end=\"\")\n    print()",
            "one_hot_linear_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=(),\n    ),\n    (alphas=alphas),\n)\n\nevaluate(one_hot_linear_pipeline, X, y, cv=ts_cv)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ],
            [
                "sklearn->Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Top terms per cluster"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-steps as categories"
            ]
        ]
    },
    "190611": {
        "jupyter_code_cell": "sb.boxplot(x=college['Private'], y=college['Outstate'], data=college, order=[\"No\", \"Yes\"])",
        "matched_tutorial_code_inds": [
            4095,
            4022,
            3822,
            4185,
            3979
        ],
        "matched_tutorial_codes": [
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots",
                "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ]
        ]
    },
    "192037": {
        "jupyter_code_cell": "import pandas as pd\ncat_retained = pd.read_csv(\"/home/data/kaggle/csv_cat_cut1.csv\", nrows=100)",
        "matched_tutorial_code_inds": [
            4021,
            1612,
            1154,
            3812,
            2952
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "import pandas as pd\n\nfeature_names = rf[:-1].get_feature_names_out()\n\nmdi_importances = (\n    rf[-1].feature_importances_, index=feature_names\n).sort_values(ascending=True)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio"
            ],
            [
                "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "1035636": {
        "jupyter_code_cell": "useless_data.append('db_creators_url')",
        "matched_tutorial_code_inds": [
            3941,
            3967,
            663,
            653,
            6228
        ],
        "matched_tutorial_codes": [
            "sns.pairplot(data=penguins, hue=\"species\")\n",
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "traced_rn18 = (rn18)\nprint()",
            "traced_model = (model)\nprint(traced_model.graph)",
            "dta.plot(figsize=(12, 8))"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Multivariate views on complex datasets",
                "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data",
                "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ]
        ]
    },
    "594910": {
        "jupyter_code_cell": "active_by_phone = udf.groupby('phone')['active'].sum() / udf.groupby('phone')['active'].size()\n\nactive_by_phone = active_by_phone.sort_values()\n_ = active_by_phone.plot(kind = 'bar', rot = 0)\n_ = plt.xlabel(\"\")\n_ = plt.title(\"Proportion of users active by type of phone\")\nplt.show()",
        "matched_tutorial_code_inds": [
            2104,
            2112,
            6016,
            1486,
            3197
        ],
        "matched_tutorial_codes": [
            "train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ]
        ]
    },
    "262091": {
        "jupyter_code_cell": "#Convert the casts of unpopularmove from a list of strings to one string\ncast1=\"\".join(unpopularmovie_casts)",
        "matched_tutorial_code_inds": [
            4274,
            6597,
            6703,
            3730,
            1728
        ],
        "matched_tutorial_codes": [
            "def rosen(x):\n...     \"\"\"The Rosenbrock function\"\"\"\n...     return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)",
            "# Assign better names for our seasonal terms\ntrue_seasonal_10_3 = terms[0]\ntrue_seasonal_100_2 = terms[1]\ntrue_sum = true_seasonal_10_3 + true_seasonal_100_2\n<br/>",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "preprocessed_random_frame = frame_preprocessing(random_frame)\nplt.imshow(preprocessed_random_frame, cmap=\"gray\")\nprint(preprocessed_random_frame.shape)"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)"
            ]
        ]
    },
    "539624": {
        "jupyter_code_cell": "trig05['WTSAF2YR'].sum()",
        "matched_tutorial_code_inds": [
            5600,
            6813,
            3723,
            5522,
            6013
        ],
        "matched_tutorial_codes": [
            "resf_logit.predict()",
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "flights.dep_time.head()",
            "glm_mod.model.data.orig_endog.sum(1)",
            "interM_lm.model.data.orig_exog[:5]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "577184": {
        "jupyter_code_cell": "print(df)\ndf[[\"A\",\"B\"]].drop_duplicates()\n\nprint(\"set the first row to na:\\n\")\ndf[0:1]=np.nan\nprint(\"after dropna:\\n\",df.dropna())\nprint(\"after fillna:\\n\",df.fillna(0))\nprint(\"df.mean\\n\",df.mean())\nvalues=dict(df.mean())\nprint(\"after fillna with mean of each column:\\n\",df.fillna(values))\ndf.head()",
        "matched_tutorial_code_inds": [
            5292,
            5285,
            4528,
            4511,
            3606
        ],
        "matched_tutorial_codes": [
            "print(res.cusum)\nfig = res.plot_cusum()",
            "print(sm.datasets.copper.DESCRLONG)\n\ndta = sm.datasets.copper.load_pandas().data\ndta.index = pd.date_range(\"1951-01-01\", \"1975-01-01\", freq=\"AS\")\nendog = dta[\"WORLDCONSUMPTION\"]\n\n# To the regressors in the dataset, we add a column of ones for an intercept\nexog = sm.add_constant(\n    dta[[\"COPPERPRICE\", \"INCOMEINDEX\", \"ALUMPRICE\", \"INVENTORYINDEX\"]]\n)",
            "print('tail prob. of normal at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' %\n...       tuple(stats.norm.sf([crit01, crit05, crit10])*100))\ntail prob. of normal at 1%, 5% and 10%   0.2857   3.4957   8.5003",
            "print('mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f' %\n...       normdiscrete.stats(moments='mvsk'))\nmean = -0.0000, variance = 6.3302, skew = 0.0000, kurtosis = -0.0076",
            "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\nFrom: sd345@city.ac.uk (Michael Collier)\nSubject: Converting images to HP LaserJet III?\nNntp-Posting-Host: hampton\n\n print(twenty_train.target_names[twenty_train.target[0]])\ncomp.graphics"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution"
            ],
            [
                "scipy->Statistics (scipy.stats)->Building specific distributions->Subclassing rv_discrete"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Loading the 20 newsgroups dataset"
            ]
        ]
    },
    "107315": {
        "jupyter_code_cell": "df_au = pd.DataFrame()\ndf_au[header_list[0]] = flag\ndf_au[header_list[1]] = state\ndf_au[header_list[2]]=abbrev\ndf_au[header_list[3]]=ISO\ndf_au[header_list[4]]=Postal\ndf_au[header_list[5]]=Type\ndf_au[header_list[6]]=Capital\ndf_au[header_list[7]]=population\ndf_au[header_list[8]]=Area",
        "matched_tutorial_code_inds": [
            28,
            6022,
            3264,
            23,
            3145
        ],
        "matched_tutorial_codes": [
            "dataloader = DataLoader(transformed_dataset, batch_size=4,\n                        shuffle=True, num_workers=4)\n\n\n# Helper function to show a batch\ndef show_landmarks_batch(sample_batched):\n    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n    images_batch, landmarks_batch = \\\n            sample_batched['image'], sample_batched['landmarks']\n    batch_size = len(images_batch)\n    im_size = images_batch.size(2)\n\n    grid = utils.make_grid(images_batch)\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n\n    for i in range(batch_size):\n        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n                    landmarks_batch[i, :, 1].numpy(),\n                    s=10, marker='.', c='r')\n\n        plt.title('Batch from dataloader')\n\nfor i_batch, sample_batched in enumerate(dataloader):\n    print(i_batch, sample_batched['image'].size(),\n          sample_batched['landmarks'].size())\n\n    # observe 4th batch and stop.\n    if i_batch == 3:\n        plt.figure()\n        show_landmarks_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "cred_intervals = []\nintervals = [0.5, 0.75, 0.95]\n\nfor interval in intervals:\n    cred_interval = list(t_post.interval(interval))\n    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n\ncred_int_df = (\n    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\n).set_index(\"interval\")\ncred_int_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "results = (list)\nn_bootstrap = 100\nrng = (seed=0)\n\nfor prevalence, X, y in zip(\n    populations[\"prevalence\"], populations[\"X\"], populations[\"y\"]\n):\n\n    results_for_prevalence = scoring_on_bootstrap(\n        estimator, X, y, rng, n_bootstrap=n_bootstrap\n    )\n    results[\"prevalence\"].append(prevalence)\n    results[\"metrics\"].append(\n        results_for_prevalence.aggregate([\"mean\", \"std\"]).unstack()\n    )\n\nresults = (results[\"metrics\"], index=results[\"prevalence\"])\nresults.index.name = \"prevalence\"\nresults\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 3: The Dataloader"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach->Region of Practical Equivalence"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ]
        ]
    },
    "781315": {
        "jupyter_code_cell": "# Figure 1\nImage(\"./exports/figures/Political Affiliation of Municipality Presidents in Serbia 2012-2016.png\", width=1000)",
        "matched_tutorial_code_inds": [
            6358,
            51,
            45,
            6558,
            3979
        ],
        "matched_tutorial_codes": [
            "Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\"/>",
            "# Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)",
            "# Specify a path\nPATH = \"entire_model.pt\"\n\n# Save\n(net, PATH)\n\n# Load\nmodel = (PATH)\nmodel.eval()",
            "# Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->3. Save model A"
            ],
            [
                "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps->5. Save and load entire model"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ]
        ]
    },
    "918807": {
        "jupyter_code_cell": "q00_strs = [\"SELECT HOUR(trips.start_date) AS start_hour,\",\n            \"trips.subscriber_type,\"\n            \"COUNT(trips.trip_id) AS num_trips\",\n            \"FROM [bigquery-public-data:san_francisco.bikeshare_trips] AS trips\",\n            \"WHERE DAYOFWEEK(trips.start_date) BETWEEN 2 AND 6\",\n            \"GROUP BY start_hour, trips.subscriber_type\",\n            \"ORDER BY start_hour, trips.subscriber_type ASC\"]\nq00_out = \"q00.csv\"\nq00_cmd = bq_builder(q00_strs,outfile=q00_out,legacy_sql=True)\nexit_stat = os.system(q00_cmd)",
        "matched_tutorial_code_inds": [
            1105,
            2140,
            23,
            2466,
            2132
        ],
        "matched_tutorial_codes": [
            "qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')",
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ]
        ]
    },
    "1382528": {
        "jupyter_code_cell": "X_CA_H, y_CA_H = utils.shuffle(X_CA_H, y_CA_H, random_state=1)\nX_CA_H_train, X_CA_H_test, y_CA_H_train, y_CA_H_test = model_selection.train_test_split(\n    X_CA_H, y_CA_H, test_size=0.4, random_state=0)\nprint((X_CA_H_train.shape), y_CA_H_train.shape)\nprint((X_CA_H_test.shape), y_CA_H_test.shape)",
        "matched_tutorial_code_inds": [
            2314,
            6902,
            4942,
            3002,
            3004
        ],
        "matched_tutorial_codes": [
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "top = mpl.colormaps['Oranges_r'].resampled(128)\nbottom = mpl.colormaps['Blues'].resampled(128)\n\nnewcolors = np.vstack((top(np.linspace(0, 1, 128)),\n                       bottom(np.linspace(0, 1, 128))))\nnewcmp = ListedColormap(newcolors, name='OrangeBlue')\nplot_examples([viridis, newcmp])\n\n\n<img alt=\"colormap manipulation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormap-manipulation_004.png\" srcset=\"../../_images/sphx_glr_colormap-manipulation_004.png, ../../_images/sphx_glr_colormap-manipulation_004_2_0x.png 2.0x\"/>",
            "sr_lle, sr_err = (\n    sr_points, n_neighbors=12, n_components=2\n)\n\nsr_tsne = (n_components=2, perplexity=40, random_state=0).fit_transform(\n    sr_points\n)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sr_lle[:, 0], sr_lle[:, 1], c=sr_color)\naxs[0].set_title(\"LLE Embedding of Swiss Roll\")\naxs[1].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss Roll\")\n\n\n<img alt=\"LLE Embedding of Swiss Roll, t-SNE Embedding of Swiss Roll\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_002.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_002.png\"/>",
            "sh_lle, sh_err = (\n    sh_points, n_neighbors=12, n_components=2\n)\n\nsh_tsne = (\n    n_components=2, perplexity=40, init=\"random\", random_state=0\n).fit_transform(sh_points)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\naxs[0].set_title(\"LLE Embedding of Swiss-Hole\")\naxs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\n\n\n<img alt=\"LLE Embedding of Swiss-Hole, t-SNE Embedding of Swiss-Hole\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_004.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_004.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Creating listed colormaps"
            ],
            [
                "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss Roll"
            ],
            [
                "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss-Hole"
            ]
        ]
    },
    "1268717": {
        "jupyter_code_cell": "import numpy\nFandango_mean = movies['Fandango_Stars'].mean()\nMetacritic_mean = movies['Metacritic_norm_round'].mean()\n\nFandango_median = movies['Fandango_Stars'].median()\nMetacritic_median = movies['Metacritic_norm_round'].median()\n\nFandango_STD = numpy.std(movies['Fandango_Stars'])\nMetacritic_STD = numpy.std(movies['Metacritic_norm_round'])\n\nprint(Fandango_mean, Metacritic_mean, Fandango_median, Metacritic_median, Fandango_STD, Metacritic_STD)\n",
        "matched_tutorial_code_inds": [
            163,
            1401,
            2280,
            3459,
            4689
        ],
        "matched_tutorial_codes": [
            "import pickle\n\nab_test_results = []\nfor env in ('environment A: mul/sum', 'environment B: bmm'):\n    for b, n in ((1, 1), (1024, 10000), (10000, 1)):\n        x = ((b, n))\n        dot_fn = (batched_dot_mul_sum if env == 'environment A: mul/sum' else batched_dot_bmm)\n        m = (\n            stmt='batched_dot(x, x)',\n            globals={'x': x, 'batched_dot': dot_fn},\n            num_threads=1,\n            label='Batched dot',\n            description=f'[{b}, {n}]',\n            env=env,\n        ).blocked_autorange(min_run_time=1)\n        ab_test_results.append(pickle.dumps(m))\n\nab_results = [pickle.loads(i) for i in ab_test_results]\ncompare = benchmark.Compare(ab_results)\ncompare.trim_significant_figures()\ncompare.colorize()\ncompare.print()\n\n\n\nOutput",
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import pandas as pd\n\nforest_importances = (importances, index=feature_names)\n\nfig, ax = ()\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n\n<img alt=\"Feature importances using MDI\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_forest_importances_001.png\" srcset=\"../../_images/sphx_glr_plot_forest_importances_001.png\"/>",
            "import matplotlib.pyplot as plt\n\nf = (figsize=(7, 5))\nfor index, image_index in enumerate(uncertainty_index):\n    image = images[image_index]\n\n    sub = f.add_subplot(2, 5, index + 1)\n    sub.imshow(image, cmap=plt.cm.gray_r)\n    ([])\n    ([])\n    sub.set_title(\n        \"predict: %i\\ntrue: %i\" % (lp_model.transduction_[image_index], y[image_index])\n    )\n\nf.suptitle(\"Learning with small amount of labeled data\")\n()\n\n\n<img alt=\"Learning with small amount of labeled data, predict: 1 true: 2, predict: 2 true: 2, predict: 8 true: 8, predict: 1 true: 8, predict: 1 true: 8, predict: 1 true: 8, predict: 3 true: 3, predict: 8 true: 8, predict: 2 true: 2, predict: 7 true: 2\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_label_propagation_digits_002.png\" srcset=\"../../_images/sphx_glr_plot_label_propagation_digits_002.png\"/>",
            "import matplotlib.pyplot as plt\nplt.figure(1)                # the first figure\nplt.subplot(211)             # the first subplot in the first figure\nplt.plot([1, 2, 3])\nplt.subplot(212)             # the second subplot in the first figure\nplt.plot([4, 5, 6])\n\n\nplt.figure(2)                # a second figure\nplt.plot([4, 5, 6])          # creates a subplot() by default\n\nplt.figure(1)                # first figure current;\n                             # subplot(212) still current\nplt.subplot(211)             # make subplot(211) in the first figure\n                             # current\nplt.title('Easy as 1, 2, 3') # subplot 211 title"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->6. Saving/Loading benchmark results"
            ],
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "sklearn->Examples->Ensemble methods->Feature importances with a forest of trees->Feature importance based on mean decrease in impurity"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Plot the most uncertain predictions"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with multiple figures and axes"
            ]
        ]
    },
    "692454": {
        "jupyter_code_cell": "students.hist(figsize=(20, 20));",
        "matched_tutorial_code_inds": [
            3861,
            3874,
            4131,
            4145,
            4130
        ],
        "matched_tutorial_codes": [
            "sns.heatmap(X.corr());",
            "smt.seasonal_decompose(y).plot();",
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips);\n",
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips);\n",
            "sns.lmplot(x=\"total_bill\", y=\"tip\", data=tips);\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Seasonality"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models",
                "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Conditioning on other variables",
                "seaborn->Statistical operations->Estimating regression fits->Conditioning on other variables"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models",
                "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models"
            ]
        ]
    },
    "225745": {
        "jupyter_code_cell": "mov3 = mov2[(mov2.Studio == 'Buena Vista Studios') | (mov2.Studio == 'Fox') | (mov2.Studio == 'Paramount Pictures') | (mov2.Studio == 'Sony') | (mov2.Studio == 'Universal') | (mov2.Studio == 'WB')]",
        "matched_tutorial_code_inds": [
            1714,
            3623,
            3798,
            3704,
            3684
        ],
        "matched_tutorial_codes": [
            "rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n'soc.religion.christian'",
            "team\nAtlanta Hawks        0.585366\nBoston Celtics       0.585366\nBrooklyn Nets        0.256098\nCharlotte Hornets    0.585366\nChicago Bulls        0.512195\ndtype: float64",
            "m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Parameter tuning using grid search"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ]
        ]
    },
    "587296": {
        "jupyter_code_cell": "car_list = glob.glob('datasets/vehicles/**/*.png')\nncar_list = glob.glob('datasets/non-vehicles/**/*.png')\nprint(len(car_list), len(ncar_list))",
        "matched_tutorial_code_inds": [
            4167,
            4031,
            4073,
            4152,
            4053
        ],
        "matched_tutorial_codes": [
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "diamonds = sns.load_dataset(\"diamonds\")\nsns.displot(diamonds, x=\"carat\", kind=\"kde\")\n",
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ]
        ]
    },
    "645460": {
        "jupyter_code_cell": "sns.jointplot(x=\"% Vacant\", y=\"PRICE\", data=merged4_MortgageListings, kind = 'reg', size = 7)\nplt.show()",
        "matched_tutorial_code_inds": [
            3822,
            4047,
            4024,
            4033,
            4029
        ],
        "matched_tutorial_codes": [
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\",\n)\n",
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", style=\"smoker\"\n)\n",
            "sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", errorbar=\"sd\",\n)\n",
            "sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\",\n    size=\"size\", sizes=(15, 200)\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Showing multiple relationships with facets",
                "seaborn->Plotting functions->Visualizing statistical relationships->Showing multiple relationships with facets"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ]
        ]
    },
    "804978": {
        "jupyter_code_cell": "pd.set_option('display.max_rows', 100)",
        "matched_tutorial_code_inds": [
            6834,
            3648,
            3671,
            3699,
            3698
        ],
        "matched_tutorial_codes": [
            "pd.get_dummies(hsb2.race.values, drop_first=False)",
            "pd.IndexSlice[:, ['ORD', 'DSM']]",
            "pd.DataFrame(js['features']).head().to_html()",
            "pd.concat([temp, sped], axis=1, join='inner')",
            "pd.concat([temp, sped], axis=1).head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->Concat Version"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->Concat Version"
            ]
        ]
    },
    "503330": {
        "jupyter_code_cell": "print(ox.linreg_ridge_lu(y,X, 5.0))",
        "matched_tutorial_code_inds": [
            5865,
            5232,
            5863,
            6919,
            5875
        ],
        "matched_tutorial_codes": [
            "print(stats.t.fit(fat_tails, f0=6))",
            "print(res3.f_test(R))",
            "print(stats.norm.fit(fat_tails))",
            "print(res.forecast(steps=2))",
            "print(prestige.head(10))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Small group effects"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Specifying the number of forecasts"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ]
        ]
    },
    "390910": {
        "jupyter_code_cell": "reviews = pd.merge(reviews,\n    (reviews.\n     groupby(['business_id']).\n     size().\n     reset_index().\n     rename(columns={0:'business_review_count'})\n    ),how='left'\n    )",
        "matched_tutorial_code_inds": [
            5694,
            3776,
            4027,
            3917,
            5716
        ],
        "matched_tutorial_codes": [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + age + yrs_married\",\n    data=data,\n    family=sm.families.Poisson(),\n)\nres_o = glm.fit()\nprint(res_o.summary())",
            "tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\",\n    hue=\"size\", palette=\"ch:r=-.5,l=.75\"\n)\n",
            "daily = (\n    indiv[['transaction_dt', 'transaction_amt']].dropna()\n        .set_index('transaction_dt')['transaction_amt']\n        .resample(\"D\")\n        .sum()\n).compute()\ndaily",
            "pd.DataFrame(\n    np.column_stack([[r.llf, r.deviance, r.pearson_chi2] for r in results_all]),\n    columns=names,\n    index=[\"llf\", \"deviance\", \"pearson chi2\"],\n)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->original data"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison"
            ]
        ]
    },
    "59583": {
        "jupyter_code_cell": "crawl.createindextables()",
        "matched_tutorial_code_inds": [
            2828,
            4187,
            5304,
            3723,
            6801
        ],
        "matched_tutorial_codes": [
            "survey.data.info()",
            "sns.axes_style()\n",
            "res.plot_cusum()",
            "flights.dep_time.head()",
            "res.forecast_components(12)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data"
            ]
        ]
    },
    "1447862": {
        "jupyter_code_cell": "#draft plot\nax = weeklymean.plot(figsize=(12,4), fontsize=8, linewidth=1.0, legend=False)\nax.set_title('MTA Ridership in NYC by week', fontsize=10)\nax.set_xlabel('Week', fontsize=8)\nax.set_ylabel('Ridership', fontsize=8)",
        "matched_tutorial_code_inds": [
            6416,
            4775,
            1476,
            6437,
            6436
        ],
        "matched_tutorial_codes": [
            "# Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "# plt.figure creates a matplotlib.figure.Figure instance\nfig = plt.figure()\nrect = fig.patch  # a rectangle instance\nrect.set_facecolor('lightgoldenrodyellow')\n\nax1 = fig.add_axes([0.1, 0.3, 0.4, 0.4])\nrect = ax1.patch\nrect.set_facecolor('lightslategray')\n\n\nfor label in ax1.xaxis.get_ticklabels():\n    # label is a Text instance\n    label.set_color('red')\n    label.set_rotation(45)\n    label.set_fontsize(16)\n\nfor line in ax1.yaxis.get_ticklines():\n    # line is a Line2D instance\n    line.set_color('green')\n    line.set_markersize(25)\n    line.set_markeredgewidth(3)\n\nplt.show()\n\n\n<img alt=\"artists\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_artists_004.png\" srcset=\"../../_images/sphx_glr_artists_004.png, ../../_images/sphx_glr_artists_004_2_0x.png 2.0x\"/>",
            "# x, y\n0.000000000000000000e+00,0.000000000000000000e+00\n1.000000000000000000e+00,1.000000000000000000e+00\n2.000000000000000000e+00,4.000000000000000000e+00\n3.000000000000000000e+00,9.000000000000000000e+00\n4.000000000000000000e+00,1.600000000000000000e+01\n5.000000000000000000e+00,2.500000000000000000e+01\n6.000000000000000000e+00,3.600000000000000000e+01\n7.000000000000000000e+00,4.900000000000000000e+01\n8.000000000000000000e+00,6.400000000000000000e+01\n9.000000000000000000e+00,8.100000000000000000e+01",
            "# Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Personal consumption', xlabel='Date', ylabel='Billions of dollars')\n\n# Plot data points\ndata.loc['1977-07-01':, 'consump'].plot(ax=ax, style='o', label='Observed')\n\n# Plot predictions\npredict.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='r--', label='One-step-ahead forecast')\nci = predict_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\npredict_dy.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='g', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='g', alpha=0.1)\n\nlegend = ax.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjIAAAEWCAYAAAB8GX3kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydeXzU5bX/32f2yc4OCSAqiAtakVVxob0KrdWK1r3tz1qXtmoXa2lre6t2ta2t93axvaJil9tqba/iXlBbdxEELC6AAioQEAjZk9nn+f1xZpJJTGCyAQnn/XrlNTPPfJdnBjEfzvmcc8Q5h2EYhmEYRn/Es683YBiGYRiG0V1MyBiGYRiG0W8xIWMYhmEYRr/FhIxhGIZhGP0WEzKGYRiGYfRbTMgYhmEYhtFvMSFjGMY+QUR+LyI/3Nf72B8QkTdEZPa+3odh9EdMyBhGP0VE3hWRiIg0ish2EblbRIr29b6M3dORgHPOHeWce3ofbckw+jUmZAyjf3Omc64IOA6YBvxnVy8gIr5e35VhGMZewoSMYQwAnHOVwOPAJAARKRWRu0Rkm4hUisgPRcSbee+zIvKCiPyXiFQDN4nIeBF5RkTqRKRKRP6avbaInCAiyzPvLReRE3Lee1pEfpC5XoOILBGRoTnv/01E3s+c+6yIHJXvZxKRK0RkTea6b4rIcZn1IzL3rc2kZD6Rc87vReQ2EXk0c97LInJo5j3JfOYdmf2sFpHs9/W0iFyec53PisjzOa+diFwlIm9nrvsDETlURF4SkXoRuU9EApljZ4vIFhH5dua7fFdEPpV570rgU8A3MpG0hzPr74rIqZnnQRH5bxHZmvn5bxEJtrv2dZnPsU1ELs33OzWMgYgJGcMYAIjIGOB0YFVm6Q9AEhgPTAbmAJfnnDID2AgMB34E/ABYAgwCRgO/zlx3MPAo8CtgCHAr8KiIDMm51sXApZlrBYCv57z3ODAh895K4M95fp7zgJuA/weUAJ8AdomIH3g4s9fhwJeAP4vIxJzTLwK+l/ks6zOfj8x3cDJwGFAGXADsymc/GT4KTAFmAt8AFqCiZAwqIC/KOXYkMBSoAC4BFojIROfcAvQ7+Jlzrsg5d2YH9/lO5h7HAh8CptM20jYSKM1c+zLgNhEZ1IXPYRgDChMyhtG/WSQitcDzwDPAj0VkBPAx4KvOuSbn3A7gv4ALc87b6pz7tXMu6ZyLAAngIKDcORd1zmWjER8H3nbO/Slz7D3AWiD3F/Ddzrm3Mte5D/0FDIBzbqFzrsE5F0OFyYdEpDSPz3U5+st+uVPWO+feQ3/BFwE/cc7FnXP/BB6hrYi43zm3zDmXREVDdj8JoBg4HBDn3Brn3LY89pLlp865eufcG8DrwBLn3EbnXB0q2Ca3O/67zrmYc+4ZVAyen+d9PgV83zm3wzm3ExVln8l5P5F5P+GcewxoBCZ2cB3DOCAwIWMY/Zt5zrky59xBzrmrMmLiIMAPbMukX2qB29EIRpbN7a7zDUCAZZl0zecy6+XAe+2OfQ+NBmR5P+d5Myo0EBGviPxERDaISD3wbuaYoeyZMcCGDtbLgc3OuXRX95MRPb8BbgO2i8gCESnJYy9Ztuc8j3TwOtdoXeOca2q3x/I879P+O29/7q6MSMvS8hkN40DEhIxhDDw2AzFgaEbklDnnSpxzuf6UNmPvnXPvO+eucM6VA58Hfisi44GtqDDKZSxQmcc+LgbOAk5FUyHjMuuS52c4tIP1rcAYEcn9f1e++8E59yvn3BTgKDTFND/zVhNQkHPoyHyutxsGiUhhuz1uzW5jD+e2/85zzzUMox0mZAxjgJFJlywBfiEiJSLiyRhTT+nsHBE5T0RGZ17WoL9sU8BjwGEicrGI+ETkAuBINJ2zJ4pRQbULFQk/7sLHuBP4uohMyZh0x4vIQcDLqOj4hoj4RXuvnAncu6cLisg0EZmR8dk0AdHMZwR4FThHRAoyAu6yLuy1M74nIgEROQk4A/hbZn07cMhuzrsH+E8RGZYxTt8A/G8v7McwBiQmZAxjYPL/UOPtm6gw+TswajfHTwNeFpFG4CHgK865d5xzu9BfwtehguQbwBnOuao89vBHNC1SmdnH0nw375z7G2rS/QvQACwCBjvn4qjx92NAFfBb4P8559bmcdkS4A70+3gv83l+nnnvv4A4KjL+QJ6m5N3wfuY+WzPX+kLOHu8Cjsyk/RZ1cO4PgVeA1cBrqEnaGgcaRieIc3uKchqGYRj5kokS/a9zbvSejjUMo+dYRMYwDMMwjH6LCRnDMAzDMPotlloyDMMwDKPfYhEZwzAMwzD6LQNyWNzQoUPduHHj9vU2DMMwDMPoJVasWFHlnBvWfn1ACplx48bxyiuv7OttGIZhGIbRS4hI+y7jgKWWDMMwDMPox5iQMQzDMAyj32JCxjAMwzCMfosJGcMwDMMw+i0mZAzDMAzD6LeYkDEMwzAMo99iQsYwDMMwjH6LCRnDMAzDMPotfSZkRGSMiPxLRNaIyBsi8pXM+mAReUJE3s48Dsqsi4j8SkTWi8hqETku51qXZI5/W0Qu6as9G4ZhGIaxn+Ec1Nfj6USz9GVEJglc55w7ApgJXC0iRwLfAp5yzk0Ansq8BvgYMCHzcyXwO1DhA9wIzACmAzdmxY9hGIZhGAMU56ChAd55BzZv3vtCxjm3zTm3MvO8AVgDVABnAX/IHPYHYF7m+VnAH52yFCgTkVHAXOAJ51y1c64GeAL4aF/t2zAMwzCMfUiugNm6FerqYPPmTg/fK7OWRGQcMBl4GRjhnNume3XbRGR45rAKIHenWzJrna23v8eVaCSHsWPH9u4HMAzDMAyjb3EOmpth+3aIx6GgAEIheOYZ+N3vOj2tz82+IlIE/B/wVedc/e4O7WDN7Wa97YJzC5xzU51zU4cN+8BwTMMwDMMw9kecg6YmeO892LIFGhvh9tvhrrv0/dNPh5//vNPT+1TIiIgfFTF/ds7dn1nenkkZkXnckVnfAozJOX00sHU364ZhGIZh9FeyEZisgGlogDvvhJNOgv/+b3j7bQAWvV3LrJeSeEaM/1BHl+nLqiUB7gLWOOduzXnrISBbeXQJ8GDO+v/LVC/NBOoyKajFwBwRGZQx+c7JrBmGYRiG0R9pboZNm/QH4Omn4eSTNfJy/PGwZAn8/OcsWlfN9U9torIx2XF+hr71yMwCPgO8JiKvZta+DfwEuE9ELgM2Aedl3nsMOB1YDzQDlwI456pF5AfA8sxx33fOVffhvg3DMAzD6AsiEdi5Ux/TaUilIBCAigo47jiYPx+OOUaPTae55flKIskPuEna0GdCxjn3PJ3qJ/6jg+MdcHUn11oILOy93RmGYRiGsdeIRKCqSr0wAPfdB7/5DcyZAz/7mYqYP/1J30un9Xhga1Nyj5feK1VLhmEYhmH0bxatquSWxevYWhuhvCzM/LkTmTf5A0XEbYlGNQLT3KyemPvvh1//Gt5/H2bNgvPOaz3WudZIzZAhUFpKedlmKmsju72FCRnDMAzDMHbLolWVXH//a0QSKQAqayNcf/9rAB2LmWgUdu1SA28gAMXF8P3vazXS9OkqZk44QY/NFTCDB0NZGfhUnsyfO7HNfTvChIxhGIZhGLvllsXrPiAmIokUtyxe11bIxGKaQmpsBBF4/HE44gj1vVx2GcyerVVJIq1VS9kITI6AyZK99i2L17Gtk72ZkDEMwzAMY7ds7SS907Iei2kEpr4ePB6tOrr1Vnj3Xfjc51TIVFToT66AKSuDQYPA7+/03vMmVzBvcgVy/foVHb1vQsYwDMMwjN1SXhbu0KtSXhqCbdtUwPh88NJL8OMfw/r1cOSRsHChGnqhNYWUSql42YOAyZc+7+xrGIZhGEb/Zv7ciYT93jZrYZ8w/5gSrUQqKoJwGN58E7xeWLAAFi+GuXP14EhE003FxXDwwTB8eK+IGDAhYxiGYRjGHpg3uYKb5x1FRWkQASoKfdx84gjm7XwD5s2Dxx7TA6+6Cp54Aj7+cU0xZQVMYaEKmBEj1Pzbi1hqyTAMwzCMjkmntQKpsZF5JVHmnTNGIy5Ll8L134JXX4Vx41rFSTCoj5EIJJNQUqKVSNn1PsCEjGEYhmEYreSIF+rq9LXPp9OoReDqq2HRIhgzBn7xC/jkJ1vTRJEIJBIqYIYM6VMBk8WEjGEYhmEc6HQmXkTguec0dfTjH6tAOf10mDkTLrigNRITjaqAKSrSyqRQaK9t3YSMYRiGYRyI7Em8PPKI+l2amzU99PbbMGWK+l+yRKMQj6uAKS/fqwImiwkZwzAMwzhQ2J14aW5WM+769XDFFSpezj4bzjxTJ1LnNquLxVTAFBTAqFFasbSPMCFjGIZhGP2MLs092lPk5eGH4ckntd/LbbfB+PHqgZk8ua14yV4nlVLhMnbsPhUwWUzIGIZhGEY/Iq+5R1nR0dCgzeraG3a//32dNp1NG51zjkZfskyb1vY66bRWK5WVaRppH6SQOsOEjGEYhmH0Izqfe7SWeRMHfVC8iMCzz8JTT8FPf6oVRkOGqHg544wPpo06Ey/BoF5rP8OEjGEYhmH0IzqfexSFzZvbipdHHtG0UTby8sUvwoQJWkKdSz8TL7mYkDEMwzCMfkR5WYjK2ugH1g8OprQEurgYXnwRrrxy94bdfixecjEhYxiGYRj9gUQCmpuZP3kw1z+7jUjKEUpEmb1xBZ946wXmbFwO2z4NN90EM2bAX/+q/V46M+z6fP1WvOTSZ0JGRBYCZwA7nHOTMmsfAv4HKALeBT7lnKvPvHc9cBmQAr7snFucWf8o8EvAC9zpnPtJX+3ZMAzDMPYr0mntlltTo8MZRZh3xBAI+PHMn89/rH6awkSUWNlgfOd+srXHi9cLJ57Yeo0BJl5y6cuIzO+B3wB/zFm7E/i6c+4ZEfkcMB/4rogcCVwIHAWUA0+KyGGZc24DTgO2AMtF5CHn3Jt9uG/DMAzD2LfEYmrara1VARIIwPbt6nf5/OeZN3EwTBsHR5wLZ5xBsLO00QAVL7n0mZBxzj0rIuPaLU8Ens08fwJYDHwXOAu41zkXA94RkfXA9Mxx651zGwFE5N7MsSZkDMMwjIFFMqlRl5oaFTJer6aTHnlE00QrVuja3Lk6Sfq73217/gEkXnLZ2x6Z14FPAA8C5wFjMusVwNKc47Zk1gA2t1uf0dGFReRK4EqAsWPH9t6ODcMwDKMTutSYriOc09RRba1GYES0R0txMSxfDhdeqOJkwgT4z//UAY3Dh7eef4CKl1z2tpD5HPArEbkBeAiIZ9Y7+rYd4Olk/YOLzi0AFgBMnTq1w2MMwzAMo7fIqzFdZ8Ri2mm3tlYjMX4/VFfD3/8Oo0frQMZJk+Cii7Tfy+TJrcLEORUvyeQBK15y2atCxjm3FpgDkPHAZCdPbaE1OgMwGtiaed7ZumEYhmHsMzpvTLeuYyGTSmnqqLpahYwn82/1xYs1dfTiiypEPvMZFTLhMPzwh63nx+N6nohOoS4t1ejNAShectmrQkZEhjvndoiIB/hPtIIJNDrzFxG5FTX7TgCWoZGaCSJyMFCJGoIv3pt7NgzDMIyO6LwxXc56NnpSV6fddkEjJ8XF+vxzn1MhM24czJ8P550HFTkiKJVq7fUSDuuE6YIC9coYQN+WX98DzAaGisgW4EagSESy7QTvB+4GcM69ISL3oSbeJHC1cy6Vuc41qCnYCyx0zr3RV3s2DMMwjHwpLwtT2YGYKS8La/QkmzpKJDQF1NAA//d/+vOXv+jU6Kuv1sZ1M2a0TR3FYq3nDRmiqaNAYC9/wv6BODfw7CRTp051r7zyyr7ehmEYhjGAae+RAQj7Pdx8SjnzxoRaoyZPPQX33QdPP62RlRkzNGV05JFtL5hIqIABFS5lZRqFOcBTR1lEZIVzbmr7devsaxiGYRjdIOuDueUfa9laF6W80Mf8KYOZd2iJRmQKCuD99+Gqq2DECLjmGjj/fC2dzpJbdRQMwsiRUFhoqaMuYELGMAzDMLrJvMPKmFd8kL5obob774ev3wfDhsE996gwefRROOqotuIkGtUITHbGUXGxChmjy5iQMQzDMIyukkpBVZU2r3v9dbjrLu26m0xqqXR2VADAMcfoYzKpAgY0dTRypFYdeTrqNGLkiwkZwzAMw+gKzc2wbZuacktKYNUqeOUVuOwyLZueOLH12HRafS/JpJp1R4zQ1JHPfv32FvZNGoZhGEY+pFKwa5f2gXn9dRUoH/4wfOELWkYdDrceG4upT8brVbFTUqLRF6PXMSFjGIZhGHsiEtEoTFMT3HYbLFgAxx4Ls2erWAmH2/Z8KSzUUQLhsKWO+hgTMoZhGIbRGem0RmF27YJ16+DrX4f167X77ne/q6XRyaQKHb8fhg5V/4vfv693PqBIu3Sn75mQMQzDMIyOyEZhkknYvFlnHo0YodVIJ5+sxzQ3q5gZPVrLra3nS6+SSqdojDeys3kneOiwJt2EjGEYhtHv6fEU6lzSafXBVFWpz2XoUG1e94MfqJgpKVFx09wMgwbp+9b3pVdJpBLUReuoidbgnENUIHaoEk3IGIZhGP2aHk2hbk80qlGYSAQWLoQ77tA+MIccAp/9rB7T3KyPY8aoF8boNaLJKLWRWupidXjEQ9gfxiMeIomO51qBCRnDMAyjn9PlKdQdkU7rXKQdO2DTJh3guHo1nH22NqyD1unVpaXa8M5KqHsF5xyRZISq5ioiiQg+j4+iQFE2CrNH7E/BMAzD6NfkNYV6d8RiGoWJx+HPf4af/Uw77S5Y0NrYLhJRsVNR0Tq52ugRaZemMdZIVaSKeCpO0BukONj179aEjGEYhtGv2e0U6t3hnHbm3blTm9UVFUFlJZx6Ktx8s3pfslGY4mI1+loUpsck00kaYg3sat5FyqUI+8OEfN3vsWN/IoZhGEa/Zv7ciR1MofYyf+7Ezk+KxXSgY3OzTqaePBmmToWbblLjrkhrFKa8XIWMVST1iHgqrgbeSA0AYX8Yr2fPJum0S/Pkxic7fd+EjGEYhtGvaZlCnU/VknOtXpht2+Db34aXXoJLL1Uh4/OpeGloUPEyfLj1hOkh0WSU6kg19bF6fB4fhYHCvPwvkUSEv6/5OwtWLGBjzcZOjxPnXG/ud79g6tSp7pVXXtnX2zAMwzD2J+Jx2L5dU0UPPAA//KFGX773PTj//NYoTCqlAx0tCtNtnHM0J5rZFdnVYuAN+/eQ6suwq3kXv3/19/z+37+nOlLNMSOO4dJjL+XaWdf+26Xcse2Pt4iMYRiGMbBxDurrNZXk98M//6mRmJNPhp//XA286TQ0NmpTu5EjLQrTTVoMvM1VJNIJgr78Dbzrq9dzx8o7+PsbfyeainLqIafyhSlfYObomUSTUa7l2g7PMyFjGIZhDFwSCRUwjY0qZsaMgTPP1PlHZ56pEZdoVI8bOVKb3VkUpssk00nqY/VUN1eTJk3IFyLk37OB1znHy5Uvc/uK21myYQlBb5BzjzyXK467gglDJuR17z4TMiKyEDgD2OGcm5RZOxb4HyAEJIGrnHPLRJNlvwROB5qBzzrnVmbOuQT4z8xlf+ic+0Nf7dkwDMMYIDinPpf339c5STfcAKtWwdNPw+DB8IlPtEZhwmEdMRAI7Otd9zviqTi10VpqIjVtGtjtiWQ6yWNvP8btr9zOq9tfZVBoENfOvJZLPnQJwwqHdWkPfRmR+T3wG+CPOWs/A77nnHtcRE7PvJ4NfAyYkPmZAfwOmCEig4EbgamAA1aIyEPOuZo+3LdhGIbRn0kk1Mzb0ABPPqnDHaNRuP761uZ22SjM8OG6ZlGYLhFJRKiOVNMYb8Tr8ebdwK4x3si9r9/LHSvvYEv9FsaVjePH//Fjzj/y/Lw9NO3pMyHjnHtWRMa1XwZKMs9Lga2Z52cBf3TqPF4qImUiMgoVOU8456oBROQJ4KPAPX21b8MwDKOfkkpphGXHDhUp8+fDY4/BccfBf/0XjB/fGqkJhdQbEwzu6133G5LpJE3xJmoiNUSTUQK+QN7+l20N27j71bv50+o/UR+rZ1r5NL43+3ucdshpeZVg74697ZH5KrBYRH4OeIATMusVwOac47Zk1jpb/wAiciVwJcDYsWN7d9eGYRjG/kssBnV1WlYNmioqKFDD7re/DV/4glYnxWJauTRsmA57tCjMHkmlU0SSEWojtTQlmhCEoC9ISahkzycDb+58k9tX3M6Dax8k5VJ8bPzH+PyUzzOlfEqv7XFvC5kvAtc65/5PRM4H7gJOpeOJlm436x9cdG4BsAC0/Lp3tmsYhmHslzinzex27dJHv1+nVf/yl/ClL+mQx9tuU7HinEZqAgEYN86iMHsgO/uoPlpPfawehyPgzT/64pzj2fee5X9W/A/PvvcsYV+YzxzzGS4/7nIOKjuoy/tJppPEU3Ho5Pf/HoWMiPwM+CEQAf4BfAj4qnPuf7u8G7gE+Erm+d+AOzPPtwBjco4bjaadtqDppdz1p7txX8MwDGMgkExqH5hduzR95PfDq6/qpOonn9TIywknqJAR0QhMLKZRmLIyrVYyPoBzjlgqRmOskZpoDWmX7lLzOlDj76K1i1iwYgFrqtYwvHA43zrxW3z66E8zKDyoy3uKJWPEU3H8Xj8VxRWQJtXRcflEZOY4574hImejwuI84F9Ad4TMVuAUVIx8BHg7s/4QcI2I3Iuafeucc9tEZDHwYxHJfgNzgOu7cV/DMAyjP9NR+igYhDPOUCEzdCh85Svw6U/DqFGtERufDw46SD0xxgeIp+ItvpdEOoHX48278ihLbbSWP6/+MwtXLeT9pveZOGQit869lXkT5xH0dS365ZwjmoySSCco8BcwpmgMYV94t2IqHyGT7Qp0OnCPc646H3UmIveg0ZShIrIFrT66AviliPiAKBlPC/BY5vrr0fLrSzMfqFpEfgAszxz3/azx1zAMw9g/WbSqMr9xAXsiN30Uiago2b5dDbzXXKMRl3nzdLzAmWe2pozica1KGjpUS60tCtOGZDpJc7yZ6kg1sVQMr3gJ+oJ59X3JZXPdZu5YeQf3vH4PzYlmThp7Er+Y+wtOOeiUvKM4WbKdgNMuTUmwhEHhQXkPktzjiAIRuRk4G00tTQfKgEecczO6tMu9iI0oMAzD2DcsWlXZ4QDHm885On8xk0xqZVF1tT73+3Ue0sKF2pXX79c00vjxHzwvGlXBM2qURm0MoK1ptznZDA6CviB+b9c7GK/atorbV9zOo28/ikc8nDXxLK6cciWThk/q3r4SEUSEQaFBlIZKO92TiKxwzk1tv77biIyIeICH0X4v9c65lIg0o+XShmEYhtGGWxavayNiACKJFLcsXrdnIRONavqork5fh8NQWQmf/Sy88472fLnuOvjUp2DEiLbnZf0yI0dCUZFFYWhr2m2IN+Ccw+/1UxQo6vK1djbt5MF1D/LAmgd4dfurFAeK+fyUz/O5yZ+jvLi8y9dLpBJEk1F8Hh/Di4ZTHCjudhn2boWMcy4tIr9wzh2fs9YENHXrboZhGMaAZmttpEvrpNOaNspNH73/vk6mPukkHSlw8MEqYD7+8dbuu9nz0mkd7jhqlPpgDvCS6s5MuwX+gi6ne5riTTy+/nEeWPMAz216jpRLcdSwo7hp9k1ceNSFeVcx5RJNRkmkEgS9QcqLyykMFHbJj9MR+XhklojIJ4H73UAclW0YhmH0GuVlYSo7EC3lZe3SPB2lj5YuhbvvhmeegUMP1cdgEP70p7bnRSIacRk0CEpLbcAjvWPaBY2UPPPeMzyw5gH+seEfRJNRRpeM5ovTvsg5h5/DxKETu7y3rIE3mU5S6C9kVNEoQr5Ql4VVZ+QjZL4GFAJJEYmivV2ccy6/bjiGYRjGAcP8uRM79MjMn5v5BRiNauVRXZ2KkVBI5x/98Ifw7ruaGpo/X6uPcn/RZdNHgYBGXyx9RCKVaBkVEEvF8Ign72GNuTjnWLFtBQ+seYCH3nqI6kg1ZcEyzj3yXD55xCeZWj61W1GTtEsTSURIuzRloTLKQmVdrmLKhz0KGedc12NHhmEYxgFJ1gfTpmrptAnMm1AK772n0RS/H7ZuhSFDoLBQe78MGwbf+AacfnprhCWdVgGTSln6KEMsGSOSiFAbrSWWirV02u1Ommd99XoeWPMAD6x9gPfq3iPkDXHqoafyySM+yexxswl4uzdEM5lOEk1G8eBhSMEQSoIl+Dx91393j1VLAJk+LhPQqdWAzlLqs131EKtaMgzD2A9IJLSjbjZ95PPBc8/BXXfB88/Dl78M3/ymllnnipNEQgWM16vpo5KSAzZ9lE3LNMWbqI/Vk0gn8IiHgDfQrYqjHU07Wky7/97+bwRh1thZnHPEOZw+/vRuCaIs8VScWDKG3+NnaMFQioJFPfa/5NKtqqXMiZej3XhHA68CM4GX0IZ2hmEYxoFKOq3RkmRSH7MCJB7X56lUa/roz39WAbNpk0ZWvvUtuPhivU5WxEQieq1gEMrLNVpzAKaPUukU0WSUhlgDDfEG0i6N1+Ml6O16rxfQidO5pt20SzNp+CRuOOUGzpp4FiOLRvZov5FEhGQ6SdgfZnTJ6G4Zi3tCPrGerwDTgKXOuQ+LyOHA9/p2W4ZhGMY+x7lWkZIVKtnBi7GYCpnscSIqOjwejbyEw7B5M2SH+C5bpuLkO9+Bj35Uj4EPpo8GDTog00dZv0t9rJ7mRDMO1+1qo+z1nn7vaR5Y8wCLNywmmowypmQM10y/hnMOP4cJQyb0aL/Z0u5UOkVxsJjB4cF5N7DrbfIRMlHnXFREEJGgc26tiHTdtmwYhmHsXzjXKlKSSf3JipR4XF/nIqLpHq9XxYbH0ypi3nsPnnhCIy6bN+vrdeu08mj8ePj1r9sOa8yKIo9Hu+8WFx9Q6SPnHPFUnOZEM3XROvW7iOD3+Ls036j9NV/Z9oqadtc9RE20hrJQGecfdT7nHH4OU8un9jhSkkwniSaiAAwKawO77nppeot8hMwWESkDFgFPiEgNOjPJMAzD6A9kBUoq1dq+PytU0um20Y+sUAkE2s4nqqqCf/xDRUpWrGzaBL/6FcyeraLlxhs1EjN2rPZ/Oe88HRMAKlUGn0YAACAASURBVGKc03tn00ejRh1Q6aPd+V164k1ZX72e+9fczwNrH2BT3SZC3hBzxs/h7MPP7pFpN0s21ZV2aQLeACOKRlAYKOxTA29XyKdq6ezM05tE5F9AKToF2zAMw9ificW01Dk7aDGb+smmf3IjJE1NOsOovVD52tfgoou0Sd03v6nnjR6tQuWjH9XKI9Dmdf/+t75u/6/+bPM659S4W1Z2wAxx7G2/S5ZNdZtafC+v7XgNj3g4ceyJXDvzWj42/mM9EkagpdOxZIxkOonf42dIwRAK/YV9Uj7dUzoVMiIyuIPl1zKPRYANbzQMw9jfcE5FQ1VVa6fcoiIVE48+quIkV6icdx589asqer76VRUhI0ZoVOX446EiM1bgsMPU5zJypEZs2hMOt51t5Fxr+sjrVYFzgKSPetvvAiosVm9fzeINi3liwxOsqVoDwDEjjuHGU27krIlnMaJoxB6usnuyXYETKY0UlYZKKQmWEPQG96p5t6vsLiKzAnBoA7z2OOCQPtmRYRiG0XVSKS113rVLBYTPBy+/DPX1cM45GoW59lpN7QwapEJl0iRt/w+69uyzGm0JdvCv7kCgVdTk3jObnkql2pZRi+h1KiqgoGDAp49iyViv+l1A2/m/sOkFFm9YzJMbn2R703Y84mFGxQxuOOUG5hwyh4MHHdwre4+n4ghCcbCYkUUjCflCvVo63Zd0KmSccz3/dgzDMIy+JZHQLrk1NSok0mm4/34tdd64EY4+Gs4+W4XF4sWtQxXbI6JjAXJJp1urltJpvT7oYzY9FQioYAkGNfLi87X6bAY4LWMBojUtUYye+l2qI9U8ufFJntjwBE+/9zTNiWYK/YXMHjebOYfO4SMHf4TB4Y4SJl0jkUoQS8ZwOAr9hQwrHEbYF+724MZ9ye5SS8ft7kTn3Mre345hGIaRF9GoNppraFBREQ7DAw/Ad7+rwubYY1n+nZ9yXWASm3/9KuXFfuafUM68XBGTjaRkf3KFikirUCks1Eefr61Q2Y/TDX1FMp1smWmUHQsQ9AV7VHq8sWYjSzYsYcmGJSzfupy0SzOycCSfPOKTzD10LsePOb5XSptzTbtBb3C/M+12l93t/he7ec9hDfEMwzDyZtGqyrZt++dObGnnnzfpNDQ3q/8lFlNR8dZbGmUpLNQ+LSedBJdfzqLiQ7j+n5uJNKs4qWxIcP2TmyASYd74Ur2ex9NWqPj9rULF5zsghUpHpNIpIskINZEamhPNPRoLkL3eyvdXsmT9EpZsXML66vUAHDH0CL48/cvMOXQOx4w4pld8KWmXJpqMkkqnWjruFgYK93nJdG+S14iC/oaNKDAMY39i0arKDgcp3nzO0fmJmfaTor1e7dly552wciVceaWWPucw6+7XqWxIfOBSFaVBXrjuZBUqA9y30hOyAw/rYnU0xhpBIOANdFsARBIRnn3vWZZsWMITG59gV2QXPo+PmaNnMvfQuZx2yGmMKR3TK3vPnTbtEQ+DQoMoChbt96bdPdGTEQV+4IvAyZmlp4HbnXMf/BtiGIZhfIBbFq9rI2IAIokUtyxet3shE4tpmihbPh0Oa6v/22/XoYvjxunU6PPPbz0nU+q8tQMRA7C1LqbRF+MDZKMX9dH6llLpgDfQbcPuzqadPLHxCZZsWMJz7z1HNBWlOFDMRw7+CHMOncOHx32Y0lBpr+0/ltSKIxGhOFBMSaiEsC/cr8VLPuSTGPsd4Ad+m3n9mcza5bs7SUQWAmcAO5xzkzJrfwWyXYHLgFrn3LGZ964HLgNSwJedc4sz6x8Ffgl4gTudcz/J+9MZhmHsB2ytjeS/ni2frq7W3i5erxp5x2T+tb5mjVYa/ehHcOqprVGVZFJ9MyIwaBDlpSEq66IfuHx5WfgDawcy2ehFQ7yBumgdaZfudqm0c463q99myYYlLN6wmFXbVuFwVBRXcPHRF3Paoacxc/TMXk3rJFIJokn9cy70FzK8cDghX6hfmna7Sz5CZppz7kM5r/8pIv/O47zfA78B/phdcM5dkH0uIr8A6jLPjwQuBI4CyoEnReSwzKG3AacBW4DlIvKQc+7NPO5vGIaxX1BeFqayA9HSRlSk01o+XVWllUh+P6xaBXfcAf/8p/aAOfZY+MlP2vZiyY4U8Pu1/0tREXi9zP/o4R2ms+bPtQkz2fEADbEG6mJ1JNNJfB4fYX+4yyXHaZfmla2v8Pj6x1myfgnv1r0LaH+X646/jjnj53Dk0CN7LSqS3Xs8FQcg5AsxqmgUBYGCfm/a7S75fOqUiBzqnNsAICKHoFGT3eKce1ZExnX0nuif6Pm0GobPAu51zsWAd0RkPTA9895659zGzHn3Zo41IWMYRr9h/tyJnYuKRKLV/5KdFv3II+p/WbtWW/xfd11rRMbvb9vqPxzW3i8FBW3MudmUVY8NxgOIlnLpSA2JdAKvx0vIFyIsXYtSOed4Y+cbLFq7iAfXPcjWhq0EvAFmjZnFlVOv5LRDTqO8uLzX9p3bZVcQCvwFDAkPIewP4/cO/AaDeyIfITMf+JeIbESb4x0EXNrD+54EbHfOvZ15XQEszXl/S2YNYHO79RkdXVBErgSuBBibnbZqGIaxH9ChqPjIwcwb5YV33mntyVJQoOmkH/xAK5BuvRXmzWttUJdt9Z9OQ2mpNrHrqHldzn0PZOECmnppTjRTE6khno7jIVMu3Y3xABtqNvDg2gdZtHYRG2o24PP4OPmgk/nWrG8x59A5PR4LkEsqnSKWipFKp/CIh5JgCUWBogMubZQP+cxaekpEJqDeFgHWZiInPeEi4J6c1511D+4oxtdhmZVzbgGwALRqqYf7MwzD6FXmTa5g3rHlWj69a5c+NqVVyNxxh3pfFi/WUuh//EMjMNkISyKhEZhsq/+SEhU+Rock00ma483UxmqJJCItvV6KfB00AtwDlQ2VPLT2IRatW8TrO15HEGaOnskVU67g4xM+3ivN6bIkUgniqThpl8bv8TMoNIjCQGG/rzbqa3bXEO+cTt46VERwzt3fnRuKiA84B5iSs7wFyK07G03rhO3O1g3DMPoH7f0vPh+8+KIKmKVLNRJz4YUqVgoKdHwA6OtEQquMRo1S/4uVTH+AVDpFPBUnkozQGG8kkoggIt3usrureRcPv/UwD657kGWVywA4dsSx3HDKDXzisE8wqnhUr+w7O9somU4C6ncZXjicsD88oPq89DW7k/RnZh6HAycAT6GRkw+jJdjdEjLAqWhUZ0vO2kPAX0TkVtTsOwFYlrnfBBE5GKhEDcEXd/O+hmEYe5dUSv0vu3a1+llCIZ0yfcUV6m254QYVMaWZMtxs1VIqpdGZUaP0HPsXeQvZyEVzopnGeKPOCRLBIx78Hn+3xEtDrIHH1z/Og2sf5LlNz5FyKQ4bchjzT5jPWRPP6pWZRtDO7yJCUaCIkmAJIV/ogDXr9pTdzVq6FEBEHgGOdM5ty7wehVYS7RYRuQeYDQwVkS3Ajc65u1AxkptWwjn3hojch5p4k8DVzrlU5jrXAIvR8uuFzrk3uvohDcMw9irJpA5r3LVLhUljI9x9t3bgvfRSOO00NfOedlpriiiVUgEDUFamP9bvBecciXSixajbFG9qiWB4PV78Xj/Fvu55UyKJCE+98xQPrn2Qp955ilgqxpiSMXxx6hc56/CzOGLoEb2S0kmmk8SSMdIujdfjbeN36S+DGfdn9tjZV0Rez/aBybz2AKtz1/Y3rLOvYRj7hHhcm9fV1GgKaPt2WLAA7rtP37vkEu3/0v6c7LiBIUOguPiAGLjYGWmXbumN0pRQ4ZJ2aQD8Xj8Bb6BHv/wTqQTPbXqORWsXsXjDYhrjjQwrGMaZh53JWYefxZRRU3pFvMRTceLJOA5HwBugLFRGgb+AgDdgfpdu0u3OvsDTIrIYjaI4NKLyr17en2EYRv8lFlPxUlenIqSoCH73O7j5ZhUo550HX/gCHHKIHp9bPh0Mdlg+faCQ9bdEk1H1tyQ1KiVItxvTtSft0iyrXMaitYt45K1HqInWUBosbREvJ4w+oceVQC1+l5RGi8L+MCOLRlqJ9F4gn6qla0TkbFpHFCxwzj3Qt9syDMPoG3pleGOWSETTR42NKlhef13HBhQUwOTJ8PnPw+WXa0oJ1PQbjWoaqaREy6dDPZ9q3J9IppNqzE1EaIg1EE9p1MIjHvxeP4X+7o0DaI9zjtXbV7No3SIeWvcQ7ze+T9gXZs6hc5h3+DxOOegUgr7OS9fzvUfuTKPiQDHFhcVWIr2XsaGRhmEcMPR4eCNoNCVbQh2JaATmuefgN7/RAY5XXw3f/nbbc7L+l8z4AEpL23bnHaDk+luaE800xhpJpHUGlM/jw+/196rBNTsi4MG1D/Lgugd5p/Yd/B4/s8fNZt7h85hz6BwK/AU9vkcspTONsuKlJFRifpe9QE9SS4ZhGAOCbg9vBI2mNDXBzp1aEh0M6gTqX/0K3n5b+7786EdwwQWt5ySTrWJn2DCNwhwA/pdkOklDrIGaSA1JlzHmipeAN9CtRnS7u8+anWtYVrmMZVuXsbxyOdubtiMIJ4w5gaumXcXHxn+MQeFBPb5XLBnT6iiEomARIwpHdGukgdH7mJAxDOOAoUvDG7Ok01pCXVWlwkREDbkATz+tKaXbboMzzmitQMo2sPP7D6j+L7FkjLpYHbWRWkSEkC9EyNN7wqU50czKbStZXrmcZVuXsWLrCpoSTQCMLhnNrDGzmFYxjbmHzmVE0Yge3y93plGBr4BhhcMI+8KWNtrP2F1DvKecc/8hIj91zn1zb27KMAyjL8hreGOWZLK1B0zW2/LHP8LChXDvvTBpkpp5c026sZj+hEJQUaF9YAa4gdc5RyQZobq5msZ4o/pcAr3jc6lqrmoRLcsrl/Pajtda5g0dMewIzjvyPKZXTGdqxVQqintnFEMilSCWjOFwatgtHHlAD2TsD+zuT2aUiJwCfCIzrLHNf5XOuZV9ujPDMIxeZrfDG7MkElp9VF2tIqSmRkuo//IXTRPNmdPqbyks1MdoVMuos115w+EBL2DSLk1jrJGqSBXxVJygN0hJqKTb13PO8U7tOypcMqmijTUbAQh5Qxw78li+OPWLTK+YzpRRUygNlfbWR2nT5yXoDTKiaAQF/gKrNuon7E7I3AB8Cx0LcGu79xytk6sNwzD6BbudCB2Pq2iprdU0UGGhrs2dq1VJ8+bBVVfBxIzoyS2hLirSIY8HQAVSIpWgId5AdXM1KZci7A8T8nX9cydSCd7Y+QbLKpe1RF2qmqsAKAuVMb1iOhdPuphpFdM4evjRPa4wak8qnSKajOpcI6+foQVDKQwU2miAfkg+DfG+65z7wV7aT69gVUuGYeRNNKrRl4YG9bi88QY8/DDcdJNGVZYsgSOP1F4v0Fq1lOcE6oFCNBmlNlJLfbweDx5C/q5V6TTGG1m5baVGWyqXsXLbypaeMWNLxzK9YjrTy6czvWI6hw4+tE9MtGmXJpqMkkqn8Hv8lIXLKPQX9rpIMvqGblctOed+ICKfoLWPzNPOuUd6e4OGYRh7jew8o6oqFSV+P6xYoSXUL72k4wEuvVR7wsyZo+ek03qOczB48AFRQu2coznRzK7ILpoTzfg9+fd5qY5U8+LmFzXisnU5b+x4g5RL4REPRw47kosmXcS0imlMK5/Wa0MYOyJ3tpHP46MsWEZRsMgmSg8g9ihkRORmYDrw58zSV0RklnPu+j7dmWEYRm8Tj2uaqKZGU0KBgKaSLr9cIzEjR8KNN8KnPtXqf8n2gPF4dIRASUlrddIAJZVO0RhvZFfzLhLpBEFfkJJgfv6XN3a+wV0r72LR2kXEUjFCvhDHjTqOL03/EtMrpnPcqOO6NdSxK7RvVFcaKqU4oI3qTLwMPPL52/hx4FjndNiFiPwBWAWYkDEMY/8nG0nJbWCXTsOWLXDEESpeBg2CW2+Fs89uHdSY7QHj88GIEeqDGeA9YOKpOPXRemqiNVq14wvn1fcllU7x1DtPccfKO3hx84uEfWEumHQB5x5xLseMOKbPTbPZxnuJVIK0S1ujugOMfP9ZUQZUZ573nlXcMAyjr4jF1PdSW6tRFb8fNm6Ee+6BRYtUmLz4ogqXv/619bzsEEe/Xw28hYUDvgdMJBGhJlpDQ6wBr8eb93yjxngj971xH3etvIt3695lVNEovnPSd7ho0kW90oSuM9IuTTwVJ5lK4nCICGFfmNKCUkK+EEFf0MTLAUQ+QuZmYJWI/AstwT4Zi8YYhtEL9OrcI1DB0tSk5t1YTCMooZA2rrv5ZnjzTX398Y9r+ijX45ItoQ6HD4ghjmmXpjnRTFVTFbFUDL/Xn3fKZ1PdJu5+9W7uee0eGuINTBk1hW+e+E0+Nv5jfRJ9yQ6WTDktm/eIh0J/IYVhNeraROkDm3zMvveIyNPANFTIfNM5935fb8wwjIFN+7lHlbURrr//NYCuiRnnVLTU1ekPqEBZvVrNuhWZa3m98OMfaxl1aU5gORLRNFJhoXbhDXfQHG8AkR0fUB2pJplOEvKF8hIwzjmWVS7jzpV38o8N/8AjHs6YcAaXHXcZx406rlf3mEglSKQ1TQQ6l6k4WEyhX8ujfR6fCRejBRsaaRjGPmHWT/7ZYZfdirIwL3wrjzZVyaRGX3bt0iZ2Pp+mkf72N+28+9578OUvwze/qWIn9xdf+ynUgwcP+BLq3PEBAGF/fq3246k4D617iDtX3slrO16jLFTGp4/5NJd86BLKi8t7vC/nnKaJ0pomAgj5QhQFigj5Qi3CxTBsaKRhGPsV3Zp7lC2brq1V/4uIpoqCQfj85+Hxx1WkHH88XHcdnH66npc7QiAe1+hMWZmKmMDAbICWLTuOJqPUx+qJJqN4Pd68xwdUNVfxp9V/4o///iM7mnYwYfAEfnrqT/nkEZ8k7O9+1Co3TeScwyMeCvwFDA4PbkkTmb/F6AomZAzD2Cd0ae5R+7Jpvx927IBnn9V+L6DVR1ddBRdeCAcf3HpuKqXRl3Ra00fDh2v6aIAZeLORjaxwiSQj4EBECHgDeftf3tz5JnetvIsH1j5ALBXjI+M+wuVzL+fkg07uVjonmU6SSCVIpnUKtt/jpyhQ1NJF1+/xW5rI6BH59JE5FNjinIuJyGzgGOCPzrnaPZy3EDgD2OGcm5Sz/iXgGiAJPOqc+0Zm/XrgMiAFfNk5tziz/lHgl4AXuNM595Muf0rDMPY79jj3KFs2XVOjKaRs2fTixZo6euklXZszR30w3/9+68WzvplsymnoUK1SGkAN7LIlx9FElMZEI03xppbSY783/8Z1oNGbJzc+yZ0r7+SFzS8Q8oW4YNIFXDb5MsYPHt/lvaXSKSKJCA5HyBeiLFRG2B+2NJHRJ+TzX9T/AVNFZDxwF/AQ8Bfg9D2c93vgN8Afswsi8mHgLOCYjDAanlk/ErgQOAooB54UkcMyp90GnAZsAZaLyEPOuTfz+3iGYeyvdDr36Mih2nE3WzYdCEBxMSxbBpdcAvX1auL91rfgvPM0EpMlmdToi3N6zsiRA2qAYyKVIJaK0RhrpDHRSCqdQhD8Xn/eJdO5tJRPr7qLd2u1fPrbJ36bi4++uFvl0/FUnGgiSsAbYETRCIoCRXn5cAyjJ+QjZNLOuaSInA38t3Pu1yKyak8nOeeeFZFx7Za/CPzEORfLHLMjs34WcG9m/R0RWY92EwZY75zbCJCZwn0WYELGMAYA8yZXqKDJLZt+912NtMRi8OCDmgo6/XRtXjdnDlxwAcyc2Zoayh3e6Pdr87rCwgHRfTc7lbk50UxDrEHTMwJe8fao0dvmus0sfHVhS/n0caOO4xuzvsHp40/vcvl0bhfdsD/MmNIx3RJVhtFd8vmbnhCRi4BLgDMza92Nzx4GnCQiPwKiwNedc8uBCmBpznFbMmsAm9utz+jowiJyJXAlwNixY7u5PcMw9grOadonGlXTblOTrgcC8Prr8Je/wGOP6fvnnKNCprgYfvnL1mskEip2oNW4Gwz26+hL1qAbSUaoj9YTT8dxzuHz+Ah4A3l12e0M5xzLty7njpV38I/1/0AQzjjsDC6bfBlTyqd0a6+RRIS0S1MaLKUsXNatKdiG0VPyETKXAl8AfuSce0dEDgb+twf3GwTMRPvS3Ccih6D9adrjgI7+udFhvbhzbgGwALT8upv7Mwyjr8imfZqaVLykUio6/H6NoIjA1Vdr193iYjj/fLjoIjj66NZr5JZNB4OaOios7LejA7IdaiOJCA3xBqKJKA6H1+Ml4A1Q5Cvq8T0aYg0s3rCYu1bdxertqykLlnHV1Ku45NjulU8nUgliyRge8TC0YCjFwWLzvRj7lHwa4r0JfDnn9TtAdw23W4D7nTavWSYiaWBoZn1MznGjga2Z552tG4axP5NKacSkuVl9LYmErvt8KkLeeUerjp59VuccDR4M554LH/6wdt7NbUzXvmy6uLhf9n3JrSxqjDfSlNBIlKCVRUXBnguXSCLC8q3LeWHzC7yw6QVWb19NyqUYP3g8Pzn1J5x7xLndKp+OJqPEk3FCvhCjikdRGCi0MmljvyCfqqVZwE3AQZnjBXDOuUO6cb9FwEeApzNm3gBQRcZALCK3ombfCcCyzL0mZKJAlagh+OJu3NcwjL4mWykUiWjEJWu69Xo1ZRQKqf/ll7+E556Dbdv0vIMOgvXrYfp0FTFZskIoldJxAf2wbDorXGLJWEtlUbYJaVcrizojkUrw6vuv8vzm53lh0wus2LaCeCqOz+Pj2JHHcs30azj5oJOZXjG9y8IjN31UHCxmVNEomyBt7HfkEw+8C7gWWIGWRueFiNwDzAaGisgW4EZgIbBQRF4H4sAlmejMGyJyH2riTQJXO6dDNUTkGmAxWn690Dn3Rr57MAyjj4nHVbA0NmrKKNtBNxBQwfHyyxpxmTZNfS5+PyxZAieeCCefDCedBO09bdFoa9n04MFaNt1Pmta1j7g0J5rbCJfeMMGm0ine3PlmS8RlaeVSmhPNCMJRw4/i0mMv5cSxJzK9YjpFge5FeJLpJNFkFEEYFBpEaai0zydYG0Z32eOIAhF52TnXocF2f8VGFBhGH5FMapSkqUnFS1KbnOH3t4qN3/wGnnkGVqxQoRMIwJe+BF/7mr6fTn8wqpL1z4AKl7KyflE23ZFwyfZyyRp0eypcnHOsr17fIlxe3PwitTFt4zV+8HhmjZnFiWNPZObomQwOD+7RvWLJGLFUjIA3wNDwUAoDhVY+bew39GREwb9E5BbgfiCWXXTOrezF/RmGsT+STremi+rrVZiApouCQXj/fY241NbqXCOAhx/Wx8su04jL9Olt/S4eT2saKpnU5/2kbDprzo0lYzTEGmhONgPqcfF5fL1Wdry5bjMvbH6B5zc9zwubX2BHk3aqqCiuYO74uZw49kROGHMCI4tG7uFKe8Y5RyQZIZVOUegvZETRCMK+sKWPjH5DPv/HyEZjclWQQ70uhmEMABatqmxtTFcaYv4pY5k3JqRGXWj1uRQVwQsvaH+X556DTZv0/QkTNOoiAg89pH6Y9sTj+uOcipnCwlbT7n6aOupMuAAEvIFe8bgA7GjawYubX2wRLpvq9HsdVjCMWWNmMWvsLGaNmcXY0rG9JjCy3XcBysJllAZLCfr6n4HaMPKpWvrwno4xDKP/smhVJdffv5pIIg1AZV2U6x99G2aXM+/QEnjlFRUtX/mKRktefFHFygkn6KDGk06CQw5pTQNlRUwyqcIlW2YdCmnUJRjcb/u95JZDN8YbiSQjLYMNe8ucC1AbreWlzS9pumjzC7y16y0ASoOlHD/6eK447gpmjZnFYUMO6/XISFaY+Tw+hhcNpzhQbOkjo1+TT9VSKWrUPTmz9AzwfedcXV9uzDCMvUAqxS2Pr2kRMQDDGquZ98bTjLrv37DlDfWu+HzaVXfKFB3MeO21H0wBZdNQWd9MIACDBmnFUSCwX/Z6yZ0Q3RBvIJqM9plwWb51OUs3L+XFLS/y2vbXcDjCvjAzKmZw3pHnMWvMLCYNn9QnoiLbfTeRTlDgL2B0yWjrvmsMGPJJLS0EXgfOz7z+DHA3cE5fbcowjD4mmVRfS00NW+tjjGiowpdOU1k6nJENu/jO0wt5e8gY+NSnNOJy/PGaVgJNCYGmiOJxrTDKllkXF+txweB+53VJpVMk0gkSqQSRZIRIIkIsqba/7ITo7lb5tGdn005ernyZl7e8zNLKpazZuQaHI+ANMHnkZL52/NeYNWYWk0dNJuDtm7RaIpUgnoqTdmlEhJJAiXXfNQYk+VQtveqcO3ZPa/sTVrVkGJ0Qj7cIGDwe2LaNh792M3NXPcljh8/iq2fOx5NOMbyxBm/FKF64dFLb8xMJvUY6ramhrM8lFFLD7n7wL/zsVOjsL/LmRDORpPZCyfYF93q8+L3+XutIW9lQqaJly1KWblnKhpoNAIR8IaaWT2Xm6JnMrJjJsSOP7VYzunxIpBIk0omWQZIhf4jiQHHL1GlrXmf0d3pStRQRkROdc89nLjQLiPT2Bg3D6ENiMRUvdXUaOXn3XS2TfvRRTvcH+Ovkufx26tkApD1e6gYP4+YTytXfEo9rBCfrcxk6VKuQsr1i9iHJdFJ/gedEWeLpODhaWv37PL4eDVhsj3OOd2vfVdFSuZSXt7zM5nodCVcSLGFa+TQunHQhMypmcPSIo/ss4pJMJ4mn4i3CJegLMjg8mLAvTNAXNOFiHDDkI2S+CPwh45URoBr4bF9uyjCMXiIahV27tNOuz6cRFI8H7r9fe71cfTXeyy+noNqLe3Er0pCgvMjH/ClDmFcR0AhMaan6XILBfeZzSbs0iVSipVFbJBEhmoqSSmuPTo94WkRLb8wnan/vt3a9xdItS1vSRdubtgMwODyYmRUzueK4K5gxegZHDD2iz4yzWdGWTKsHKegNMig0iAJ/AQFvwAy7saS/hgAAIABJREFUxgHLHlNLLQeKlAA45+r7dEe9gKWWjAMa57TvS1WVPnq98PzzGoG57jo45RSNzni9OjEaNPISiWjUJdfn4t+73Vydc6RcqiUtFElEiCQjxFNxBAGhpdmc3+PvE7NqMp3kzZ1v8tKWl3h5y8u8XPkytVFtQDeyaCTHjz6eGaNnMLNiJuMHj+8zw2wqnWqJuGT9NcWBYgoCBQS9QRMuxgFHl1NLIvJp59z/isjX2q0D4Jy7tdd3aRhG93FO+77s3KmpJI9HxwH89rewdi2MGaProNVEoCmjSESjNSNGqIDZi1GXbNVQU7yJSDLSUjVERhtkBUtf9jeJJWOs3r66JU20fOtyGuONAIwrHcfcQ+eqx2X0TMaUjOkz4ZL9LpLppDbY8/ooCZZQ4C8g6AvahGnD6ITd/c3IlCZQvDc2YhiG0qY5XVmY+XMnMm9yRecnpNM6LqCqSv0s4bBGVebNg+XLYeJE+NWv4BOfaI2wJBIqYAIBGDVKBcxe8rukXZpoMkp9tJ6GeANpl8bn8fVqZ9zd3XtL/RbWVq1l9fbVvFz5Miu3riSa0vEIE4dM5JwjzmFmxUymV0xnVPGoPt1LLBkj5VI45/B7/BQHiyn0F5pwMYwukHdqqT9hqSWjv6LN6V4jkmidzxr2e7n5nKM/KGZSqVYBk0yqOLn/frjkEhUsDz6ooubUU1tFSiymYicYhGHD1PuyFyqNUukU0WSUulgdTfGmFvHSl5OUq5qrWFO1hnVV61hbtZa1VWt5a9dbNCWaAE1RHTXsqJZoy/SK6T2eVbQ7ss32EqkEIoJHPBQHiikMFBL0Bm0oo2Hsge6kln61uws6577cGxszDKOVWxavayNiACKJFLcsXtcqZJJJnXtUXa3RmKYmWLgQ/vAHNfWOHw+zZ8NZZ7VeJDtROhzWFNNeGMiYNebWRmppTmhrf5+396MuTfEm1u1qFSvZn12RXS3HDA4P5vChh3PhpAuZOGQihw89nIlDJ/Za35iOyHpcsuZcv0enXxcVFhHwBvqsmskwDjR2F7tcsdd2YRgGAFtrO+5ssLU2okKkrk4FTFYI3Hwz3HuvRlo+/vH/3955x0dV5f3/faZPeqcHQpGSECIdQUSQUGQRsCCsCjZELLi76opY1roqrro+tkddZH1EwB+IoAgoIFUQQQFDkRJaCEJIT2Yy7Z7fH3dmSCCUhAQInPfrdV+5c+65956bk8x85nu+BR58ENq3149JeVzAhIdDw4aV10CqQTw+D06PkwJXAWVefbnGbDATZj13weD2ucnMz6wgVn7P/T1YlwggxBxC69jWpLdIp3WcLljaxLYhPjT+nO9/JgJ5XDSpZ0k2GY77uFiMFmVxUShqiVMKGSnlf8/nQBQKBTSMsnOoEjHTMNwCe/fqAsbthpgY3Rrzyy8wfDjcfz+0aKF3DkQt+Xx66HR0tL6UVEsEoovynfm4fC4EAoup+llyy/ux7Di2I7g0tCd/Dx7NA+gioUV0C9Lqp3Fryq20iW1Dm7g2NIlscl7yp5RPuhdYnrea9HDoQAI65eOiUJwfTre09DXBPJgnI6UcWisjUiguYx4b0PpkHxmj4LHOsXrk0bvvwk8/6Vt4OHz99fFSAJqmCxhN08VLVFStVJWWUgYz5haUFeD2uTEIgx4ebK1abIBP8/HL4V/YfGQzvx/7ne3HtlfwYwFoHNGYNnFtuK7FdUHB0jy6+Xmt1Bx4Zq/mRfrfFu0mO5EhkdhMNpXHRaG4gJzuK8Pr520UCoUCgGGp9cHpZMqyTLJLPDQMNfKa9QA9n3sR1q7Vxcnddx8/wWTShYvDoVtrYmJ0K0wN1zmSUuLy6WHShWWFeDQPBmHAarISbqqaeHF6nKw6sIpFuxfxfeb35DnzgON+LCOTRwZ9WFrHtq6yOKoJAo65Xp8uXAzCQKgllBhzDDaTDbPRrDLnKhQXCSpqSaG40AR8WYqKdB8YOJ6Mbts26N8f6teH++7TizgGijYGktgZDBAbqye3q8EcMIGKyaXuUgpdhXg1L0aDsVrJ2PKd+Szdu5TFuxfzw74fcHqdRFgjuC7pOtJbptOtUTfiQ+IvWDXmEx1zTQYToeZQwqy6Y25tJd9TKBRnT3Wilr6QUt4ihPiNSpaYpJSpZ7jhVGAIcFRKmeJv+wdwL5Dj7/aklPJb/7FJwN2AD3hYSrnY3z4Q+DdgBD6WUr5yhmdVKOoGHo8ePp2fr++bTPrrOXN0C8tjj0G7djBtGvTufdzPxePRhY/ZrCexCw+vsRwwgRwvJe4SCssKg2HSVpMVu6hascNDxYdYvHsxi3YvYl3WOnzSR/3Q+tySfAsDWw6ke+PuFyRyJ1DuwCd9ep0iITAZTMEcLsoxV6GoW5zO/jzR/3NINa89DXgH+PSE9jellBWWrYQQ7YBbgWSgIbBECHGF//C7QH8gC/hZCDFfSrmtmmNSKC4sAT+W/Hw9bNpg0K0oa9bo0UfLlumWlquu0sOsTSbdIgO6k6/LpQuYhg2P10061yGVS1BX5C5CSonJYMJutldp+URKye+5v7No9yIW71nMliNbAGgV04oJXSYwsOVAUuulnrclGU1qeDUvXs2LJjWklHrGXL8wC2TMVY65CkXd5nRRS4f9P/cH2oQQcUCuPIv1KCnlSiFEs7Mcxw3ATCmlC9grhNgNdPUf2y2lzPTff6a/rxIyirqFy6XneCko0IWKxaJbUgBefx3efBMSEmD8eBg58ngEUuBcl0vP/dK4cY0ksdOkhtPjpMilZ9cNiJdQc2iVllACzroLdy9k8e7F7CvcB0CnBp2YfPVk0luk0zKm5TmN9UycKFgAkAQFS4Q1AqvRGswerJxyFYpLi9MtLXUHXkGvdv0C8H9AHGAQQtwhpVxUzXs+KIS4A9gA/E1KmQ80AtaV65PlbwM4eEJ7t1OMdxwwDiAxMbGaQ1MojlPlUgEn4vPpVpe8PF2IGI36stCCBbr15ZFHoG9fXbikpur75Z10nU7dKhMaqvvI2Ku2tHPScCrJrms2mqssXsq8Zaw+sJrFuxfzXeZ3HHMcw2ww0yuxF+O7jCe9eTr1wuqd01grIyBYfJpPr0ckRFCA2Uw2wi3hWE1WzAazEiwKxWXE6eyp7wBPApHAMmCQlHKdEKINMAOojpB5H10USf/PfwF3ESwRVwEJVGaDrtQaJKX8EPgQdGffaoxNoQhyYqmAQwVOJn35G8DpxUzAcbewUHfeBd36sm2bLl6+/loXKK1a6UIH9Ey7TZro+16vfj7o9Y9iYs4piV158VLiKkEiMRvNVc6uW1hWyLK9y1i0ZxE/7P2BUk8pYZYw+iX1Y0DLAfRt1rfGoosCOVp8mu7DEnh3MAgDNqONcJsSLAqF4jinEzImKeV3AEKI56WU6wCklDuq670vpTwS2BdCfAR843+ZBTQp17UxkO3fP1W7QlFrnFWpgPJU5rhrMOjLQD4fPPCALmyGD4dbb4WOHY8vDwXEj9d73IE3NLTaIdQ+zYfT66SwTLe8gF4aINRSNcvL4eLDLN6zmMV7FvPjwR/xal4SQhMY3nY4A1sM5KomV51zLpdAfhaPz4NEBmsQlRcsgQrYSrAoFIrKON07pVZu/8RUo9WyeAghGgR8b4DhQIZ/fz7wuRDiDXRn31bAevTvYq2EEEnAIXSH4NHVubdCURVOWyogQGWOuwYDrF6tW18yMmDdOl2cTJsGzZvrwiZAwHnXYNBzv0RE6JFJ1fiicGJdI4nEYrRUSbxIKdmdt5tFexaxePdifv3jVwCSopIY13EcA1sO5MoGV56zs26g6nNgeSjMEkZcSBxmo1kJFoVCUWVOJ2Q6CCGK0MWE3b+P//UZbd1CiBlAHyBOCJEFPAv0EUKkoQuhfcB9AFLKrUKIL9CdeL3AA1JKn/86DwKL0cOvp0opt1b1IRWKqnLKUgFR9sodd/PzdbEyezbk5upWlZtvPh5llJKiX6B89l27XY8+CgmpVv4Xr+bF4XZQ5Cqi1FOKEKLKdY2klGz6YxOLdi9i4e6F7MnfA8CV9a/kiV5PMLDFQFrGtDznHCpezYvL60KTGkaDkQhrBGGWMGwmm0osp1AozgmVEE+hqIQTfWQA7CYD/7ymAcMS7ccdd71ePRndqlVw222Qnq477/bpU3FpKFC80WTSs/OGh1erfEBAvBS6CnF6daFV1UrKXs3LT1k/sXD3QhbtXsThksMYhZGrmlzFwJYDGdBiAA3CG1R5bCfi9rlxe91B61CULSpYQFEll1MoFFXlVAnxlJBRKMojpW5l8Xr5atMhpizbS3aRi4ahJh7rHMuwtnF6ocYZM+Cbb+D22+Ef/9AtLHl5EBd3/FonOu5GRelWmCp+iJevKO30OBFCVFm8lHnLWLl/JYt2L+K7Pd+RX5aPzWjjmmbXMKjVIK5Luo5oe3SVxnUigTIGXp+eHdduthNpjcRutqsEcwqF4pypcmZfheKSxusNChbcbl1wuFy61cQv7ofFwLBbmunWF4sF/vMfmPAJ7NunC5Mbb9Sdd0H3c4mLqxHH3UDm2UC0kdPjDFaUrkpkULGrmKV7l7Jw98JgpFGgLMCgVoPo06wPIeaQM1/oNPg0Hy6fC03TEEIQbgknPDQcm8mmfF0UCsV5QQkZxaVLQKh4vbpAcbl0keF26xaUgGUk4KRrMun+Kl4vHDwIe/fq29ixet+tW/V8Lo88AtdfXyOOu4FQ44DVpdRTisvnCmahtZqsVRIvxxzH+G7PdyzctZDVB1fj9rmJD4lneNvhDGo5iKuaXHXOZQE8Pg9un1vPQ2MwE2WNItQSitVkVf4uCoXivKOEjKJuU16sBJZyAv4oWrnAOyF0y0pArGgaZGXpQqVTJ71t3jyYMkUXMV7v8XP794emTfVj5S0rJzruNmqk/zyF466UEq/mxe1zU+Ytw+FxUOYtQ5O6NcMojJgMJsIsZ++sC5BVlMXC3QtZuGshP2f/jCY1EiMTGZs2lsEtB9OxQcdzso4EQ6Q1D1JKrEYrcSFxyt9FoVBcFCgho6gTfPXrIaYs2kF2YRkNwy081i2BYc1CdREhpS5UyosVu11vP3xYd6yNiIDNm/VSAJmZcOCALnZAjzTq0QOio/UijUOGQFKSHi6dlKQ788JxEVPecTcm5pSOux6fB4/mweV1Ueouxel1IpEgwWAwYDZUPTEd6MJiV94uvt31LYt2L+K3o3qivrZxbZnYbSKDWg2iXVy7cxIYFUKkEYRaQokPjcdmsqm6RAqF4qJCvSMpLnq+2niQSXMzcHp1C8uhYjeTlh+Cvk0Y1jpGFxUWCxw5Ah99dHxJaP9+XXT8z//AiBHHrTCtW8PAgbpISUo6Hhrdu7e+VUbA2iOlLlwCJQP8YsGrefH4/KLFo4sWTdOQSIwG3dJSHdESQJMam//YrFtedi8kMz8TgI4NOvLU1U8xsOVAkqKTqnXtAEF/F6khEETaIgmzhGE1WpW/i0KhuGhRQkZxceNwMGXhNpxeDavXzQ1bl5OUn03T/Gxa/ecwFP6h+6w8+KAuNqZO1ZeBkpL0EOikJH3pCODKK2HJktPfT8rjy1SBEgJSVnDc9RmEbmlxFeHwOHB6nHg03bpjEIZg7Z9z9Rfxal7WZa0L5nj5o+SPYJj03VfeXSNh0gF/F4lesyjaFq37uxitaslIoVDUCZSQUVyceDxw7BgUFvJHkQsMRnzCwMuL30ETBg5G1WdfdANaDemnCxTQk8vt2nXm5HLlxUpgaSrQbjTqTrrh4WC1opmMuIWGxwAOr5PS4lw8Pg8CAUKvsGwxWbCJ6tdDClDsKmZrzla2Ht3KpiObWLZ3GQVlBcEw6Sd6PXHOYdLl/V0AbCYbCaEJ2M32c3YCVigUiguBEjKKiwsp9Yy5OTm6mJk2jW+nf8WQP0/BYzTTe/zHHAmLxWcw0ijcTL87U46fG/CRCVwn4Ajs8x13/BVCjyyyWPQQaptNt7YYjUijEY/QcPvcegSROx+3063nspZ6vSKzwYzNdO6i5WjpUTKOZgS3rUe3sq9wX/B4XEgcfZv1ZWDLgVybdO05hUkH/F180nfc38Wq/F0UCsWlgXoXU1w8OJ3wxx96KPP69fDMM7BvH6F9BhCjuThiNJMdkQCA3SR4rEeDistA5ZM7CqGLldBQXayYTMc3v9gJRBC5vC5KykpOiiAyG82EmaoWQXQiUkr2F+6vIFgycjI4Wno02KdpZFOSE5K5JeUWUuJTSElIoV5YvXO6b3l/F4MwqJIACoXikkUJGcWFx+vV6xPl5+siZtIk+PZbPWpoxgwa9+7NpN/zmLImm+wSj55lt1MMw5rYdPESEqIvB1ksJ4mVAD7Np/u1eEpwOBw4PA58mq/GnHFB9zfZlbergmjZmrOVYncxAEZh5IrYK+jdtDcpCSmkxKeQnJBMhDXinH595e9f3t8lxh5DiDlE+bsoFIpLGiVkFBcOKaGoCI4e1S0oERG6ZSU3F/7+d7jvPl2gaBrDGpoZdtsVevbc8mKlkg9oKSUenxu3z43D46DUXYrH50Eig86455q8zeFxBP1ZAlaW34/9jsvnAsBustM2vi3D2w4PWllax7WukWWpAMrfRVGbeDwesrKyKAuU2VAozhM2m43GjRtjNp9daRMlZBQXhrIyfRnJ5YJNm/T8Lh9/rOdlmT1b92MBcDh0/5b69XWhU4lwCeRrKfOWBfO1IAFBcInIarJWe6h5zrwK/iwZRzPIzM/Uc8IA0bZoUhJSuOvKu0hJSCE5Ppnm0c1rJWRZ+bsozhdZWVmEh4fTrFkzZdFTnDeklOTm5pKVlUVS0tmllFDvfIoq8dWvh5iy+HeyC5w0jLLz2IDWDLuy0dlfoPwyUmEhvPoqzJ0LiYmQna0LGYPheP2jyEiIjw8mo/NpPr2qss9NqacUh8eB5nfkDSSZCzWHVuuNV0pJdnH2ccGSo//MLs4O9mkU3oiUhBSGtRmmi5aEZBqGNazVN3rl76K4EJSVlSkRozjvCCGIjY0lJyfnrM9RQkZx1nz16yEmffkbTo+eX+VQgZNJX+pZZc8oZqSE4mI9aR3AF1/A66/rFplAHhi7Xbe+lJbqkURNmyJtNn2JyFlMYVkhLp8rGPociCCqzoe5JjUy8zNPsrTkl+UDIBC0jGlJt0bdgoIlOT6ZGHtMle9VVXyaD6/mxat5lb+L4oKi/tYUF4Kq/t0pIaM4a6Ys/j0oYgI4PT6mLP799ELG5dIFjMOhRxEZjbB2rZ6o7oUXdKde0KOWfD58cbG4wmyUeIspyj+ET/NhEAYsxqpVfw7g9rnZmbuTjKMZ/HbkNzJyMtiWsw2HxwGAxWihdWxrBrUcRHJCMikJKbSLb3fOlaHPRKBgpE/z4ZO+YKFIk8GE3WwnxByi/F0UCoXiDCghozhrsgucVWrH54O8PH0rLoa33tIdeFu0gHfe0cOihQCPB3dJIc5QK0VRFpwyD1ksq5Uht9RdyracbUELy29Hf2Nn7s6gQ2yoOZTkhGRuTb6VlHq6E26rmFa1LhYCJQw0qSGlDEZL2Yw2wm3hWE1WzAYzZqNZLRcp6iTnvOxcCVlZWTzwwANs27YNTdMYMmQIU6ZM4fPPP2fDhg288847NTT6miEsLIySkpILPYzLDiVkFGdNwyg7hyoRLQ2j7BUbpISSEt0K4/XqVaVfeUVv69QJWrRAs1lxeRyUFuVSpJXhjYsGm8BiFIQZzy53y5mccGPsMbRPaE+fZn1ITkimfUJ7mkU1q1WhUH5ZKJCTBsBqtBJuDcdusmM2mjEZTMo5V3HJcE7LzqdASsmIESO4//77mTdvHj6fj3HjxjF58mSSk5NrbOwBvF4vJpP6n6yL1NqsCSGmAkOAo1LKlBOOPQpMAeKllMeE/m7/b2Aw4ADGSil/8fcdAzzlP/VFKeV/a2vMitPz2IDWFd6sAOxmI48NaH28k8ulh1M7HLB7Nzz1lB6V1KMHnhefx9WiKUWlf1BSnAuahiE6BmtMEjbT6cPsCssKWZ+9ni1/bDmtE+7wNsODy0MNwhrU2hr/mZaF7CY7FpMFs0EXLcrXQHEpU+1l59OwbNkybDYbd955JwBGo5E333yTpKQkXnjhBQ4ePMjAgQPZu3cvo0eP5tlnn6W0tJRbbrmFrKwsfD4fTz/9NCNHjmTjxo389a9/paSkhLi4OKZNm0aDBg3o06cPV111FWvWrKFv37588sknZGZmYjAYcDgctG7dmszMTA4cOMADDzxATk4OISEhfPTRR7Rp0yZ4b6/Xy8CBA8/596ioHrUpP6cB7wCflm8UQjQB+gMHyjUPAlr5t27A+0A3IUQM8CzQGT2gdqMQYr6UMr8Wx604BYE3pErNxz6fHol07JiemC48HDl/PhzKwvHGq+QM6o1LcyMKD2F2ewkNj0bExel9K6HMW8bP2T+z+sBq1hxYw+Yjm4NVmVvEtKBrw656Url6KbXuhHuqZSGr0aqWhRQKqrHsfBZs3bqVToGCr34iIiJITEzE6/Wyfv16MjIyCAkJoUuXLlx//fXs37+fhg0bsmDBAgAKCwvxeDw89NBDzJs3j/j4eGbNmsXkyZOZOnUqAAUFBaxYsQKAX375hRUrVnDttdfy9ddfM2DAAMxmM+PGjeODDz6gVatW/PTTT0yYMIFly5YxceJE7r//fu644w7efffdaj+r4tyoNSEjpVwphGhWyaE3gceBeeXabgA+lVJKYJ0QIkoI0QDoA3wvpcwDEEJ8DwwEZtTWuBWnZ9iVjU7+hlVuGUn79ltcTRpQ1LEdJffehHbXMAgPx4og3C3AZIUmjfVsvOXwaT62HNnC6oOrWX1gNRsObaDMV4ZRGLmywZU83PVheib2pEO9DoRaQmvl2QLZf32ar9JlIZvRhtloDi4NKRQKnbNedq4CUspKLZmB9v79+xMbGwvAiBEjWL16NYMHD+bRRx/l73//O0OGDOHqq68mIyODjIwM+vfvD4DP56NBg+NV40eOHFlhf9asWVx77bXMnDmTCRMmUFJSwo8//sjNN98c7Ody6Ykv16xZw5w5cwC4/fbb+fvf/17t51VUn/P6biyEGAocklJuPuEPtBFwsNzrLH/bqdoru/Y4YBxAYmJiDY5acUrcbsjJwV2Qi2vvbsz/eB7bxs24bxhIScrTWCOidQtFWRm4XRAdA1FRYDAgpWR33m5WH9CFy49ZP1LkKgKgbVxbbutwG1cnXk33xt0Js5xbvaMTqRDeHKjP5A/nDjHpkUJmo1ktCykUZ8lZLTtXkeTk5KBICFBUVMTBgwcxGo0n/V8KIbjiiivYuHEj3377LZMmTSI9PZ3hw4eTnJzM2rVrK71PaOjxL0ZDhw5l0qRJ5OXlsXHjRvr27UtpaSlRUVFs2rSp0vPV+8OF57wJGSFECDAZSK/scCVt8jTtJzdK+SHwIUDnzp0r7aOoAaREK3PiKsyl9EgWxcW5RH70KdHTv0SLCCf/5WdwjvgTdoNBX25ylkCIHeIaku06xurts1l1YBU/HviRP0r/AKBJRBOGtBpCr8Re9EzsSVxIXI0MVZNaBcfbE/1YbCYbFqMlaGFRy0IKRfU47bJzNenXrx9PPPEEn376KXfccQc+n4+//e1vjB07lpCQEL7//nvy8vKw2+189dVXTJ06lezsbGJiYrjtttsICwtj2rRpPPHEE+Tk5LB27Vp69OiBx+Nh586dlToMh4WF0bVrVyZOnMiQIUMwGo1ERESQlJTE//t//4+bb74ZKSVbtmyhQ4cO9OzZk5kzZ3Lbbbcxffr0aj+r4tw4nxaZFkASELDGNAZ+EUJ0Rbe0NCnXtzGQ7W/vc0L78vMwVkV5vF7cjmLK8o9RlP8HDo8DaQCTPYzopWuI/r/ZOEaOoOivDyCjIvWoJYeDfG8JP5btYvW+Daw+uJrM/EwAYu2x9EzsSa8mveiV2IumUU3PaXhSyqBg8Wm+oPw1CAN2k51wS3gwhb/yY1EoaodKl53PASEEc+fOZcKECbzwwgtomsbgwYN5+eWXmTFjBr169eL2229n9+7djB49ms6dO7N48WIee+wxPcu32cz777+PxWJh9uzZPPzwwxQWFuL1ennkkUdOGfk0cuRIbr75ZpYvXx5smz59Ovfffz8vvvgiHo+HW2+9lQ4dOvDvf/+b0aNH8+9//5sbb7yxxp5dUTVE0LReGxfXfWS+OTFqyX9sH9DZH7V0PfAgetRSN+BtKWVXv7PvRqCj/7RfgE4Bn5lT0blzZ7lhw4Yae47LDinxlTlxlRRQknuYktI8PNKHMJiwmG2Ebt2JweHAdfVV4PVi2rkbb7s2OL1O1mf/zOo/1rM6fzO/5W5HIgkxh9C9cXd6JerCpW1c22qJiZMEC1RwvLWb7NjMfsFiMNdKrSOF4nJh+/bttG3b9kIPQ3GZUtnfnxBio5Sy84l9azP8ega6NSVOCJEFPCul/M8pun+LLmJ2o4df3wkgpcwTQrwA/Ozv9/yZRIyiekivF3dpEWVFeRTmZVPmdiCFwGS1Y/NA9I8bsa5Yg3XljxgLCvEmNSV7wUw25e9gtbae1d//i425v+HWPJgNZjo26MjfevyNXom9SKufhtl4dlVMy6NJDY/PE0zVD7rjbYQ1ApvJpvKxKBQKhaJWo5ZGneF4s3L7EnjgFP2mAlNrdHAKkBJvmQNXaSEluYcpLs7VI3WMJswWO1HZ+XjbXgFCEPX0a4TMW4AjLpJVg5JZlRrJysgC1n15HSXeUgCSI1txV8rt9GpxLd0ad69Wev9AQUif1K0tBmEgxBxCjDkmKFzUspBCoVAoyqO+yl5GSK8Xl6MIZ4Hu6+JyO0AYMFqt2DFjX7se28ofsa5cgzEnl23z/sNP4QX8MtDA+p6t2Fy2H5f2I7igmaMJwxL70ysmjZ7N+xDToLleQ6kKeHwePJqenwXAZDARbg0nxByiO+EazCoiQKFQKBSnRQmZSxxvmYM+gS/7AAAgAElEQVSykgKKcw9TUnxMDwUzGrFYQwkzRYPFjGXdz0TfOYEdMRqrr7CxamQM6+Jj2fPr3QCYDSbaR7dlbOItdI5tT6eINtQzR+m1kuLi9J9nQEqJ2+eusExkM9mIsccEo4fUEpFCoVAoqor65KjDVFakbWhqPVylRTiLcinMzcbtceohx1Y7ocYQbOt+xrryR7xrVrFqdE9WdqvPxpJf+GWyiQKDGygj1uqkc3wqt8Z1oEt8B9pHtMLmEyA1MBghLEyvYm2360UfK+F0y0RWkxWL0aKWiRQKhUJxzighU0eprEjb32dv4vBeQd8mBowmMxZbCOEh4SAlpQ+NZ8WRX1nb0MeapgY23yXxibmwBVpHtuD6VtfTOb4DneM6kBTWGOHxgNcDCDBYIDJcFy4WS6XiJbBMFIgmMhvMaplIoVAoFLWOEjJ1lNcWbT+pSJvLBzN+dTH82D52/fQtP5HFyl5N2HBsC4evOgJAiLCQFteeBxPS6BzfgY5x7YmyROjJ61wuPQdMWZludQmL14XLCRVhA9FEHs0TzIxrM9mItkVjN9vVMpFCoagRsrKyeOCBB9i2bRuapjFkyBCmTJmC5RQ12mqCl19+mSeffLLWrh8gLCyMkpKSGr9unz59eP311+ncuWKU8qpVqxg/fjxms5m1a9dit1e/fERNUFBQwOeff86ECRPO+Vrq06YO4SotwlGUS/7RAxwudAXbfRTT9sg8EoqWczDiCC00icNfpaHRsQK6xqfROS6VLvEdaBvVShcZUuolBjxe8JSAxQoxMbq/i9UKQhyv8Oxx4tW8CH+b2WDGZrapZSKFQlFrSCkZMWIE999/P/PmzcPn8zFu3DgmT57MlClTau2+50vInG+mT5/Oo48+GqwmfiZ8Ph/GKgZwVIWCggLee+89JWQudaSm4XaWUFqYQ8HRg3g8TgwG3VG3XekKPEVfszKxGJfpEFnNwKhB0+Iobo9NI61dPzo36ETDkHrHL+jzgcsNWhkIg164MS4OrFa8BvwVnn1o7hIEAoPBgNVoJcwahs1sC9YeUonmFIrLkD59Tm675RaYMAEcDhg8+OTjY8fq27FjcNNNFY+Vy5xbGcuWLcNmswU/eI1GI2+++SZJSUk899xzfPHFF8yfPx+Hw8GePXsYPnw4r732GgDfffcdzz77LC6XixYtWvDJJ58QFlaxZtvhw4cZOXIkRUVFeL1e3n//fRYsWIDT6SQtLY3k5GSmT5/OZ599xttvv43b7aZbt2689957GI1GwsLCuO+++/jhhx+Ijo5m5syZxMfHn/Qcw4YN4+DBg5SVlTFx4kTGjRsXPDZ58mS++eYb7HY78+bNo169euTk5DB+/HgOHDgAwFtvvUXPnj1Zv349jzzyCE6nE7vdzieffELr1q1xOp3ceeedbNu2jbZt2+J0nly88+OPP+aLL75g8eLFLFmyhM8++4zHH3+chQsXIoTgqaeeYuTIkSxfvpznnnuOBg0asGnTJrZt23bK51+0aBFPPvkkPp+PuLg4li5desoxbt26lTvvvBO3242macyZM4enn36aPXv2kJaWRv/+/c9JnCohc5Fxonjxel0IIbDawti15QcWbpnDfLmDvXE+jDGQ4EjCab0Dm/cKwoxteLJ/PQa09OdwCVpd3PprkxlfeBhemxmvyYgmQAgJWhlWYSXMEobdZFcVnhUKxQVn69atdOrUqUJbREQEiYmJ7N69G4BNmzbx66+/YrVaad26NQ899BB2u50XX3yRJUuWEBoayquvvsobb7zBM888U+Fan3/+OQMGDGDy5Mn4fD4cDgdXX30177zzTrBA5Pbt25k1axZr1qzBbDYzYcIEpk+fzh133EFpaSkdO3bkX//6F88//zzPPfcc77zzzknPMXXqVGJiYnA6nXTp0oUbb7yR2NhYSktL6d69Oy+99BKPP/44H330EU899RQTJ07kL3/5C7169eLAgQMMGDCA7du306ZNG1auXInJZGLJkiU8+eSTzJkzh/fff5+QkBC2bNnCli1b6Nix40ljuOeee1i9ejVDhgzhpptuYs6cOWzatInNmzdz7NgxunTpQu/evQFYv349GRkZJCUlnfL5Bw0axL333svKlStJSkoiL0/PU3uqMX7wwQdMnDiRP//5z7jdbnw+H6+88goZGRmnLMZZFdQn1UWA1DRcjiJKC3IoPHYIj6cMo9GE2WJja85Wvslby4K933HIlYMpBPoWRPEX49VYWo1i6p4ojpZqJIQaGN85nAHNreB0ovk8eKWG12ZBiw4HqwVMZkxGE3aTHbvJjsVkURWeFQrF2XE6C0pIyOmPx8Wd0QJzIlLKSt+Xyrf369ePyMhIANq1a8f+/fspKChg27Zt9OzZEwC3202PHj1Ouk6XLl2466678Hg8DBs2jLS0tJP6LF26lI0bN9KlSxcAnE4nCQkJABgMBkaOHAnAbbfdxogRIyp9jrfffpu5c+cCcPDgQXbt2kVsbCwWi4UhQ4YA0KlTJ77//nsAlixZwrZt24LnFxUVUVxcTGFhIWPGjGHXrl0IIfB4PACsXLmShx9+GIDU1FRSU1NP/Uv1s3r1akaNGoXRaKRevXpcc801/Pzzz0RERNC1a1eSkpJO+/zr1q2jd+/ewX4xMTEApxxjjx49eOmll8jKymLEiBG0atXqjGOsCkrIXCAC4qUk/wiFxw7h9bqPi5eNP7Bw61fMN+7mUJiGxWCmd/3uTCrsS9+efyYy7nhhtiGdNLzuMrzuMnxaCcUOJyIsAkNoJPaQSMItIapgokKhqHMkJyczZ86cCm1FRUUcPHiQFi1asHHjRqxWa/CY0WjE6/UipaR///7MmDGjwrk//fQT9913HwDPP/88Q4cOZeXKlSxYsIDbb7+dxx57jDvuuKPCOVJKxowZwz//+c8zjlcIwcGDB/nTn/4EwPjx42nTpg1Llixh7dq1hISE0KdPH8rKygAwm49HcgbGDqBpWqXOuA899BDXXnstc+fOZd++ffQpt9RX1S+ip6uxGBoaWqFfZc8/f/78Su/59NNPVzrG0aNH061bNxYsWMCAAQP4+OOPad68eZXGfDrUp9p5RGoazqI8jh38nczNy9m/fR0FOQcxWe385jrAa3P+So9Pr2Fo1mtMDd3JlWXRvB8yks0jvue/fd5i+A2PExnXCKlplDmKKCnIobQ4F2EyE9GoOQ3adKFp+6tp0aIzLRum0CiqCbEhsYRaQrGarErEKBSKOkO/fv1wOBx8+umngO58+re//Y2xY8cSEnLqEijdu3dnzZo1weUnh8PBzp076datG5s2bWLTpk0MHTqU/fv3k5CQwL333svdd9/NL7/8AugCI2BJ6NevH7Nnz+bo0aMA5OXlsX//fkAXHLNnzwb0ZapevXrRpEmT4D3Gjx9PYWEh0dHRhISEsGPHDtatW3fG505PT6+wRBVYeiksLKRRI/1L7LRp04LHe/fuzfTp0wHIyMhgy5YtZ7xH7969mTVrFj6fj5ycHFauXEnXrl1P6neq5+/RowcrVqxg7969wfbTjTEzM5PmzZvz8MMPM3ToULZs2UJ4eDjFxcVnHOvZoCwyNUBliekC5eylplFWUkBJgW558fm8mEwWjEYTm3+az4Kd3zA/IY9jrnzsZhMDimO5Pq4v11x7J6FRxx3HpKbhKivB43YihIGwyHgiExtjC4vCaK69UESFQqG4EAghmDt3LhMmTOCFF15A0zQGDx7Myy+/fNrz4uPjmTZtGqNGjcLl0qM7X3zxRa644ooK/ZYvX86UKVMwm82EhYUFBdO4ceNITU2lY8eOTJ8+nRdffJH09HQ0TcNsNvPuu+/StGlTQkNDg348kZGRzJo166SxDBw4kA8++IDU1FRat25N9+7dz/jcb7/9Ng888ACpqal4vV569+7NBx98wOOPP86YMWN444036Nu3b7D//fffz5133klqaippaWmVCpITGT58OGvXrqVDhw4IIXjttdeoX78+O3bsqNCvXbt2lT5/9+7d+fDDDxkxYgSappGQkMD3339/yjHOmjWLzz77DLPZTP369XnmmWeIiYmhZ8+epKSkMGjQoHNy9hWnMzHVVTp37iw3bNhwXu51YmI6ALvZwHODmtOnoZfC3ENomobJZMFgMLFh9Sy+3bWAr20HyLVLQtxwXUJ3BqcMo1/DXoSYjpsTy4sXg9FEaHgskXFKvCgUitpn+/bttG3b9kIP46KltvLAKHQq+/sTQmyUUnY+sa+yyJwjUxb/flJiOqdH4/Xvd9NpqB2j0cJPf6zlm7x1LN63hALNQXgIDCppwPXx/el57R3Yw6OD50pNo8xZjNdThsFoIiwinnqJ7bBHxGAwqulSKBQKhaI86pOxmvg8blyOIrILTo7ZN3tLiNz3JU9PW8O3IYcotEG4OZT0xD4MK2zAVdfchi0kItj/uHjRQ60johsQEdcQW1iUEi8KhUJxEaKsMRcP6lPyLAkIF2dxPsX5R3CVFSOEgfgQOOoAjTLKDL/QImcaW+Oz2d0MostgqKMJg5oOpXuf27Aajy8HaT4vrrJSvB4XBoOB8Kj6SrwoFAqFQlFF1CfmKfC6y3A7S3AU5VKcfwSP24mUGkajGYstFJPBzKb18+iQs4K12j62xjtBeHEkWOiW3Yi+zQdz2+gxmG3HfV40nxeXswSv143BYCAythFh0fWUeFEoFAqFopqoT08/XncZrtIinCX5FYSLyWTFbLXjMQk25Gxm/fq5/JS1lo2RTrxGMMZAhzw7ib7BuHxdSbR14LZbooLZdSsTL+ExDbCFRSEMKhxaoVAoFIpzodaEjBBiKjAEOCqlTPG3vQDcAGjAUWCslDJb6Jl1/g0MBhz+9l/854wBnvJf9kUp5X9rYnwB4eIozqU4/ygetwOBAaPJjMUWgttRzC/rv2Ld/h/50buXX2Nc+NAwY6SztDLRmUq3Zr1I6zq0Qpg06OLFWVIQTHIXGdeIsKh6SrwoFAqFQlHD1Oan6jRg4AltU6SUqVLKNOAbIFD8YhDQyr+NA94HEELEAM8C3YCuwLNCiGiqgafMQWn+UY7u20rmpuXs2fwDh3b/SlHuYcwWK16blTUlW5ny81vc8L99aLdwCKPzP+aDkG1YpYGJsYOZ0fc9tt2ygtkPreKv4z6hZ/rdQRHj9bhwFOdRUpiDy1lCZFwjElt3pUVaX+IT22KPiFEiRqFQKKqA0WgMFnDs0KEDb7zxBpqmXZCxbNiwIVgK4GxYtWoVycnJpKWlVVrI8XwTqDZ9KpxOJ9dccw0+nx6FO3DgQKKiooJlFAIsW7aMjh07kpKSwpgxY4IZiadMmUJaWhppaWmkpKRgNBqDifLefPNNkpOTSUlJYdSoUcHsxrfeeiu7du0652er1TwyQohmwDcBi8wJxyYBiVLK+4UQ/wssl1LO8B/7HegT2KSU9/nbK/Q7FZ07d5ZrV6/E5SjCUZRLSUEOXq9LXyoy27BY7RQdy2bD+nn8dGgda7R9/BblRgqwGSx0zbFylb0V3VtcQ0rXP2EPjTzpHl6PC3dZKZrmQ0oNqy2c8Oh6hEbFYw2JUKJFoVDUaS6GPDLlc7UcPXqU0aNH07NnT5577rkLOq6zYfz48XTr1i1YvftM+Hw+jEZjrY1n3759DBkyhIyMjEqPv/vuu3i9XiZOnAjodZYcDgf/+7//yzfffAPo2YybNm3K0qVLueKKK3jmmWdo2rQpd999d4Vrff3117z55pssW7aMQ4cO0atXL7Zt24bdbueWW25h8ODBjB07lhUrVvDZZ5/x0UcfnTSeizqPjBDiJeAOoBC41t/cCDhYrluWv+1U7ZVddxy6NQdL/Zb0fPUH7mwvSG9ux2K1U+IpZW1RBuuObOTnDfPZGqlnfLSHQPeiCCZZrqbzNaNIi02uEF0UwOMuw+Ny4PN5AYnVFk50QlPs4dFYQyJUgjqFQnHJ8siiR9j0x7lXKS5PWv003hr41ln3T0hI4MMPP6RLly784x//oHfv3vzP//xPsNhjz549ef/99/nyyy85cOAAmZmZHDhwgEceeSRoSRk2bBgHDx6krKyMiRMnMm7cOEAXTA888ABLliwhOjqal19+mccff5wDBw7w1ltvMXToUJYvX87rr7/ON998Q0lJCQ899BAbNmxACMGzzz7LjTfeGBzrxx9/zBdffMHixYtZsmQJn332GY8//jgLFy5ECMFTTz3FyJEjWb58Oc899xwNGjRg06ZNbNu2jc8++4y3334bt9tNt27deO+99zAajSxatIgnn3wSn89HXFwcS5cuZf369TzyyCM4nU7sdjuffPIJrVu3ZuvWrdx555243W40TWPOnDk8/fTT7Nmzh7S0NPr3739SJt3p06fz+eefB1/369eP5ScU+szNzcVqtQazJPfv359//vOfJwmZGTNmMGrUqOBrr9eL0+nEbDbjcDho2LAhAFdffTVjx47F6/ViMlVfjpx3ISOlnAxM9ltkHkRfOqqs4pU8TXtl1/0Q+BDA2qCVdOVm8vX871gRksFm6yF2ROm1M0JMdnrY4rixrAldr+hLcudBWGwn1+0oL1yEEFjt4cTUT8IeFo3FHqaEi0KhUJxnmjdvjqZpHD16lHvuuYdp06bx1ltvsXPnTlwuF6mpqXz55Zfs2LGDH374geLiYlq3bs3999+P2Wxm6tSpxMTE4HQ66dKlCzfeeCOxsbGUlpbSp08fXn31VYYPH85TTz3F999/z7Zt2xgzZgxDhw6tMI4XXniByMhIfvvtNwDy8/MrHL/nnntYvXo1Q4YM4aabbmLOnDls2rSJzZs3c+zYMbp06ULv3r0BWL9+PRkZGSQlJbF9+3ZmzZrFmjVrMJvNTJgwgenTpzNo0CDuvfdeVq5cSVJSUnDJpk2bNqxcuRKTycSSJUt48sknmTNnDh988AETJ07kz3/+M263G5/PxyuvvEJGRkawdlN53G43mZmZNGvW7LS//7i4ODweDxs2bKBz587Mnj2bgwcPVujjcDhYtGhRsF5Uo0aNePTRR0lMTMRut5Oenk56ejqgVxBv2bIlmzdvplOnTmf5V3AyFzJq6XNgAbqQyQKalDvWGMj2t/c5oX35mS6syUy2xD7MllgId0HP4mhGudvR8fp7aR/bBrPBfNI5HncZ7rKSYFVQJVwUCoVCpyqWk9om8B59880388ILLzBlyhSmTp3K2LFjg32uv/56rFYrVquVhIQEjhw5QuPGjXn77beZO3cuAAcPHmTXrl3ExsZisVgYOFB36Wzfvj1WqxWz2Uz79u3Zt2/fSWNYsmQJM2fODL6Ojj696+bq1asZNWoURqORevXqcc011/Dzzz8TERFB165dSUpKAvTlnI0bN9KlSxdA91tJSEhg3bp19O7dO9gvJiYG0Is0jhkzhl27diGECBa77NGjBy+99BJZWVmMGDGCVq1anXZ8x44dIyoq6rR9QK9/NXPmTP7yl7/gcrlIT08/yZLy9ddf07Nnz+AY8/PzmTdvHnv37iUqKoqbb76Zzz77jNtuuw3QLW3Z2dl1R8gIIVpJKQOePUOBQIWq+cCDQoiZ6I69hVLKw0KIxcDL5Rx804FJZ7wPZgbuSaXE2ovs6Kv5z8NNKxyXmobHo1tcAv8UNnsEcQ1bYQuNxBoaofK6KBQKxUVGZmYmRqORhIQEhBD079+fefPm8cUXX1C+vp7Vag3uG41GvF4vy5cvZ8mSJaxdu5aQkBD69OkTdDo1m83owbO6lSBwvsFgCDqzlkdKGex/NpzOFzU0NLRCvzFjxvDPf/6zQp/58+dXer+nn36aa6+9lrlz57Jv3z769OkDwOjRo+nWrRsLFixgwIABfPzxxzRv3vyUY7Db7cHfxZno0aMHq1atAuC7775j586dFY7PnDmzwrLSkiVLSEpKIj5eD4wZMWIEP/74Y1DIlJWVYbfbORdqzSNVCDEDWAu0FkJkCSHuBl4RQmQIIbagi5KJ/u7fApnAbuAjYAKAlDIPeAH42b897287w72bsL3hsxyM7UdcuA2pabhdDkqLjlFccJTS4lyMRjNxDVuR2KYbLa/sR2JyD6IbJKmaRgqFQnERkpOTw/jx43nwwQeDH+r33HMPDz/8MF26dAlaAE5FYWEh0dHRhISEsGPHDtatW1ftsaSnpweXTuDkpaUT6d27N7NmzcLn85GTk8PKlSsrrVLdr18/Zs+ezdGjRwHIy8tj//799OjRgxUrVrB3795ge+CZGjXS3UanTZsWvE5mZibNmzfn4YcfZujQoWzZsoXw8HCKi4srHV90dDQ+n++sxExgbC6Xi1dffZXx48cHjxUWFrJixQpuuOGGYFtiYiLr1q3D4dANB0uXLq3gxLtz506Sk5PPeN/TUWtCRko5SkrZQEppllI2llL+R0p5o5QyxR+C/Scp5SF/XymlfEBK2UJK2V5KuaHcdaZKKVv6t0+qMgarEcYmy6BwiW/UmqZtu9Pyyn40aduN6AZJKquuQqFQXKQ4nc5g+PV1111Heno6zz77bPB4p06diIiIOKvIoIEDB+L1eklNTeXpp5+me/fu1R7XU089RX5+PikpKXTo0IEffvjhtP2HDx9OamoqHTp0oG/fvrz22mvUr1//pH7t2rXjxRdfJD09ndTUVPr378/hw4eJj4/nww8/ZMSIEXTo0IGRI0cC8PjjjzNp0iR69uwZDJsGmDVrFikpKaSlpbFjxw7uuOMOYmNj6dmzJykpKTz22GMn3Ts9PZ3Vq1cHX1999dXcfPPNLF26lMaNG7N48WJAD7Nu27Ytqamp/OlPf6Jv377Bc+bOnUt6enoFK1O3bt246aab6NixI+3bt0fTtKCT9ZEjR7Db7TRo0OBsfu2npFbDry8U1gatZNqEt5l4dUNGdG6KxR6mxIpCoVBUgYsh/PpMZGdn06dPH3bs2IFBpbw4J3799VfeeOMN/u///u+83fPNN98kIiLipKgnuMjDr88H7RtF8tPTgy70MBQKhUJRS3z66adMnjyZN954Q4mYGuDKK6/k2muvrfV8NuWJiori9ttvP+frXJIWmc6dO8vyjl8KhUKhqBp1wSKjuHSpikVGyViFQqFQVMql+EVXcfFT1b87JWQUCoVCcRI2m43c3FwlZhTnFSklubm52Gy2sz7nkvSRUSgUCsW50bhxY7KyssjJybnQQ1FcZthsNho3bnzW/ZWQUSgUCsVJmM3mYCZZheJiRi0tKRQKhUKhqLMoIaNQKBQKhaLOooSMQqFQKBSKOsslmUdGCFEI7Dpjx5onEii8APeNA46d53teqGe9EPdV83pp3lfN66V538tpXuHy+h23klJGnth4qTr7zpJSjjvfNxVCfHiB7ruhsiRBtXzPC/Ws5/2+al4vzfuqeb0073s5zav/vpfT7/jDytov1aWlry+z+14ILqffsZrXS/O+al4vzfteTvMKl9fvuNL7XpJLS5cbF+qbgKJ2UfN6aaLm9dJEzeuF41K1yFxuVGpuU9R51Lxemqh5vTRR83qBUBYZhUKhUCgUdRZlkVEoFAqFQlFnUUJGoVAoFApFnUUJmYsQIcRUIcRRIURGubYOQoi1QojfhBBfCyEi/O1/FkJsKrdpQog0/7FR/v5bhBCLhBBxF+qZFDU6ryP9c7pVCPHahXoehU4V59UshPivv327EGJSuXMGCiF+F0LsFkI8cSGeRXGcGpzXk66jqGGklGq7yDagN9ARyCjX9jNwjX//LuCFSs5rD2T6903AUSDO//o14B8X+tku562G5jUWOADE+1//F+h3oZ/tct6qMq/AaGCmfz8E2Ac0A4zAHqA5YAE2A+0u9LNdzltNzOuprqO2mt2UReYiREq5Esg7obk1sNK//z1wYyWnjgJm+PeFfwsVQgggAsiu+dEqzpYamtfmwE4pZY7/9ZJTnKM4T1RxXiX6/6QJsANuoAjoCuyWUmZKKd3ATOCG2h674tTU0Lye6jqKGkQJmbpDBjDUv38z0KSSPiPxf+BJKT3A/cBv6AKmHfCf2h+moopUaV6B3UAbIUQz/5vmsFOco7iwnGpeZwOlwGF0y9rrUso8oBFwsNz5Wf42xcVFVedVcR5QQqbucBfwgBBiIxCOrviDCCG6AQ4pZYb/tRldyFwJNAS2AJNQXGxUaV6llPno8zoLWIVuwvaezwErzopTzWtXwIf+P5kE/E0I0RzdenoiKjfGxUdV51VxHrhUay1dckgpdwDpAEKIK4DrT+hyK8e/tQOk+c/b4z/nC0A5EF5kVGNekVJ+jT9VtxBiHPobqOIi4jTzOhpY5LeYHhVCrAE6o1tjylvWGqOWgi86qjGvmRdkoJcZyiJTRxBCJPh/GoCngA/KHTOgmzlnljvlENBOCBHvf90f2H5+Rqs4W6oxr+XPiQYmAB+fr/Eqzo7TzOsBoK/QCQW6AzvQnUhbCSGShBAWdAE7//yPXHE6qjGvivOAEjIXIUKIGcBaoLUQIksIcTcwSgixE/2fIxv4pNwpvYEsKWVQ/Usps4HngJVCiC3oFpqXz9czKE6mJubVz7+FENuANcArUsqd52H4ilNQxXl9FwhD97X4GfhESrlFSukFHgQWo3/h+EJKufU8P4qiHDUxr6e5jqIGUSUKFAqFQqFQ1FmURUahUCgUCkWdRQkZhUKhUCgUdRYlZBQKhUKhUNRZlJBRKBQKhUJRZ1FCRqFQKBQKRZ1FCRmFQnHRIYSIEkJM8O83FELMvtBjUigUFycq/FqhUFx0CCGaAd9IKVMu8FAUCsVFjipRoFAoLkZeAVoIITYBu4C2UsoUIcRY9EKZRiAF+BdgAW4HXMBgKWWeEKIFepKyeMAB3OtPL69QKC4x1NKSQqG4GHkC2COlTAMeO+FYCnptm67AS+hFNa9Ez556h7/Ph8BDUspOwKPAe+dl1AqF4ryjLDIKhaKu8YOUshgoFkIU4i+gCfwGpAohwjjoPnQAAADKSURBVICrgP8nRLCotPX8D1OhUJwPlJBRKBR1DVe5fa3caw39Pc0AFPitOQqF4hJHLS0pFIqLkWIgvDonSimLgL1CiJsB/BWJO9Tk4BQKxcWDEjIKheKiQ0qZC6wRQmQAU6pxiT8DdwshNgNbgRtqcnwKheLiQYVfKxQKhUKhqLMoi4xCoVAoFIo6ixIyCoVCoVAo6ixKyCgUCoVCoaizKCGjUCgUCoWizqKEjEKhUCgUijqLEjIKhUKhUCjqLErIKBQKhUKhqLP8fzrYmiZkmrKgAAAAAElFTkSuQmCC\n\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Axis containers"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Save the data to csv file using"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ]
        ]
    },
    "1245662": {
        "jupyter_code_cell": "# TODO: Read and plot episode rewards\ndf_stats = pd.read_csv('landing_stats.csv')\ndf_stats[['total_reward']].plot(title=\"Episode Rewards\")",
        "matched_tutorial_code_inds": [
            3730,
            3857,
            6703,
            6953,
            6951
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "# Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ]
        ]
    },
    "792057": {
        "jupyter_code_cell": "df, m, h = stats.t.fit(r)  #Fit a location-scale t distribution to r.\nVaR_t = -stats.t.ppf(0.01, df, loc=m, scale=h)\nVaR_t",
        "matched_tutorial_code_inds": [
            4533,
            5615,
            5195,
            2026,
            4421
        ],
        "matched_tutorial_codes": [
            "print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...       stats.normaltest((x-x.mean())/x.std()))\nnormaltest teststat = 30.379 pvalue = 0.0000  # random",
            "kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()  # Estimate the densities",
            "X = sm.add_constant(X)\ny = np.dot(X, beta) + e",
            "Spectral clustering: kmeans, 2.04s\nSpectral clustering: discretize, 1.83s\nSpectral clustering: cluster_qr, 1.82s",
            "evals_all, evecs_all = eigh(X)"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Special tests for normal distributions"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Fitting with the default arguments"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation"
            ],
            [
                "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions"
            ],
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Examples"
            ]
        ]
    },
    "696137": {
        "jupyter_code_cell": "df['hour'] = df['date'].map(lambda x: x.hour).astype('category')\ndf_hour = pd.get_dummies(df['hour'],drop_first=False)",
        "matched_tutorial_code_inds": [
            3638,
            3644,
            3855,
            3779,
            3636
        ],
        "matched_tutorial_codes": [
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ]
        ]
    },
    "575836": {
        "jupyter_code_cell": "from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)",
        "matched_tutorial_code_inds": [
            2319,
            3448,
            6796,
            2666,
            2791
        ],
        "matched_tutorial_codes": [
            "from sklearn.ensemble import \n\nclf = (max_samples=100, random_state=0)\nclf.fit(X_train)",
            "from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)",
            "from statsmodels.tsa.forecasting.theta import ThetaModel\n\ntm = ThetaModel(housing)\nres = tm.fit()\nprint(res.summary())",
            "from sklearn.linear_model import \n\nlm = (alpha=0.01, precompute=gram)\nlm.fit(X_centered, y, sample_weight=normalized_weights)",
            "from sklearn import linear_model\n\nols = ()\n_ = ols.fit(X_train, y_train)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->IsolationForest example->Training of the model"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Sparsity Example: Fitting only features 1  and 2"
            ]
        ]
    },
    "1088479": {
        "jupyter_code_cell": "observations_2 = test_data['observations_2'].unstack(level=0).values\nobservation_dim = test_data['observations_2'].ndim\nobservations_2 = observations.reshape(-1, observation_dim, num_test_episodes)\n\nevaluation.setup_evaluation(train_data=train_matrices, test_observations=observations,\n                            test_groundtruth=groundtruth)",
        "matched_tutorial_code_inds": [
            2314,
            3549,
            2883,
            2888,
            5351
        ],
        "matched_tutorial_codes": [
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\norder_centroids = original_space_centroids.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names_out()\n\nfor i in range(true_k):\n    print(f\"Cluster {i}: \", end=\"\")\n    for ind in order_centroids[i, :10]:\n        print(f\"{terms[ind]} \", end=\"\")\n    print()",
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, with regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, with regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\"/>",
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Lasso model, optimum regularization, normalized variables\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Lasso model, optimum regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\"/>",
            "covb = res_ols.cov_params()\nprediction_var = res_ols.mse_resid + (X * np.dot(covb, X.T).T).sum(1)\nprediction_std = np.sqrt(prediction_var)\ntppf = stats.t.ppf(0.975, res_ols.df_resid)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "sklearn->Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Top terms per cluster"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS"
            ]
        ]
    },
    "1191539": {
        "jupyter_code_cell": "pca = PCA(n_components=0.95)\nX_pca = pca.fit_transform(X)\nprint(X_pca.shape)",
        "matched_tutorial_code_inds": [
            6898,
            6166,
            6765,
            5240,
            6734
        ],
        "matched_tutorial_codes": [
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation"
            ]
        ]
    },
    "226200": {
        "jupyter_code_cell": "color = ['red', 'blue']\nplt.figure()\nfor color, i, name in zip(color, [0,1], ['no_churn', 'churn']):\n    plt.scatter(df_fa[df_fa['is_churn'] == i]['date_featuresdatelistening_tenure'],\n               df_fa[df_fa['is_churn'] == i]['within_days_7num_unqmean'], color = color, alpha = 0.2, label = name)\nplt.legend(loc = 'best')\nplt.xlabel('Listening Tenure')\nplt.ylabel('Mean Number of Unique listening Periods in the last 7 days')",
        "matched_tutorial_code_inds": [
            3290,
            3291,
            3825,
            4682,
            3674
        ],
        "matched_tutorial_codes": [
            "cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "cvs = [\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n()\n\n\n\n<img alt=\"KFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_006.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_006.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_007.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_007.png\"/>\n<img alt=\"ShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_008.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_008.png\"/>\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_009.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_009.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_010.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_010.png\"/>\n<img alt=\"GroupShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_011.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_011.png\"/>\n<img alt=\"StratifiedShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_012.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_012.png\"/>\n<img alt=\"TimeSeriesSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_013.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_013.png\"/>",
            "cmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True)\n\n(df.select_dtypes(include=[np.number])\n   .pipe(core)\n   .pipe(sns.PairGrid)\n   .map_upper(plt.scatter, marker='.', alpha=.25)\n   .map_diag(sns.kdeplot)\n   .map_lower(plt.hexbin, cmap=cmap, gridsize=20)\n);",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize cross-validation indices for many CV objects"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "pandas_toms_blog->Indexes"
            ]
        ]
    },
    "1307607": {
        "jupyter_code_cell": "import pandas as pd\n\npath=r'noExpired.xlsx'\nall=pd.read_excel(path).dropna()",
        "matched_tutorial_code_inds": [
            3812,
            6824,
            6209,
            634,
            6280
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "import pandas as pd\n\nurl = \"https://stats.idre.ucla.edu/stat/data/hsb2.csv\"\nhsb2 = pd.read_table(url, delimiter=\",\")",
            "import numpy as np\n\ngen = np.random.default_rng(98765432101234567890)\nexog = pd.DataFrame(gen.integers(100, size=(300, 2)), columns=[\"exog1\", \"exog2\"])\nexog.head()",
            "import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)",
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Example Data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Custom Deterministic Terms"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data"
            ]
        ]
    },
    "1421220": {
        "jupyter_code_cell": "tf_and = 1\ndf_and = 1 \ntf_and * (np.log(n_docs / df_and) + 1)",
        "matched_tutorial_code_inds": [
            1716,
            6466,
            6911,
            1486,
            324
        ],
        "matched_tutorial_codes": [
            "dof = len(before_sample) - 1\n\np_value = stats.distributions.t.cdf(t_value, dof)\n\nprint(\"The t value is {} and the p value is {}.\".format(t_value, p_value))",
            "delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "tensor(2.2155, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(0.0802, grad_fn=&lt;NllLossBackward0&gt;)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Calculating the test statistics"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Refactor using optim"
            ]
        ]
    },
    "1043714": {
        "jupyter_code_cell": "\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport random as rand",
        "matched_tutorial_code_inds": [
            5464,
            6222,
            6140,
            1660,
            5980
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable",
            "import pandas as pd\nimport numpy as np\nfrom scipy.stats.distributions import norm, poisson\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data"
            ],
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->What you\u2019ll need"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ]
        ]
    },
    "421411": {
        "jupyter_code_cell": "f1 = data[\"V1\"].values\nf2 = data[\"V2\"].values\nX = np.array(list(zip(f1,f2)))  \nplt.scatter(f1,f2,c=\"black\",s=7) # \u2018c\u2019\t\u2018color\u2019 ",
        "matched_tutorial_code_inds": [
            2132,
            2314,
            1634,
            4535,
            1701
        ],
        "matched_tutorial_codes": [
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "rvs1 = stats.norm.rvs(loc=5, scale=10, size=500)\n rvs2 = stats.norm.rvs(loc=5, scale=10, size=500)\n stats.ttest_ind(rvs1, rvs2)\nTtest_indResult(statistic=-0.5489036175088705, pvalue=0.5831943748663959)  # random",
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "scipy->Statistics (scipy.stats)->Comparing two samples->Comparing means"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset"
            ]
        ]
    },
    "1468887": {
        "jupyter_code_cell": "(cast\n .loc[cast.character.isin(['Superman', 'Batman'])]\n .pivot_table(index='year', columns='character', aggfunc='size')\n .fillna(0)\n .apply(lambda x: 1 if x['Batman'] > x['Superman'] else 0, axis=1)\n .sum())",
        "matched_tutorial_code_inds": [
            3805,
            3660,
            4016,
            4018,
            3712
        ],
        "matched_tutorial_codes": [
            "(sm.Logit.from_formula('home_win ~ strength_diff + rest_spread',\n                       df.assign(strength_diff=df.home_strength - df.away_strength))\n    .fit().summary())",
            "(df.dropna(subset=['dep_time', 'unique_carrier'])\n   .loc[df['unique_carrier']\n       .isin(df['unique_carrier'].value_counts().index[:5])]\n   .set_index('dep_time')\n   # TimeGrouper to resample &amp; groupby at once\n   .groupby(['unique_carrier', pd.TimeGrouper(\"H\")])\n   .fl_num.count()\n   .unstack(0)\n   .fillna(0)\n   .rolling(24)\n   .sum()\n   .rename_axis(\"Flights per Day\", axis=1)\n   .plot()\n)\nsns.despine()",
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\", marker=\"cut\")\n    .add(so.Dots())\n    .scale(\n        color=so.Continuous(\"crest\", norm=(0, 3), trans=\"sqrt\"),\n        marker=so.Nominal([\"o\", \"+\", \"x\"], order=[\"Ideal\", \"Premium\", \"Good\"]),\n    )\n)\n",
            "(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\")\n    .add(so.Dots())\n    .scale(\n        x=so.Continuous().tick(every=0.5),\n        y=so.Continuous().label(like=\"${x:.0f}\"),\n        color=so.Continuous().tick(at=[1, 2, 3, 4]),\n    )\n)\n",
            "(weather.reset_index(level='station')\n .query('station in @locs')\n .groupby(['station', pd.TimeGrouper('H')])).mean()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Method Chaining->Method Chaining->Application"
            ],
            [
                "seaborn->User guide and tutorial->The seaborn.objects interface->Customizing the appearance->Parameterizing scales",
                "seaborn->Objects interface->The seaborn.objects interface->Customizing the appearance->Parameterizing scales"
            ],
            [
                "seaborn->User guide and tutorial->The seaborn.objects interface->Customizing the appearance->Customizing legends and ticks",
                "seaborn->Objects interface->The seaborn.objects interface->Customizing the appearance->Customizing legends and ticks"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ]
        ]
    },
    "740194": {
        "jupyter_code_cell": "for feature in features_for_ks:\n    hist(data_agreement[data_agreement.signal == 1][feature].values,\n         weights=data_agreement[data_agreement.signal == 1]['weight'].values, label='MC', **hist_kw)\n    hist(data_agreement[data_agreement.signal == 0][feature].values,\n         weights=data_agreement[data_agreement.signal == 0]['weight'].values, label='real', **hist_kw)\n    title(feature)\n    legend()\n    show()",
        "matched_tutorial_code_inds": [
            2968,
            6638,
            2178,
            6364,
            58
        ],
        "matched_tutorial_codes": [
            "for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>",
            "for i in range(niter):\n    mod.update_variances(store_obs_cov[i], store_state_cov[i])\n    sim.simulate()\n\n    # 1. Sample states\n    store_states[i + 1] = sim.simulated_state.T\n\n    # 2. Simulate obs cov\n    fitted = np.matmul(mod['design'].transpose(2, 0, 1), store_states[i + 1][..., None])[..., 0]\n    resid = mod.endog - fitted\n    store_obs_cov[i + 1] = invwishart.rvs(v10 + mod.nobs, S10 + resid.T @ resid)\n\n    # 3. Simulate state cov variances\n    resid = store_states[i + 1, 1:] - store_states[i + 1, :-1]\n    sse = np.sum(resid**2, axis=0)\n\n    for j in range(mod.k_states):\n        rv = invgamma.rvs((vi20 + mod.nobs - 1) / 2, scale=(Si20 + sse[j]) / 2)\n        store_state_cov[i + 1, j] = rv",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->5. Zero the gradients while training the network"
            ]
        ]
    },
    "113426": {
        "jupyter_code_cell": "labels = ['Ticket', 'Add. services', 'Reductions']\ncomp_mat = np.array([[1,7,9],[1/7,1,3],[1/9,1/3,1]])\ncomp_df = pd.DataFrame(comp_mat, index = labels, columns=labels)\ndisplay(comp_df)\nind, mat_E = inconsistency(comp_df)\nprint('The consistency index is:',ind,'\\n')\nprint('The consistency deviation matrix:')\nprint(mat_E-1,'\\n')",
        "matched_tutorial_code_inds": [
            3281,
            2773,
            4682,
            4683,
            2974
        ],
        "matched_tutorial_codes": [
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "names = ['group_a', 'group_b', 'group_c']\nvalues = [1, 10, 100]\n\nplt.figure(figsize=(9, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\n\n\n<img alt=\"Categorical Plotting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_006.png\" srcset=\"../../_images/sphx_glr_pyplot_006.png, ../../_images/sphx_glr_pyplot_006_2_0x.png 2.0x\"/>",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with categorical variables"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ]
        ]
    },
    "567628": {
        "jupyter_code_cell": "import matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn \nimport pandas as pd",
        "matched_tutorial_code_inds": [
            1516,
            4741,
            4654,
            4175,
            6505
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np",
            "import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np",
            "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->What you\u2019ll need",
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM"
            ],
            [
                "matplotlib->Tutorials->Introductory->Animations using Matplotlib"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics",
                "seaborn->Figure aesthetics->Controlling figure aesthetics"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ]
        ]
    },
    "1481291": {
        "jupyter_code_cell": "grid = sns.FacetGrid(df, col='quality',col_wrap = 2)\ngrid.map(plt.scatter,'pH','total sulfur dioxide',alpha = 0.2)",
        "matched_tutorial_code_inds": [
            5401,
            5390,
            3822,
            5386,
            6183
        ],
        "matched_tutorial_codes": [
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)",
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "fig = sm.graphics.plot_partregress(\"prestige\", \"income\", [\"education\"], data=prestige)\nfig.tight_layout(pad=1.0)",
            "fig = plt.figure(figsize=(16, 9))\nfig = res.plot_diagnostics(fig=fig, lags=30)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_29_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_29_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dynamics"
            ]
        ]
    },
    "335237": {
        "jupyter_code_cell": "df = pd.concat(df_list, ignore_index = True, join = 'inner')",
        "matched_tutorial_code_inds": [
            6005,
            3891,
            5363,
            5371,
            3700
        ],
        "matched_tutorial_codes": [
            "df_infl = infl.summary_frame()",
            "df.visualize(rankdir='LR')",
            "df = generate_nested()",
            "df = generate_crossed()",
            "pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True).head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->Merge Version"
            ]
        ]
    },
    "925431": {
        "jupyter_code_cell": "class_data.info()",
        "matched_tutorial_code_inds": [
            5600,
            5680,
            6351,
            4187,
            5304
        ],
        "matched_tutorial_codes": [
            "resf_logit.predict()",
            "data.describe()",
            "res_filardo.summary()",
            "sns.axes_style()\n",
            "res.plot_cusum()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Filardo (1994) Time-Varying Transition Probabilities"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money"
            ]
        ]
    },
    "618636": {
        "jupyter_code_cell": "# count null values by sex and class and use unstack() to make each sex a column in the \n# resulting table\nnull_ages = df[df.Age.isnull()].groupby(['Pclass', 'Sex']).count().Name.unstack()\nnull_ages",
        "matched_tutorial_code_inds": [
            2176,
            7013,
            6664,
            32,
            115
        ],
        "matched_tutorial_codes": [
            "# The ordinal encoder will first output the categorical features, and then the\n# continuous (passed-through) features\n\nhist_native = (\n    ordinal_encoder,\n    (\n        random_state=42,\n        categorical_features=categorical_columns,\n    ),\n).set_output(transform=\"pandas\")",
            "# You can also use `scipy.optimize.minimize` and write your own cost function.\n# This does not give you the parameter errors though ... you'd have\n# to estimate the HESSE matrix separately ...\nfrom scipy.optimize import minimize\n\n\ndef chi2(pars):\n    \"\"\"Cost function.\"\"\"\n    y_model = pars[0] * data[\"x\"] + pars[1]\n    chi = (data[\"y\"] - y_model) / data[\"y_err\"]\n    return np.sum(chi ** 2)\n\n\nresult = minimize(fun=chi2, x0=[0, 0])\npopt = result.x\nprint(\"a = {0:10.3f}\".format(popt[0]))\nprint(\"b = {0:10.3f}\".format(popt[1]))",
            "# Here we follow the same procedure as above, but now we instantiate the\n# Theano wrapper `Loglike` with the UC model instance instead of the\n# SARIMAX model instance\nloglike_uc = Loglike(mod_uc)\n\nwith pm.Model():\n    # Priors\n    sigma2level = pm.InverseGamma(\"sigma2.level\", 1, 1)\n    sigma2ar = pm.InverseGamma(\"sigma2.ar\", 1, 1)\n    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n\n    # convert variables to tensor vectors\n    theta_uc = tt.as_tensor_variable([sigma2level, sigma2ar, arL1])\n\n    # use a DensityDist (use a lamdba function to \"call\" the Op)\n    pm.DensityDist(\"likelihood\", loglike_uc, observed=theta_uc)\n\n    # Draw samples\n    trace_uc = pm.sample(\n        ndraws,\n        tune=nburn,\n        return_inferencedata=True,\n        cores=1,\n        compute_convergence_checks=False,\n    )",
            "# import the modules used here in this recipe\nimport torch\nimport torch.quantization\nimport torch.nn as nn\nimport copy\nimport os\nimport time\n\n# define a very, very simple LSTM for demonstration purposes\n# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n# inspired by\n# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\nclass lstm_for_demonstration():\n  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n     Not to be used for anything other than demonstration.\n  \"\"\"\n  def __init__(self,in_dim,out_dim,depth):\n     super(lstm_for_demonstration,self).__init__()\n     self.lstm = (in_dim,out_dim,depth)\n\n  def forward(self,inputs,hidden):\n     out,hidden = self.lstm(inputs,hidden)\n     return out, hidden\n\n\n(29592)  # set the seed for reproducibility\n\n#shape parameters\nmodel_dimension=8\nsequence_length=20\nbatch_size=1\nlstm_depth=1\n\n# random data for input\ninputs = (sequence_length,batch_size,model_dimension)\n# hidden is actually is a tuple of the initial hidden state and the initial cell state\nhidden = ((lstm_depth,batch_size,model_dimension), (lstm_depth,batch_size,model_dimension))",
            "# Measurement objects store the results of multiple repeats, and provide\n# various utility features.\nfrom torch.utils.benchmark import \n\nm:  = timer.blocked_autorange(min_run_time=1)\nprint(m)\n\n\n\n<strong>Snippet wall time.</strong>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with native categorical support"
            ],
            [
                "statsmodels->Examples->User Notes->Least squares fitting of models to data->Linear models->Check against self-written cost function"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->1: Set Up"
            ],
            [
                "torch->PyTorch Recipes->Timer quick start->2. Wall time: Timer.blocked_autorange(\u2026)"
            ]
        ]
    },
    "1277599": {
        "jupyter_code_cell": "oc_pattern = os.path.join(ecv_base, 'occci-v2.0/data/geographic/netcdf/monthly/chlor_a/2010/*.nc')\nst_pattern = os.path.join(ecv_base, 'sst/data/lt/Analysis/L4/v01.1/2010/*.nc')\n\noc_ds = xr.open_mfdataset(oc_pattern)\nsst_ds = xr.open_mfdataset(st_pattern)",
        "matched_tutorial_code_inds": [
            23,
            5606,
            5605,
            6998,
            5992
        ],
        "matched_tutorial_codes": [
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "ex = add_constant(data2[['pared', 'public', 'gpa']], prepend=False)\nmod_logit = Logit(data2['apply'].cat.codes, ex)\n\nres_logit = mod_logit.fit(method='bfgs', disp=False)",
            "mod_log = OrderedModel(data2['apply'],\n                        data2[['pared', 'public', 'gpa']],\n                        distr='logit')\n\nres_log = mod_log.fit(method='bfgs', disp=False)\nres_log.summary()",
            "url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "rslt, scales = [], []\n\nfor hyp in 0, 1:\n    s, t = dosim(hyp, sm.cov_struct.Exchangeable(), mcrep=100)\n    rslt.append(s)\n    scales.append(t)\n\nrslt = pd.DataFrame(rslt, index=[\"H0\", \"H1\"], columns=[\"Mean\", \"Prop(p&lt;0.1)\"])\n\nprint(rslt)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ]
        ]
    },
    "905025": {
        "jupyter_code_cell": "# Here we set the bandwidth. This function automatically derives a bandwidth\n# number based on an inspection of the distances among points in the data.\nbandwidth = estimate_bandwidth(X_train2, quantile=0.2, n_samples=500)\n\n# Declare and fit the model.\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X_train2)\n\n# Extract cluster assignments for each data point.\nlabels = ms.labels_\n\n# Coordinates of the cluster centers.\ncluster_centers = ms.cluster_centers_\n\n# Count our clusters.\nn_clusters_ = len(np.unique(labels))\n\nprint(\"Number of estimated clusters: {}\".format(n_clusters_))",
        "matched_tutorial_code_inds": [
            6664,
            1165,
            3509,
            6635,
            7013
        ],
        "matched_tutorial_codes": [
            "# Here we follow the same procedure as above, but now we instantiate the\n# Theano wrapper `Loglike` with the UC model instance instead of the\n# SARIMAX model instance\nloglike_uc = Loglike(mod_uc)\n\nwith pm.Model():\n    # Priors\n    sigma2level = pm.InverseGamma(\"sigma2.level\", 1, 1)\n    sigma2ar = pm.InverseGamma(\"sigma2.ar\", 1, 1)\n    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n\n    # convert variables to tensor vectors\n    theta_uc = tt.as_tensor_variable([sigma2level, sigma2ar, arL1])\n\n    # use a DensityDist (use a lamdba function to \"call\" the Op)\n    pm.DensityDist(\"likelihood\", loglike_uc, observed=theta_uc)\n\n    # Draw samples\n    trace_uc = pm.sample(\n        ndraws,\n        tune=nburn,\n        return_inferencedata=True,\n        cores=1,\n        compute_convergence_checks=False,\n    )",
            "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n# in seconds. We use CUDA events and synchronization for the most accurate\n# measurements.\ndef timed(fn):\n    start = (enable_timing=True)\n    end = (enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    ()\n    return result, start.elapsed_time(end) / 1000\n\n# Generates random input and targets data for the model, where `b` is\n# batch size.\ndef generate_data(b):\n    return (\n        (b, 3, 128, 128).to().cuda(),\n        (1000, (b,)).cuda(),\n    )\n\nN_ITERS = 10\n\nfrom torchvision.models import \ndef init_model():\n    return ().to().cuda()",
            "# To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\n\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nlasso_cv = (alphas=alphas, random_state=0, max_iter=10000)\nk_fold = (3)\n\nprint(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\nprint()\nprint(\"Alpha parameters maximising the generalization score on different\")\nprint(\"subsets of the data:\")\nfor k, (train, test) in enumerate(k_fold.split(X, y)):\n    lasso_cv.fit(X[train], y[train])\n    print(\n        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\n            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\n        )\n    )\nprint()\nprint(\"Answer: Not very much since we obtained different alphas for different\")\nprint(\"subsets of the data and moreover, the scores for these alphas differ\")\nprint(\"quite substantially.\")\n\n()",
            "# Here, for illustration purposes only, we plot the time-varying\n# coefficients conditional on an ad-hoc parameterization\n\n# Recall that `initial_res` contains the Kalman filtering and smoothing,\n# and the `states.smoothed` attribute contains the smoothed states\nplot_coefficients_by_equation(initial_res.states.smoothed);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\"/>",
            "# You can also use `scipy.optimize.minimize` and write your own cost function.\n# This does not give you the parameter errors though ... you'd have\n# to estimate the HESSE matrix separately ...\nfrom scipy.optimize import minimize\n\n\ndef chi2(pars):\n    \"\"\"Cost function.\"\"\"\n    y_model = pars[0] * data[\"x\"] + pars[1]\n    chi = (data[\"y\"] - y_model) / data[\"y_err\"]\n    return np.sum(chi ** 2)\n\n\nresult = minimize(fun=chi2, x0=[0, 0])\npopt = result.x\nprint(\"a = {0:10.3f}\".format(popt[0]))\nprint(\"b = {0:10.3f}\".format(popt[1]))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups"
            ],
            [
                "sklearn->Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Bonus: how much can you trust the selection of alpha?"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q"
            ],
            [
                "statsmodels->Examples->User Notes->Least squares fitting of models to data->Linear models->Check against self-written cost function"
            ]
        ]
    },
    "950328": {
        "jupyter_code_cell": "pal = sns.diverging_palette(220,10, sep=80, n=2)\ncmap = sns.diverging_palette(220,10, sep=80, as_cmap=True)\nsns.palplot(pal)",
        "matched_tutorial_code_inds": [
            4167,
            4185,
            4110,
            3822,
            4936
        ],
        "matched_tutorial_codes": [
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "g = sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"violin\", inner=None)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", color=\"k\", size=3, ax=g.ax)\n",
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "copper = mpl.colormaps['copper'].resampled(8)\n\nprint('copper(range(8))', copper(range(8)))\nprint('copper(np.linspace(0, 1, 8))', copper(np.linspace(0, 1, 8)))"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots",
                "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Getting colormaps and accessing their values->LinearSegmentedColormap"
            ]
        ]
    },
    "167514": {
        "jupyter_code_cell": "data['plot_clean'] = data['plot'].apply(clean_sentence)",
        "matched_tutorial_code_inds": [
            6911,
            6813,
            7002,
            2138,
            6973
        ],
        "matched_tutorial_codes": [
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "data.endog.index = dates\nendog = data.endog\nendog",
            "data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]",
            "sm_probit_canned = sm.Probit(endog, exog).fit()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ]
        ]
    },
    "778867": {
        "jupyter_code_cell": "normalSampleDF = sc.parallelize(np.ones(n))\\\n                 .map(lambda y: multivariateNormalVector(y,mu,mormalizerMatrix))\\\n                 .toDF(['x_1','x_2'])",
        "matched_tutorial_code_inds": [
            5368,
            5375,
            5264,
            5647,
            5456
        ],
        "matched_tutorial_codes": [
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "resid_fit = sm.OLS(\n    np.asarray(ols_resid)[1:], sm.add_constant(np.asarray(ols_resid)[:-1])\n).fit()\nprint(resid_fit.tvalues[1])\nprint(resid_fit.pvalues[1])",
            "glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\nres = glm_binom.fit()\nprint(res.summary())",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Generalized Least Squares"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Fit and summary"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ]
        ]
    },
    "253067": {
        "jupyter_code_cell": "cuisine = df[['CUISINE DESCRIPTION','RESTAURANT']].drop_duplicates(subset='RESTAURANT')\ncuisine['CUISINE DESCRIPTION'].value_counts()[:20].plot(kind = 'bar')",
        "matched_tutorial_code_inds": [
            3644,
            3889,
            3859,
            3855,
            6953
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "# Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ]
        ]
    },
    "687339": {
        "jupyter_code_cell": "print(\"Accuracy on Training Data: %f\" % clf.score(X_train,y_train))\nprint(\"Accuracy on Test Data: %f\" % clf.score(X_test,y_test))",
        "matched_tutorial_code_inds": [
            2950,
            1576,
            1580,
            1569,
            1572
        ],
        "matched_tutorial_codes": [
            "print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")",
            "print(\"The data type of training labels: {}\".format(y_train.dtype))\nprint(\"The data type of test labels: {}\".format(y_test.dtype))",
            "print(\"The data type of training labels: {}\".format(training_labels.dtype))\nprint(\"The data type of test labels: {}\".format(test_labels.dtype))",
            "print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))",
            "print(\"The data type of training images: {}\".format(training_images.dtype))\nprint(\"The data type of test images: {}\".format(test_images.dtype))"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Accuracy of the Model",
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format"
            ]
        ]
    },
    "219007": {
        "jupyter_code_cell": "wss = []  # within-cluster sum of squares\nns = range(2, 11)\nfor i in ns:\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=500, n_init=20, random_state=0)\n    kmeans.fit(X)\n    wss.append(kmeans.inertia_)",
        "matched_tutorial_code_inds": [
            3197,
            3281,
            2101,
            5933,
            5244
        ],
        "matched_tutorial_codes": [
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = (random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)",
            "all_betas = []\nfor i in range(mc_iter):\n    y = np.dot(X, beta_true) + np.random.normal(size=200)\n    random_idx = np.random.randint(0, nobs, size=int(contaminate * nobs))\n    y[random_idx] = np.random.uniform(-750, 750)\n    beta_hat = sm.RLM(y, X).fit().params\n    all_betas.append(beta_hat)",
            "norm_x = X.values\nfor i, name in enumerate(X):\n    if name == \"const\":\n        continue\n    norm_x[:, i] = X[name] / np.linalg.norm(X[name])\nnorm_xtx = np.dot(norm_x.T, norm_x)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Total impurity of leaves vs effective alphas of pruned tree"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Condition number"
            ]
        ]
    },
    "383696": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt\n\nsal_by_reg = pd.read_csv('salaries-by-region.csv', index_col='Region')\nclean_df = sal_by_reg.dropna(axis=0).replace({'\\,':''}, regex = True).replace({'\\$':''}, regex = True)\nten_num = pd.to_numeric(clean_df['Mid-Career 10th Percentile Salary'], errors='coerce')",
        "matched_tutorial_code_inds": [
            3557,
            4871,
            3469,
            4776,
            2548
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, (ax0, ax1) = (ncols=2, figsize=(16, 6), sharey=True)\n\ndf = (evaluations[::-1]).set_index(\"estimator\")\ndf_std = (evaluations_std[::-1]).set_index(\"estimator\")\n\ndf.drop(\n    [\"train_time\"],\n    axis=\"columns\",\n).plot.barh(ax=ax0, xerr=df_std)\nax0.set_xlabel(\"Clustering scores\")\nax0.set_ylabel(\"\")\n\ndf[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\nax1.set_xlabel(\"Clustering time (s)\")\n()\n\n\n<img alt=\"plot document clustering\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_clustering_001.png\" srcset=\"../../_images/sphx_glr_plot_document_clustering_001.png\"/>",
            "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-2 * np.pi, 2 * np.pi, 100)\ny = np.sinc(x)\n\nfig, ax = plt.subplots()\nax.plot(x, y)\n\n\n<img alt=\"autoscale\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_autoscale_001.png\" srcset=\"../../_images/sphx_glr_autoscale_001.png, ../../_images/sphx_glr_autoscale_001_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\nxx, yy = ((-3, 3, 500), (-3, 3, 500))\n(0)\nX = (300, 2)\nY = (X[:, 0]  0, X[:, 1]  0)\n\n# fit the model\nclf = (gamma=\"auto\")\nclf.fit(X, Y)\n\n# plot the decision function for each datapoint on the grid\nZ = clf.decision_function([xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n(\n    Z,\n    interpolation=\"nearest\",\n    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n    aspect=\"auto\",\n    origin=\"lower\",\n    cmap=plt.cm.PuOr_r,\n)\ncontours = (xx, yy, Z, levels=[0], linewidths=2, linestyles=\"dashed\")\n(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired, edgecolors=\"k\")\n(())\n(())\n([-3, 3, -3, 3])\n()",
            "import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\nfig, ax = plt.subplots()\nax.plot(100*np.random.rand(20))\n\n# Use automatic StrMethodFormatter\nax.yaxis.set_major_formatter('${x:1.2f}')\n\nax.yaxis.set_tick_params(which='major', labelcolor='green',\n                         labelleft=False, labelright=True)\n\nplt.show()\n\n\n<img alt=\"dollar ticks\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_dollar_ticks_001.png\" srcset=\"../../_images/sphx_glr_dollar_ticks_001.png, ../../_images/sphx_glr_dollar_ticks_001_2_0x.png 2.0x\"/>",
            "import pandas as pd\n\ndf = (grid_search.cv_results_)[\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n]\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\ndf = df.rename(\n    columns={\n        \"param_n_components\": \"Number of components\",\n        \"param_covariance_type\": \"Type of covariance\",\n        \"mean_test_score\": \"BIC score\",\n    }\n)\ndf.sort_values(by=\"BIC score\").head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Working with text documents->Clustering text documents using k-means->Clustering evaluation summary"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Autoscaling"
            ],
            [
                "sklearn->Examples->Support Vector Machines->Non-linear SVM"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Tick containers"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the BIC scores"
            ]
        ]
    },
    "1363504": {
        "jupyter_code_cell": "import pandas as pd \nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc \nfrom datetime import datetime as dt \nfrom matplotlib.font_manager import FontProperties\nimport seaborn as sns \nimport numpy as np\nfrom mapsplotlib import mapsplot as mplt",
        "matched_tutorial_code_inds": [
            3007,
            6376,
            2345,
            2053,
            6197
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.ensemble import",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nplt.rc(\"figure\", figsize=(16, 9))\nplt.rc(\"font\", size=16)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms"
            ]
        ]
    },
    "798429": {
        "jupyter_code_cell": "df3['degree_of_freedom'] = df3['degree_of_freedom'].astype(int)",
        "matched_tutorial_code_inds": [
            6911,
            1495,
            1757,
            3792,
            6813
        ],
        "matched_tutorial_codes": [
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "df['home_win'] = df.home_points &gt; df.away_points",
            "pca_model = PCA(dta.T, standardize=False, demean=True)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 1: Create an outcome variable"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "1328929": {
        "jupyter_code_cell": "candperc = pd.DataFrame({c : OH[c] / OH[u'Total Voters'] for c in candidates})\ncandperc[['County Name','Precinct Name','Total Voters']] = OH[['County Name','Precinct Name','Total Voters']]\ncandperc = candperc.set_index(['County Name','Precinct Name'])",
        "matched_tutorial_code_inds": [
            4432,
            3917,
            442,
            3776,
            5494
        ],
        "matched_tutorial_codes": [
            "N = 21\n D = np.diag(0.5*np.ones(N-1), k=1) - np.diag(0.5*np.ones(N-1), k=-1)\n D[0] = D[-1] = 0 # take away edge effects\n Dop = FirstDerivative(N, dtype=np.float64)",
            "daily = (\n    indiv[['transaction_dt', 'transaction_amt']].dropna()\n        .set_index('transaction_dt')['transaction_amt']\n        .resample(\"D\")\n        .sum()\n).compute()\ndaily",
            "test_loss = evaluate(model, )\ntest_ppl = math.exp(test_loss)\nprint('=' * 89)\nprint(f'| End of training | test loss {test_loss:5.2f} | '\n      f'test ppl {test_ppl:8.2f}')\nprint('=' * 89)",
            "tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.cdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.cdf(support), label=\"Probit\")\nax.legend()"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Use of LinearOperator"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "torch->Text->Language Modeling with nn.Transformer and TorchText->Evaluate the best model on the test dataset"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Exercise: Logit vs Probit"
            ]
        ]
    },
    "1203352": {
        "jupyter_code_cell": "# Establish and fit the model, with default settings.\nmlp4 = MLPClassifier(activation='logistic', hidden_layer_sizes=(1000, 1000))\nmlp4.fit(X, Y)",
        "matched_tutorial_code_inds": [
            1260,
            6828,
            5240,
            3328,
            6848
        ],
        "matched_tutorial_codes": [
            "def setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer",
            "from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "from patsy.contrasts import Diff\n\ncontrast = Diff().code_without_intercept(levels)\nprint(contrast.matrix)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Backward Difference Coding"
            ]
        ]
    },
    "809529": {
        "jupyter_code_cell": "y_pred = np.tile(np.mean(y_train, axis=0),(y_test.shape[0], 1))",
        "matched_tutorial_code_inds": [
            1623,
            6813,
            6217,
            6973,
            2990
        ],
        "matched_tutorial_codes": [
            "x_ray_image_gaussian_gradient = ndimage.gaussian_gradient_magnitude(xray_image, sigma=2)",
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "sm_probit_canned = sm.Probit(endog, exog).fit()",
            "n_neighbors = 12  # neighborhood which is used to recover the locally linear structure\nn_components = 2  # number of coordinates for the manifold"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Gaussian gradient magnitude method"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning"
            ]
        ]
    },
    "1328995": {
        "jupyter_code_cell": "def setax(ax):\n    ax.set_title(\"Height vs Weight\", fontsize=20)\n    ax.set_xlabel(\"Height\", fontsize=15)\n    ax.set_ylabel(\"Weight\", fontsize=15)",
        "matched_tutorial_code_inds": [
            4858,
            3730,
            5554,
            184,
            6264
        ],
        "matched_tutorial_codes": [
            "def annotate_axes(ax, text, fontsize=18):\n    ax.text(0.5, 0.5, text, transform=ax.transAxes,\n            ha=\"center\", va=\"center\", fontsize=fontsize, color=\"darkgrey\")",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->High-level methods for making grids->Basic 2x2 grid"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ]
        ]
    },
    "1166229": {
        "jupyter_code_cell": "\n#the residual has a lot of NaN Values, eliminating all of them\nress1 = ress1.fillna(0)\n\n#assigning index values to train normal data\n\ncd1 = np.arange(35000,59991)\ncd = np.arange(0,13500)\nc = np.concatenate((cd,cd1))",
        "matched_tutorial_code_inds": [
            4360,
            2681,
            2974,
            3830,
            653
        ],
        "matched_tutorial_codes": [
            "# Normalized inverse: no scaling factor\n idct(dct(x, type=2), type=2)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "ElasticNet(alpha=0.1, l1_ratio=0.7)\nr^2 on test data : 0.642515",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "traced_model = (model)\nprint(traced_model.graph)"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->DCT and IDCT"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->ElasticNet"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm"
            ]
        ]
    },
    "256272": {
        "jupyter_code_cell": "from IPython.display import HTML\ns = \"\"\"<table>\n<tr>\n<th>Model</th>\n<th>RLMSE</th>\n<th># exact predictions</th>\n<th>most extreme difference</th>\n</tr>\n<tr>\n<td>GradientBoostingRegressor</td>\n<td>0.705</td>\n<td>31</td>\n<td>440</td>\n</tr>\n<tr>\n<td>KNeighboursRegressor</td>\n<td>1.027</td>\n<td>13</td>\n<td>598</td>\n</tr>\n<tr>\n<td>RandomForestRegressor</td>\n<td>0.319</td>\n<td>67</td>\n<td>456</td>\n</tr>\n<tr>\n<td>AdaBoostRegressor</td>\n<td>0.799</td>\n<td>11</td>\n<td>589</td>\n</tr>\n<tr>\n<td>DecisionTreeRegressor</td>\n<td>0.394</td>\n<td>57</td>\n<td>603</td>\n</tr>\n<tr>\n<td>BaggingRegressor</td>\n<td>0.315</td>\n<td>77</td>\n<td>442</td>\n</tr>\n</table>\"\"\"\nh = HTML(s); h",
        "matched_tutorial_code_inds": [
            2808,
            2177,
            5994,
            3028,
            516
        ],
        "matched_tutorial_codes": [
            "from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "from sklearn.model_selection import \nimport matplotlib.pyplot as plt\n\nscoring = \"neg_mean_absolute_percentage_error\"\nn_cv_folds = 3\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\n\ndef plot_results(figure_title):\n    fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\n    plot_info = [\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n    ]\n\n    x, width = (4), 0.9\n    for key, title, ax, y_limit in plot_info:\n        items = [\n            dropped_result[key],\n            one_hot_result[key],\n            ordinal_result[key],\n            native_result[key],\n        ]\n\n        mape_cv_mean = [(np.abs(item)) for item in items]\n        mape_cv_std = [(item) for item in items]\n\n        ax.bar(\n            x=x,\n            height=mape_cv_mean,\n            width=width,\n            yerr=mape_cv_std,\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n        )\n        ax.set(\n            xlabel=\"Model\",\n            title=title,\n            xticks=x,\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n            ylim=y_limit,\n        )\n    fig.suptitle(figure_title)\n\n\nplot_results(\"Gradient Boosting on Ames Housing\")\n\n\n<img alt=\"Gradient Boosting on Ames Housing, Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\"/>",
            "from urllib.request import urlopen\nimport numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nimport pandas as pd\n\npd.set_option(\"display.width\", 100)\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.api import interaction_plot, abline_plot\nfrom statsmodels.stats.anova import anova_lm\n\ntry:\n    salary_table = pd.read_csv(\"salary.table\")\nexcept:  # recent pandas can read URL without urlopen\n    url = \"http://stats191.stanford.edu/data/salary.table\"\n    fh = urlopen(url)\n    salary_table = pd.read_table(fh)\n    salary_table.to_csv(\"salary.table\")\n\nE = salary_table.E\nM = salary_table.M\nX = salary_table.X\nS = salary_table.S",
            "from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>",
            "from timeit import default_timer as timer\nNUM_EPOCHS = 18\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n\n\n# function to generate output sequence using greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = (1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = (prob, dim=1)\n        next_word = next_word.item()\n\n        ys = ([ys,\n                        (1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\n\n# actual function to translate input sentence into target language\ndef translate(model: , src_sentence: str):\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = ((num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"&lt;bos&gt;\", \"\").replace(\"&lt;eos&gt;\", \"\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Model comparison"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves"
            ],
            [
                "torch->Text->Language Translation with nn.Transformer and torchtext->Collation"
            ]
        ]
    },
    "364571": {
        "jupyter_code_cell": "sum_blbr = trace_5_8['br'] + trace_5_8['bl']\npm.kdeplot(sum_blbr);",
        "matched_tutorial_code_inds": [
            6911,
            3794,
            5482,
            6563,
            2026
        ],
        "matched_tutorial_codes": [
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "g = sns.FacetGrid(wins.reset_index(), hue='team', size=7, aspect=.5, palette=['k'])\ng.map(sns.pointplot, 'is_home', 'win_pct').set(ylim=(0, 1));",
            "respondent1000 = dta.iloc[1000]\nprint(respondent1000)",
            "dta.plot(figsize=(12,4));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\"/>",
            "Spectral clustering: kmeans, 2.04s\nSpectral clustering: discretize, 1.83s\nSpectral clustering: cluster_qr, 1.82s"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions"
            ]
        ]
    },
    "1239657": {
        "jupyter_code_cell": "def calculate_pi_loop(N): \n    hits = np.zeros(N)\n    for i in range(N): \n        point = np.random.rand(2)\n        dist = point[0]**2 + point[1]**2\n        hits[i] = dist < 1\n    return 4*hits.sum()/N",
        "matched_tutorial_code_inds": [
            6145,
            1779,
            6144,
            1727,
            3667
        ],
        "matched_tutorial_codes": [
            "def gen_outcome(otype, mtime0):\n    if otype == \"full\":\n        lp = 0.5 * mtime0\n    elif otype == \"no\":\n        lp = exp\n    else:\n        lp = exp + mtime0\n    mn = np.exp(-lp)\n    ytime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    ystatus = (ctime = ytime0).astype(int)\n    ytime = np.where(ytime0 &lt;= ctime, ytime0, ctime)\n    return ytime, ystatus",
            "def loss_f(A, Y):\n    # define value of epsilon to prevent zero division error inside a log\n    epsilon = 1e-5\n    # Implement formula for negative log likelihood\n    loss = (- Y * np.log(A + epsilon)\n            - (1 - Y) * np.log(1 - A + epsilon))\n    # Return loss\n    return np.squeeze(loss)",
            "def gen_mediator():\n    mn = np.exp(exp)\n    mtime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    mstatus = (ctime = mtime0).astype(int)\n    mtime = np.where(mtime0 &lt;= ctime, mtime0, ctime)\n    return mtime0, mtime, mstatus",
            "def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "def get_ids(network):\n    url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\n    r = requests.get(url.format(network))\n    md = pd.io.json.json_normalize(r.json()['features'])\n    md['network'] = network\n    return md"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Training the Network"
            ],
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)"
            ],
            [
                "pandas_toms_blog->Indexes"
            ]
        ]
    },
    "967874": {
        "jupyter_code_cell": "masked_region_df_pbr['subject_id'] = pd.Categorical(masked_region_df_pbr['subject_id'], pca_results.subject_id)",
        "matched_tutorial_code_inds": [
            1495,
            5712,
            3830,
            5533,
            2160
        ],
        "matched_tutorial_codes": [
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\nX_reconstructed_kernel_pca = kernel_pca.inverse_transform(kernel_pca.transform(X_test))"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "sklearn->Examples->Decomposition->Kernel PCA->Projecting into the original feature space"
            ]
        ]
    },
    "1209585": {
        "jupyter_code_cell": "data.number_emergency.value_counts().head(20)",
        "matched_tutorial_code_inds": [
            5560,
            5680,
            5679,
            5561,
            6801
        ],
        "matched_tutorial_codes": [
            "data_student.head(5)",
            "data.describe()",
            "data = sm.datasets.fair.load_pandas().data",
            "data_student.dtypes",
            "res.forecast_components(12)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data"
            ]
        ]
    },
    "319711": {
        "jupyter_code_cell": "f = pd.read_csv('foods.csv')\nf.head()",
        "matched_tutorial_code_inds": [
            6809,
            3676,
            3684,
            4153,
            4152
        ],
        "matched_tutorial_codes": [
            "data = sm.datasets.fertility.load_pandas().data\ndata.head()",
            "weather = pd.read_hdf(\"weather.h5\", \"weather\").sort_index()\n\nweather.head()",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ]
        ]
    },
    "59750": {
        "jupyter_code_cell": "df_mw['tot pop'] = np.round(df_mw['tot unemp'].values / (df_mw['tot unemp quote'].values/100)).astype(int)\ndf_mw.sort_values(by='canton',ascending=True).head()",
        "matched_tutorial_code_inds": [
            3644,
            5789,
            5536,
            3776,
            3638
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ]
        ]
    },
    "603991": {
        "jupyter_code_cell": "Salary_Data[\"salary\"].iloc[6179] = (387500 + 350000) / 2\nSalary_Data[\"salary\"].iloc[12007] = 170000",
        "matched_tutorial_code_inds": [
            5777,
            2359,
            2630,
            6885,
            2706
        ],
        "matched_tutorial_codes": [
            "Intercept        3.756282e-141\nrate_marriage     0.000000e+00\nage               2.221918e-11\nyrs_married       1.219200e-02\ndtype: float64",
            "sigma = 0.5 + X.ravel() / 10\nnoise = rng.lognormal(sigma=sigma) - (sigma**2 / 2)\ny = expected_y + noise",
            "rng = (4)\nX_train = rng.uniform(0, 5, 10).reshape(-1, 1)\ny_train = ((X_train[:, 0] - 2.5) ** 2)\nn_samples = 5",
            "Literacy:Wealth    0.018176\ndtype: float64\n\nLiteracy           0.427386\nWealth             1.080987\nLiteracy:Wealth   -0.013609\ndtype: float64",
            "Matrix density : 0.626%\nSparse Lasso done in 0.117s\nDense Lasso done in  0.805s\nDistance between coefficients : 8.65e-12"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Dataset and Gaussian process generation"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->Multiplicative interactions"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Sparse data"
            ]
        ]
    },
    "700685": {
        "jupyter_code_cell": "sender_count = Counter(in_set[:,0])\nreceiver_count = Counter(in_set[:,1])\ntop_5_senders = sender_count.most_common(5)\ntop_5_receivers = receiver_count.most_common(5)\n\nprint(top_5_senders)\nprint(top_5_receivers)",
        "matched_tutorial_code_inds": [
            2106,
            5799,
            6884,
            2845,
            4040
        ],
        "matched_tutorial_codes": [
            "iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)",
            "model1 = sm.GLM.from_formula(\n    \"blotch ~ 0 + C(variety) + C(site)\", family=sm.families.Binomial(), data=df\n)\nresult1 = model1.fit(scale=\"X2\")\nprint(result1.summary())",
            "res1 = ols(formula=\"Lottery ~ Literacy : Wealth - 1\", data=df).fit()\nres2 = ols(formula=\"Lottery ~ Literacy * Wealth - 1\", data=df).fit()\nprint(res1.params, \"\\n\")\nprint(res2.params)",
            "feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "dots = sns.load_dataset(\"dots\").query(\"align == 'dots'\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\",\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->Multiplicative interactions"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings"
            ]
        ]
    },
    "269949": {
        "jupyter_code_cell": "# output\ndf.to_csv(pwd + 'output/selangor.csv', sep=';', index=False )",
        "matched_tutorial_code_inds": [
            3812,
            3684,
            4021,
            3226,
            2692
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "from sklearn.datasets import \n\nX, y = (return_X_y=True, as_frame=True)\nX.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Dataset"
            ]
        ]
    },
    "424934": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\n\nd_data = {\n    'names' : ['tom' , 'tim', 'jim', 'ali'] ,\n    'age' : [12, 44, 56, 9]   \n}\n\ndf = pd.DataFrame(d_data)\ndf",
        "matched_tutorial_code_inds": [
            6280,
            2053,
            3103,
            3461,
            5635
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "import sys\nfrom time import \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.random_projection import \nfrom sklearn.random_projection import \nfrom sklearn.datasets import \nfrom sklearn.datasets import \nfrom sklearn.metrics.pairwise import euclidean_distances",
            "import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ]
        ]
    },
    "381195": {
        "jupyter_code_cell": "\nreg_labels = reg_list[0]\n\nreg_list = reg_list[1:]\nreg_list[:5]  #To view first five rows as example",
        "matched_tutorial_code_inds": [
            2845,
            4022,
            2443,
            5341,
            4053
        ],
        "matched_tutorial_codes": [
            "feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "name = [\"t value\", \"p value\"]\ntest = sms.linear_harvey_collier(results)\nlzip(name, test)",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Regression Diagnostics->Linearity"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ]
        ]
    },
    "93910": {
        "jupyter_code_cell": "voices.isnull().values.any()",
        "matched_tutorial_code_inds": [
            3720,
            1500,
            3723,
            3734,
            6801
        ],
        "matched_tutorial_codes": [
            "flights.dep_time.unique()",
            "china_mask.nonzero()",
            "flights.dep_time.head()",
            "delays.nsmallest(5).sort_values()",
            "res.forecast_components(12)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data"
            ]
        ]
    },
    "1508176": {
        "jupyter_code_cell": "breed_acc_df.sort_values(by='inception_accuracy', ascending=True).head(10).plot.barh(figsize=(8, 10))",
        "matched_tutorial_code_inds": [
            3817,
            7007,
            4031,
            6194,
            3830
        ],
        "matched_tutorial_codes": [
            "df.plot.scatter(x='carat', y='depth', c='k', alpha=.15)\nplt.tight_layout()",
            "fig = pandas_ar_res.plot_predict(start=\"2005\", end=\"2027\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\"/>",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "fig = res_glob.plot_predict(start=714, end=732)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\"/>",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Pandas Built-in Plotting"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "70396": {
        "jupyter_code_cell": "### Create personal ratings vector\nmy_ratings = M_copy.ix['Daniel Lee']\nmy_ratings[np.isnan(my_ratings)] = 0\nmy_ratings",
        "matched_tutorial_code_inds": [
            6434,
            3857,
            6703,
            6735,
            6848
        ],
        "matched_tutorial_codes": [
            "# In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# Set sampling params\nndraws = 3000  #  3000 number of draws from the distribution\nnburn = 600  # 600 number of \"burn-in points\" (which will be discarded)",
            "from patsy.contrasts import Diff\n\ncontrast = Diff().code_without_intercept(levels)\nprint(contrast.matrix)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->Bayesian estimation"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Backward Difference Coding"
            ]
        ]
    },
    "1382167": {
        "jupyter_code_cell": "X['international_plan'] = pd.get_dummies(X.international_plan)[' yes']\nX['voice_mail'] = pd.get_dummies(X.voice_mail)[' yes']\n#pd.get_dummies(X.voice_mail)",
        "matched_tutorial_code_inds": [
            3821,
            5586,
            1811,
            3812,
            3820
        ],
        "matched_tutorial_codes": [
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "encoded = enc.transform(([[\"dog\"], [\"snake\"], [\"cat\"], [\"rabbit\"]]))\n(encoded, columns=enc.get_feature_names_out())\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Grouping infrequent categories in OneHotEncoder"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "833317": {
        "jupyter_code_cell": "tstrt = 20.00 * 60  # time in minutes at which the battery inverter is switched on\nratingPV = 6000.  # W minimum installed panel capacity\nbatvolt = 48 # nominal battery voltage\nbatCap = 800. * batvolt  # Wh  battery capacity in Ah * voltage\nbatDoD = 0.27 # depth of dischage\nbatACEff = 0.9 # battery discharge efficiency only this much is available as AC power\nbatDCEff = 0.9 # battery charge efficiency only this much is available as charge\nmaxInverter = 6000  # W maximum that the inverter can supply\nmaxBatCharge = 40 # maximum battery charge current\n\ncalcProfile(tstrt, ratingPV, batvolt, batCap, batDoD, batACEff, batDCEff,\n            maxInverter,maxBatCharge,reqAC,removedAC,cost=1.5, storedinGrid=False);",
        "matched_tutorial_code_inds": [
            3674,
            4954,
            1677,
            1679,
            4682
        ],
        "matched_tutorial_codes": [
            "stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))",
            "delta = 0.1\nx = np.arange(-3.0, 4.001, delta)\ny = np.arange(-4.0, 3.001, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (0.9*Z1 - 0.5*Z2) * 2\n\n# select a divergent colormap\ncmap = cm.coolwarm\n\nfig, (ax1, ax2) = plt.subplots(ncols=2)\npc = ax1.pcolormesh(Z, cmap=cmap)\nfig.colorbar(pc, ax=ax1)\nax1.set_title('Normalize()')\n\npc = ax2.pcolormesh(Z, norm=colors.CenteredNorm(), cmap=cmap)\nfig.colorbar(pc, ax=ax2)\nax2.set_title('CenteredNorm()')\n\nplt.show()\n\n\n<img alt=\"Normalize(), CenteredNorm()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_002.png\" srcset=\"../../_images/sphx_glr_colormapnorms_002.png, ../../_images/sphx_glr_colormapnorms_002_2_0x.png 2.0x\"/>",
            "output = julia(mesh, c=-0.75 + 0.4j, num_iter=20)\nkwargs = {'title': 'f(z) = z^2 - \\dfrac{3}{4} + 0.4i', 'cmap': 'Greens_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\" src=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\"/>",
            "output = mandelbrot(mesh, num_iter=50)\nkwargs = {'title': 'Mandelbrot \\ set', 'cmap': 'hot'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/20dd449f2a6134d4842d5337133c4150c22d59046f8a6076cf707296b245644c.png\" src=\"../_images/20dd449f2a6134d4842d5337133c4150c22d59046f8a6076cf707296b245644c.png\"/>",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Centered"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Julia set"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Mandelbrot set"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ]
        ]
    },
    "901914": {
        "jupyter_code_cell": "p = 13\nrun = 50\n\nn = [3,4,6]\n\ndf = pd.DataFrame(sobol_seq.i4_sobol_generate(6*k, run*2**p))",
        "matched_tutorial_code_inds": [
            4167,
            2630,
            4166,
            3812,
            6884
        ],
        "matched_tutorial_codes": [
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "rng = (4)\nX_train = rng.uniform(0, 5, 10).reshape(-1, 1)\ny_train = ((X_train[:, 0] - 2.5) ** 2)\nn_samples = 5",
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "res1 = ols(formula=\"Lottery ~ Literacy : Wealth - 1\", data=df).fit()\nres2 = ols(formula=\"Lottery ~ Literacy * Wealth - 1\", data=df).fit()\nprint(res1.params, \"\\n\")\nprint(res2.params)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Dataset and Gaussian process generation"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->Multiplicative interactions"
            ]
        ]
    },
    "860767": {
        "jupyter_code_cell": "Median_Salary_Sorted = Median.sort_values('Salary', ascending=False)\nMedian_Salary_Sorted.Salary.plot(kind='bar', figsize = (10, 5))\nplt.ylabel('Median Salary per team (in terms of $10M)')",
        "matched_tutorial_code_inds": [
            3857,
            3979,
            4432,
            1918,
            2586
        ],
        "matched_tutorial_codes": [
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "N = 21\n D = np.diag(0.5*np.ones(N-1), k=1) - np.diag(0.5*np.ones(N-1), k=-1)\n D[0] = D[-1] = 0 # take away edge effects\n Dop = FirstDerivative(N, dtype=np.float64)",
            "Accuracy (train) for L1 logistic: 83.3%\nAccuracy (train) for L2 logistic (Multinomial): 82.7%\nAccuracy (train) for L2 logistic (OvR): 79.3%\nAccuracy (train) for Linear SVC: 82.0%\nAccuracy (train) for GPC: 82.7%\n\n\n\n<br/>",
            "gaussian_process = (\n    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9\n)\ngaussian_process.fit(X_train, y_train_noisy)\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Use of LinearOperator"
            ],
            [
                "sklearn->Examples->Classification->Plot classification probability"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noisy targets"
            ]
        ]
    },
    "657375": {
        "jupyter_code_cell": "# Jitter plot\n# How does online backup service affect monthly charges?\nfrom plotnine import *\n(ggplot(df_churn, aes(x='OnlineBackup', y='MonthlyCharges')) + geom_jitter(position=position_jitter(0.4)))",
        "matched_tutorial_code_inds": [
            6416,
            1138,
            6531,
            406,
            6437
        ],
        "matched_tutorial_codes": [
            "# Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "# Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "# Create the model\nextended_mod = ExtendedDFM(endog)\ninitial_extended_res = extended_mod.fit(maxiter=1000, disp=False)\nextended_res = extended_mod.fit(initial_extended_res.params, method='nm', maxiter=1000)\nprint(extended_res.summary(separate_params=False))",
            "# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
            "# Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Appendix 1: Extending the dynamic factor model"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ]
        ]
    },
    "995131": {
        "jupyter_code_cell": "sns.barplot(x='Month',y='RSPM/PM10',data=df[df['City/Town/Village/Area']=='Bongaigaon']).set_title('Mean RSPM/PM10 level in Bonaigaon')",
        "matched_tutorial_code_inds": [
            3938,
            3936,
            4084,
            4063,
            4085
        ],
        "matched_tutorial_codes": [
            "sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n",
            "sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n",
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", levels=[.01, .05, .1, .8])\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ]
        ]
    },
    "715594": {
        "jupyter_code_cell": "ax = df_count.rolling(96).mean().plot()\nplt.show()",
        "matched_tutorial_code_inds": [
            3817,
            6806,
            3820,
            3821,
            4110
        ],
        "matched_tutorial_codes": [
            "df.plot.scatter(x='carat', y='depth', c='k', alpha=.15)\nplt.tight_layout()",
            "ax = res.plot_predict(24, theta=2)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_20_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_20_0.png\"/>",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "g = sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"violin\", inner=None)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", color=\"k\", size=3, ax=g.ax)\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Pandas Built-in Plotting"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots",
                "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots"
            ]
        ]
    },
    "1054586": {
        "jupyter_code_cell": "sns.distplot(cdf['Fare'], bins=5, kde=False)\nplt.show()",
        "matched_tutorial_code_inds": [
            3936,
            3938,
            3820,
            4063,
            4084
        ],
        "matched_tutorial_codes": [
            "sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ]
        ]
    },
    "969048": {
        "jupyter_code_cell": "for c in ['Gold', 'Silver', 'Bronze', 'Total']:\n    dfn[c]=dfn[c].astype(int)",
        "matched_tutorial_code_inds": [
            3975,
            3979,
            1451,
            3857,
            3859
        ],
        "matched_tutorial_codes": [
            "flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "Sigma = np.zeros((3, 768, 1024))\nfor j in range(3):\n    np.fill_diagonal(Sigma[j, :, :], s[j, :])",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ]
        ]
    },
    "1514550": {
        "jupyter_code_cell": "import pymc3 as pm  \nfrom scipy.optimize import fmin_powell",
        "matched_tutorial_code_inds": [
            3802,
            4273,
            4175,
            3203,
            659
        ],
        "matched_tutorial_codes": [
            "import statsmodels.formula.api as sm\n\ndf['home_win'] = df.home_win.astype(int)  # for statsmodels",
            "import numpy as np\n from scipy.optimize import minimize",
            "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n",
            "from sklearn.datasets import \nfrom sklearn.linear_model import \n\nX, y = (return_X_y=True)\nlr = ()",
            "import torch\nimport torch.fx\nimport torchvision.models as models\n\nrn18 = ()\n()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics",
                "seaborn->Figure aesthetics->Controlling figure aesthetics"
            ],
            [
                "sklearn->Examples->Model Selection->Plotting Cross-Validated Predictions",
                "sklearn->Examples->Model Selection->Plotting Learning Curves and Checking Models' Scalability"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX"
            ]
        ]
    },
    "87548": {
        "jupyter_code_cell": "noshowappointments['PatientId'] = str(noshowappointments['PatientId'])\nnoshowappointments['AppointmentID'] = str(noshowappointments['AppointmentID'])",
        "matched_tutorial_code_inds": [
            1757,
            1579,
            156,
            6449,
            1569
        ],
        "matched_tutorial_codes": [
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
            "m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format"
            ]
        ]
    },
    "1243566": {
        "jupyter_code_cell": "import pandas as pd\n\n# #downloading dataset\n## !wget -nv -O /resources/data/china_gdp.csv https://ibm.box.com/shared/static/ccd2tu4wvkwi1f6yp4mm1ztsyt1ygphv.csv\n# df = pd.read_csv(\"/resources/data/china_gdp.csv\")\n\n# JAG this is to be switche with DSWB path\ndf = pd.read_csv(\"https://ibm.box.com/shared/static/7tr6tai74kdk3vik815k2frl54kfuw4h.csv\")\n\ndf.head()",
        "matched_tutorial_code_inds": [
            2548,
            3249,
            3392,
            4417,
            2650
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\n\ndf = (grid_search.cv_results_)[\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n]\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\ndf = df.rename(\n    columns={\n        \"param_n_components\": \"Number of components\",\n        \"param_covariance_type\": \"Type of covariance\",\n        \"mean_test_score\": \"BIC score\",\n    }\n)\ndf.sort_values(by=\"BIC score\").head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import pandas as pd\n\nresults_df = (search.cv_results_)\nresults_df = results_df.sort_values(by=[\"rank_test_score\"])\nresults_df = results_df.set_index(\n    results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n).rename_axis(\"kernel\")\nresults_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import pandas as pd\n\ncv_results = (search_cv.cv_results_)\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\ncv_results[\n    [\n        \"mean_test_score\",\n        \"std_test_score\",\n        \"param_preprocessor__num__imputer__strategy\",\n        \"param_preprocessor__cat__selector__percentile\",\n        \"param_classifier__C\",\n    ]\n].head(5)\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "import numpy as np\n from scipy import linalg\n A = np.array([[1,2,3],[4,5,6]])\n A\narray([[1, 2, 3],\n      [4, 5, 6]])\n M,N = A.shape\n U,s,Vh = linalg.svd(A)\n Sig = linalg.diagsvd(s,M,N)\n U, Vh = U, Vh\n U\narray([[-0.3863177 , -0.92236578],\n      [-0.92236578,  0.3863177 ]])\n Sig\narray([[ 9.508032  ,  0.        ,  0.        ],\n      [ 0.        ,  0.77286964,  0.        ]])\n Vh\narray([[-0.42866713, -0.56630692, -0.7039467 ],\n      [ 0.80596391,  0.11238241, -0.58119908],\n      [ 0.40824829, -0.81649658,  0.40824829]])\n U.dot(Sig.dot(Vh)) #check computation\narray([[ 1.,  2.,  3.],\n      [ 4.,  5.,  6.]])\n\n\n<aside class=\"footnote-list brackets\">\n<aside class=\"footnote brackets\" id=\"id3\" role=\"note\">\n[1]",
            "import pandas as pd\nfrom sklearn.linear_model import , , \n\nolr = ().fit(X, y)\nbrr = (compute_score=True, n_iter=30).fit(X, y)\nard = (compute_score=True, n_iter=30).fit(X, y)\ndf = (\n    {\n        \"Weights of true generative process\": true_weights,\n        \"ARDRegression\": ard.coef_,\n        \"BayesianRidge\": brr.coef_,\n        \"LinearRegression\": olr.coef_,\n    }\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the BIC scores"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "scipy->Linear Algebra (scipy.linalg)->Decompositions->Singular value decomposition"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Models robustness to recover the ground truth weights->Fit the regressors"
            ]
        ]
    },
    "110251": {
        "jupyter_code_cell": "df1.columns",
        "matched_tutorial_code_inds": [
            6119,
            3718,
            6006,
            5561,
            5776
        ],
        "matched_tutorial_codes": [
            "dta_c.T[0]",
            "flights.dep_time",
            "df_infl[:5]",
            "data_student.dtypes",
            "res_o.pvalues"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder"
            ]
        ]
    },
    "665484": {
        "jupyter_code_cell": "boundary_distances2 = pd.read_csv(\"MR_Data/Earthquake_boundary_distances2\")\nboundary_distances2 = boundary_distances2.drop(\"Unnamed: 0\", axis = 1)",
        "matched_tutorial_code_inds": [
            3731,
            3833,
            5240,
            3730,
            6811
        ],
        "matched_tutorial_codes": [
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "102825": {
        "jupyter_code_cell": "import pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nsys.path.append('../utils')\nimport DataAggregation as da\nimport AlgoUtils as au\ncmap_bold = ListedColormap(['#00FF00','#FF0000'])",
        "matched_tutorial_code_inds": [
            2969,
            859,
            1616,
            3267,
            2892
        ],
        "matched_tutorial_codes": [
            "from collections import \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import \nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import \n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import \nfrom sklearn.model_selection import",
            "import torch\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nnew_h, new_C = rnn(X, (h, C))",
            "import numpy as np\nnum_imgs = 9\n\ncombined_xray_images_1 = np.array(\n    [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)]\n)",
            "import pandas as pd\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom scipy.stats import \nimport numpy as np\n\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import \nfrom sklearn.ensemble import",
            "from sklearn.model_selection import \n\ntarget_name = \"hourly wage\"\nX, y = df.drop(columns=target_name), df[target_name]\nX_train, X_test, y_train, y_test = (X, y, test_size=0.2, random_state=0)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features"
            ],
            [
                "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Motivation and Example"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Combine images into a multidimensional array to demonstrate progression"
            ],
            [
                "sklearn->Examples->Model Selection->Successive Halving Iterations"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Description of the simulated data"
            ]
        ]
    },
    "451367": {
        "jupyter_code_cell": "df_jdbc.loc[0, 'lines'] = 250\ndf_jdbc.head()",
        "matched_tutorial_code_inds": [
            6911,
            3801,
            3830,
            2325,
            6005
        ],
        "matched_tutorial_codes": [
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "gbdt_no_cst = ()\ngbdt_no_cst.fit(X, y)",
            "df_infl = infl.summary_frame()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "185810": {
        "jupyter_code_cell": "## Seaborn offers a powerful tool called FacetGrid for making small multiples of matplotlib graphs:\n\n### Create an empty set of grids:\nfacet_histograms = sns.FacetGrid(ein_empl_lim, col='year', hue='year')\n\n## \"map' a histogram to each grid:\nfacet_histograms = facet_histograms.map(plt.hist, 'avg_wage')\n\n## Data Sourcing:\nplt.annotate('Source: MO Department of Labor', xy=(0.6,-0.35), xycoords=\"axes fraction\")\nplt.show()",
        "matched_tutorial_code_inds": [
            5110,
            6635,
            5118,
            5116,
            6628
        ],
        "matched_tutorial_codes": [
            "# Load modules and data\nIn [1]: import numpy as np\n\nIn [2]: import statsmodels.api as sm\n\nIn [3]: spector_data = sm.datasets.spector.load()\n\nIn [4]: spector_data.exog = sm.add_constant(spector_data.exog, prepend=False)\n\n# Fit and summarize OLS model\nIn [5]: mod = sm.OLS(spector_data.endog, spector_data.exog)\n\nIn [6]: res = mod.fit()\n\nIn [7]: print(res.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  GRADE   R-squared:                       0.416\nModel:                            OLS   Adj. R-squared:                  0.353\nMethod:                 Least Squares   F-statistic:                     6.646\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):            0.00157\nTime:                        17:12:47   Log-Likelihood:                -12.978\nNo. Observations:                  32   AIC:                             33.96\nDf Residuals:                      28   BIC:                             39.82\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGPA            0.4639      0.162      2.864      0.008       0.132       0.796\nTUCE           0.0105      0.019      0.539      0.594      -0.029       0.050\nPSI            0.3786      0.139      2.720      0.011       0.093       0.664\nconst         -1.4980      0.524     -2.859      0.008      -2.571      -0.425\n==============================================================================\nOmnibus:                        0.176   Durbin-Watson:                   2.346\nProb(Omnibus):                  0.916   Jarque-Bera (JB):                0.167\nSkew:                           0.141   Prob(JB):                        0.920\nKurtosis:                       2.786   Cond. No.                         176.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "# Here, for illustration purposes only, we plot the time-varying\n# coefficients conditional on an ad-hoc parameterization\n\n# Recall that `initial_res` contains the Kalman filtering and smoothing,\n# and the `states.smoothed` attribute contains the smoothed states\nplot_coefficients_by_equation(initial_res.states.smoothed);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\"/>",
            "# Load the data from Spector and Mazzeo (1980)\nIn [1]: import statsmodels.api as sm\n\nIn [2]: spector_data = sm.datasets.spector.load_pandas()\n\nIn [3]: spector_data.exog = sm.add_constant(spector_data.exog)\n\n# Logit Model\nIn [4]: logit_mod = sm.Logit(spector_data.endog, spector_data.exog)\n\nIn [5]: logit_res = logit_mod.fit()\nOptimization terminated successfully.\n         Current function value: 0.402801\n         Iterations 7\n\nIn [6]: print(logit_res.summary())\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  GRADE   No. Observations:                   32\nModel:                          Logit   Df Residuals:                       28\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.3740\nTime:                        17:12:32   Log-Likelihood:                -12.890\nconverged:                       True   LL-Null:                       -20.592\nCovariance Type:            nonrobust   LLR p-value:                  0.001502\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -13.0213      4.931     -2.641      0.008     -22.687      -3.356\nGPA            2.8261      1.263      2.238      0.025       0.351       5.301\nTUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\nPSI            2.3787      1.065      2.234      0.025       0.292       4.465\n==============================================================================",
            "# Load modules and data\nIn [1]: import statsmodels.api as sm\n\nIn [2]: data = sm.datasets.stackloss.load()\n\nIn [3]: data.exog = sm.add_constant(data.exog)\n\n# Fit model and print summary\nIn [4]: rlm_model = sm.RLM(data.endog, data.exog, M=sm.robust.norms.HuberT())\n\nIn [5]: rlm_results = rlm_model.fit()\n\nIn [6]: print(rlm_results.params)\nconst       -41.026498\nAIRFLOW       0.829384\nWATERTEMP     0.926066\nACIDCONC     -0.127847\ndtype: float64",
            "# Plot the inflation data along with simulated trends\nfig, axes = plt.subplots(2, figsize=(15, 6))\n\n# Plot data and KFS simulations\ndta.infl.plot(ax=axes[0], color='k')\naxes[0].set_title('Simulations based on KFS approach, MLE parameters')\nsimulated_state_kfs.plot(ax=axes[0], color='C0', alpha=0.25, legend=False)\n\n# Plot data and CFA simulations\ndta.infl.plot(ax=axes[1], color='k')\naxes[1].set_title('Simulations based on CFA approach, MLE parameters')\nsimulated_state_cfa.plot(ax=axes[1], color='C0', alpha=0.25, legend=False)\n\n# Add a legend, clean up layout\nhandles, labels = axes[0].get_legend_handles_labels()\naxes[0].legend(handles[:2], ['Data', 'Simulated state'])\nfig.tight_layout();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->User Guide->Regression and Linear Models->Linear Regression->Examples"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q"
            ],
            [
                "statsmodels->User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Examples"
            ],
            [
                "statsmodels->User Guide->Regression and Linear Models->Robust Linear Models->Examples"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Local level model"
            ]
        ]
    },
    "609621": {
        "jupyter_code_cell": "selected_vars = np.array(selected_vars)\nimportances = result.feature_importances_\nimportant_names = selected_vars[importances > np.mean(importances)]\nprint (important_names)",
        "matched_tutorial_code_inds": [
            5375,
            3257,
            5454,
            5368,
            5246
        ],
        "matched_tutorial_codes": [
            "oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "# initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "eigs = np.linalg.eigvals(norm_xtx)\ncondition_number = np.sqrt(eigs.max() / eigs.min())\nprint(condition_number)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Condition number"
            ]
        ]
    },
    "432689": {
        "jupyter_code_cell": "cold_days = freezing_days[freezing_days.min_temp >= 20]\ncold_days.info()",
        "matched_tutorial_code_inds": [
            3684,
            4153,
            1714,
            162,
            2822
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "135317": {
        "jupyter_code_cell": "dfWAR.Nationality.value_counts()",
        "matched_tutorial_code_inds": [
            6013,
            4421,
            2903,
            2431,
            2436
        ],
        "matched_tutorial_codes": [
            "interM_lm.model.data.orig_exog[:5]",
            "evals_all, evecs_all = eigh(X)",
            "X_train.info()",
            "df[\"count\"].max()",
            "X[\"weather\"].value_counts()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Examples"
            ],
            [
                "sklearn->Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->Bike sharing dataset preprocessing",
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset",
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset",
                "sklearn->Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->Bike sharing dataset preprocessing"
            ]
        ]
    },
    "1161768": {
        "jupyter_code_cell": "df = pd.read_csv('pandas_dataframe_importing_csv/example.csv', header=None)\ndf",
        "matched_tutorial_code_inds": [
            6759,
            3641,
            7002,
            3967,
            2435
        ],
        "matched_tutorial_codes": [
            "df = pred.summary_frame(alpha=0.05)\ndf",
            "f = pd.DataFrame({'a':[1,2,3,4,5], 'b':[10,20,30,40,50]})\nf",
            "data.endog.index = dates\nendog = data.endog\nendog",
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "X = df.drop(\"count\", axis=\"columns\")\nX"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->SettingWithCopy"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset"
            ]
        ]
    },
    "1409703": {
        "jupyter_code_cell": "classes = list(set(data['activity']))\nfor activity in classes:\n    nb = np.sum(data['activity'] == activity)\n    print(\"{:<15}{:<9d}{:<5.2f} %\".format(activity, nb, 100. * nb / data.shape[0]))\nprint()\nprint(\"Number of objects: {:d}\".format(data.shape[0]))",
        "matched_tutorial_code_inds": [
            2101,
            2773,
            3290,
            3726,
            1978
        ],
        "matched_tutorial_codes": [
            "clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = (random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "unique_labels = set(labels)\ncore_samples_mask = (labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in (0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\n(f\"Estimated number of clusters: {n_clusters_}\")\n()\n\n\n<img alt=\"Estimated number of clusters: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_002.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Total impurity of leaves vs effective alphas of pruned tree"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Plot results"
            ]
        ]
    },
    "850244": {
        "jupyter_code_cell": "one_tenth = google.sample(frac = .1, random_state=np.random.randint(10))",
        "matched_tutorial_code_inds": [
            3607,
            4183,
            3637,
            4621,
            6477
        ],
        "matched_tutorial_codes": [
            "twenty_train.target[:10]\narray([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])",
            "sinplot()\nsns.despine()\n",
            "first.ix[10:15, ['fl_date', 'tail_num']]",
            "ndi_sum(image, labels, [0, 2])\narray([178.0, 80.0])",
            "t = y.shape[0]\nma_res.predict(t - 3, t - 1)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Tutorials->Working With Text Data->Loading the 20 newsgroups dataset"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "scipy->Multidimensional image processing (scipy.ndimage)->Object measurements"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->Prediction with MA components"
            ]
        ]
    },
    "982874": {
        "jupyter_code_cell": "indices = []\nfor i in range(0,len(data['Zip_code'])):\n\n    if(data['Zip_code'][i].isdigit()):\n        x = int(data['Zip_code'][i])\n    else:\n        indices.append(i)",
        "matched_tutorial_code_inds": [
            184,
            403,
            3730,
            4274,
            6765
        ],
        "matched_tutorial_codes": [
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "def rosen(x):\n...     \"\"\"The Rosenbrock function\"\"\"\n...     return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ]
        ]
    },
    "384142": {
        "jupyter_code_cell": "import dill\nwith open('tutorial_results.pkl', 'rb') as f:\n    successes = dill.load(f)\n    failures = dill.load(f)",
        "matched_tutorial_code_inds": [
            1612,
            3812,
            3448,
            6546,
            5058
        ],
        "matched_tutorial_codes": [
            "import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)",
            "from pandas_datareader.data import DataReader\nendog = DataReader('UNRATE', 'fred', start='1954-01-01')\nendog.index.freq = endog.index.inferred_freq",
            "import mpl_toolkits.axisartist as AA\nfig = plt.figure()\nfig.add_axes([0.1, 0.1, 0.8, 0.8], axes_class=AA.Axes)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ],
            [
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist"
            ]
        ]
    },
    "642078": {
        "jupyter_code_cell": "dfs_piv_1 = dfs_piv[dfs_piv['group']==1]\nfig, axs = plt.subplots(1,3)\ndfs_piv_1.boxplot(column='randomness',ax=axs[0],return_type='dict')\ndfs_piv_1.boxplot(column='real_person',ax=axs[1],return_type='dict')\ndfs_piv_1.boxplot(column='control',ax=axs[2],return_type='dict')",
        "matched_tutorial_code_inds": [
            3008,
            6022,
            5719,
            6195,
            2974
        ],
        "matched_tutorial_codes": [
            "diabetes = ()\nX = (diabetes.data, columns=diabetes.feature_names)\ny = diabetes.target\n\ntree = ()\nmlp = (\n    (),\n    (hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0),\n)\ntree.fit(X, y)\nmlp.fit(X, y)",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "res_ar5 = AutoReg(ind_prod, 5, old_names=False).fit()\npredictions = pd.DataFrame(\n    {\n        \"AR(5)\": res_ar5.predict(start=714, end=726),\n        \"AR(13)\": res.predict(start=714, end=726),\n        \"Restr. AR(13)\": res_glob.predict(start=714, end=726),\n    }\n)\n_, ax = plt.subplots()\nax = predictions.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\"/>",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Train models on the diabetes dataset"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ]
        ]
    },
    "994549": {
        "jupyter_code_cell": "route = data.groupby(data['route_name']).sum()[['ons','offs']]\nroute['avg']=route.mean(axis=1)\nroute['avg'][route['avg']==route['avg'].max()].index[0]",
        "matched_tutorial_code_inds": [
            3919,
            3889,
            3783,
            6512,
            3644
        ],
        "matched_tutorial_codes": [
            "subset = daily.loc['2011':'2016']\nax = subset.div(1000).plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",)\nsns.despine();",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "df['home_win'] = df['home_points'] &gt; df['away_points']\ndf['rest_spread'] = df['home_rest'] - df['away_rest']\ndf.dropna().head()",
            "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Caution: VARMA(p,q) specifications"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ]
        ]
    },
    "687027": {
        "jupyter_code_cell": "fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(comps_sqrt_2diff_sdiff.dropna(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(comps_sqrt_2diff_sdiff.dropna(), lags=40, ax=ax2)",
        "matched_tutorial_code_inds": [
            6230,
            6564,
            6248,
            6576,
            6247
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(12, 8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)",
            "fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)",
            "fig = plt.figure(figsize=(12, 8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(resid.values.squeeze(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)",
            "fig = plt.figure(figsize=(12,4))\nax = fig.add_subplot(111)\nfig = qqplot(resid, line='q', ax=ax, fit=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_21_0.png\"/>",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nfig = qqplot(resid, line=\"q\", ax=ax, fit=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_21_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_21_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ]
        ]
    },
    "1150175": {
        "jupyter_code_cell": "imgfiles = import_data(datafolder)\ndf, feature_names = extract_features(imgfiles, feature_funcs, 10, 10)",
        "matched_tutorial_code_inds": [
            5438,
            4166,
            5454,
            6898,
            5240
        ],
        "matched_tutorial_codes": [
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ]
        ]
    },
    "353538": {
        "jupyter_code_cell": "for i in range(2,24,1): \n    LIMIT_L = 1990\n    LIMIT_H = LIMIT_L+3040 #2 TAU\n    PULSE_R = 1738\n    hdf5_file     = ''.join([path,'cal_1u.h5.z'])\n    Channel       = i\n    event_range      = range(0,500,1)\n\n    coeff[i,0] = find_coeff_II( LIMIT_L, LIMIT_H, PULSE_R,\n                                hdf5_file, Channel, event_range )\n    print(\"CH\",i,'=',coeff[i,0])",
        "matched_tutorial_code_inds": [
            2968,
            58,
            6638,
            80,
            6364
        ],
        "matched_tutorial_codes": [
            "for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>",
            "for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')",
            "for i in range(niter):\n    mod.update_variances(store_obs_cov[i], store_state_cov[i])\n    sim.simulate()\n\n    # 1. Sample states\n    store_states[i + 1] = sim.simulated_state.T\n\n    # 2. Simulate obs cov\n    fitted = np.matmul(mod['design'].transpose(2, 0, 1), store_states[i + 1][..., None])[..., 0]\n    resid = mod.endog - fitted\n    store_obs_cov[i + 1] = invwishart.rvs(v10 + mod.nobs, S10 + resid.T @ resid)\n\n    # 3. Simulate state cov variances\n    resid = store_states[i + 1, 1:] - store_states[i + 1, :-1]\n    sse = np.sum(resid**2, axis=0)\n\n    for j in range(mod.k_states):\n        rv = invgamma.rvs((vi20 + mod.nobs - 1) / 2, scale=(Si20 + sse[j]) / 2)\n        store_state_cov[i + 1, j] = rv",
            "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        # Runs the forward pass under autocast.\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            # output is float16 because linear layers autocast to float16.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss is float32 because mse_loss layers autocast to float32.\n            assert loss.dtype is torch.float32\n\n        # Exits autocast before backward().\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->5. Zero the gradients while training the network"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC"
            ],
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->Adding autocast"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ]
        ]
    },
    "64732": {
        "jupyter_code_cell": "plot = df.plot(x='I',y='V', kind='scatter', \n               title=r'$V$ $\\rm vs.$ $I$ (with error bars and trendline)',\n              xerr = 0.05, yerr = 0.1);\n\nplot.set_xlabel(r'$I$ $\\rm (Amperes)$')\nplot.set_ylabel(r'$V$ $\\rm (Volts)$')\n\nx1 = 1; y1 = a + b*x1\nx2 = 12; y2 = a + b*x2\n\nplt.plot([x1,x2],[y1,y2],'k-')\nplt.text(7, 6000, r'$y = %.2f x + (%.2f) $' % (b, a), fontsize=14)",
        "matched_tutorial_code_inds": [
            6552,
            3916,
            4826,
            4869,
            4828
        ],
        "matched_tutorial_codes": [
            "fig, axes = plt.subplots(2, figsize=(13,5));\naxes[0].set(title='Level/trend component')\naxes[0].plot(endog.index, res_uc.level.smoothed, label='UC')\naxes[0].plot(endog.index, res_ucarima.level.smoothed, label='UC-ARIMA(2,0)')\naxes[0].plot(hp_trend, label='HP Filter')\naxes[0].legend(loc='upper left')\naxes[0].grid()\n\naxes[1].set(title='Cycle component')\naxes[1].plot(endog.index, res_uc.cycle.smoothed, label='UC')\naxes[1].plot(endog.index, res_ucarima.autoregressive.smoothed, label='UC-ARIMA(2,0)')\naxes[1].plot(hp_cycle, label='HP Filter')\naxes[1].legend(loc='upper left')\naxes[1].grid()\n\nfig.tight_layout();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_cycles_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_cycles_11_0.png\"/>",
            "ax = occupation_avg.sort_values(ascending=False).plot.barh(color='k', width=0.9);\nlim = ax.get_ylim()\nax.vlines(total_avg, *lim, color='C1', linewidth=3)\nax.legend(['Average donation'])\nax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\")\nsns.despine()",
            "fig = plt.figure(figsize=(4, 6), layout=\"constrained\")\n\ngs0 = fig.add_gridspec(6, 2)\n\nax1 = fig.add_subplot(gs0[:3, 0])\nax2 = fig.add_subplot(gs0[3:, 0])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\nax = fig.add_subplot(gs0[0:2, 1])\nexample_plot(ax, hide_labels=True)\nax = fig.add_subplot(gs0[2:4, 1])\nexample_plot(ax, hide_labels=True)\nax = fig.add_subplot(gs0[4:, 1])\nexample_plot(ax, hide_labels=True)\nfig.suptitle('Overlapping Gridspecs')\n\n\n<img alt=\"Overlapping Gridspecs, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_021.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_021.png, ../../_images/sphx_glr_constrainedlayout_guide_021_2_0x.png 2.0x\"/>",
            "fig = plt.figure(layout=\"constrained\")\ngs0 = fig.add_gridspec(1, 2)\n\ngs00 = gs0[0].subgridspec(2, 2)\ngs01 = gs0[1].subgridspec(3, 1)\n\nfor a in range(2):\n    for b in range(2):\n        ax = fig.add_subplot(gs00[a, b])\n        annotate_axes(ax, f'axLeft[{a}, {b}]', fontsize=10)\n        if a == 1 and b == 1:\n            ax.set_xlabel('xlabel')\nfor a in range(3):\n    ax = fig.add_subplot(gs01[a])\n    annotate_axes(ax, f'axRight[{a}, {b}]')\n    if a == 2:\n        ax.set_ylabel('ylabel')\n\nfig.suptitle('nested gridspecs')\n\n\n<img alt=\"nested gridspecs\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_012.png\" srcset=\"../../_images/sphx_glr_arranging_axes_012.png, ../../_images/sphx_glr_arranging_axes_012_2_0x.png 2.0x\"/>",
            "fig = plt.figure(layout=\"constrained\")\nsfigs = fig.subfigures(1, 2, width_ratios=[1, 2])\n\naxs_left = sfigs[0].subplots(2, 1)\nfor ax in axs_left.flat:\n    example_plot(ax)\n\naxs_right = sfigs[1].subplots(2, 2)\nfor ax in axs_right.flat:\n    pcm = ax.pcolormesh(arr, **pc_kwargs)\n    ax.set_xlabel('x-label')\n    ax.set_ylabel('y-label')\n    ax.set_title('title')\nfig.colorbar(pcm, ax=axs_right)\nfig.suptitle('Nested plots using subfigures')\n\n\n<img alt=\"Nested plots using subfigures, Title, Title, title, title, title, title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_023.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_023.png, ../../_images/sphx_glr_constrainedlayout_guide_023_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Trends and cycles in unemployment->Graphical comparison"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->Low-level and advanced grid methods->Nested layouts with SubplotSpec"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ]
        ]
    },
    "642959": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\n\nimport collections\n\nimport matplotlib.pyplot as plt\nimport pymc3 as pm\n\nimport scipy.stats as stats",
        "matched_tutorial_code_inds": [
            5558,
            6289,
            6561,
            6505,
            4175
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel",
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics",
                "seaborn->Figure aesthetics->Controlling figure aesthetics"
            ]
        ]
    },
    "1133605": {
        "jupyter_code_cell": "pivot_capacity_level1 = pd.pivot_table(data_selection[data_selection.energy_source_level_1 == True],\n                                       index=('country','year','source'),\n                                       columns='technology',\n                                       values='capacity',\n                                       aggfunc=sum,\n                                       margins=False)\n\npivot_capacity_level1",
        "matched_tutorial_code_inds": [
            5719,
            6022,
            6061,
            6805,
            5717
        ],
        "matched_tutorial_codes": [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)",
            "forecasts = pd.DataFrame(\n    {\n        \"ln PCE\": np.log(pce.PCE),\n        \"theta=1.2\": res.forecast(12, theta=1.2),\n        \"theta=2\": res.forecast(12),\n        \"theta=3\": res.forecast(12, theta=3),\n        \"No damping\": res.forecast(12, theta=np.inf),\n    }\n)\n_ = forecasts.tail(36).plot()\nplt.title(\"Forecasts of ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\"/>",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\", data=data, family=sm.families.Poisson()\n)\nres_o2 = glm.fit()\n# print(res_f2.summary())\nres_o2.pearson_chi2 - res_o.pearson_chi2, res_o2.deviance - res_o.deviance, res_o2.llf - res_o.llf"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ]
        ]
    },
    "1143077": {
        "jupyter_code_cell": "trips = trips[trips['trip_duration'] < 3*3600]",
        "matched_tutorial_code_inds": [
            6449,
            4092,
            1757,
            1495,
            3744
        ],
        "matched_tutorial_codes": [
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "idx = idx[idx.get_level_values(0) &lt;= idx.get_level_values(1)]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ]
        ]
    },
    "381263": {
        "jupyter_code_cell": "loan = preprocessing.LoanDefault()\nloan.logistic_bootstrap(3)",
        "matched_tutorial_code_inds": [
            4185,
            4053,
            4167,
            4152,
            4022
        ],
        "matched_tutorial_codes": [
            "sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ]
        ]
    },
    "359291": {
        "jupyter_code_cell": "fig = nba.bokeh_shot_chart(curry_shots_df, fill_color=\"color\",\n                           hover_tool=True, tooltips=tooltips)\nshow(fig)",
        "matched_tutorial_code_inds": [
            4155,
            4107,
            6311,
            6303,
            4168
        ],
        "matched_tutorial_codes": [
            "g = sns.FacetGrid(tips, row=\"smoker\", col=\"time\", margin_titles=True)\ng.map(sns.regplot, \"size\", \"total_bill\", color=\".3\", fit_reg=False, x_jitter=.1)\n",
            "sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\",\n    kind=\"violin\", bw=.15, cut=0,\n)\n",
            "fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "g = sns.PairGrid(iris, hue=\"species\")\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots",
                "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical."
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ]
        ]
    },
    "1325342": {
        "jupyter_code_cell": "import os\nfrom selenium import webdriver\nimport pandas as pd\nimport datetime, time, csv",
        "matched_tutorial_code_inds": [
            4370,
            4469,
            2820,
            4386,
            6505
        ],
        "matched_tutorial_codes": [
            "import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "import numpy as np\n from scipy import spatial\n import matplotlib.pyplot as plt",
            "import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Signal Processing (scipy.signal)->B-splines",
                "scipy->Signal Processing (scipy.signal)->Filtering->Convolution/Correlation",
                "scipy->Signal Processing (scipy.signal)->Filtering->Convolution/Correlation"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter",
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter",
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter",
                "scipy->Signal Processing (scipy.signal)->Filtering->Analog Filter Design",
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements",
                "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Spectral Analysis using Welch\u00e2\u0080\u0099s Method",
                "scipy->Signal Processing (scipy.signal)->Detrend"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ]
        ]
    },
    "85441": {
        "jupyter_code_cell": "# Initialize some variables / containers\nfolder_list = os.listdir('f_center') # folder list\n\n# create dictionary that maps key to number of files per name folder\nlen_dict = {}\n\nfor i, folder in enumerate(folder_list):\n    num_files = len(os.listdir('f_center\\%s' %folder)) \n    len_dict[i] = num_files\n    \n# Create hash that maps previous key to actual names\ndf_target_info = pd.DataFrame(index = list(len_dict.keys()), data = os.listdir('f_center'), columns = ['name'])\ndf_target_info['n_im'] = len_dict.values()",
        "matched_tutorial_code_inds": [
            1782,
            5621,
            473,
            2419,
            5110
        ],
        "matched_tutorial_codes": [
            "# To store predicted sentiments\npredictions = {}\n\n# define the length of a paragraph\npara_len = 100\n\n# Retrieve trained values of the parameters\nif os.path.isfile('tutorial-nlp-from-scratch/parameters.npy'):\n    parameters = np.load('tutorial-nlp-from-scratch/parameters.npy', allow_pickle=True).item()\n\n# This is the prediction loop.\nfor index, text in enumerate(X_pred):\n    # split each speech into paragraphs\n    paras = textproc.text_to_paras(text, para_len)\n    # To store the network outputs\n    preds = []\n\n    for para in paras:\n        # split text sample into words/tokens\n        para_tokens = textproc.word_tokeniser(para)\n        # Forward Propagation\n        caches = forward_prop(para_tokens,\n                              parameters,\n                              input_dim)\n\n        # Retrieve the output of the fully connected layer\n        sent_prob = caches['fc_values'][0]['a2'][0][0]\n        preds.append(sent_prob)\n\n    threshold = 0.5\n    preds = np.array(preds)\n    # Mark all predictions &gt; threshold as positive and &lt; threshold as negative\n    pos_indices = np.where(preds &gt; threshold)  # indices where output &gt; 0.5\n    neg_indices = np.where(preds &lt; threshold)  # indices where output &lt; 0.5\n    # Store predictions and corresponding piece of text\n    predictions[speakers[index]] = {'pos_paras': paras[pos_indices[0]],\n                                    'neg_paras': paras[neg_indices[0]]}",
            "# Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, (ker_name, ker_class) in enumerate(kernel_switch.items()):\n\n    # Initialize the kernel object\n    kernel = ker_class()\n\n    # Sample from the domain\n    domain = kernel.domain or [-3, 3]\n    x_vals = np.linspace(*domain, num=2 ** 10)\n    y_vals = kernel(x_vals)\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(ker_name))\n    ax.plot(x_vals, y_vals, lw=3, label=\"{}\".format(ker_name))\n    ax.scatter([0], [0], marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim(domain)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_21_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_21_0.png\"/>",
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "# Get data\nX2 = ()[\"data\"][:, [6, 9]]  # \"banana\"-shaped\n\n# Learn a frontier for outlier detection with several classifiers\nxx2, yy2 = ((-1, 5.5, 500), (-2.5, 19, 500))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    (2)\n    clf.fit(X2)\n    Z2 = clf.decision_function([xx2.ravel(), yy2.ravel()])\n    Z2 = Z2.reshape(xx2.shape)\n    legend2[clf_name] = (\n        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i]\n    )\n\nlegend2_values_list = list(legend2.values())\nlegend2_keys_list = list(legend2.keys())\n\n# Plot the results (= shape of the data points cloud)\n(2)  # \"banana\" shape\n(\"Outlier detection on a real data set (wine recognition)\")\n(X2[:, 0], X2[:, 1], color=\"black\")\n((xx2.min(), xx2.max()))\n((yy2.min(), yy2.max()))\n(\n    (\n        legend2_values_list[0].collections[0],\n        legend2_values_list[1].collections[0],\n        legend2_values_list[2].collections[0],\n    ),\n    (legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),\n    loc=\"upper center\",\n    prop=(size=11),\n)\n(\"color_intensity\")\n(\"flavanoids\")\n\n()\n\n\n<img alt=\"Outlier detection on a real data set (wine recognition)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\" srcset=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\"/>",
            "# Load modules and data\nIn [1]: import numpy as np\n\nIn [2]: import statsmodels.api as sm\n\nIn [3]: spector_data = sm.datasets.spector.load()\n\nIn [4]: spector_data.exog = sm.add_constant(spector_data.exog, prepend=False)\n\n# Fit and summarize OLS model\nIn [5]: mod = sm.OLS(spector_data.endog, spector_data.exog)\n\nIn [6]: res = mod.fit()\n\nIn [7]: print(res.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  GRADE   R-squared:                       0.416\nModel:                            OLS   Adj. R-squared:                  0.353\nMethod:                 Least Squares   F-statistic:                     6.646\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):            0.00157\nTime:                        17:12:47   Log-Likelihood:                -12.978\nNo. Observations:                  32   AIC:                             33.96\nDf Residuals:                      28   BIC:                             39.82\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGPA            0.4639      0.162      2.864      0.008       0.132       0.796\nTUCE           0.0105      0.019      0.539      0.594      -0.029       0.050\nPSI            0.3786      0.139      2.720      0.011       0.093       0.664\nconst         -1.4980      0.524     -2.859      0.008      -2.571      -0.425\n==============================================================================\nOmnibus:                        0.176   Durbin-Watson:                   2.346\nProb(Omnibus):                  0.916   Jarque-Bera (JB):                0.167\nSkew:                           0.141   Prob(JB):                        0.920\nKurtosis:                       2.786   Cond. No.                         176.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Outlier detection on a real data set->Second example"
            ],
            [
                "statsmodels->User Guide->Regression and Linear Models->Linear Regression->Examples"
            ]
        ]
    },
    "1081613": {
        "jupyter_code_cell": "# Build the frequency distribution table\nquotes_freq = nltk.FreqDist(quotes)\nquotes_freq = pd.DataFrame(quotes_freq.most_common(200), columns=['word','n'])\nquotes_freq['relative_frequency'] = (quotes_freq['n']/float(len(quotes)))\nquotes_freq.head(20)",
        "matched_tutorial_code_inds": [
            6699,
            6538,
            6531,
            6649,
            5638
        ],
        "matched_tutorial_codes": [
            "# Plot the updated dataset\nfig, ax = plt.subplots(figsize=(15, 3))\ny_post.plot(ax=ax)\nax.hlines(0, '2009', '2017-06', linewidth=1.0)\nax.set_xlim('2009', '2017-06');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_43_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_43_0.png\"/>",
            "# Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>",
            "# Create the model\nextended_mod = ExtendedDFM(endog)\ninitial_extended_res = extended_mod.fit(maxiter=1000, disp=False)\nextended_res = extended_mod.fit(initial_extended_res.params, method='nm', maxiter=1000)\nprint(extended_res.summary(separate_params=False))",
            "# Plot the series\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\nax.plot(inf.index, inf, label=r\"$\\Delta \\log CPI$\", lw=2)\nax.legend(loc=\"lower left\")\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_6_0.png\"/>",
            "# Plot the fit line\nfig, ax = pylab.subplots()\n\nax.scatter(x, y)\nax.plot(smoothed[:, 0], smoothed[:, 1], c=\"k\")\npylab.autoscale(enable=True, axis=\"x\", tight=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_lowess_4_0.png\" src=\"../../../_images/examples_notebooks_generated_lowess_4_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Data"
            ],
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Appendix 1: Extending the dynamic factor model"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->2. Download and plot the data on US CPI"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ]
        ]
    },
    "1175637": {
        "jupyter_code_cell": "tips_df.loc[4, 'Tip amount ($)']",
        "matched_tutorial_code_inds": [
            4093,
            4078,
            4061,
            4178,
            4071
        ],
        "matched_tutorial_codes": [
            "sns.pairplot(penguins)\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n",
            "sns.set_theme()\nsinplot()\n",
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics",
                "seaborn->Figure aesthetics->Controlling figure aesthetics"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ]
        ]
    },
    "797363": {
        "jupyter_code_cell": "display.Image(filename='Figures/plates.png')",
        "matched_tutorial_code_inds": [
            6396,
            5680,
            4074,
            4087,
            4093
        ],
        "matched_tutorial_codes": [
            "sunspots.plot(figsize=(12, 8))",
            "data.describe()",
            "sns.displot(diamonds, x=\"carat\")\n",
            "sns.displot(diamonds, x=\"color\", y=\"clarity\")\n",
            "sns.pairplot(penguins)\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ]
        ]
    },
    "1254723": {
        "jupyter_code_cell": "linear_svc_c100 = svm.SVC(C=100, kernel=\"linear\")\nlinear_svc_c100.fit(ex1_X, ex1_y)\n\nvisualizeBoundaryLinear(linear_svc_c100)",
        "matched_tutorial_code_inds": [
            6734,
            5901,
            5647,
            5457,
            6700
        ],
        "matched_tutorial_codes": [
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "rlm_mod = sm.RLM(y, X, sm.robust.norms.TrimmedMean(0.5)).fit()\nabline_plot(model_results=rlm_mod, ax=ax, color=\"red\")",
            "glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\nres = glm_binom.fit()\nprint(res.summary())",
            "poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())",
            "mod_pre = sm.tsa.DynamicFactor(y_pre, exog=const_pre, k_factors=1, factor_order=6)\nres_pre = mod_pre.fit()\nprint(res_pre.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Fit and summary"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ]
        ]
    },
    "630340": {
        "jupyter_code_cell": "from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_new_data, y_new_data, test_size=0.2,random_state=0)",
        "matched_tutorial_code_inds": [
            2800,
            3138,
            3226,
            2078,
            1094
        ],
        "matched_tutorial_codes": [
            "from sklearn.model_selection import \nfrom sklearn.linear_model import \n\n\ndf_train, df_test, X_train, X_test = (df, X, random_state=0)",
            "from sklearn.model_selection import \n\nestimator = ()\nextract_score((estimator, X, y, scoring=scoring, cv=10))\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "from sklearn.cross_decomposition import \n\ncca = (n_components=2)\ncca.fit(X_train, Y_train)\nX_train_r, Y_train_r = cca.transform(X_train, Y_train)\nX_test_r, Y_test_r = cca.transform(X_test, Y_test)",
            "from torch.quantization import convert\nmodel_ft_tuned.cpu()\n\nmodel_quantized_and_trained = convert(model_ft_tuned, inplace=False)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Frequency model \u2013 Poisson distribution"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Cross-validation of likelihood ratios"
            ],
            [
                "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning"
            ],
            [
                "sklearn->Examples->Cross decomposition->Compare cross decomposition methods->CCA (PLS mode B with symmetric deflation)"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model"
            ]
        ]
    },
    "244408": {
        "jupyter_code_cell": "wp_posts_with_eng = wp_posts_with_eng.sort_values('ID') ",
        "matched_tutorial_code_inds": [
            5712,
            1759,
            3830,
            6813,
            1495
        ],
        "matched_tutorial_codes": [
            "results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ]
        ]
    },
    "1019751": {
        "jupyter_code_cell": "_es = elasticsearch_base.connect(settings.ES_URL)\npositive_hs_filter = \"_exists_:hs_keyword_matches\"\nnegative_hs_filter = \"!_exists_:hs_keyword_matches\"\n\nhs_keywords = set(file_ops.read_csv_file(\"refined_hs_keywords\", settings.TWITTER_SEARCH_PATH))",
        "matched_tutorial_code_inds": [
            237,
            387,
            3960,
            3101,
            1256
        ],
        "matched_tutorial_codes": [
            "model = ().to(device)\nprint(model)",
            "model_ft = (pretrained=True)\nnum_ftrs = .in_features\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n = (num_ftrs, 2)\n\nmodel_ft = ()\n\n = ()\n\n# Observe that all parameters are being optimized\n = ((), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n = (, step_size=7, gamma=0.1)",
            "flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "svc_disp = (svc, X_test, y_test)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\"/>",
            "model = Net().to(rank)\nmodel = DDP(model)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Introduction to PyTorch->Build the Neural Network->Define the Class"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Finetuning the convnet"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data"
            ],
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Plotting the ROC Curve"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP"
            ]
        ]
    },
    "581774": {
        "jupyter_code_cell": "# log transform target variable and recalculate skewness\n\ntarget = np.log(train.SalePrice)\nprint(\"Skew is:\", target.skew())\n\nplt.hist(target, color = 'blue')\nplt.show()",
        "matched_tutorial_code_inds": [
            3931,
            6892,
            6703,
            403,
            6434
        ],
        "matched_tutorial_codes": [
            "# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n",
            "f = \"Lottery ~ Literacy * Wealth\"\ny, X = patsy.dmatrices(f, df, return_type=\"dataframe\")\nprint(y[:5])\nprint(X[:5])",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "# In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->Using formulas with models that do not (yet) support them"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ]
        ]
    },
    "143534": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt \nimport numpy as np\nstatictable = pd.read_excel('Lab5/staticmethoddata.xlsx')\nstatictablereverse = pd.read_excel('Lab5/staticreverse.xlsx')\nstaticdata = statictable.as_matrix()\nstaticdatareverse = statictablereverse.as_matrix()\nzero = 0.170\nstaticdata[:, 1] -= zero\nstaticdatareverse[:,1] -= zero\nstatictable",
        "matched_tutorial_code_inds": [
            1401,
            4728,
            471,
            4974,
            3392
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\"/>",
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nx1 = np.linspace(0.0, 5.0, 100)\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]')\nax.set_ylabel('Damped oscillation [V]')\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_002.png\" srcset=\"../../_images/sphx_glr_text_intro_002.png, ../../_images/sphx_glr_text_intro_002_2_0x.png 2.0x\"/>",
            "import pandas as pd\n\ncv_results = (search_cv.cv_results_)\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\ncv_results[\n    [\n        \"mean_test_score\",\n        \"std_test_score\",\n        \"param_preprocessor__num__imputer__strategy\",\n        \"param_preprocessor__cat__selector__percentile\",\n        \"param_classifier__C\",\n    ]\n].head(5)\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Plotting the Results"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ]
        ]
    },
    "1487228": {
        "jupyter_code_cell": "user_names = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\nusers = pd.read_table(url2, sep='|', header=None, names=user_names)\nusers.head()",
        "matched_tutorial_code_inds": [
            3727,
            3889,
            3833,
            3730,
            3644
        ],
        "matched_tutorial_codes": [
            "files = glob.glob('weather/*.csv')\nweather_dfs = [pd.read_csv(fp, names=columns) for fp in files]\nweather = pd.concat(weather_dfs)",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ]
        ]
    },
    "772060": {
        "jupyter_code_cell": "X_train_valid = np.concatenate((X_train, X_valid), axis = 0)\ny_train_valid = np.concatenate((y_train, y_valid), axis = 0)",
        "matched_tutorial_code_inds": [
            1579,
            1759,
            2967,
            1757,
            1573
        ],
        "matched_tutorial_codes": [
            "training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
            "train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "train_importances = (\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = (\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)",
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "The data type of training images: float64\nThe data type of test images: float64\n\n\n\n\n<blockquote>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format"
            ]
        ]
    },
    "357432": {
        "jupyter_code_cell": "prices = jd.PriceData()\nprices.plot()",
        "matched_tutorial_code_inds": [
            3967,
            2138,
            653,
            3960,
            1256
        ],
        "matched_tutorial_codes": [
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]",
            "traced_model = (model)\nprint(traced_model.graph)",
            "flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "model = Net().to(rank)\nmodel = DDP(model)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP"
            ]
        ]
    },
    "1467939": {
        "jupyter_code_cell": "minyr = data[data.YearBuilt==1196].index[0]\nmaxyr = data[data.YearBuilt==2106].index[0]",
        "matched_tutorial_code_inds": [
            156,
            6449,
            2990,
            3892,
            6466
        ],
        "matched_tutorial_codes": [
            "m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "n_neighbors = 12  # neighborhood which is used to recover the locally linear structure\nn_components = 2  # number of coordinates for the manifold",
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ]
        ]
    },
    "1026713": {
        "jupyter_code_cell": "md = {}\nmd['normal'] = np.random.normal(popMean, scale=100, size=100)",
        "matched_tutorial_code_inds": [
            6911,
            5615,
            749,
            6813,
            1623
        ],
        "matched_tutorial_codes": [
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()  # Estimate the densities",
            "ft_compute_sample_grad = (ft_compute_grad, in_dims=(None, None, 0, 0))",
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "x_ray_image_gaussian_gradient = ndimage.gaussian_gradient_magnitude(xray_image, sigma=2)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Fitting with the default arguments"
            ],
            [
                "torch->Frontend APIs->Per-sample-gradients->Per-sample-grads, the efficient way, using function transforms"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Gaussian gradient magnitude method"
            ]
        ]
    },
    "1280451": {
        "jupyter_code_cell": "cantons_pairs_de",
        "matched_tutorial_code_inds": [
            1491,
            5943,
            3910,
            3906,
            3902
        ],
        "matched_tutorial_codes": [
            "nbcases_ma",
            "beta_true",
            "by_occupation",
            "avg_transaction",
            "total_by_employee"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ]
        ]
    },
    "999816": {
        "jupyter_code_cell": "grouped = df.groupby('key1')",
        "matched_tutorial_code_inds": [
            237,
            5969,
            5371,
            3734,
            6969
        ],
        "matched_tutorial_codes": [
            "model = ().to(device)\nprint(model)",
            "groups_var = 1\nlevel1_var = 2\nlevel2_var = 3\nresid_var = 4",
            "df = generate_crossed()",
            "delays.nsmallest(5).sort_values()",
            "exog = sm.add_constant(exog, prepend=True)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Introduction to PyTorch->Build the Neural Network->Define the Class"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ]
        ]
    },
    "989685": {
        "jupyter_code_cell": "nbDays = '90'\ndf = lf.retrieve_top_tracks_as_dataframe(cursor, nbDays, 10)\n\niplot(lf.create_figure(df.Track, df.ArtistAlbumTrack, df.PlayCount, 'tracks', nbDays))",
        "matched_tutorial_code_inds": [
            5214,
            2389,
            4954,
            5955,
            5344
        ],
        "matched_tutorial_codes": [
            "nsample = 50\ngroups = np.zeros(nsample, int)\ngroups[20:40] = 1\ngroups[40:] = 2\n# dummy = (groups[:,None] == np.unique(groups)).astype(float)\n\ndummy = pd.get_dummies(groups).values\nx = np.linspace(0, 20, nsample)\n# drop reference category\nX = np.column_stack((x, dummy[:, 1:]))\nX = sm.add_constant(X, prepend=False)\n\nbeta = [1.0, 3, -3, 10]\ny_true = np.dot(X, beta)\ne = np.random.normal(size=nsample)\ny = y_true + e",
            "n_components = 150\n\nprint(\n    \"Extracting the top %d eigenfaces from %d faces\" % (n_components, X_train.shape[0])\n)\nt0 = ()\npca = (n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(X_train)\nprint(\"done in %0.3fs\" % (() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint(\"Projecting the input data on the eigenfaces orthonormal basis\")\nt0 = ()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(\"done in %0.3fs\" % (() - t0))",
            "delta = 0.1\nx = np.arange(-3.0, 4.001, delta)\ny = np.arange(-4.0, 3.001, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (0.9*Z1 - 0.5*Z2) * 2\n\n# select a divergent colormap\ncmap = cm.coolwarm\n\nfig, (ax1, ax2) = plt.subplots(ncols=2)\npc = ax1.pcolormesh(Z, cmap=cmap)\nfig.colorbar(pc, ax=ax1)\nax1.set_title('Normalize()')\n\npc = ax2.pcolormesh(Z, norm=colors.CenteredNorm(), cmap=cmap)\nfig.colorbar(pc, ax=ax2)\nax2.set_title('CenteredNorm()')\n\nplt.show()\n\n\n<img alt=\"Normalize(), CenteredNorm()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_002.png\" srcset=\"../../_images/sphx_glr_colormapnorms_002.png, ../../_images/sphx_glr_colormapnorms_002_2_0x.png 2.0x\"/>",
            "nsample = 50\nx1 = np.linspace(0, 20, nsample)\nX = np.column_stack((x1, (x1 - 5) ** 2))\nX = sm.add_constant(X)\nsig = 0.3  # smaller error variance makes OLS&lt;-RLM contrast bigger\nbeta = [5, 0.5, -0.0]\ny_true2 = np.dot(X, beta)\ny2 = y_true2 + sig * 1.0 * np.random.normal(size=nsample)\ny2[[39, 41, 43, 45, 48]] -= 5  # add some outliers (10% of nsample)",
            "nsample = 50\nx = np.linspace(0, 20, nsample)\nX = np.column_stack((x, (x - 5) ** 2))\nX = sm.add_constant(X)\nbeta = [5.0, 0.5, -0.01]\nsig = 0.5\nw = np.ones(nsample)\nw[nsample * 6 // 10 :] = 3\ny_true = np.dot(X, beta)\ne = np.random.normal(size=nsample)\ny = y_true + sig * w * e\nX = X[:, [0, 1]]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Centered"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->WLS Estimation->Artificial data: Heteroscedasticity 2 groups"
            ]
        ]
    },
    "100476": {
        "jupyter_code_cell": "## With ensembling\nif toEnsemble:\n    pred_test_l = model_l.predict_proba(test_f.iloc[:, 1:])\n    pred_test_r = model_r.predict_proba(test_f.iloc[:, 1:])\n    pred_test_x = model_x.predict_proba(test_f.iloc[:, 1:])\n    x = 0.0*pred_test_l + 0.5*pred_test_r + 0.5*pred_test_x",
        "matched_tutorial_code_inds": [
            42,
            228,
            6671,
            1138,
            1127
        ],
        "matched_tutorial_codes": [
            "# Additional information\nEPOCH = 5\nPATH = \"model.pt\"\nLOSS = 0.4\n\n({\n            'epoch': EPOCH,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': LOSS,\n            }, PATH)",
            "# Display image and label.\n,  = next(iter())\nprint(f\"Feature batch shape: {.size()}\")\nprint(f\"Labels batch shape: {.size()}\")\n = [0].squeeze()\n = [0]\nplt.imshow(, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {}\")\n\n\n<img alt=\"data tutorial\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_002.png\" srcset=\"../../_images/sphx_glr_data_tutorial_002.png\"/>",
            "# Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n\n# Plot estimate of the level term\nres_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\nres_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n\nax.legend(loc=\"lower left\");\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\"/>",
            "# Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "# Perform warm-up iterations\nfor _ in range(3):\n     = inputs1[0]\n     = inputs2[0]\n     = grad_outputs[0]\n    # Run model, forward and backward\n     = (\n        ,\n        ,\n        ,\n        ,\n        ,\n        normalization_axis=2,\n        dropout_prob=0.1,\n    )\n    ()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->4. Save the general checkpoint"
            ],
            [
                "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterate through the DataLoader"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->nvFuser & Dynamic Shapes"
            ]
        ]
    },
    "545262": {
        "jupyter_code_cell": "from sklearn import datasets\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nimport numpy as np",
        "matched_tutorial_code_inds": [
            2660,
            3461,
            3398,
            2259,
            3328
        ],
        "matched_tutorial_codes": [
            "from sklearn.linear_model import \n\nn_order = 3\nX_train = (x_train, n_order + 1, increasing=True)\nX_test = (x_test, n_order + 1, increasing=True)\nreg = (tol=1e-6, fit_intercept=False, compute_score=True)",
            "import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "import numpy as np\nfrom sklearn.datasets import \n\nX, y = (n_samples=10_000, noise=100, random_state=0)\ny = ((y + abs(y.min())) / 200)\ny_trans = (y)",
            "from sklearn.ensemble import \n\nada_discrete = (\n    estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME\",\n)\nada_discrete.fit(X_train, y_train)",
            "from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Curve Fitting with Bayesian Ridge Regression->Fit by cubic polynomial"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Synthetic example"
            ],
            [
                "sklearn->Examples->Ensemble methods->Discrete versus Real AdaBoost->Adaboost with discrete SAMME and real SAMME.R"
            ],
            [
                "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation"
            ]
        ]
    },
    "1507299": {
        "jupyter_code_cell": "## Split the input features and outcome variable\n\ngot_data_X = got_data.drop('dead',axis=1)\ngot_data_Y = got_data['dead']\ngot_data_X.head()",
        "matched_tutorial_code_inds": [
            6434,
            6558,
            6735,
            6942,
            6703
        ],
        "matched_tutorial_codes": [
            "# In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "# Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "# Set sampling params\nndraws = 3000  #  3000 number of draws from the distribution\nnburn = 600  # 600 number of \"burn-in points\" (which will be discarded)",
            "# Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->Bayesian estimation"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example",
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ]
        ]
    },
    "755868": {
        "jupyter_code_cell": "df = pd.read_csv('storm_data.csv', encoding = 'utf8')",
        "matched_tutorial_code_inds": [
            3891,
            3813,
            5363,
            6005,
            5371
        ],
        "matched_tutorial_codes": [
            "df.visualize(rankdir='LR')",
            "df.info()",
            "df = generate_nested()",
            "df_infl = infl.summary_frame()",
            "df = generate_crossed()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ]
        ]
    },
    "659175": {
        "jupyter_code_cell": "display(pd.read_csv(os.path.join('data','description','features_description.csv'), encoding='iso-8859-1'))",
        "matched_tutorial_code_inds": [
            128,
            666,
            4031,
            4435,
            6305
        ],
        "matched_tutorial_codes": [
            "print(delta.transform(extract_fn_name).filter(lambda fn: \"TensorIterator\" in fn))\n\n\n\n<strong>Instruction count delta (filter)</strong>",
            "interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "word_list = open('/usr/share/dict/words').readlines()\n word_list = map(str.strip, word_list)",
            "print(sm.tsa.stattools.adfuller(dta[\"unemp\"])[:3])"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Timer quick start->6. A/B testing with Callgrind"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment"
            ]
        ]
    },
    "1001963": {
        "jupyter_code_cell": "# Grab the first file from the glob list.\ntest_spec = spec_files[0]\ntest_spec",
        "matched_tutorial_code_inds": [
            3928,
            6705,
            3930,
            1179,
            3929
        ],
        "matched_tutorial_codes": [
            "# Import seaborn\nimport seaborn as sns\n",
            "# Show the summary of the news results\nprint(news.summary())",
            "# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n",
            "# Reset since we are using a different mode.\n()\n\ncompile_f1 = (f1)\nprint(\"compile 1, 1:\", test_fns(f1, compile_f1, (, )))\nprint(\"compile 1, 2:\", test_fns(f1, compile_f1, (-, )))\nprint(\"~\" * 10)",
            "# Apply the default theme\nsns.set_theme()\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ]
        ]
    },
    "597142": {
        "jupyter_code_cell": "for i in range(49):\n    button = driver.find_element_by_css_selector('button.btn.btn-secondary-rt.mb-load-btn')\n    driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n    button.click()",
        "matched_tutorial_code_inds": [
            184,
            4176,
            1063,
            1484,
            580
        ],
        "matched_tutorial_codes": [
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "def sinplot(n=10, flip=1):\n    x = np.linspace(0, 14, 100)\n    for i in range(1, n + 1):\n        plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip)\n",
            "def print_size_of_model(model):\n    ((), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)",
            "import matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
            "with open(\"../_static/img/sample_file.jpeg\", 'rb') as f:\n    image_bytes = f.read()\n    tensor = transform_image(image_bytes=image_bytes)\n    print(tensor)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics",
                "seaborn->Figure aesthetics->Controlling figure aesthetics"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->4. Test dynamic quantization"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Inference->Preparing the image"
            ]
        ]
    },
    "207208": {
        "jupyter_code_cell": "avalon = h2s_avalon[np.logical_and(h2s_avalon != np.array(None), h2s_liberty != np.array(None))]\nliberty = h2s_liberty[np.logical_and(h2s_avalon != np.array(None), h2s_liberty != np.array(None))]",
        "matched_tutorial_code_inds": [
            2966,
            2967,
            3854,
            5608,
            6449
        ],
        "matched_tutorial_codes": [
            "train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()",
            "train_importances = (\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = (\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)",
            "fl_date         datetime64[ns]\norigin                category\ncrs_dep_time    datetime64[ns]\ndep_time        datetime64[ns]\ncrs_arr_time    datetime64[ns]\narr_time        datetime64[ns]\ndtype: object",
            "res_logit_hac = mod_logit.fit(method='bfgs', disp=False, cov_type=\"hac\", cov_kwds={\"maxlags\": 2})\nres_log_hac = mod_log.fit(method='bfgs', disp=False, cov_type=\"hac\", cov_kwds={\"maxlags\": 2})",
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA"
            ]
        ]
    },
    "1511473": {
        "jupyter_code_cell": "# Assigns each data point to a centroid\ndef assignment(X, centroids):\n    C = dict.fromkeys(range(X.shape[0]), np.inf)\n    Z = {}\n    for i in centroids.keys():\n        for j in range(X.shape[0]):\n            \n            # Euclidean dist\n            dist = abs(np.linalg.norm(X[j] - centroids[i]))\n            \n            # Change assignment if dist is lesser than previous\n            if dist < C[j]:\n                C[j] = dist\n                Z[j] = i\n    return Z",
        "matched_tutorial_code_inds": [
            1777,
            1776,
            1121,
            484,
            401
        ],
        "matched_tutorial_codes": [
            "# Update the parameters using Adam optimization\ndef update_parameters(parameters, gradients, v, s,\n                      learning_rate=0.01, beta1=0.9, beta2=0.999):\n    for key in parameters:\n        # Moving average of the gradients\n        v['d' + key] = (beta1 * v['d' + key]\n                        + (1 - beta1) * gradients['d' + key])\n\n        # Moving average of the squared gradients\n        s['d' + key] = (beta2 * s['d' + key]\n                        + (1 - beta2) * (gradients['d' + key] ** 2))\n\n        # Update parameters\n        parameters[key] = (parameters[key] - learning_rate\n                           * v['d' + key] / np.sqrt(s['d' + key] + 1e-8))\n    # Return updated parameters and moving averages\n    return parameters, v, s",
            "# initialise the moving averages\ndef initialise_mav(hidden_dim, input_dim, params):\n    v = {}\n    s = {}\n    # Initialize dictionaries v, s\n    for key in params:\n        v['d' + key] = np.zeros(params[key].shape)\n        s['d' + key] = np.zeros(params[key].shape)\n    # Return initialised moving averages\n    return v, s",
            "# Utility to profile the workload\ndef profile_workload(forward_func, , iteration_count=100, label=\"\"):\n    # Perform warm-up iterations\n    for _ in range(3):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before starting the timer\n    ()\n    start = time.perf_counter()\n    for _ in range(iteration_count):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before stopping the timer\n    ()\n    stop = time.perf_counter()\n    iters_per_second = iteration_count / (stop - start)\n    if label:\n        print(label)\n    print(\"Average iterations per second: {:.2f}\".format(iters_per_second))",
            "# Make category, input, and target tensors from a random category, line pair\ndef randomTrainingExample():\n    category, line = randomTrainingPair()\n    category_tensor = categoryTensor(category)\n    input_line_tensor = inputTensor(line)\n    target_line_tensor = targetTensor(line)\n    return category_tensor, input_line_tensor, target_line_tensor",
            "# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = (perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Updating the Parameters"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Updating the Parameters"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Setup and Performance Metrics"
            ],
            [
                "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Training->Preparing for Training"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->FGSM Attack"
            ]
        ]
    },
    "365388": {
        "jupyter_code_cell": "df_round_trip = pd.DataFrame()\ndf_round_trip = df2016.loc[df2016['start station id'] == df2016['end station id'], :]\nlen(df_round_trip)/len(df2016)",
        "matched_tutorial_code_inds": [
            3644,
            5171,
            5789,
            3880,
            3731
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "statsmodels->User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ]
        ]
    },
    "1317623": {
        "jupyter_code_cell": "sns.countplot(x = 'Avdsurg3', data=va, hue = 'txgot_binary')",
        "matched_tutorial_code_inds": [
            3936,
            3938,
            3958,
            4084,
            4106
        ],
        "matched_tutorial_codes": [
            "sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n",
            "sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n",
            "sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\", kind=\"violin\",\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data"
            ],
            [
                "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data",
                "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots",
                "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots"
            ]
        ]
    },
    "942816": {
        "jupyter_code_cell": "subset.shape",
        "matched_tutorial_code_inds": [
            1453,
            1478,
            3718,
            5629,
            5561
        ],
        "matched_tutorial_codes": [
            "reconstructed.shape",
            "load_xy.shape",
            "flights.dep_time",
            "kde.entropy",
            "data_student.dtypes"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Our arrays as a csv file"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ]
        ]
    },
    "1023140": {
        "jupyter_code_cell": "matplotlib.rcParams['font.size'] = 7\nplot(space, sample(list(space.index.values), 1000))",
        "matched_tutorial_code_inds": [
            5033,
            4735,
            4193,
            4737,
            4736
        ],
        "matched_tutorial_codes": [
            "matplotlib.use('pgf')",
            "import matplotlib.pyplot as plt\n plt.style.use('./images/presentation.mplstyle')",
            "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\nsinplot()\n",
            "import matplotlib.pyplot as plt\n plt.style.use(['dark_background', 'presentation'])",
            "import matplotlib.pyplot as plt\n plt.style.use(&lt;style-name)"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Defining your own style"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Scaling plot elements",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Scaling plot elements"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Composing styles"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Distributing styles"
            ]
        ]
    },
    "1017405": {
        "jupyter_code_cell": "one_mode_multilevel = RCT2.networkMultiLevel('keywords', 'authorsFull')\nmk.graphStats(one_mode_multilevel)",
        "matched_tutorial_code_inds": [
            6765,
            6898,
            3857,
            3979,
            5240
        ],
        "matched_tutorial_codes": [
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ]
        ]
    },
    "810669": {
        "jupyter_code_cell": "airline_dictionary = {'9E': 'Pinnacle Airlines', \n              'AA': 'American Airlines', \n              'AS': 'Alaska Airlines',\n              'B6': 'JetBlue',\n              'DL': 'Delta Air Lines',\n              'EV': 'Atlantic Southeast Airlines',\n              'F9': 'Frontier Airlines',\n              'G4': 'Allegiant Air',\n              'HA': 'Hawaiian Airlines',\n              'MQ': 'Envoy Air',\n              'NK': 'Spirit Airlines',\n              'OH': 'Comair',\n              'OO': 'SkyWest  Airlines',\n              'UA': 'United Airlines',\n              'VX': 'Virgin America',\n              'WN': 'Southwest Airlines',\n              'YV': 'Mesa Airlines',\n              'YX': 'Midwest Airlines'\n             }",
        "matched_tutorial_code_inds": [
            6752,
            190,
            221,
            1151,
            1499
        ],
        "matched_tutorial_codes": [
            "austourists_data = [\n    30.05251300,\n    19.14849600,\n    25.31769200,\n    27.59143700,\n    32.07645600,\n    23.48796100,\n    28.47594000,\n    35.12375300,\n    36.83848500,\n    25.00701700,\n    30.72223000,\n    28.69375900,\n    36.64098600,\n    23.82460900,\n    29.31168300,\n    31.77030900,\n    35.17787700,\n    19.77524400,\n    29.60175000,\n    34.53884200,\n    41.27359900,\n    26.65586200,\n    28.27985900,\n    35.19115300,\n    42.20566386,\n    24.64917133,\n    32.66733514,\n    37.25735401,\n    45.24246027,\n    29.35048127,\n    36.34420728,\n    41.78208136,\n    49.27659843,\n    31.27540139,\n    37.85062549,\n    38.83704413,\n    51.23690034,\n    31.83855162,\n    41.32342126,\n    42.79900337,\n    55.70835836,\n    33.40714492,\n    42.31663797,\n    45.15712257,\n    59.57607996,\n    34.83733016,\n    44.84168072,\n    46.97124960,\n    60.01903094,\n    38.37117851,\n    46.97586413,\n    50.73379646,\n    61.64687319,\n    39.29956937,\n    52.67120908,\n    54.33231689,\n    66.83435838,\n    40.87118847,\n    51.82853579,\n    57.49190993,\n    65.25146985,\n    43.06120822,\n    54.76075713,\n    59.83447494,\n    73.25702747,\n    47.69662373,\n    61.09776802,\n    66.05576122,\n]\nindex = pd.date_range(\"1999-03-01\", \"2015-12-01\", freq=\"3MS\")\naustourists = pd.Series(austourists_data, index=index)\naustourists.plot()\nplt.ylabel(\"Australian Tourists\")",
            "classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\n()\n, y = [0][0], [0][1]\nwith ():\n     = model()\n    predicted, actual = classes[[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')",
            "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
            "total_trials = 48  # total evaluation budget\n\nfrom ax.modelbridge.dispatch_utils import choose_generation_strategy\n\ngs = choose_generation_strategy(\n    search_space=experiment.search_space,\n    optimization_config=experiment.optimization_config,\n    num_trials=total_trials,\n  )",
            "china_mask = (\n    (locations[:, 1] == \"China\")\n    &amp; (locations[:, 0] != \"Hong Kong\")\n    &amp; (locations[:, 0] != \"Taiwan\")\n    &amp; (locations[:, 0] != \"Macau\")\n    &amp; (locations[:, 0] != \"Unspecified*\")\n)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Loading Models"
            ],
            [
                "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterating and Visualizing the Dataset"
            ],
            [
                "torch->Model Optimization->Multi-Objective NAS with Ax->Choosing the GenerationStrategy"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ]
        ]
    },
    "1163019": {
        "jupyter_code_cell": "clean_table = data.set_index('date').pivot(columns = 'ticker')\n\n# Check the data now\nclean_table.head()",
        "matched_tutorial_code_inds": [
            3638,
            3779,
            3684,
            3962,
            3821
        ],
        "matched_tutorial_codes": [
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "flights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n",
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "715874": {
        "jupyter_code_cell": "allfeatures = list(df.columns.copy())\nallfeatures.remove('customer_id')\ndf_mat = df.head(20).as_matrix(columns=allfeatures)\ndst.heatmap(df_mat, rownames=df.head(20).index, colnames=allfeatures, xlabel='feature', ylabel='customer ID', figsize=(10,5))",
        "matched_tutorial_code_inds": [
            2866,
            2848,
            3784,
            2883,
            1761
        ],
        "matched_tutorial_codes": [
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, small regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\"/>",
            "coefs = (\n    model[-1].regressor_.coef_ * X_train_preprocessed.std(axis=0),\n    columns=[\"Coefficient importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Coefficient values corrected by the feature's std. dev.\")\n(\"Ridge model, small regularization\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_005.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_005.png\"/>",
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, with regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, with regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\"/>",
            "speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ]
        ]
    },
    "1118804": {
        "jupyter_code_cell": "mpl.rcParams['font.size'] = 30.0\n#labels = ['business','technology','lifestyle','social media','world','entertainment']\nplt.figure(figsize = (30, 30))",
        "matched_tutorial_code_inds": [
            5089,
            7,
            6377,
            3812,
            5017
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')",
            "import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())",
            "plt.rc(\"figure\", figsize=(16, 12))\nplt.rc(\"font\", size=13)",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "# math text\nplt.title(r'$\\alpha  \\beta$')"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Toolkits->The mplot3d toolkit"
            ],
            [
                "torch->PyTorch Recipes->Loading data in PyTorch->5. [Optional] Visualize the data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "matplotlib->Tutorials->Text->Writing mathematical expressions"
            ]
        ]
    },
    "850496": {
        "jupyter_code_cell": "# print the types of heart disease\nset(heart.loc[:, \"diagnosis\"].values)",
        "matched_tutorial_code_inds": [
            6688,
            3929,
            5016,
            3732,
            6705
        ],
        "matched_tutorial_codes": [
            "# Print the news, computed by the `news` method\nprint(news.news)\n\n# Manually compute the news\nprint()\nprint((y_update.iloc[0] - phi_hat * y_pre.iloc[-1]).round(6))",
            "# Apply the default theme\nsns.set_theme()\n",
            "# plain text\nplt.title('alpha  beta')",
            "# Select the 5 largest delays\ndelays.nlargest(5).sort_values()",
            "# Show the summary of the news results\nprint(news.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Addendum: manually computing the news, weights, and impacts"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "matplotlib->Tutorials->Text->Writing mathematical expressions"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ]
        ]
    },
    "508146": {
        "jupyter_code_cell": "new_merged = merged.merge(climate_new,how='inner',on = ['Country Name','years'])",
        "matched_tutorial_code_inds": [
            3684,
            3638,
            5586,
            1714,
            2845
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "643703": {
        "jupyter_code_cell": "#This is our known metadata for the outputs\nmetadata = pd.DataFrame({\n         'orgName': [\n                   #these don't have depth\n                   'icephl_latlon','ben_latlon',\n                   'aice_latlon',\n                   #these do\n                 'phs_latlon','phl_latlon','mzl_latlon','cop_latlon','ncao_latlon','ncas_latlon','eup_latlon','det_latlon',\n                 'temp_latlon',\n                 'u_latlon',\n                 'v_latlon',],\n         'name': ['Ice Phytoplankton Concentration',\n                  'Benthos Concentration',\n                  'Sea Ice Area Fraction',\n                  'Small Phytoplankton Concentration',\n                  'Large Phytoplankton Concentration',\n                  'Large Microzooplankton Concentration',\n                  'Small Coastal Copepod Concentration',\n                  'Offshore Neocalanus Concentration',\n                  'Neocalanus Concentration',\n                  'Euphausiids Concentration',\n                  'Detritus Concentration',\n                  'Sea Water Temperature',\n                  'Zonal (U) Current',\n                  'Meridional (V) Current'],\n         'units': ['mgC/m2','mgC/m2',\n                   'Fraction',\n                   'mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3',\n                   'degrees C',\n                   'm/s',\n                   'm/s']\n       })",
        "matched_tutorial_code_inds": [
            6545,
            4694,
            406,
            6416,
            383
        ],
        "matched_tutorial_codes": [
            "# Create Table I\ntable_i = np.zeros((5,6))\n\nstart = dta.index[0]\nend = dta.index[-1]\ntime_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\nmodels = [\n    ('US GNP', time_range, 'None'),\n    ('US Prices', time_range, 'None'),\n    ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n    ('US monetary base', time_range, 'None'),\n    ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n]\nindex = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\nparameter_symbols = [\n    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n]\n\ni = 0\nfor res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n    if res.model.stochastic_level:\n        (sigma_irregular, sigma_level, sigma_trend,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n    else:\n        (sigma_irregular, sigma_level,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n        sigma_trend = np.nan\n    period_cycle = 2 * np.pi / frequency_cycle\n\n    table_i[i, :] = [\n        sigma_level*1e7, sigma_trend*1e7,\n        sigma_cycle*1e7, damping_cycle, period_cycle,\n        sigma_irregular*1e7\n    ]\n    i += 1\n\npd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\ntable_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\ntable_i",
            "# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n# make up some data in the open interval (0, 1)\ny = np.random.normal(loc=0.5, scale=0.4, size=1000)\ny = y[(y  0) & (y &lt; 1)]\ny.sort()\nx = np.arange(len(y))\n\n# plot with various axes scales\nplt.figure()\n\n# linear\nplt.subplot(221)\nplt.plot(x, y)\nplt.yscale('linear')\nplt.title('linear')\nplt.grid(True)\n\n# log\nplt.subplot(222)\nplt.plot(x, y)\nplt.yscale('log')\nplt.title('log')\nplt.grid(True)\n\n# symmetric log\nplt.subplot(223)\nplt.plot(x, y - y.mean())\nplt.yscale('symlog', linthresh=0.01)\nplt.title('symlog')\nplt.grid(True)\n\n# logit\nplt.subplot(224)\nplt.plot(x, y)\nplt.yscale('logit')\nplt.title('logit')\nplt.grid(True)\n# Adjust the subplot layout, because the logit one may take more space\n# than usual, due to y-tick labels like \"1 - 10^{-3}\"\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()\n\n\n<img alt=\"linear, log, symlog, logit\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_010.png\" srcset=\"../../_images/sphx_glr_pyplot_010.png, ../../_images/sphx_glr_pyplot_010_2_0x.png 2.0x\"/>",
            "# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
            "# Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Model"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Logarithmic and other nonlinear axes"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ]
        ]
    },
    "1159324": {
        "jupyter_code_cell": "from sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression(C = 10000, random_state = 0)\nlogr.fit(X_train_std, y_train)\nimport VisualFuncs as vf\n\n\nvf.VDR(X_test_std, y_test, classifier = logr)",
        "matched_tutorial_code_inds": [
            2156,
            2790,
            1904,
            2719,
            3157
        ],
        "matched_tutorial_codes": [
            "from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=0)",
            "from sklearn import datasets\nimport numpy as np\n\nX, y = (return_X_y=True)\nindices = (0, 1)\n\nX_train = X[:-20, indices]\nX_test = X[-20:, indices]\ny_train = y[:-20]\ny_test = y[-20:]",
            "from sklearn.metrics import \n\nscore = (y_test, clf_probs)\ncal_score = (y_test, cal_clf_probs)\n\nprint(\"Log-loss of\")\nprint(f\" * uncalibrated classifier: {score:.3f}\")\nprint(f\" * calibrated classifier: {cal_score:.3f}\")",
            "from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5)",
            "from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5, random_state=0)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Kernel PCA->Projecting data: PCA vs. KernelPCA"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Sparsity Example: Fitting only features 1  and 2"
            ],
            [
                "sklearn->Examples->Calibration->Probability Calibration for 3-class classification->Compare probabilities"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "sklearn->Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->The dataset"
            ]
        ]
    },
    "331630": {
        "jupyter_code_cell": "#There are games with duplicate names (For each platform for example), so let's deal with this\nx = vg_df.groupby(['Genre', 'Name']).sum().reset_index().groupby('Genre')\n\n#A dataframe that will hold rankings, for nice display\nbest_selling_titles_by_genre_df = pd.DataFrame()\n\nfor name, group in x:\n    temp_col = group.sort_values('Global_Sales', ascending=False).head(10).Name.reset_index(drop=True)\n    best_selling_titles_by_genre_df[name] = temp_col",
        "matched_tutorial_code_inds": [
            6340,
            6345,
            6556,
            2419,
            3286
        ],
        "matched_tutorial_codes": [
            "# Get the RGNP data to replicate Hamilton\ndta = pd.read_stata(\"https://www.stata-press.com/data/r14/rgnp.dta\").iloc[1:]\ndta.index = pd.DatetimeIndex(dta.date, freq=\"QS\")\ndta_hamilton = dta.rgnp\n\n# Plot the data\ndta_hamilton.plot(title=\"Growth rate of Real GNP\", figsize=(12, 3))\n\n# Fit the model\nmod_hamilton = sm.tsa.MarkovAutoregression(\n    dta_hamilton, k_regimes=2, order=4, switching_ar=False\n)\nres_hamilton = mod_hamilton.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\"/>",
            "# Get the dataset\new_excs = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/ew_excs.prn\").content\nraw = pd.read_table(BytesIO(ew_excs), header=None, skipfooter=1, engine=\"python\")\nraw.index = pd.date_range(\"1926-01-01\", \"1995-12-01\", freq=\"MS\")\n\ndta_kns = raw.loc[:\"1986\"] - raw.loc[:\"1986\"].mean()\n\n# Plot the dataset\ndta_kns[0].plot(title=\"Excess returns\", figsize=(12, 3))\n\n# Fit the model\nmod_kns = sm.tsa.MarkovRegression(\n    dta_kns, k_regimes=3, trend=\"n\", switching_variance=True\n)\nres_kns = mod_kns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\"/>",
            "# Load Dataset\ndf.index = pd.date_range(start='%d-01-01' % df.date[0], end='%d-01-01' % df.iloc[-1, 0], freq='AS')\n\n# Log transform\ndf['lff'] = np.log(df['ff'])\n\n# Setup the model\nmod = LocalLinearTrend(df['lff'])\n\n# Fit it using MLE (recall that we are fitting the three variance parameters)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "# Get data\nX2 = ()[\"data\"][:, [6, 9]]  # \"banana\"-shaped\n\n# Learn a frontier for outlier detection with several classifiers\nxx2, yy2 = ((-1, 5.5, 500), (-2.5, 19, 500))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    (2)\n    clf.fit(X2)\n    Z2 = clf.decision_function([xx2.ravel(), yy2.ravel()])\n    Z2 = Z2.reshape(xx2.shape)\n    legend2[clf_name] = (\n        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i]\n    )\n\nlegend2_values_list = list(legend2.values())\nlegend2_keys_list = list(legend2.keys())\n\n# Plot the results (= shape of the data points cloud)\n(2)  # \"banana\" shape\n(\"Outlier detection on a real data set (wine recognition)\")\n(X2[:, 0], X2[:, 1], color=\"black\")\n((xx2.min(), xx2.max()))\n((yy2.min(), yy2.max()))\n(\n    (\n        legend2_values_list[0].collections[0],\n        legend2_values_list[1].collections[0],\n        legend2_values_list[2].collections[0],\n    ),\n    (legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),\n    loc=\"upper center\",\n    prop=(size=11),\n)\n(\"color_intensity\")\n(\"flavanoids\")\n\n()\n\n\n<img alt=\"Outlier detection on a real data set (wine recognition)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\" srcset=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\"/>",
            "# Generate the class/group data\nn_points = 100\nX = rng.randn(100, 10)\n\npercentiles_classes = [0.1, 0.3, 0.6]\ny = ([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\n\n# Generate uneven groups\ngroup_prior = rng.dirichlet([2] * 10)\ngroups = ((10), rng.multinomial(100, group_prior))\n\n\ndef visualize_groups(classes, groups, name):\n    # Visualize dataset groups\n    fig, ax = ()\n    ax.scatter(\n        range(len(groups)),\n        [0.5] * len(groups),\n        c=groups,\n        marker=\"_\",\n        lw=50,\n        cmap=cmap_data,\n    )\n    ax.scatter(\n        range(len(groups)),\n        [3.5] * len(groups),\n        c=classes,\n        marker=\"_\",\n        lw=50,\n        cmap=cmap_data,\n    )\n    ax.set(\n        ylim=[-1, 5],\n        yticks=[0.5, 3.5],\n        yticklabels=[\"Data\\ngroup\", \"Data\\nclass\"],\n        xlabel=\"Sample index\",\n    )\n\n\nvisualize_groups(y, groups, \"no groups\")\n\n\n<img alt=\"plot cv indices\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cv_indices_001.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Hamilton (1989) switching model of GNP"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Kim, Nelson, and Startz (1998) Three-state Variance Switching"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Outlier detection on a real data set->Second example"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize our data"
            ]
        ]
    },
    "1026300": {
        "jupyter_code_cell": "def group_A_fit(training_data):\n    crimedata = pd.read_csv(training_data)\n    m1 = smf.ols(formula=\"ViolentCrimesPerPop ~ NumUnderPov + MalePctDivorce + PctPersDenseHous + pctUrban + racepctblack + \\\n        np.power(racepctblack,2) + PctKids2Par\", data=crimedata).fit()\n    return m1\n    \nm1 = group_A_fit(\"crime-train.csv\")\nprint(m1.summary())",
        "matched_tutorial_code_inds": [
            3126,
            3122,
            3125,
            3124,
            6382
        ],
        "matched_tutorial_codes": [
            "def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "def get_full_score(X_full, y_full):\n    full_scores = (\n        regressor, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n    return full_scores.mean(), full_scores.std()\n\n\nmses_california[0], stds_california[0] = get_full_score(X_california, y_california)\nmses_diabetes[0], stds_diabetes[0] = get_full_score(X_diabetes, y_diabetes)\nx_labels.append(\"Full data\")",
            "def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")",
            "def get_impute_knn_score(X_missing, y_missing):\n    imputer = (missing_values=, add_indicator=True)\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return knn_impute_scores.mean(), knn_impute_scores.std()\n\n\nmses_california[2], stds_california[2] = get_impute_knn_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"KNN Imputation\")",
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Estimate the score"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->kNN-imputation of the missing values"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting"
            ]
        ]
    },
    "997656": {
        "jupyter_code_cell": "features = ['per_listen', 'listen_duration', 'n_loops', 'is_loop']\n\ng = sns.clustermap(df_data[features].corr(), \n                   linewidths=1, \n                   cmap=\"YlGnBu\", \n                   square=True, \n                   annot=True, \n                   fmt='.3f', \n                   figsize = (5, 5))\nplt.setp(g.ax_heatmap.get_xticklabels(), rotation=90); ",
        "matched_tutorial_code_inds": [
            2896,
            3678,
            3677,
            6600,
            3726
        ],
        "matched_tutorial_codes": [
            "features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")",
            "airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.svg', transparent=True);",
            "airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.png');",
            "time_s = np.s_[:100]\n\nfig3 = plt.figure()\nax3 = fig3.add_subplot(111)\nh31, = ax3.plot(idx[time_s], res_f.freq_seasonal[1].filtered[time_s] + res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh32, = ax3.plot(idx[time_s], res_tf.freq_seasonal[0].filtered[time_s] + res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh33, = ax3.plot(idx[time_s], true_sum[time_s], label='True Seasonal 100(2)')\nh34, = ax3.plot(idx[time_s], res_lf.freq_seasonal[0].filtered[time_s], label='Lazy Freq. Seas')\nh35, = ax3.plot(idx[time_s], res_lt.seasonal.filtered[time_s], label='Lazy Time Seas')\n\nplt.legend([h31, h32, h33, h34, h35], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth', 'Lazy Freq. Seas', 'Lazy Time Seas'], loc=1)\nplt.title('Seasonal components combined')\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\"/>",
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ]
        ]
    },
    "1448218": {
        "jupyter_code_cell": "sql_with_features = attach_query_result_by_header(sql_data_path)",
        "matched_tutorial_code_inds": [
            2822,
            6809,
            5712,
            4031,
            5586
        ],
        "matched_tutorial_codes": [
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "data = sm.datasets.fertility.load_pandas().data\ndata.head()",
            "results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ]
        ]
    },
    "1076097": {
        "jupyter_code_cell": "sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df[cols], size = 2.5)\nplt.show()",
        "matched_tutorial_code_inds": [
            3944,
            3150,
            4072,
            3315,
            3932
        ],
        "matched_tutorial_codes": [
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n",
            "rng = (0)\nX, y = (n_samples=1000, random_state=rng)\n\ngammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\nCs = [1, 10, 100, 1e3, 1e4, 1e5]\nparam_grid = {\"gamma\": gammas, \"C\": Cs}\n\nclf = (random_state=rng)\n\ntic = ()\ngsh = (\n    estimator=clf, param_grid=param_grid, factor=2, random_state=rng\n)\ngsh.fit(X, y)\ngsh_time = () - tic\n\ntic = ()\ngs = (estimator=clf, param_grid=param_grid)\ngs.fit(X, y)\ngs_time = () - tic",
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "nca = (max_iter=30, random_state=0)\nnca = nca.fit(X, y)\n\n(2)\nax2 = ()\nX_embedded = nca.transform(X)\nrelate_point(X_embedded, i, ax2)\n\nfor i in range(len(X)):\n    ax2.text(X_embedded[i, 0], X_embedded[i, 1], str(i), va=\"center\", ha=\"center\")\n    ax2.scatter(X_embedded[i, 0], X_embedded[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax2.set_title(\"NCA embedding\")\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\nax2.axis(\"equal\")\n()\n\n\n<img alt=\"NCA embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nca_illustration_002.png\" srcset=\"../../_images/sphx_glr_plot_nca_illustration_002.png\"/>",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization"
            ],
            [
                "sklearn->Examples->Model Selection->Comparison between grid search and successive halving"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration->Learning an embedding"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ]
        ]
    },
    "180012": {
        "jupyter_code_cell": "sns.boxplot(y=\"tenure\", data=churn, fliersize=5)\nplt.show()\n\nsns.boxplot(y=\"MonthlyCharges\", data=churn, fliersize=5)\nplt.show()\n\nsns.boxplot(y=\"TotalCharges\", data=churn, fliersize=10)\nplt.show()",
        "matched_tutorial_code_inds": [
            3730,
            5554,
            184,
            6811,
            3931
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ]
        ]
    },
    "319062": {
        "jupyter_code_cell": "for fpr, tpr, roc_label in zip(fprs, tprs, roc_labels):\n    plt.plot(fpr, tpr, label=roc_label)\n\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.title(\"ROC Curves\")\nplt.legend()\nplt.xlim([0, .07])\nplt.ylim([.98, 1])\nplt.show()",
        "matched_tutorial_code_inds": [
            2178,
            6364,
            2968,
            6761,
            3191
        ],
        "matched_tutorial_codes": [
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>",
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average"
            ]
        ]
    },
    "1091790": {
        "jupyter_code_cell": "# import csv and create dataframe\ngrad_rates = os.path.join('Resources','gradrates.csv')\ngrad_df = pd.read_csv(grad_rates)\ngrad_df = grad_df.rename(columns={\"State\":\"ST\"})\ngrad_df.head()",
        "matched_tutorial_code_inds": [
            6423,
            1762,
            3730,
            3631,
            5456
        ],
        "matched_tutorial_codes": [
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "zf = zipfile.ZipFile(\"data/flights.csv.zip\")\nfp = zf.extract(zf.filelist[0].filename, path='data/')\ndf = pd.read_csv(fp, parse_dates=[\"FL_DATE\"]).rename(columns=str.lower)\n\ndf.info()",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Get the Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ]
        ]
    },
    "1424865": {
        "jupyter_code_cell": "# http://scikit.ml/api/api/skmultilearn.adapt.html#skmultilearn.adapt.MLkNN\n\nfrom skmultilearn.adapt import MLkNN\nfrom scipy.sparse import csr_matrix, lil_matrix",
        "matched_tutorial_code_inds": [
            630,
            7015,
            2210,
            3226,
            2015
        ],
        "matched_tutorial_codes": [
            "# Some standard imports\nimport io\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx",
            "# TODO: we could use the examples from here:\n# http://probfit.readthedocs.org/en/latest/api.html#probfit.costfunc.Chi2Regression",
            "from sklearn.ensemble import \n\nrf_pipeline = (tree_preprocessor, (random_state=42))\nrf_pipeline",
            "from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "from sklearn.neighbors import \n\nconnectivity = (X, n_neighbors=10, include_self=False)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ],
            [
                "statsmodels->Examples->User Notes->Least squares fitting of models to data->Non-linear models"
            ],
            [
                "sklearn->Examples->Ensemble methods->Combine predictors using stacking->Stack of predictors on a single data set"
            ],
            [
                "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning"
            ],
            [
                "sklearn->Examples->Clustering->Hierarchical clustering: structured vs unstructured ward->We are defining k-Nearest Neighbors with 10 neighbors"
            ]
        ]
    },
    "161674": {
        "jupyter_code_cell": "#merging dataframes order_prior and products\norder_prior = pd.merge(order_prior, products, on='product_id', how='left')",
        "matched_tutorial_code_inds": [
            6703,
            3257,
            6321,
            3776,
            5454
        ],
        "matched_tutorial_codes": [
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "# Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()",
            "tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ]
        ]
    },
    "187377": {
        "jupyter_code_cell": "from geopy.distance import great_circle\n\n#Specify mile limit and default location\nwithin_x_miles = 1\nlatitude_default, longitude_default = 43.6544, -79.3807 # Toronto Eaton Centre\nmy_location = (latitude_default, longitude_default)\n\n#Filter out business_id within the mile limit\nrestaurants_info_my_dataset = pd.read_csv('restaurants_info_my_dataset.csv')\ndistances_to_my_location = restaurants_info_my_dataset.apply(lambda x: great_circle(np.array(x[['latitude','longitude']]), my_location).miles, axis=1) \nrestaurants_info_my_dataset['distances_to_my_location'] = distances_to_my_location\nrestaurants_info_my_dataset = restaurants_info_my_dataset.set_index('business_id')\nbusiness_id_within_x_miles = list(restaurants_info_my_dataset[np.array(restaurants_info_my_dataset['distances_to_my_location'])<=within_x_miles].index)",
        "matched_tutorial_code_inds": [
            5896,
            5546,
            5015,
            3324,
            5005
        ],
        "matched_tutorial_codes": [
            "from matplotlib.patches import Ellipse\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(\n    111,\n    xlabel=\"log(Temp)\",\n    ylabel=\"log(Light)\",\n    title=\"Hertzsprung-Russell Diagram of Star Cluster CYG OB1\",\n)\nax.scatter(*dta.values.T)\n# highlight outliers\ne = Ellipse((3.5, 6), 0.2, 1, alpha=0.25, color=\"r\")\nax.add_patch(e)\nax.annotate(\n    \"Red giants\",\n    xy=(3.6, 6),\n    xytext=(3.8, 6),\n    arrowprops=dict(facecolor=\"black\", shrink=0.05, width=2),\n    horizontalalignment=\"left\",\n    verticalalignment=\"bottom\",\n    clip_on=True,  # clip to the axes bounding box\n    fontsize=16,\n)\n# annotate these with their index\nfor i, row in dta.loc[dta[\"log.Te\"] &lt; 3.8].iterrows():\n    ax.annotate(i, row, row + 0.01, fontsize=14)\nxlim, ylim = ax.get_xlim(), ax.get_ylim()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_75_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_75_0.png\"/>",
            "from statsmodels.graphics.api import abline_plot\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, ylabel=\"Observed Values\", xlabel=\"Fitted Values\")\nax.scatter(yhat, y)\ny_vs_yhat = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()\nfig = abline_plot(model_results=y_vs_yhat, ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\"/>",
            "from matplotlib.patches import ConnectionPatch\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(6, 3))\nxy = (0.3, 0.2)\ncon = ConnectionPatch(xyA=xy, coordsA=ax1.transData,\n                      xyB=xy, coordsB=ax2.transData)\n\nfig.add_artist(con)\n\n\n<img alt=\"annotations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_annotations_022.png\" srcset=\"../../_images/sphx_glr_annotations_022.png, ../../_images/sphx_glr_annotations_022_2_0x.png 2.0x\"/>",
            "from sklearn.base import clone\n\n# Hyper-parameters. These were set by cross-validation,\n# using a GridSearchCV. Here we are not performing cross-validation to\n# save time.\nrbm.learning_rate = 0.06\nrbm.n_iter = 10\n\n# More components tend to give better prediction performance, but larger\n# fitting time\nrbm.n_components = 100\nlogistic.C = 6000\n\n# Training RBM-Logistic Pipeline\nrbm_features_classifier.fit(X_train, Y_train)\n\n# Training the Logistic regression classifier directly on the pixel\nraw_pixel_classifier = clone(logistic)\nraw_pixel_classifier.C = 100.0\nraw_pixel_classifier.fit(X_train, Y_train)",
            "from matplotlib.patches import Circle\nfrom mpl_toolkits.axes_grid1.anchored_artists import AnchoredDrawingArea\n\nfig, ax = plt.subplots(figsize=(3, 3))\nada = AnchoredDrawingArea(40, 20, 0, 0,\n                          loc='upper right', pad=0., frameon=False)\np1 = Circle((10, 10), 10)\nada.drawing_area.add_artist(p1)\np2 = Circle((30, 10), 5, fc=\"r\")\nada.drawing_area.add_artist(p2)\nax.add_artist(ada)\n\n\n<img alt=\"annotations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_annotations_012.png\" srcset=\"../../_images/sphx_glr_annotations_012.png, ../../_images/sphx_glr_annotations_012_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "matplotlib->Tutorials->Text->Annotations->Coordinate systems for annotations->Using ConnectionPatch"
            ],
            [
                "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Training"
            ],
            [
                "matplotlib->Tutorials->Text->Annotations->Advanced annotation->Placing Artist at anchored Axes locations"
            ]
        ]
    },
    "437875": {
        "jupyter_code_cell": "df=df[(df.case_status!='Withdrawn')]\ndf=df.replace({'Certified-Expired':'Certified'})",
        "matched_tutorial_code_inds": [
            6251,
            3644,
            3731,
            6877,
            3889
        ],
        "matched_tutorial_codes": [
            "table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "df = dta.data[[\"Lottery\", \"Literacy\", \"Wealth\", \"Region\"]].dropna()\ndf.head()",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->OLS regression using formulas"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ]
        ]
    },
    "204993": {
        "jupyter_code_cell": "#!pip install fitbit\n#!pip install -r requirements/base.txt\n#!pip install -r requirements/dev.txt\n#!pip install -r requirements/test.txt\nfrom time import sleep\nimport fitbit\nimport cherrypy\nimport requests\nimport json\nimport datetime\nimport scipy.stats\nimport pandas as pd\nimport numpy as np\n\n# plotting\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns",
        "matched_tutorial_code_inds": [
            137,
            1138,
            6437,
            6416,
            2762
        ],
        "matched_tutorial_codes": [
            "# (omitting some columns)\n# -------------------------------------------------------  ------------  ------------\n#                                                    Name     Self CUDA    CUDA total\n# -------------------------------------------------------  ------------  ------------\n#                                         model_inference       0.000us      11.666ms\n#                                            aten::conv2d       0.000us      10.484ms\n#                                       aten::convolution       0.000us      10.484ms\n#                                      aten::_convolution       0.000us      10.484ms\n#                              aten::_convolution_nogroup       0.000us      10.484ms\n#                                       aten::thnn_conv2d       0.000us      10.484ms\n#                               aten::thnn_conv2d_forward      10.484ms      10.484ms\n# void at::native::im2col_kernel&lt;float&gt;(long, float co...       3.844ms       3.844ms\n#                                       sgemm_32x32x32_NN       3.206ms       3.206ms\n#                                   sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n# -------------------------------------------------------  ------------  ------------\n# Self CPU time total: 23.015ms\n# Self CUDA time total: 11.666ms",
            "# Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "# Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "# Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Profiler->Steps->3. Using profiler to analyze execution time"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ]
        ]
    },
    "184313": {
        "jupyter_code_cell": "print(\"NO. of values in different varibles for train_df\")\nprint(train_df.count())\nprint(\"NO. of values in different varibles for test_df\")\nprint(test_df.count())",
        "matched_tutorial_code_inds": [
            5536,
            6029,
            1473,
            6037,
            6041
        ],
        "matched_tutorial_codes": [
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "min_lm = ols(\"JPERF ~ TEST\", data=jobtest_table).fit()\nprint(min_lm.summary())",
            "array_out = np.block([x[:, np.newaxis], y[:, np.newaxis]])\nprint(\"the output array has shape \", array_out.shape, \" with values:\")\nprint(array_out)",
            "min_lm3 = ols(\"JPERF ~ TEST + MINORITY\", data=jobtest_table).fit()\nprint(min_lm3.summary())",
            "min_lm4 = ols(\"JPERF ~ TEST * MINORITY\", data=jobtest_table).fit()\nprint(min_lm4.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Rearrange the data into a single 2D array"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data"
            ]
        ]
    },
    "995106": {
        "jupyter_code_cell": "# take a look at the current structure of counties_df\n\ncounties_df.head()",
        "matched_tutorial_code_inds": [
            3929,
            6705,
            3732,
            5016,
            607
        ],
        "matched_tutorial_codes": [
            "# Apply the default theme\nsns.set_theme()\n",
            "# Show the summary of the news results\nprint(news.summary())",
            "# Select the 5 largest delays\ndelays.nlargest(5).sort_values()",
            "# plain text\nplt.title('alpha  beta')",
            "# New inputs\n,  = (3, 4), (3, 4)\ntraced_cell(, )"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "matplotlib->Tutorials->Text->Writing mathematical expressions"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Using Scripting to Convert Modules"
            ]
        ]
    },
    "1129237": {
        "jupyter_code_cell": "print(departments_df[departments_df.department == 'missing'])\nmissing_prod_df = products_df[products_df.department_id == 21].reset_index()\nprint (len(missing_prod_df), \"products items are associated with department='missing'\")\n\n#checking for any missing labled aisles\nprint(aisles_df[aisles_df.aisle == 'missing'])\n\nmissing_prod_df.head()",
        "matched_tutorial_code_inds": [
            6195,
            2466,
            6902,
            3020,
            5719
        ],
        "matched_tutorial_codes": [
            "res_ar5 = AutoReg(ind_prod, 5, old_names=False).fit()\npredictions = pd.DataFrame(\n    {\n        \"AR(5)\": res_ar5.predict(start=714, end=726),\n        \"AR(13)\": res.predict(start=714, end=726),\n        \"Restr. AR(13)\": res_glob.predict(start=714, end=726),\n    }\n)\n_, ax = plt.subplots()\nax = predictions.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\"/>",
            "last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "tree_disp = (tree, X, [\"age\"])\nmlp_disp = (\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\"/>",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for one feature"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ]
        ]
    },
    "510941": {
        "jupyter_code_cell": "idxstats_df = getTableFromDB('select * from idxstats_reads_per_chromosome;',database_path)\nidxstats_df.index = idxstats_df['region']\nreads_per_chr_df = reorderDFbyChrOrder(idxstats_df.drop('region', 1))\nprint ('this table shows million reads per chromosome')\nreads_per_chr_df.divide(1000000)",
        "matched_tutorial_code_inds": [
            6022,
            6902,
            3150,
            3268,
            2314
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "rng = (0)\nX, y = (n_samples=1000, random_state=rng)\n\ngammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\nCs = [1, 10, 100, 1e3, 1e4, 1e5]\nparam_grid = {\"gamma\": gammas, \"C\": Cs}\n\nclf = (random_state=rng)\n\ntic = ()\ngsh = (\n    estimator=clf, param_grid=param_grid, factor=2, random_state=rng\n)\ngsh.fit(X, y)\ngsh_time = () - tic\n\ntic = ()\ngs = (estimator=clf, param_grid=param_grid)\ngs.fit(X, y)\ngs_time = () - tic",
            "rng = (0)\n\nX, y = (n_samples=400, n_features=12, random_state=rng)\n\nclf = (n_estimators=20, random_state=rng)\n\nparam_dist = {\n    \"max_depth\": [3, None],\n    \"max_features\": (1, 6),\n    \"min_samples_split\": (2, 11),\n    \"bootstrap\": [True, False],\n    \"criterion\": [\"gini\", \"entropy\"],\n}\n\nrsh = (\n    estimator=clf, param_distributions=param_dist, factor=2, random_state=rng\n)\nrsh.fit(X, y)",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "sklearn->Examples->Model Selection->Comparison between grid search and successive halving"
            ],
            [
                "sklearn->Examples->Model Selection->Successive Halving Iterations"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ]
        ]
    },
    "1021331": {
        "jupyter_code_cell": "# for gender, before one hot encoding. .head() is used to display first 5 records.\ndf['DemGender'].head(5)",
        "matched_tutorial_code_inds": [
            6703,
            3777,
            6166,
            6953,
            6951
        ],
        "matched_tutorial_codes": [
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# For each team... get number of days between games\ntidy.groupby('team')['date'].diff().dt.days - 1",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "# Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "# Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ]
        ]
    },
    "68031": {
        "jupyter_code_cell": "def get_count_df( df ):\n    lead_digit_counts = { str(n): 0 for n in range(1,10)}\n    \n    for t in df.columns:\n        prices = df[t].dropna()\n        d = count_lead_digits( prices )\n        for digit, count in d.items():\n            lead_digit_counts[digit] += count\n    \n    df_counts = pd.DataFrame.from_dict(lead_digit_counts, orient='index')\n    df_counts.columns = ['frequency']\n    df_counts.sort_index(inplace=True)\n    return df_counts",
        "matched_tutorial_code_inds": [
            1727,
            6146,
            3151,
            2411,
            6634
        ],
        "matched_tutorial_codes": [
            "def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "def build_df(ytime, ystatus, mtime0, mtime, mstatus):\n    df = pd.DataFrame(\n        {\n            \"ytime\": ytime,\n            \"ystatus\": ystatus,\n            \"mtime\": mtime,\n            \"mstatus\": mstatus,\n            \"exp\": exp,\n        }\n    )\n    return df",
            "def make_heatmap(ax, gs, is_sh=False, make_cbar=False):\n    \"\"\"Helper to make a heatmap.\"\"\"\n    results = (gs.cv_results_)\n    results[[\"param_C\", \"param_gamma\"]] = results[[\"param_C\", \"param_gamma\"]].astype(\n        \n    )\n    if is_sh:\n        # SH dataframe: get mean_test_score values for the highest iter\n        scores_matrix = results.sort_values(\"iter\").pivot_table(\n            index=\"param_gamma\",\n            columns=\"param_C\",\n            values=\"mean_test_score\",\n            aggfunc=\"last\",\n        )\n    else:\n        scores_matrix = results.pivot(\n            index=\"param_gamma\", columns=\"param_C\", values=\"mean_test_score\"\n        )\n\n    im = ax.imshow(scores_matrix)\n\n    ax.set_xticks((len(Cs)))\n    ax.set_xticklabels([\"{:.0E}\".format(x) for x in Cs])\n    ax.set_xlabel(\"C\", fontsize=15)\n\n    ax.set_yticks((len(gammas)))\n    ax.set_yticklabels([\"{:.0E}\".format(x) for x in gammas])\n    ax.set_ylabel(\"gamma\", fontsize=15)\n\n    # Rotate the tick labels and set their alignment.\n    (ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    if is_sh:\n        iterations = results.pivot_table(\n            index=\"param_gamma\", columns=\"param_C\", values=\"iter\", aggfunc=\"max\"\n        ).values\n        for i in range(len(gammas)):\n            for j in range(len(Cs)):\n                ax.text(\n                    j,\n                    i,\n                    iterations[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\",\n                    fontsize=20,\n                )\n\n    if make_cbar:\n        fig.subplots_adjust(right=0.8)\n        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n        fig.colorbar(im, cax=cbar_ax)\n        cbar_ax.set_ylabel(\"mean_test_score\", rotation=-90, va=\"bottom\", fontsize=15)\n\n\nfig, axes = (ncols=2, sharey=True)\nax1, ax2 = axes\n\nmake_heatmap(ax1, gsh, is_sh=True)\nmake_heatmap(ax2, gs, make_cbar=True)\n\nax1.set_title(\"Successive Halving\\ntime = {:.3f}s\".format(gsh_time), fontsize=15)\nax2.set_title(\"GridSearch\\ntime = {:.3f}s\".format(gs_time), fontsize=15)\n\n()\n\n\n<img alt=\"Successive Halving time = 1.240s, GridSearch time = 5.943s\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\" srcset=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\"/>",
            "def plot_influence(conf, mse_values, prediction_times, complexities):\n    \"\"\"\n    Plot influence of model complexity on both accuracy and latency.\n    \"\"\"\n\n    fig = ()\n    fig.subplots_adjust(right=0.75)\n\n    # first axes (prediction error)\n    ax1 = fig.add_subplot(111)\n    line1 = ax1.plot(complexities, mse_values, c=\"tab:blue\", ls=\"-\")[0]\n    ax1.set_xlabel(\"Model Complexity (%s)\" % conf[\"complexity_label\"])\n    y1_label = conf[\"prediction_performance_label\"]\n    ax1.set_ylabel(y1_label)\n\n    ax1.spines[\"left\"].set_color(line1.get_color())\n    ax1.yaxis.label.set_color(line1.get_color())\n    ax1.tick_params(axis=\"y\", colors=line1.get_color())\n\n    # second axes (latency)\n    ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)\n    line2 = ax2.plot(complexities, prediction_times, c=\"tab:orange\", ls=\"-\")[0]\n    ax2.yaxis.tick_right()\n    ax2.yaxis.set_label_position(\"right\")\n    y2_label = \"Time (s)\"\n    ax2.set_ylabel(y2_label)\n    ax1.spines[\"right\"].set_color(line2.get_color())\n    ax2.yaxis.label.set_color(line2.get_color())\n    ax2.tick_params(axis=\"y\", colors=line2.get_color())\n\n    (\n        (line1, line2), (\"prediction error\", \"prediction latency\"), loc=\"upper center\"\n    )\n\n    (\n        \"Influence of varying '%s' on %s\"\n        % (conf[\"changing_param\"], conf[\"estimator\"].__name__)\n    )\n\n\nfor conf in configurations:\n    prediction_performances, prediction_times, complexities = benchmark_influence(conf)\n    plot_influence(conf, prediction_performances, prediction_times, complexities)\n()\n\n\n\n<img alt=\"Influence of varying 'l1_ratio' on SGDClassifier\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_001.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_001.png\"/>\n<img alt=\"Influence of varying 'nu' on NuSVR\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_002.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_002.png\"/>\n<img alt=\"Influence of varying 'n_estimators' on GradientBoostingRegressor\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_003.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_003.png\"/>",
            "def plot_coefficients_by_equation(states):\n    fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n\n    # The way we defined Z_t implies that the first 5 elements of the\n    # state vector correspond to the first variable in y_t, which is GDP growth\n    ax = axes[0, 0]\n    states.iloc[:, :5].plot(ax=ax)\n    ax.set_title('GDP growth')\n    ax.legend()\n\n    # The next 5 elements correspond to inflation\n    ax = axes[0, 1]\n    states.iloc[:, 5:10].plot(ax=ax)\n    ax.set_title('Inflation rate')\n    ax.legend();\n\n    # The next 5 elements correspond to unemployment\n    ax = axes[1, 0]\n    states.iloc[:, 10:15].plot(ax=ax)\n    ax.set_title('Unemployment equation')\n    ax.legend()\n\n    # The last 5 elements correspond to the interest rate\n    ax = axes[1, 1]\n    states.iloc[:, 15:20].plot(ax=ax)\n    ax.set_title('Interest rate equation')\n    ax.legend();\n\n    return ax\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)"
            ],
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "sklearn->Examples->Model Selection->Comparison between grid search and successive halving"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Model Complexity Influence->Run the code and plot the results"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q"
            ]
        ]
    },
    "1293630": {
        "jupyter_code_cell": "odds = np.exp(answer2.params[1:])\nodds",
        "matched_tutorial_code_inds": [
            1497,
            5570,
            6759,
            5809,
            3450
        ],
        "matched_tutorial_codes": [
            "china_total = china_masked.data\nchina_total",
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "df = pred.summary_frame(alpha=0.05)\ndf",
            "norms = sm.robust.norms",
            "y_train = (y)\ny_train[unlabeled_set] = -1"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ]
        ]
    },
    "583059": {
        "jupyter_code_cell": "# Check out what the data looks like.\n\nplt.figure(figsize = (15,5))\nplt.hist(muons.M, bins = 500, range=(0,130))\nplt.xlabel(\"Invariant mass (GeV)\", fontsize = 15)\nplt.ylabel(\"Number of events \\n\", fontsize = 15)\nplt.show()",
        "matched_tutorial_code_inds": [
            6688,
            6932,
            6632,
            3294,
            51
        ],
        "matched_tutorial_codes": [
            "# Print the news, computed by the `news` method\nprint(news.news)\n\n# Manually compute the news\nprint()\nprint((y_update.iloc[0] - phi_hat * y_pre.iloc[-1]).round(6))",
            "# Step 1: append a new observation to the sample and refit the parameters\nappend_res = training_res.append(endog[training_obs:training_obs + 1], refit=True)\n\n# Print the re-estimated parameters\nprint(append_res.params)",
            "# Create an instance of our TVPVAR class with our observed dataset y\nmod = TVPVAR(y)",
            "import sys\n\ntry:\n    import nmslib\nexcept ImportError:\n    print(\"The package 'nmslib' is required to run this example.\")\n    ()\n\ntry:\n    from pynndescent import PyNNDescentTransformer\nexcept ImportError:\n    print(\"The package 'pynndescent' is required to run this example.\")\n    ()",
            "# Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Addendum: manually computing the news, weights, and impacts"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->TVP-VAR model in Statsmodels"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Approximate nearest neighbors in TSNE"
            ],
            [
                "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->3. Save model A"
            ]
        ]
    },
    "598798": {
        "jupyter_code_cell": "YOATP = rATP/rO",
        "matched_tutorial_code_inds": [
            2723,
            5922,
            5358,
            2083,
            2721
        ],
        "matched_tutorial_codes": [
            "OLS R2 score 0.7436926291700356",
            "%R print(mod)",
            "%R library(lme4)",
            "PCR r-squared -0.026\nPLS r-squared 0.658",
            "NNLS R2 score 0.8225220806196526"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Linear Mixed-Effects"
            ],
            [
                "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->Projection on one component and predictive power"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ]
        ]
    },
    "1432865": {
        "jupyter_code_cell": "giss_temp = pd.read_table('data/temperatures/GLB.Ts+dSST.txt', sep='\\s+',\n                          skiprows=7, skip_footer=11, engine='python')\ngiss_temp",
        "matched_tutorial_code_inds": [
            5719,
            3932,
            1486,
            6902,
            1105
        ],
        "matched_tutorial_codes": [
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training"
            ]
        ]
    },
    "1045719": {
        "jupyter_code_cell": "import dill\nf = open('twitter_sentiment_model.pkl','wb')\nr = RegexPreprocess()\ndill.dump(sentiment_lr, f)\nf.close()",
        "matched_tutorial_code_inds": [
            1726,
            3631,
            1762,
            5664,
            5456
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\n\nenv.seed(42)\nenv.reset()\nrandom_frame = env.render(mode=\"rgb_array\")\nprint(random_frame.shape)\nplt.imshow(random_frame)",
            "zf = zipfile.ZipFile(\"data/flights.csv.zip\")\nfp = zf.extract(zf.filelist[0].filename, path='data/')\ndf = pd.read_csv(fp, parse_dates=[\"FL_DATE\"]).rename(columns=str.lower)\n\ndf.info()",
            "glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "data2 = sm.datasets.scotland.load()\ndata2.exog = sm.add_constant(data2.exog, prepend=False)\nprint(data2.exog.head())\nprint(data2.endog.head())",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Get the Data"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gamma for proportional count response->Load Scottish Parliament Voting data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ]
        ]
    },
    "1247369": {
        "jupyter_code_cell": "data['genre_game-show'] = data[['genre_game-show', 'genre_reality-tv']].max(axis=1)\ndata['genre_music'] = data[['genre_musical', 'genre_music']].max(axis=1)\ndata['genre_thriller'] = 1-data[['genre_comedy']]",
        "matched_tutorial_code_inds": [
            6423,
            6703,
            3919,
            6512,
            3783
        ],
        "matched_tutorial_codes": [
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "subset = daily.loc['2011':'2016']\nax = subset.div(1000).plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",)\nsns.despine();",
            "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "df['home_win'] = df['home_points'] &gt; df['away_points']\ndf['rest_spread'] = df['home_rest'] - df['away_rest']\ndf.dropna().head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Caution: VARMA(p,q) specifications"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ]
        ]
    },
    "191170": {
        "jupyter_code_cell": "info=pd.read_csv(\"customer_info_numeric.csv\")\nrfm_train=pd.read_csv(\"rfm train seq.csv\")",
        "matched_tutorial_code_inds": [
            4022,
            5586,
            4031,
            7007,
            4030
        ],
        "matched_tutorial_codes": [
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "fig = pandas_ar_res.plot_predict(start=\"2005\", end=\"2027\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\"/>",
            "dowjones = sns.load_dataset(\"dowjones\")\nsns.relplot(data=dowjones, x=\"Date\", y=\"Price\", kind=\"line\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots"
            ]
        ]
    },
    "625632": {
        "jupyter_code_cell": "banks_data = labels[labels.Bank1.notnull()]\nbanks_data.Bank1 = banks_data.Bank1.apply(clean)\nbanks_data.Bank2 = banks_data.Bank2.apply(clean)",
        "matched_tutorial_code_inds": [
            5240,
            6811,
            5451,
            5456,
            4094
        ],
        "matched_tutorial_codes": [
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "anes_data = sm.datasets.anes96.load()\nanes_exog = anes_data.exog\nanes_exog = sm.add_constant(anes_exog)",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ]
        ]
    },
    "1220759": {
        "jupyter_code_cell": "sns.countplot(y='LanguageRecommendationSelect', data=mc, palette='Spectral')",
        "matched_tutorial_code_inds": [
            3969,
            3935,
            3936,
            4083,
            4086
        ],
        "matched_tutorial_codes": [
            "sns.catplot(data=anagrams_long, x=\"solutions\", y=\"score\", hue=\"attnr\", kind=\"point\")\n",
            "sns.displot(data=tips, x=\"total_bill\", col=\"time\", kde=True)\n",
            "sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "sns.displot(diamonds, x=\"price\", y=\"clarity\", log_scale=(True, False))\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ]
        ]
    },
    "279837": {
        "jupyter_code_cell": "input_dim = X_train.shape[1]\nencoding_dim = 15",
        "matched_tutorial_code_inds": [
            3450,
            6217,
            1497,
            5570,
            1502
        ],
        "matched_tutorial_codes": [
            "y_train = (y)\ny_train[unlabeled_set] = -1",
            "auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "china_total = china_masked.data\nchina_total",
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ]
        ]
    },
    "954218": {
        "jupyter_code_cell": "def context_tag(comb_q1):\n    context_tags = []\n    for i in range(len(comb_q1)):\n        context_tag = []\n        context = context_a_sent(comb_q1[i])\n        \n        for item in context:\n            x = list(zip(item.keys(), item.values()))  \n            context_tag.append(x[0][0] + x[0][1]+\" \"+x[1][0] + x[1][1])\n        context_tags.append(context_tag)\n    return context_tags",
        "matched_tutorial_code_inds": [
            5810,
            1953,
            1672,
            4722,
            1766
        ],
        "matched_tutorial_codes": [
            "def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax",
            "def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "def julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
            "def currency(x, pos):\n    \"\"\"The two arguments are the value and tick position\"\"\"\n    if x = 1e6:\n        s = '${:1.1f}M'.format(x*1e-6)\n    else:\n        s = '${:1.0f}K'.format(x*1e-3)\n    return s",
            "def fp_input_gate(concat, parameters):\n    it = sigmoid(np.dot(parameters['Wi'], concat)\n                 + parameters['bi'])\n    cmt = np.tanh(np.dot(parameters['Wcm'], concat)\n                  + parameters['bcm'])\n    return it, cmt"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression"
            ],
            [
                "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Julia set"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Forward Propagation"
            ]
        ]
    },
    "554471": {
        "jupyter_code_cell": "df = pd.read_csv(\"Crimes_-_2001_to_present.csv\")",
        "matched_tutorial_code_inds": [
            3676,
            4031,
            7007,
            4153,
            4152
        ],
        "matched_tutorial_codes": [
            "weather = pd.read_hdf(\"weather.h5\", \"weather\").sort_index()\n\nweather.head()",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "fig = pandas_ar_res.plot_predict(start=\"2005\", end=\"2027\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\"/>",
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ]
        ]
    },
    "1049448": {
        "jupyter_code_cell": "sw = stopwords.words('english')\n\ndef tokenizer_for_LDAvis(sentence):\n    tokens = ret.tokenize(sentence)\n    sentence = [lem.lemmatize(t.lower()) for t in tokens if t.lower() not in sw and len(t) >1]\n    string_tokenize_title = ' '.join(sentence)#map(str, sentence))\n    return string_tokenize_title",
        "matched_tutorial_code_inds": [
            1056,
            4438,
            6598,
            2140,
            1761
        ],
        "matched_tutorial_codes": [
            "ntokens = len(corpus.dictionary)\n\nmodel = (\n    ntoken = ntokens,\n    ninp = 512,\n    nhid = 256,\n    nlayers = 5,\n)\n\n(\n    (\n        model_data_filepath + 'word_language_model_quantize.pth',\n        map_location=('cpu')\n        )\n    )\n\n()\nprint(model)",
            "word_bytes = np.ndarray((word_list.size, word_list.itemsize),\n...                         dtype='uint8',\n...                         buffer=word_list.data)\n # each unicode character is four bytes long. We only need first byte\n # we know that there are three characters in each word\n word_bytes = word_bytes[:, ::word_list.itemsize//3]\n word_bytes.shape\n(586, 3)    # may vary",
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>",
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>",
            "speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->3. Load the pre-trained model"
            ],
            [
                "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ]
        ]
    },
    "321538": {
        "jupyter_code_cell": "users = merged['personId'].unique() ; print(\"# of users\" , len(users))",
        "matched_tutorial_code_inds": [
            2822,
            5482,
            5488,
            4435,
            4455
        ],
        "matched_tutorial_codes": [
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "respondent1000 = dta.iloc[1000]\nprint(respondent1000)",
            "respondent1000 = dta.iloc[[1000]]\naffair_mod.predict(respondent1000)",
            "word_list = open('/usr/share/dict/words').readlines()\n word_list = map(str.strip, word_list)",
            "points = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n tri = Delaunay(points)\n np.unique(tri.simplices.ravel())\narray([0, 1, 2, 3], dtype=int32)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations->Coplanar points"
            ]
        ]
    },
    "810835": {
        "jupyter_code_cell": "VL_registered = registered[registered['Region Gewest'] == 'VLA']\n(100*VL_registered.groupby('Titular Type')['Titular Type'].count()/(len(VL_registered))).plot(kind='barh', )\nplt.xlabel('% aantal wagens')\nplt.ylabel(' ')\nplt.gca().set_yticklabels(['Rechtsperson', 'Natuurlijk persoon'])",
        "matched_tutorial_code_inds": [
            2621,
            2847,
            3478,
            4682,
            6016
        ],
        "matched_tutorial_codes": [
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "X_train_preprocessed = (\n    model[:-1].transform(X_train), columns=feature_names\n)\n\nX_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))\n(\"Feature ranges\")\n(\"Std. dev. of feature values\")\n(left=0.3)\n\n\n<img alt=\"Feature ranges\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\"/>",
            "C_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = (C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Support Vector Machines->RBF SVM parameters->Train classifiers"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "161998": {
        "jupyter_code_cell": "symbol = \"CMG\"\nstart = \"2012-01-01\"\nend = \"2016-01-01\"\nprices = get_pricing(symbol, start_date=start, end_date=end, fields=\"price\")\n\n# Resample daily prices to get monthly prices using median. \nmonthly_prices = prices.resample('M').median()\nmonthly_prices.head(24)",
        "matched_tutorial_code_inds": [
            1486,
            3932,
            4682,
            3281,
            6061
        ],
        "matched_tutorial_codes": [
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ]
        ]
    },
    "507192": {
        "jupyter_code_cell": "df2['intercept']=1\nmodel=sm.Logit(df2['converted'],df2[['intercept','ab_page']])\nresult=model.fit()",
        "matched_tutorial_code_inds": [
            5171,
            5454,
            5289,
            5108,
            6512
        ],
        "matched_tutorial_codes": [
            "sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "mod = AR(endog)\naic = []\nfor lag in range(1,11):\n    res = mod.fit(maxlag=lag)\n    aic.append(res.aic)",
            "mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "statsmodels->User Guide->Background->Pitfalls->Repeated calls to fit with different parameters"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Caution: VARMA(p,q) specifications"
            ]
        ]
    },
    "918063": {
        "jupyter_code_cell": "try:\n    df = df.reset_index()\nexcept:\n    pass\n    \n# copy specific columns\ndf1 = df[['date','alt','lat','count']]\ndf1['date'] = pd.to_datetime(df1['date'])\n\ndf1['count'] = df1['count'] + pp(40000) - pp(df1['alt']) # subtract altitude relation\ndf1['count'] = df1['count'] + p(60) - p(df1['lat']) # subtract latitude relation\n\ndf1 = df1.set_index('date')\ndf1 = df1.sort_index()\n\ndf1.to_csv('./radiation.csv') # save data ",
        "matched_tutorial_code_inds": [
            6053,
            6026,
            2039,
            6359,
            428
        ],
        "matched_tutorial_codes": [
            "try:\n    rehab_table = pd.read_csv(\"rehab.table\")\nexcept:\n    url = \"http://stats191.stanford.edu/data/rehab.csv\"\n    rehab_table = pd.read_table(url, delimiter=\",\")\n    rehab_table.to_csv(\"rehab.table\")\n\nfig, ax = plt.subplots(figsize=(8, 6))\nfig = rehab_table.boxplot(\"Time\", \"Fitness\", ax=ax, grid=False)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_53_0.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_53_0.png\"/>",
            "try:\n    jobtest_table = pd.read_table(\"jobtest.table\")\nexcept:  # do not have data already\n    url = \"http://stats191.stanford.edu/data/jobtest.table\"\n    jobtest_table = pd.read_table(url)\n\nfactor_group = jobtest_table.groupby([\"MINORITY\"])\n\nfig, ax = plt.subplots(figsize=(6, 6))\ncolors = [\"purple\", \"green\"]\nmarkers = [\"o\", \"v\"]\nfor factor, group in factor_group:\n    ax.scatter(\n        group[\"TEST\"],\n        group[\"JPERF\"],\n        color=colors[factor],\n        marker=markers[factor],\n        s=12 ** 2,\n    )\nax.set_xlabel(\"TEST\")\nax.set_ylabel(\"JPERF\")",
            "try:  # Scipy = 1.10\n    from scipy.datasets import \nexcept ImportError:\n    from scipy.misc import \n\nraccoon_face = (gray=True)\n\nprint(f\"The dimension of the image is {raccoon_face.shape}\")\nprint(f\"The data used to encode the image is of type {raccoon_face.dtype}\")\nprint(f\"The number of bytes taken in RAM is {raccoon_face.nbytes}\")",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "with (use_cuda=False) as :\n     = model()\nwith (use_cuda=False) as :\n     = scripted_model()\nwith (use_cuda=False) as :\n     = scripted_quantized_model()\nwith (use_cuda=False) as :\n     = optimized_scripted_quantized_model()\nwith (use_cuda=False) as :\n     = ptl()\n\nprint(\"original model: {:.2f}ms\".format(/1000))\nprint(\"scripted model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized &amp; optimized model: {:.2f}ms\".format(/1000))\nprint(\"lite model: {:.2f}ms\".format(/1000))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA->One-way ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data"
            ],
            [
                "sklearn->Examples->Clustering->Vector Quantization Example->Original image"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed",
                "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed"
            ]
        ]
    },
    "78081": {
        "jupyter_code_cell": "folium.Marker([43.6426, -79.3871], popup='<i>CN Tower</i>').add_to(m)\nfolium.Marker([43.6677, -79.3948], popup='<b>Royal Ontario Museum</b>').add_to(m)\nm",
        "matched_tutorial_code_inds": [
            6179,
            4167,
            2630,
            1188,
            4185
        ],
        "matched_tutorial_codes": [
            "sel = ar_select_order(yoy_housing, 13, old_names=False)\nsel.ar_lags",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "rng = (4)\nX_train = rng.uniform(0, 5, 10).reshape(-1, 1)\ny_train = ((X_train[:, 0] - 2.5) ** 2)\nn_samples = 5",
            "try:\n    (f3)\nexcept:\n    tb.print_exc()\n\ntry:\n    (f3)\nexcept:\n    tb.print_exc()",
            "sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dynamics"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Dataset and Gaussian process generation"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ]
        ]
    },
    "136245": {
        "jupyter_code_cell": "X = mushroom_col.iloc[:, 0:9].values\nY = mushroom_col.iloc[:, 1].values\n\nX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, random_state=1)\nlinreg.fit(X_train, Y_train)\nY_pred = linreg.predict(X_test)\n\nprint(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))",
        "matched_tutorial_code_inds": [
            2624,
            3449,
            1760,
            94,
            3171
        ],
        "matched_tutorial_codes": [
            "X = ([\"AGCT\", \"AGC\", \"AACT\", \"TAA\", \"AAA\", \"GAACA\"])\nY = ([1.0, 1.0, 2.0, 2.0, 3.0, 3.0])\n\ntraining_idx = [0, 1, 3, 4]\ngp = (kernel=kernel)\ngp.fit(X[training_idx], Y[training_idx])\n\n(figsize=(8, 5))\n((len(X)), gp.predict(X), color=\"b\", label=\"prediction\")\n(training_idx, Y[training_idx], width=0.2, color=\"r\", alpha=1, label=\"training\")\n((len(X)), X)\n(\"Regression on sequences\")\n()\n()\n\n\n<img alt=\"Regression on sequences\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_on_structured_data_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_on_structured_data_002.png\"/>",
            "X = digits.data[indices[:340]]\ny = digits.target[indices[:340]]\nimages = digits.images[indices[:340]]\n\nn_total_samples = len(y)\nn_labeled_points = 40\n\nindices = (n_total_samples)\n\nunlabeled_set = indices[n_labeled_points:]",
            "X_train = textproc.cleantext(train_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:2000]\n\nX_test = textproc.cleantext(test_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:1000]\n\ny_train = train_df['sentiment'].to_numpy()[0:2000]\ny_test = test_df['sentiment'].to_numpy()[0:1000]",
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "X, y = (n_samples=8000, random_state=42)\n\n# The scorers can be either one of the predefined metric strings or a scorer\n# callable, like the one returned by make_scorer\nscoring = {\"AUC\": \"roc_auc\", \"Accuracy\": ()}\n\n# Setting refit='AUC', refits an estimator on the whole dataset with the\n# parameter setting that has the best cross-validated AUC score.\n# That estimator is made available at ``gs.best_estimator_`` along with\n# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n# ``gs.best_index_``\ngs = (\n    (random_state=42),\n    param_grid={\"min_samples_split\": range(2, 403, 20)},\n    scoring=scoring,\n    refit=\"AUC\",\n    n_jobs=2,\n    return_train_score=True,\n)\ngs.fit(X, y)\nresults = gs.cv_results_"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian processes on discrete data structures->Regression"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars"
            ],
            [
                "sklearn->Examples->Model Selection->Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV->Running GridSearchCV using multiple evaluation metrics"
            ]
        ]
    },
    "1306957": {
        "jupyter_code_cell": "y = dm['HomeWin']  \nX = dm[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']] \nX = sm.add_constant(X)  ",
        "matched_tutorial_code_inds": [
            462,
            3684,
            6984,
            5205,
            6731
        ],
        "matched_tutorial_codes": [
            "input = lineToTensor('Albert')\n = (1, n_hidden)\n\n,  = rnn(input[0], )\nprint()",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "y = medpar.los\nX = medpar[[\"type2\", \"type3\", \"hmo\", \"white\"]].copy()\nX[\"constant\"] = 1",
            "res = sm.OLS(y, X).fit()\nprint(res.summary())",
            "mod = MultipleYsModel(i_hat, s_t, m_hat)\nres = mod.fit()\n\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Creating the Network"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->What do we need to modify?->2) The update() function"
            ]
        ]
    },
    "346112": {
        "jupyter_code_cell": "pn_output_path = os.path.join(DTPM_TRXDir,'3_DAILY/daily_pn.csv')\nzp_output_path = os.path.join(DTPM_TRXDir,'3_DAILY/daily_zp.csv')",
        "matched_tutorial_code_inds": [
            1759,
            4387,
            3854,
            1757,
            398
        ],
        "matched_tutorial_codes": [
            "train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "b1 = signal.firwin(40, 0.5)\n b2 = signal.firwin(41, [0.3, 0.8])\n w1, h1 = signal.freqz(b1)\n w2, h2 = signal.freqz(b2)",
            "fl_date         datetime64[ns]\norigin                category\ncrs_dep_time    datetime64[ns]\ndep_time        datetime64[ns]\ncrs_arr_time    datetime64[ns]\narr_time        datetime64[ns]\ndtype: object",
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "epsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"data/lenet_mnist_model.pth\"\nuse_cuda=True"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Inputs"
            ]
        ]
    },
    "1030915": {
        "jupyter_code_cell": "lr = MLPClassifier(hidden_layer_sizes=(10, 10, 10,), batch_size=100,\n                   solver='sgd', learning_rate_init=0.01, early_stopping=True)\n\nstart = time.time()\nscores = cross_val_score(estimator=lr,\n                         X=X, \n                         y=y,\n                         cv=5)\n\nprint(\"\\nAccuracy: {}% (+/- {})\".format(round(scores.mean() * 100, 2), round(scores.std(), 3) * 2))\nprint('Finished in {}sec\\n'.format(round(time.time() - start, 2)))",
        "matched_tutorial_code_inds": [
            1892,
            2386,
            3349,
            3504,
            2688
        ],
        "matched_tutorial_codes": [
            "lr = (C=1.0)\nsvc = NaivelyCalibratedLinearSVC(max_iter=10_000)\nsvc_isotonic = (svc, cv=2, method=\"isotonic\")\nsvc_sigmoid = (svc, cv=2, method=\"sigmoid\")\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (svc, \"SVC\"),\n    (svc_isotonic, \"SVC + Isotonic\"),\n    (svc_sigmoid, \"SVC + Sigmoid\"),\n]",
            "lfw_people = (min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint(\"Total dataset size:\")\nprint(\"n_samples: %d\" % n_samples)\nprint(\"n_features: %d\" % n_features)\nprint(\"n_classes: %d\" % n_classes)",
            "clf = (\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", ())]\n)\n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.2, random_state=0)\n\nclf.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))",
            "lw = 2\n\nsvrs = [svr_rbf, svr_lin, svr_poly]\nkernel_label = [\"RBF\", \"Linear\", \"Polynomial\"]\nmodel_color = [\"m\", \"c\", \"g\"]\n\nfig, axes = (nrows=1, ncols=3, figsize=(15, 10), sharey=True)\nfor ix, svr in enumerate(svrs):\n    axes[ix].plot(\n        X,\n        svr.fit(X, y).predict(X),\n        color=model_color[ix],\n        lw=lw,\n        label=\"{} model\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[svr.support_],\n        y[svr.support_],\n        facecolor=\"none\",\n        edgecolor=model_color[ix],\n        s=50,\n        label=\"{} support vectors\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[((len(X)), svr.support_)],\n        y[((len(X)), svr.support_)],\n        facecolor=\"none\",\n        edgecolor=\"k\",\n        s=50,\n        label=\"other training data\",\n    )\n    axes[ix].legend(\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, 1.1),\n        ncol=1,\n        fancybox=True,\n        shadow=True,\n    )\n\nfig.text(0.5, 0.04, \"data\", ha=\"center\", va=\"center\")\nfig.text(0.06, 0.5, \"target\", ha=\"center\", va=\"center\", rotation=\"vertical\")\nfig.suptitle(\"Support Vector Regression\", fontsize=14)\n()\n\n\n<img alt=\"Support Vector Regression\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_regression_001.png\"/>",
            "lasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X, y)\n\nbic_criterion = zou_et_al_criterion_rescaling(\n    lasso_lars_ic[-1].criterion_,\n    n_samples,\n    lasso_lars_ic[-1].noise_variance_,\n)\n\nindex_alpha_path_bic = (\n    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\n)[0]"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Calibration->Probability Calibration curves->Calibration curves->Linear support vector classifier"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "sklearn->Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels->Look at the results"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso model selection via information criteria"
            ]
        ]
    },
    "1341423": {
        "jupyter_code_cell": "nba['mademissnum'] = num",
        "matched_tutorial_code_inds": [
            1491,
            2336,
            5972,
            5594,
            5970
        ],
        "matched_tutorial_codes": [
            "nbcases_ma",
            "n_jobs = -1",
            "n = n_groups * group_size * level1_size * level2_size",
            "modfd_logit.k_constant",
            "n_groups = 100"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "sklearn->Examples->Ensemble methods->Pixel importances with a parallel forest of trees->Loading the data and model fitting"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure"
            ]
        ]
    },
    "951841": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport warnings\nwarnings.simplefilter('ignore',DeprecationWarning)\nimport seaborn as sns\nimport time\nimport copy\n\nfrom pylab import rcParams\n#import hdbscan\n\n\nfrom tabulate import tabulate\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nfrom __future__ import print_function",
        "matched_tutorial_code_inds": [
            5312,
            1854,
            4728,
            1401,
            519
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "import gymnasium as gym\nimport math\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple, deque\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nenv = gym.make(\"CartPole-v1\")\n\n# set up matplotlib\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\n# if gpu is to be used\n = (\"cuda\" if () else \"cpu\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial"
            ]
        ]
    },
    "605365": {
        "jupyter_code_cell": "from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\ny_pred",
        "matched_tutorial_code_inds": [
            3177,
            2791,
            3168,
            2517,
            2319
        ],
        "matched_tutorial_codes": [
            "from sklearn.linear_model import \n\nclassifier = ()\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)",
            "from sklearn import linear_model\n\nols = ()\n_ = ols.fit(X_train, y_train)",
            "from sklearn.metrics import \n\ny_pred = grid_search.predict(X_test)\nprint((y_test, y_pred))",
            "from sklearn.metrics import \n\ny_pred = anova_svm.predict(X_test)\nprint((y_test, y_pred))",
            "from sklearn.ensemble import \n\nclf = (max_samples=100, random_state=0)\nclf.fit(X_train)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->Load and prepare data"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Sparsity Example: Fitting only features 1  and 2"
            ],
            [
                "sklearn->Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->Tuning hyper-parameters"
            ],
            [
                "sklearn->Examples->Feature Selection->Pipeline ANOVA SVM"
            ],
            [
                "sklearn->Examples->Ensemble methods->IsolationForest example->Training of the model"
            ]
        ]
    },
    "1137947": {
        "jupyter_code_cell": "# Descriptive statistics\nnwid_train_org.describe()",
        "matched_tutorial_code_inds": [
            3929,
            3928,
            6705,
            3930,
            3732
        ],
        "matched_tutorial_codes": [
            "# Apply the default theme\nsns.set_theme()\n",
            "# Import seaborn\nimport seaborn as sns\n",
            "# Show the summary of the news results\nprint(news.summary())",
            "# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n",
            "# Select the 5 largest delays\ndelays.nlargest(5).sort_values()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ]
        ]
    },
    "777040": {
        "jupyter_code_cell": "# we need to tell tensorflow how many inputs to expect and what the data types will be\n# for this early example, everything is just numeric, real valued\nfeatures_tf = [layers.real_valued_column('', dimension=X_train.shape[1])]\nclf = SKCompat(# wrap with SKCompat for easy usage like sklearn\n            learn.DNNClassifier(hidden_units=[50], feature_columns=features_tf)\n        )\n\nclf.fit(X_train,y_train,steps=100)",
        "matched_tutorial_code_inds": [
            32,
            6611,
            2025,
            2176,
            112
        ],
        "matched_tutorial_codes": [
            "# import the modules used here in this recipe\nimport torch\nimport torch.quantization\nimport torch.nn as nn\nimport copy\nimport os\nimport time\n\n# define a very, very simple LSTM for demonstration purposes\n# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n# inspired by\n# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\nclass lstm_for_demonstration():\n  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n     Not to be used for anything other than demonstration.\n  \"\"\"\n  def __init__(self,in_dim,out_dim,depth):\n     super(lstm_for_demonstration,self).__init__()\n     self.lstm = (in_dim,out_dim,depth)\n\n  def forward(self,inputs,hidden):\n     out,hidden = self.lstm(inputs,hidden)\n     return out, hidden\n\n\n(29592)  # set the seed for reproducibility\n\n#shape parameters\nmodel_dimension=8\nsequence_length=20\nbatch_size=1\nlstm_depth=1\n\n# random data for input\ninputs = (sequence_length,batch_size,model_dimension)\n# hidden is actually is a tuple of the initial hidden state and the initial cell state\nhidden = ((lstm_depth,batch_size,model_dimension), (lstm_depth,batch_size,model_dimension))",
            "# Here we restrict the first three parameters to specific values\nwith mod.fix_params({'sigma2.irregular': 1, 'sigma2.level': 0, 'sigma2.trend': 1. / 129600}):\n    # Now we fit any remaining parameters, which in this case\n    # is just `sigma2.seasonal`\n    res_restricted = mod.fit()",
            "# Computing a few extra eigenvectors may speed up the eigen_solver.\n# The spectral clustering quality may also benetif from requesting\n# extra regions for segmentation.\nn_regions_plus = 3\n\n# Apply spectral clustering using the default eigen_solver='arpack'.\n# Any implemented solver can be used: eigen_solver='arpack', 'lobpcg', or 'amg'.\n# Choosing eigen_solver='amg' requires an extra package called 'pyamg'.\n# The quality of segmentation and the speed of calculations is mostly determined\n# by the choice of the solver and the value of the tolerance 'eigen_tol'.\n# TODO: varying eigen_tol seems to have no effect for 'lobpcg' and 'amg' #21243.\nfor assign_labels in (\"kmeans\", \"discretize\", \"cluster_qr\"):\n    t0 = ()\n    labels = (\n        graph,\n        n_clusters=(n_regions + n_regions_plus),\n        eigen_tol=1e-7,\n        assign_labels=assign_labels,\n        random_state=42,\n    )\n\n    t1 = ()\n    labels = labels.reshape(rescaled_coins.shape)\n    (figsize=(5, 5))\n    (rescaled_coins, cmap=plt.cm.gray)\n\n    (())\n    (())\n    title = \"Spectral clustering: %s, %.2fs\" % (assign_labels, (t1 - t0))\n    print(title)\n    (title)\n    for l in range(n_regions):\n        colors = [plt.cm.nipy_spectral((l + 4) / float(n_regions + 4))]\n        (labels == l, colors=colors)\n        # To view individual segments as appear comment in plt.pause(0.5)\n()\n\n# TODO: After #21194 is merged and #21243 is fixed, check which eigen_solver\n# is the best and set eigen_solver='arpack', 'lobpcg', or 'amg' and eigen_tol\n# explicitly in this example.\n\n\n\n<img alt=\"Spectral clustering: kmeans, 2.04s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\"/>\n<img alt=\"Spectral clustering: discretize, 1.83s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\"/>\n<img alt=\"Spectral clustering: cluster_qr, 1.82s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\"/>",
            "# The ordinal encoder will first output the categorical features, and then the\n# continuous (passed-through) features\n\nhist_native = (\n    ordinal_encoder,\n    (\n        random_state=42,\n        categorical_features=categorical_columns,\n    ),\n).set_output(transform=\"pandas\")",
            "# AMP for JIT mode is enabled by default, and is divergent with its eager mode counterpart\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16):\n    model = torch.jit.trace(model, (example_input))\n    model = torch.jit.freeze(model)\n    # a couple of warmup runs\n    model(example_input)\n    model(example_input)\n    # speedup would be observed in subsequent runs.\n    model(example_input)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->1: Set Up"
            ],
            [
                "statsmodels->Examples->State space models->Fixed / constrained parameters in state space models->Adding a seasonal component"
            ],
            [
                "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with native categorical support"
            ],
            [
                "torch->PyTorch Recipes->Performance Tuning Guide->CPU specific optimizations->Use oneDNN Graph with TorchScript for inference"
            ]
        ]
    },
    "1028081": {
        "jupyter_code_cell": "byyear = complaintdates.groupby(by='Year').count()\nbyyear",
        "matched_tutorial_code_inds": [
            3892,
            1495,
            2387,
            1502,
            5570
        ],
        "matched_tutorial_codes": [
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "Total dataset size:\nn_samples: 1288\nn_features: 1850\nn_classes: 7",
            "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ]
        ]
    },
    "1434696": {
        "jupyter_code_cell": "split = 100\nx_test = X[-split:,:,:]\ny_test = Y[-split:,:]\ns_test = sequence_lengths[-split:]\n\nx = X[0:-split,:,:]\ny = Y[0:-split,:]\nsequence_lengths = sequence_lengths[0:-split]",
        "matched_tutorial_code_inds": [
            5193,
            2731,
            2352,
            403,
            6695
        ],
        "matched_tutorial_codes": [
            "nsample = 100\nx = np.linspace(0, 10, 100)\nX = np.column_stack((x, x ** 2))\nbeta = np.array([1, 0.1, 10])\ne = np.random.normal(size=nsample)",
            "n_alphas = 200\nalphas = (-10, -2, n_alphas)\n\ncoefs = []\nfor a in alphas:\n    ridge = (alpha=a, fit_intercept=False)\n    ridge.fit(X, y)\n    coefs.append(ridge.coef_)",
            "xt = X[:20]\n\npred1 = reg1.predict(xt)\npred2 = reg2.predict(xt)\npred3 = reg3.predict(xt)\npred4 = ereg.predict(xt)",
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "# Previous dataset runs through 2017-02\ny_pre = infl.loc[:'2017-01'].copy()\nconst_pre = np.ones(len(y_pre))\nprint(y_pre.tail())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Plot Ridge coefficients as a function of the regularization->Compute paths"
            ],
            [
                "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions->Making predictions"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ]
        ]
    },
    "899572": {
        "jupyter_code_cell": "factors.corrwith(port)",
        "matched_tutorial_code_inds": [
            6199,
            6212,
            6202,
            5522,
            5600
        ],
        "matched_tutorial_codes": [
            "det_proc.out_of_sample(15)",
            "det_proc.out_of_sample(10)",
            "det_proc.out_of_sample(12)",
            "glm_mod.model.data.orig_endog.sum(1)",
            "resf_logit.predict()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Basic Use"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Custom Deterministic Terms"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Using a Date-like Index"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ]
        ]
    },
    "452802": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import cross_validation",
        "matched_tutorial_code_inds": [
            3007,
            6280,
            6999,
            6966,
            2053
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "import numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.base.model import GenericLikelihoodModel",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ]
        ]
    },
    "1016193": {
        "jupyter_code_cell": "test_X['churn'] = test_y['churn']\ntest_X.head(2)",
        "matched_tutorial_code_inds": [
            663,
            1480,
            6911,
            3830,
            3967
        ],
        "matched_tutorial_codes": [
            "traced_rn18 = (rn18)\nprint()",
            "x = load_xy[:, 0]\ny = load_xy[:, 1]\nprint(x)\nprint(y)",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Our arrays as a csv file"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ]
        ]
    },
    "124416": {
        "jupyter_code_cell": "pov_wide_n24['ubi_reduction'] = 1 - (\n    pov_wide_n24.afti_ubi / pov_wide_n24.afti_keep)\npov_wide_n24['tubi_reduction'] = 1 - (\n    pov_wide_n24.afti_tubi / pov_wide_n24.afti_keep)\npov_wide_n24 * 100",
        "matched_tutorial_code_inds": [
            2882,
            2887,
            2865,
            1486,
            6466
        ],
        "matched_tutorial_codes": [
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>",
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ]
        ]
    },
    "741644": {
        "jupyter_code_cell": "# get residuals\nresids = arma.resid.squeeze()\n\nplt.title('Residuals')\nplt.plot(resids)",
        "matched_tutorial_code_inds": [
            5454,
            6703,
            3857,
            5451,
            6321
        ],
        "matched_tutorial_codes": [
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "anes_data = sm.datasets.anes96.load()\nanes_exog = anes_data.exog\nanes_exog = sm.add_constant(anes_exog)",
            "# Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable"
            ]
        ]
    },
    "400358": {
        "jupyter_code_cell": "g = sns.PairGrid(data = sfcrimes, x_vars = ['X'], y_vars = ['Y'], size = 5.5*1.3)\ng = g.map(plt.scatter)",
        "matched_tutorial_code_inds": [
            4170,
            4094,
            6734,
            6166,
            5384
        ],
        "matched_tutorial_codes": [
            "g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n",
            "g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "fig = sm.graphics.plot_partregress(\n    \"prestige\", \"income\", [\"income\", \"education\"], data=prestige\n)\nfig.tight_layout(pad=1.0)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)"
            ]
        ]
    },
    "871432": {
        "jupyter_code_cell": "df.loc[df['Fare'].isnull()].shape",
        "matched_tutorial_code_inds": [
            6006,
            2467,
            2592,
            3792,
            2469
        ],
        "matched_tutorial_codes": [
            "df_infl[:5]",
            "naive_linear_pipeline[:-1].transform(X).shape",
            "co2_data.index.min(), co2_data.index.max()",
            "df['home_win'] = df.home_points &gt; df.away_points",
            "one_hot_linear_pipeline[:-1].transform(X).shape"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 1: Create an outcome variable"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions"
            ]
        ]
    },
    "1214459": {
        "jupyter_code_cell": "# simplest way to draw a graph\nnx.draw(G)",
        "matched_tutorial_code_inds": [
            3929,
            5016,
            630,
            6705,
            3732
        ],
        "matched_tutorial_codes": [
            "# Apply the default theme\nsns.set_theme()\n",
            "# plain text\nplt.title('alpha  beta')",
            "# Some standard imports\nimport io\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx",
            "# Show the summary of the news results\nprint(news.summary())",
            "# Select the 5 largest delays\ndelays.nlargest(5).sort_values()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "matplotlib->Tutorials->Text->Writing mathematical expressions"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ]
        ]
    },
    "603279": {
        "jupyter_code_cell": "#find all infant moratlity rates\ninfmort = []\nfor child in document.getroot():\n    try:\n        infmort.append([child.find('name').text, float(child.find('infant_mortality').text)])\n    except AttributeError:\n        continue\n\n#create dataframe for infant mortality rate then sort to get lowest 10\ndf_infmort = pd.DataFrame(infmort, columns=['country', 'infant_mortality'])\ndf_infmort.sort('infant_mortality', ascending=True).head(10)",
        "matched_tutorial_code_inds": [
            2762,
            3104,
            6754,
            364,
            1782
        ],
        "matched_tutorial_codes": [
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "# range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "# fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# create grid of images\nimg_grid = torchvision.utils.make_grid(images)\n\n# show images\nmatplotlib_imshow(img_grid, one_channel=True)\n\n# write to tensorboard\nwriter.add_image('four_fashion_mnist_images', img_grid)",
            "# To store predicted sentiments\npredictions = {}\n\n# define the length of a paragraph\npara_len = 100\n\n# Retrieve trained values of the parameters\nif os.path.isfile('tutorial-nlp-from-scratch/parameters.npy'):\n    parameters = np.load('tutorial-nlp-from-scratch/parameters.npy', allow_pickle=True).item()\n\n# This is the prediction loop.\nfor index, text in enumerate(X_pred):\n    # split each speech into paragraphs\n    paras = textproc.text_to_paras(text, para_len)\n    # To store the network outputs\n    preds = []\n\n    for para in paras:\n        # split text sample into words/tokens\n        para_tokens = textproc.word_tokeniser(para)\n        # Forward Propagation\n        caches = forward_prop(para_tokens,\n                              parameters,\n                              input_dim)\n\n        # Retrieve the output of the fully connected layer\n        sent_prob = caches['fc_values'][0]['a2'][0][0]\n        preds.append(sent_prob)\n\n    threshold = 0.5\n    preds = np.array(preds)\n    # Mark all predictions &gt; threshold as positive and &lt; threshold as negative\n    pos_indices = np.where(preds &gt; threshold)  # indices where output &gt; 0.5\n    neg_indices = np.where(preds &lt; threshold)  # indices where output &lt; 0.5\n    # Store predictions and corresponding piece of text\n    predictions[speakers[index]] = {'pos_paras': paras[pos_indices[0]],\n                                    'neg_paras': paras[neg_indices[0]]}"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->2. Writing to TensorBoard"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ]
        ]
    },
    "1118023": {
        "jupyter_code_cell": "from IPython.display import Latex\nLatex(r\"\"\"\\begin{eqnarray}\n{\\mathbf{t}}\\,  {\\mathbf{f}}{\\mathbf{(t,d)}} & = \\frac{{\\mathbf{f}}{\\mathbf{(t,d)}}\\, {\\mathbf{+}}\\, \n{\\mathbf{1}} }{\\mathbf{||x||}}\\\\\n\\end{eqnarray}\"\"\")",
        "matched_tutorial_code_inds": [
            289,
            6011,
            4978,
            2556,
            3028
        ],
        "matched_tutorial_codes": [
            "from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow([0].reshape((28, 28)), cmap=\"gray\")\nprint(.shape)\n\n\n<img alt=\"nn tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_nn_tutorial_001.png\" srcset=\"../_images/sphx_glr_nn_tutorial_001.png\"/>",
            "from statsmodels.stats.api import anova_lm\n\ntable1 = anova_lm(lm, interX_lm)\nprint(table1)\n\ninterM_lm = ols(\"S ~ X + C(E)*C(M)\", data=salary_table).fit()\nprint(interM_lm.summary())\n\ntable2 = anova_lm(lm, interM_lm)\nprint(table2)",
            "from matplotlib.font_manager import FontProperties\n\nfont = FontProperties()\nfont.set_family('serif')\nfont.set_name('Times New Roman')\nfont.set_style('italic')\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]', fontsize='large', fontweight='bold')\nax.set_ylabel('Damped oscillation [V]', fontproperties=font)\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_006.png\" srcset=\"../../_images/sphx_glr_text_intro_006.png, ../../_images/sphx_glr_text_intro_006_2_0x.png 2.0x\"/>",
            "from sklearn.linear_model import \n\nridge = ().fit(training_data, training_noisy_target)\n\n(data, target, label=\"True signal\", linewidth=2)\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n(data, ridge.predict(data), label=\"Ridge regression\")\n()\n(\"data\")\n(\"target\")\n_ = (\"Limitation of a linear model such as ridge\")\n\n\n<img alt=\"Limitation of a linear model such as ridge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_002.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_002.png\"/>",
            "from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Learning PyTorch->What is torch.nn really?->MNIST data setup"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Limitations of a simple linear model"
            ],
            [
                "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves"
            ]
        ]
    },
    "1098909": {
        "jupyter_code_cell": "df_wine.columns = ['Identifier','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']\ndf_wine.head(5)",
        "matched_tutorial_code_inds": [
            6005,
            3829,
            6674,
            2462,
            3801
        ],
        "matched_tutorial_codes": [
            "df_infl = infl.summary_frame()",
            "df = sns.load_dataset('titanic')\n\nclf = RandomForestClassifier()\nparam_grid = dict(max_depth=[1, 2, 5, 10, 20, 30, 40],\n                  min_samples_split=[2, 5, 10],\n                  min_samples_leaf=[2, 3, 5])\nest = GridSearchCV(clf, param_grid=param_grid, n_jobs=4)\n\ny = df['survived']\nX = df.drop(['survived', 'who', 'alive'], axis=1)\n\nX = pd.get_dummies(X, drop_first=True)\nX = X.fillna(value=X.median())\nest.fit(X, y);",
            "y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>",
            "hour_df = (\n    (0, 26, 1000).reshape(-1, 1),\n    columns=[\"hour\"],\n)\nsplines = periodic_spline_transformer(24, n_splines=12).fit_transform(hour_df)\nsplines_df = (\n    splines,\n    columns=[f\"spline_{i}\" for i in range(splines.shape[1])],\n)\n([hour_df, splines_df], axis=\"columns\").plot(x=\"hour\", cmap=plt.cm.tab20b)\n_ = (\"Periodic spline-based encoding for the 'hour' feature\")\n\n\n<img alt=\"Periodic spline-based encoding for the 'hour' feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\"/>",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Periodic spline features"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ]
        ]
    },
    "422152": {
        "jupyter_code_cell": "from arcgis.gis import GIS\nimport pandas as pd",
        "matched_tutorial_code_inds": [
            4539,
            3740,
            5874,
            5655,
            6223
        ],
        "matched_tutorial_codes": [
            "from scipy import stats\n import matplotlib.pyplot as plt",
            "from utils import download_airports\nimport zipfile",
            "from statsmodels.graphics.api import abline_plot\nfrom statsmodels.formula.api import ols, rlm",
            "from statsmodels.graphics.api import abline_plot",
            "from statsmodels.graphics.api import qqplot"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Plots"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data",
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ]
        ]
    },
    "396707": {
        "jupyter_code_cell": "\nfrom sklearn.datasets import make_classification\nimport numpy as np\nimport pandas as pd \nimport xgboost as xgb \nfrom matplotlib import pylab as plt \nimport seaborn as sns",
        "matched_tutorial_code_inds": [
            2097,
            2345,
            3461,
            3398,
            2717
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.tree import",
            "import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.ensemble import",
            "import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "import numpy as np\nfrom sklearn.datasets import \n\nX, y = (n_samples=10_000, noise=100, random_state=0)\ny = ((y + abs(y.min())) / 200)\ny_trans = (y)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning"
            ],
            [
                "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Synthetic example"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ]
        ]
    },
    "717129": {
        "jupyter_code_cell": "ax = df.groupby('runtime').popularity.mean().plot(kind='line', title =\"Runtime over Film Popularity\", figsize=(15, 10), legend=True, fontsize=12)\nax.set_xlabel(\"Runtime\", fontsize=12)\nax.set_ylabel(\"Popularity\", fontsize=12)\nplt.show()\n\n",
        "matched_tutorial_code_inds": [
            6577,
            3916,
            3881,
            5393,
            6509
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(resid, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)",
            "ax = occupation_avg.sort_values(ascending=False).plot.barh(color='k', width=0.9);\nlim = ax.get_ylim()\nax.vlines(total_avg, *lim, color='C1', linewidth=3)\nax.legend(['Average donation'])\nax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\")\nsns.despine()",
            "ax = y.plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_ylabel(\"Monthly Flights\")\nplt.legend()\nsns.despine()",
            "fig = sm.graphics.plot_ccpr_grid(prestige_model)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\"/>",
            "ax = res.impulse_responses(10, orthogonalized=True, impulse=[1, 0]).plot(figsize=(13,3))\nax.set(xlabel='t', title='Responses to a shock to `dln_inv`');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Component-Component plus Residual (CCPR) Plots"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ]
        ]
    },
    "611386": {
        "jupyter_code_cell": "#set the ending date = 17/05/2015\ndata_2009.loc[lambda data_2009: data_2009.iloc[:,1] == '17/05/2015', :]\ndata_2009 = data_2009.iloc[87:,:].reset_index(drop=True)\ndata_2009.head()\n#save the data for further use\ndata_2009.to_hdf('result/n4.h5','obamaapproval',table=True,mode='a')",
        "matched_tutorial_code_inds": [
            5575,
            6754,
            2762,
            370,
            473
        ],
        "matched_tutorial_codes": [
            "# using a SciPy distribution\nres_exp = OrderedModel(data_student['apply'],\n                           data_student[['pared', 'public', 'gpa']],\n                           distr=stats.expon).fit(method='bfgs', disp=False)\nres_exp.summary()",
            "# fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Ordinal regression with a custom cumulative cLogLog distribution:"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ]
        ]
    },
    "951620": {
        "jupyter_code_cell": "# Show the equivalent dataframe (i.e., dense matrix version).\npd.DataFrame(dtm_A_train.toarray(), columns=vect_A.get_feature_names())",
        "matched_tutorial_code_inds": [
            6632,
            6942,
            6558,
            51,
            6434
        ],
        "matched_tutorial_codes": [
            "# Create an instance of our TVPVAR class with our observed dataset y\nmod = TVPVAR(y)",
            "# Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "# Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "# Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)",
            "# In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->TVP-VAR model in Statsmodels"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example",
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->3. Save model A"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ]
        ]
    },
    "117203": {
        "jupyter_code_cell": "mr_df.sort_values().head(10)",
        "matched_tutorial_code_inds": [
            5522,
            5560,
            3734,
            5324,
            6801
        ],
        "matched_tutorial_codes": [
            "glm_mod.model.data.orig_endog.sum(1)",
            "data_student.head(5)",
            "delays.nsmallest(5).sort_values()",
            "res.nobs[10:15]",
            "res.forecast_components(12)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->Expanding Sample"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data"
            ]
        ]
    },
    "86231": {
        "jupyter_code_cell": "payoff = s[:,-1]-k",
        "matched_tutorial_code_inds": [
            5266,
            5631,
            3610,
            6119,
            707
        ],
        "matched_tutorial_codes": [
            "rho = resid_fit.params[1]",
            "kde.evaluate(-1)",
            "count_vect.vocabulary_.get(u'algorithm')\n4690",
            "dta_c.T[0]",
            "get_perf(, \"without vmap\",  , \"vmap\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Generalized Least Squares"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables"
            ],
            [
                "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->Computing the Jacobian"
            ]
        ]
    },
    "1077041": {
        "jupyter_code_cell": "weather.corr() # pairwise correlation",
        "matched_tutorial_code_inds": [
            3683,
            3709,
            4093,
            3689,
            4178
        ],
        "matched_tutorial_codes": [
            "weather.loc['DSM'].head()",
            "weather.head()",
            "sns.pairplot(penguins)\n",
            "temp.head().to_frame()",
            "sns.set_theme()\nsinplot()\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics",
                "seaborn->Figure aesthetics->Controlling figure aesthetics"
            ]
        ]
    },
    "148908": {
        "jupyter_code_cell": "lab = df_dbscan['clusterLabel']\nlab.SVD = df_dbscan['clusterLabel.SVD']\nlab.raw = df_dbscan['clusterLabel.raw']",
        "matched_tutorial_code_inds": [
            4167,
            4166,
            5352,
            5438,
            3979
        ],
        "matched_tutorial_codes": [
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "pred_ols = res_ols.get_prediction()\niv_l_ols = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u_ols = pred_ols.summary_frame()[\"obs_ci_upper\"]",
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ]
        ]
    },
    "1095879": {
        "jupyter_code_cell": "# File location\nurl = 'https://www.dropbox.com/s/shg31hm4voydqnl/Thinkful%20Workshops%20-%20Predicting%20the%20Oscars.zip?dl=1'\n\nr = requests.get(url)",
        "matched_tutorial_code_inds": [
            3812,
            142,
            4697,
            5480,
            3758
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "# git clone https://github.com/brendangregg/FlameGraph\n# cd FlameGraph\n# ./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt &gt; perf_viz.svg",
            "img = np.asarray(Image.open('../../doc/_static/stinkbug.png'))\nprint(repr(img))",
            "mfx = affair_mod.get_margeff()\nprint(mfx.summary())",
            "%%time\nr = gcd_vec(pairs['LATITUDE_1'], pairs['LONGITUDE_1'],\n            pairs['LATITUDE_2'], pairs['LONGITUDE_2'])"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Profiler->Steps->7. Visualizing data as a flamegraph"
            ],
            [
                "matplotlib->Tutorials->Introductory->Image tutorial->Importing image data into Numpy arrays"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ]
        ]
    },
    "362761": {
        "jupyter_code_cell": "df.loc[1:5, 'Country':'HasTraffic']",
        "matched_tutorial_code_inds": [
            3633,
            3891,
            6006,
            3647,
            3645
        ],
        "matched_tutorial_codes": [
            "df.ix[10:15, ['fl_date', 'tail_num']]",
            "df.visualize(rankdir='LR')",
            "df_infl[:5]",
            "hdf.loc[pd.IndexSlice[:, ['ORD', 'DSM']], ['dep_time', 'dep_delay']]",
            "hdf.loc[['AA', 'DL', 'US'], ['dep_time', 'dep_delay']]"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ]
        ]
    },
    "838692": {
        "jupyter_code_cell": "max_polynomial = 10\nmse = np.zeros(max_polynomial)\nr2 = np.zeros(max_polynomial)\nmse_train = np.zeros(max_polynomial)\nr2_train = np.zeros(max_polynomial)\n\nfor i in range(max_polynomial):\n    mse[i], r2[i],mse_train[i], r2_train[i] = compute_polynomial_regression_holdout(df,'LSTAT','MEDV',i+1)",
        "matched_tutorial_code_inds": [
            1486,
            2054,
            5214,
            3281,
            2140
        ],
        "matched_tutorial_codes": [
            "totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "n_features = 100\n# simulation covariance matrix (AR(1) process)\nr = 0.1\nreal_cov = (r ** (n_features))\ncoloring_matrix = (real_cov)\n\nn_samples_range = (6, 31, 1)\nrepeat = 100\nlw_mse = ((n_samples_range.size, repeat))\noa_mse = ((n_samples_range.size, repeat))\nlw_shrinkage = ((n_samples_range.size, repeat))\noa_shrinkage = ((n_samples_range.size, repeat))\nfor i, n_samples in enumerate(n_samples_range):\n    for j in range(repeat):\n        X = ((size=(n_samples, n_features)), coloring_matrix.T)\n\n        lw = (store_precision=False, assume_centered=True)\n        lw.fit(X)\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\n        lw_shrinkage[i, j] = lw.shrinkage_\n\n        oa = (store_precision=False, assume_centered=True)\n        oa.fit(X)\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\n        oa_shrinkage[i, j] = oa.shrinkage_\n\n# plot MSE\n(2, 1, 1)\n(\n    n_samples_range,\n    lw_mse.mean(1),\n    yerr=lw_mse.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_mse.mean(1),\n    yerr=oa_mse.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"Squared error\")\n(loc=\"upper right\")\n(\"Comparison of covariance estimators\")\n(5, 31)\n\n# plot shrinkage coefficient\n(2, 1, 2)\n(\n    n_samples_range,\n    lw_shrinkage.mean(1),\n    yerr=lw_shrinkage.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_shrinkage.mean(1),\n    yerr=oa_shrinkage.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"n_samples\")\n(\"Shrinkage\")\n(loc=\"lower right\")\n(()[0], 1.0 + (()[1] - ()[0]) / 10.0)\n(5, 31)\n\n()\n\n\n<img alt=\"Comparison of covariance estimators\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\" srcset=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\"/>",
            "nsample = 50\ngroups = np.zeros(nsample, int)\ngroups[20:40] = 1\ngroups[40:] = 2\n# dummy = (groups[:,None] == np.unique(groups)).astype(float)\n\ndummy = pd.get_dummies(groups).values\nx = np.linspace(0, 20, nsample)\n# drop reference category\nX = np.column_stack((x, dummy[:, 1:]))\nX = sm.add_constant(X, prepend=False)\n\nbeta = [1.0, 3, -3, 10]\ny_true = np.dot(X, beta)\ne = np.random.normal(size=nsample)\ny = y_true + e",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ]
        ]
    },
    "394300": {
        "jupyter_code_cell": "plt.plot(mcp325405['On Time'],mcp325405['350Signal'],c='blue',linestyle='-',label='mcp 325')\nplt.plot(mcp325405['On Time'],mcp325405['405Signal'],c='red',linestyle='-',label='mcp 405')\nplt.xlabel('Time (Hours)')\nplt.ylabel('Signal')\nplt.legend()\nplt.show()",
        "matched_tutorial_code_inds": [
            5418,
            4392,
            4404,
            4390,
            1514
        ],
        "matched_tutorial_codes": [
            "plt.rcParams[\"figure.subplot.bottom\"] = 0.23  # keep labels visible\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # make plot larger in notebook\nage = [data.exog[\"age\"][data.endog == id] for id in party_ID]\nfig = plt.figure()\nax = fig.add_subplot(111)\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\n# plt.show()",
            "plt.title('Digital filter frequency response')\n plt.plot(w, 20*np.log10(np.abs(h)))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response [dB]')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with amplitude response on the Y axis vs Frequency on the X axis. A single trace shows a smooth low-pass filter with the left third passband near 0 dB. The right two-thirds are about 60 dB down with two sharp narrow valleys dipping down to -100 dB.\"' class=\"plot-directive\" src=\"../_images/signal-6.png\"/>\n</figure>",
            "plt.plot(t, y, '-rx')\n plt.plot(t, yconst, '-bo')\n plt.plot(t, ylin, '-k+')\n plt.grid()\n plt.legend(['signal', 'const. detrend', 'linear detrend'])\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with no units. A red trace corresponding to the original signal curves from the bottom left to the top right. A blue trace has the constant detrend applied and is below the red trace with zero Y offset. The last black trace has the linear detrend applied and is almost flat from left to right highlighting the curve of the original signal. This last trace has an average slope of zero and looks very different.\"' class=\"plot-directive\" src=\"../_images/signal-10.png\"/>\n</figure>",
            "plt.title('Digital filter frequency response')\n plt.plot(w, np.abs(h))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. A single trace forms a shape similar to a heartbeat signal.\"' class=\"plot-directive\" src=\"../_images/signal-5.png\"/>\n</figure>",
            "plt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Detrend"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Fitting Data"
            ]
        ]
    },
    "1025041": {
        "jupyter_code_cell": "#2010 records\nusers_2010 = data_features[data_features['Date'] >= '2010-01-01']\nusers_2010 = users_2010[users_2010['Date'] <= '2010-12-31']\nusers_2010\n#2011 records\nusers_2011 = data_features[data_features['Date'] >= '2011-01-01']\nusers_2011 = users_2011[users_2011['Date'] <= '2011-12-31']\nusers_2011\n#2012 records\nusers_2012 = data_features[data_features['Date'] >= '2012-01-01']\nusers_2012 = users_2012[users_2012['Date'] <= '2012-12-31']\nusers_2012\n#2013 records\nusers_2013 = data_features[data_features['Date'] >= '2013-01-01']\nusers_2013 = users_2013[users_2013['Date'] <= '2013-12-31']\nusers_2013",
        "matched_tutorial_code_inds": [
            2705,
            6540,
            370,
            5133,
            1611
        ],
        "matched_tutorial_codes": [
            "# make a copy of the previous data\nXs = X.copy()\n# make Xs sparse by replacing the values lower than 2.5 with 0s\nXs[Xs &lt; 2.5] = 0.0\n# create a copy of Xs in sparse format\nXs_sp = (Xs)\nXs_sp = Xs_sp.tocsc()\n\n# compute the proportion of non-zero coefficient in the data matrix\nprint(f\"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%\")\n\nalpha = 0.1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = ()\nsparse_lasso.fit(Xs_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(Xs, y)\nprint(f\"Dense Lasso done in  {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")",
            "# Output\noutput_mod = sm.tsa.UnobservedComponents(dta['US GNP'], **unrestricted_model)\noutput_res = output_mod.fit(method='powell', disp=False)\n\n# Prices\nprices_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **unrestricted_model)\nprices_res = prices_mod.fit(method='powell', disp=False)\n\nprices_restricted_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **restricted_model)\nprices_restricted_res = prices_restricted_mod.fit(method='powell', disp=False)\n\n# Money\nmoney_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **unrestricted_model)\nmoney_res = money_mod.fit(method='powell', disp=False)\n\nmoney_restricted_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **restricted_model)\nmoney_restricted_res = money_restricted_mod.fit(method='powell', disp=False)",
            "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "# some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)",
            "# The training set metrics.\ny_training_error = [\n    store_training_loss[i] / float(len(training_images))\n    for i in range(len(store_training_loss))\n]\nx_training_error = range(1, len(store_training_loss) + 1)\ny_training_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_training_accuracy = range(1, len(store_training_accurate_pred) + 1)\n\n# The test set metrics.\ny_test_error = [\n    store_test_loss[i] / float(len(test_images)) for i in range(len(store_test_loss))\n]\nx_test_error = range(1, len(store_test_loss) + 1)\ny_test_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_test_accuracy = range(1, len(store_test_accurate_pred) + 1)\n\n# Display the plots.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\naxes[0].set_title(\"Training set error, accuracy\")\naxes[0].plot(x_training_accuracy, y_training_accuracy, label=\"Training set accuracy\")\naxes[0].plot(x_training_error, y_training_error, label=\"Training set error\")\naxes[0].set_xlabel(\"Epochs\")\naxes[1].set_title(\"Test set error, accuracy\")\naxes[1].plot(x_test_accuracy, y_test_accuracy, label=\"Test set accuracy\")\naxes[1].plot(x_test_error, y_test_error, label=\"Test set error\")\naxes[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\" src=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Sparse data"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Model"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard"
            ],
            [
                "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Compose the model and begin training and testing it"
            ]
        ]
    },
    "1405201": {
        "jupyter_code_cell": "from scipy.stats import mode\n\nlabels = np.zeros_like(clusters)\nfor i in range(4):\n    mask = (clusters == i)\n    labels[mask] = mode(training_target[mask])[0]\n#np.dtype(labels)\n# for x in range(0,len(training_target)):\n#      print(np.dtype(labels[x,]))\nfrom sklearn.metrics import accuracy_score\naccuracy_score(training_target.astype('int'), labels.astype('int'),normalize=False)",
        "matched_tutorial_code_inds": [
            2034,
            2361,
            2324,
            2771,
            1969
        ],
        "matched_tutorial_codes": [
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "from sklearn.ensemble import \nfrom sklearn.metrics import , \n\n\nall_models = {}\ncommon_params = dict(\n    learning_rate=0.05,\n    n_estimators=200,\n    max_depth=2,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\nfor alpha in [0.05, 0.5, 0.95]:\n    gbr = (loss=\"quantile\", alpha=alpha, **common_params)\n    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)",
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "from sklearn.linear_model import \n\nquantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_normal).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_normal\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_normal\n        )",
            "from sklearn.cluster import \n\nmbk = (\n    init=\"k-means++\",\n    n_clusters=3,\n    batch_size=batch_size,\n    n_init=10,\n    max_no_improvement=10,\n    verbose=0,\n)\nt0 = ()\nmbk.fit(X)\nt_mini_batch = () - t0"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Fitting non-linear quantile and least squares regressors"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "sklearn->Examples->Clustering->Comparison of the K-Means and MiniBatchKMeans clustering algorithms->Compute clustering with MiniBatchKMeans"
            ]
        ]
    },
    "196336": {
        "jupyter_code_cell": "#Chicago Salaries Dataset\n#Resource https://data.cityofchicago.org/Administration-Finance/Current-Employee-Names-Salaries-and-Position-Title/xzkq-xp2w\ndf_salaries = pd.read_json('https://data.cityofchicago.org/resource/tt4n-kn4t.json')\ndf_salaries.info()",
        "matched_tutorial_code_inds": [
            6688,
            6932,
            3929,
            6358,
            2813
        ],
        "matched_tutorial_codes": [
            "# Print the news, computed by the `news` method\nprint(news.news)\n\n# Manually compute the news\nprint()\nprint((y_update.iloc[0] - phi_hat * y_pre.iloc[-1]).round(6))",
            "# Step 1: append a new observation to the sample and refit the parameters\nappend_res = training_res.append(endog[training_obs:training_obs + 1], refit=True)\n\n# Print the re-estimated parameters\nprint(append_res.params)",
            "# Apply the default theme\nsns.set_theme()\n",
            "Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\"/>",
            "Mean AvgClaim Amount per policy:              71.78\nMean AvgClaim Amount | NbClaim  0:           1951.21\nPredicted Mean AvgClaim Amount | NbClaim  0: 1940.95\nPredicted Mean AvgClaim Amount (dummy) | NbClaim  0: 1978.59"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Addendum: manually computing the news, weights, and impacts"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ]
        ]
    },
    "601298": {
        "jupyter_code_cell": "inputs = AND[['x1','x2']]\ntarget = AND['y']\n\nw = train(inputs, target, w, 0.25, 10)",
        "matched_tutorial_code_inds": [
            156,
            993,
            1664,
            653,
            4440
        ],
        "matched_tutorial_codes": [
            "m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "layer = LinearSymmetric(3)\nout = layer((8, 3))",
            "z = [4, 1-0.2j, 1.6]\nf(z)",
            "traced_model = (model)\nprint(traced_model.graph)",
            "i1 = word_list.searchsorted('ape')\n i2 = word_list.searchsorted('man')\n word_list[i1]\n'ape'\n word_list[i2]\n'man'"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange"
            ],
            [
                "torch->Model Optimization->Parametrizations Tutorial->Implementing parametrizations by hand"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Warmup"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm"
            ],
            [
                "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders"
            ]
        ]
    },
    "1047227": {
        "jupyter_code_cell": "smuts = [i['selected'] for i in gametes[0]]",
        "matched_tutorial_code_inds": [
            3967,
            4187,
            4093,
            6228,
            3734
        ],
        "matched_tutorial_codes": [
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "sns.axes_style()\n",
            "sns.pairplot(penguins)\n",
            "dta.plot(figsize=(12, 8))",
            "delays.nsmallest(5).sort_values()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ]
        ]
    },
    "197762": {
        "jupyter_code_cell": "df3= df[(df['type_1']=='Water') | (df['type_1']=='Normal') \n                                | (df['type_1']=='Grass')]\ndf3= df3[['type_1','attack','defense','hp','speed']]",
        "matched_tutorial_code_inds": [
            2595,
            3784,
            5590,
            3801,
            6674
        ],
        "matched_tutorial_codes": [
            "co2_data = co2_data.resample(\"M\").mean().dropna(axis=\"index\", how=\"any\")\nco2_data.plot()\n(\"Monthly average of CO$_2$ concentration (ppm)\")\n_ = (\n    \"Monthly average of air samples measurements\\nfrom the Mauna Loa Observatory\"\n)\n\n\n<img alt=\"Monthly average of air samples measurements from the Mauna Loa Observatory\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_co2_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_co2_002.png\"/>",
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "modfd_logit = OrderedModel.from_formula(\"apply ~ 1 + pared + public + gpa + C(dummy)\", data_student,\n                                      distr='logit')\nresfd_logit = modfd_logit.fit(method='bfgs')\nprint(resfd_logit.summary())",
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset"
            ]
        ]
    },
    "267469": {
        "jupyter_code_cell": "fiftiers = dh_data[dh_data.PctDeadhead > .5]\nfiftiers['Run'] = fiftiers['Run'].astype('str')",
        "matched_tutorial_code_inds": [
            6765,
            3857,
            3644,
            3731,
            3855
        ],
        "matched_tutorial_codes": [
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ]
        ]
    },
    "509262": {
        "jupyter_code_cell": "train_df = pd.read_json(\"./input/train.json\")\nprint(train_df.shape)\ntrain_df.info()",
        "matched_tutorial_code_inds": [
            4166,
            3731,
            4697,
            5504,
            3889
        ],
        "matched_tutorial_codes": [
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "img = np.asarray(Image.open('../../doc/_static/stinkbug.png'))\nprint(repr(img))",
            "dta = sm.datasets.star98.load_pandas().data\nprint(dta.columns)",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "matplotlib->Tutorials->Introductory->Image tutorial->Importing image data into Numpy arrays"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ]
        ]
    },
    "1164439": {
        "jupyter_code_cell": "print(\"Train accuracy: {:.1f}%\".format(model_mlp.score(X_train, y_train)[\"acc\"] * 100))\nprint(\"Valid accuracy: {:.1f}%\".format(model_mlp.score(X_valid, y_valid)[\"acc\"] * 100))",
        "matched_tutorial_code_inds": [
            1576,
            1580,
            1569,
            1572,
            2950
        ],
        "matched_tutorial_codes": [
            "print(\"The data type of training labels: {}\".format(y_train.dtype))\nprint(\"The data type of test labels: {}\".format(y_test.dtype))",
            "print(\"The data type of training labels: {}\".format(training_labels.dtype))\nprint(\"The data type of test labels: {}\".format(test_labels.dtype))",
            "print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))",
            "print(\"The data type of training images: {}\".format(training_images.dtype))\nprint(\"The data type of test images: {}\".format(test_images.dtype))",
            "print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Accuracy of the Model",
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "833395": {
        "jupyter_code_cell": "logreg_hitters_params = optimal_parameters.lr_hitters_params(to_predict_hitters,x_hitters, hitter_predictions)",
        "matched_tutorial_code_inds": [
            3388,
            57,
            6900,
            362,
            4299
        ],
        "matched_tutorial_codes": [
            "search_cv.fit(X_train, y_train)\n\nprint(\"Best params:\")\nprint(search_cv.best_params_)",
            "net = Net()\ncriterion = ()\noptimizer = (net.parameters(), lr=0.001, momentum=0.9)",
            "ypred = olsres.predict(X)\nprint(ypred)",
            "criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
            "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess='2-point')"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->4. Define a Loss function and optimizer"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->In-sample prediction"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard"
            ],
            [
                "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Nonlinear Constraints:"
            ]
        ]
    },
    "1189520": {
        "jupyter_code_cell": "english = stopwords.words('english')\nfrench = stopwords.words('french')\nspanish = stopwords.words('spanish')\nportuguese = stopwords.words('portuguese')\nmine = ['yeah', 'get', 'got', 'would', 'nan', 'ca']\nstop = english + french + spanish + portuguese + mine\n#stop",
        "matched_tutorial_code_inds": [
            1778,
            2773,
            4865,
            82,
            6103
        ],
        "matched_tutorial_codes": [
            "hidden_dim = 64\ninput_dim = emb_matrix['memory'].shape[0]\nlearning_rate = 0.001\nepochs = 10\nparameters = initialise_params(hidden_dim,\n                               input_dim)\nv, s = initialise_mav(hidden_dim,\n                      input_dim,\n                      parameters)",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "inner = [['innerA'],\n         ['innerB']]\nouter = [['upper left',  inner],\n          ['lower left', 'lower right']]\n\nfig, axd = plt.subplot_mosaic(outer, layout=\"constrained\")\nfor k in axd:\n    annotate_axes(axd[k], f'axd[\"{k}\"]')\n\n\n<img alt=\"arranging axes\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_008.png\" srcset=\"../../_images/sphx_glr_arranging_axes_008.png, ../../_images/sphx_glr_arranging_axes_008_2_0x.png 2.0x\"/>",
            "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\nscaler = (enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Training the Network"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->High-level methods for making grids->Nested Axes layouts"
            ],
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->All together: \u201cAutomatic Mixed Precision\u201d"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ]
        ]
    },
    "1213431": {
        "jupyter_code_cell": "tl = TomekLinks(n_jobs=4, ratio='majority')\nX_res, y_res = tl.fit_sample(X, y)",
        "matched_tutorial_code_inds": [
            6734,
            6724,
            6166,
            5240,
            6898
        ],
        "matched_tutorial_codes": [
            "mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "mod = TVRegressionExtended(y_t, x_t, w_t)\nres = mod.fit(maxiter=2000)  # it doesn't converge with 50 iters\nprint(res.summary())",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 2: time-varying parameters with non identity transition matrix->1) Change the starting parameters function"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ]
        ]
    },
    "334256": {
        "jupyter_code_cell": "def daterange(start_date, end_date):\n    for n in range(int ((end_date - start_date).days)):\n        yield start_date + timedelta(n)\n\n# Initialize date range start at 6/30 to compensate for UTC\nstart_date = date(2017, 6, 30)\nend_date = date(2017, 11, 1)\n\ndates = []\n\nfor single_date in daterange(start_date, end_date):\n        dates.append(single_date.strftime(\"%Y-%m-%d\"))",
        "matched_tutorial_code_inds": [
            1705,
            6145,
            1078,
            4987,
            6146
        ],
        "matched_tutorial_codes": [
            "def moving_mean(a, n):\n    ret = np.cumsum(a, dtype=float, axis=0)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\npollutants_A_24hr_avg = moving_mean(pollutants_A, 24)\npollutants_B_8hr_avg = moving_mean(pollutants_B, 8)[-(pollutants_A_24hr_avg.shape[0]):]",
            "def gen_outcome(otype, mtime0):\n    if otype == \"full\":\n        lp = 0.5 * mtime0\n    elif otype == \"no\":\n        lp = exp\n    else:\n        lp = exp + mtime0\n    mn = np.exp(-lp)\n    ytime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    ystatus = (ctime = ytime0).astype(int)\n    ytime = np.where(ytime0 &lt;= ctime, ytime0, ctime)\n    return ytime, ystatus",
            "def time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n    eval_duration_time = eval_end_time - eval_start_time\n    print(result)\n    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\n# Evaluate the original FP32 BERT model\ntime_model_evaluation(model, configs, tokenizer)\n\n# Evaluate the INT8 BERT model after the dynamic quantization\ntime_model_evaluation(quantized_model, configs, tokenizer)",
            "def formatoddticks(x, pos):\n    \"\"\"Format odd tick positions.\"\"\"\n    if x % 2:\n        return f'{x:1.2f}'\n    else:\n        return ''\n\n\nfig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\nax.plot(x1, y1)\nlocator = matplotlib.ticker.MaxNLocator(nbins=6)\nax.xaxis.set_major_formatter(formatoddticks)\nax.xaxis.set_major_locator(locator)\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_015.png\" srcset=\"../../_images/sphx_glr_text_intro_015.png, ../../_images/sphx_glr_text_intro_015_2_0x.png 2.0x\"/>",
            "def build_df(ytime, ystatus, mtime0, mtime, mstatus):\n    df = pd.DataFrame(\n        {\n            \"ytime\": ytime,\n            \"ystatus\": ystatus,\n            \"mtime\": mtime,\n            \"mstatus\": mstatus,\n            \"exp\": exp,\n        }\n    )\n    return df"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Moving averages"
            ],
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on BERT->3. Apply the dynamic quantization->3.2 Evaluate the inference accuracy and time"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Ticks and ticklabels->Tick Locators and Formatters"
            ],
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ]
        ]
    },
    "913352": {
        "jupyter_code_cell": "# aggregated stats of numerica variables\ntemp_agg = agg_numeric(bureau_balance, parent_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n\n# value count categorical variables\ntemp_count = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n\n# merge two together\ntemp_merged = temp_agg.merge(temp_count, on = 'SK_ID_BUREAU', how ='left')",
        "matched_tutorial_code_inds": [
            3104,
            3250,
            5133,
            364,
            6754
        ],
        "matched_tutorial_codes": [
            "# range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "# create df of model scores ordered by performance\nmodel_scores = results_df.filter(regex=r\"split\\d*_test_score\")\n\n# plot 30 examples of dependency between cv fold and AUC scores\nfig, ax = ()\n(\n    data=model_scores.transpose().iloc[:30],\n    dashes=False,\n    palette=\"Set1\",\n    marker=\"o\",\n    alpha=0.5,\n    ax=ax,\n)\nax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\nax.set_ylabel(\"Model AUC\", size=12)\nax.tick_params(bottom=True, labelbottom=False)\n()\n\n# print correlation of AUC scores across folds\nprint(f\"Correlation of models:\\n {model_scores.transpose().corr()}\")\n\n\n<img alt=\"plot grid search stats\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\"/>",
            "# some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)",
            "# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# create grid of images\nimg_grid = torchvision.utils.make_grid(images)\n\n# show images\nmatplotlib_imshow(img_grid, one_channel=True)\n\n# write to tensorboard\nwriter.add_image('four_fashion_mnist_images', img_grid)",
            "# fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search"
            ],
            [
                "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->2. Writing to TensorBoard"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method"
            ]
        ]
    },
    "627501": {
        "jupyter_code_cell": "#Use seaborn to create graphs that use logistic regression to predict the survival % at different ages\n#seperated by class\n               \ng = sns.lmplot(x='age', y='survived', hue='pclass', data=titanic_df, y_jitter=.02, logistic=True, size=6, aspect=4)\n\ng.set(xlim=(0,80),title='Survival Rate of All Passengers\\nSeperated by Class\\nUsing Logistic Regression')",
        "matched_tutorial_code_inds": [
            383,
            1565,
            6635,
            5118,
            406
        ],
        "matched_tutorial_codes": [
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "# Display 5 random images from the training set.\nnum_examples = 5\nseed = 147197952744\nrng = np.random.default_rng(seed)\n\nfig, axes = plt.subplots(1, num_examples)\nfor sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")\n\n\n\n\n<img alt=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\" src=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\"/>",
            "# Here, for illustration purposes only, we plot the time-varying\n# coefficients conditional on an ad-hoc parameterization\n\n# Recall that `initial_res` contains the Kalman filtering and smoothing,\n# and the `states.smoothed` attribute contains the smoothed states\nplot_coefficients_by_equation(initial_res.states.smoothed);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\"/>",
            "# Load the data from Spector and Mazzeo (1980)\nIn [1]: import statsmodels.api as sm\n\nIn [2]: spector_data = sm.datasets.spector.load_pandas()\n\nIn [3]: spector_data.exog = sm.add_constant(spector_data.exog)\n\n# Logit Model\nIn [4]: logit_mod = sm.Logit(spector_data.endog, spector_data.exog)\n\nIn [5]: logit_res = logit_mod.fit()\nOptimization terminated successfully.\n         Current function value: 0.402801\n         Iterations 7\n\nIn [6]: print(logit_res.summary())\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  GRADE   No. Observations:                   32\nModel:                          Logit   Df Residuals:                       28\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.3740\nTime:                        17:12:32   Log-Likelihood:                -12.890\nconverged:                       True   LL-Null:                       -20.592\nCovariance Type:            nonrobust   LLR p-value:                  0.001502\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -13.0213      4.931     -2.641      0.008     -22.687      -3.356\nGPA            2.8261      1.263      2.238      0.025       0.351       5.301\nTUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\nPSI            2.3787      1.065      2.234      0.025       0.292       4.465\n==============================================================================",
            "# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q"
            ],
            [
                "statsmodels->User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Examples"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples"
            ]
        ]
    },
    "597055": {
        "jupyter_code_cell": "embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ndf_data[\"Embarked\"] = df_data[\"Embarked\"].map(embarked_mapping)\n# split Embanked into df_train and df_test:\ndf_train[\"Embarked\"] = df_data[\"Embarked\"][:891]\ndf_test[\"Embarked\"] = df_data[\"Embarked\"][891:]",
        "matched_tutorial_code_inds": [
            1634,
            5319,
            2397,
            2459,
            3266
        ],
        "matched_tutorial_codes": [
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "exog_vars = [\"Mkt-RF\", \"SMB\", \"HML\"]\nexog = sm.add_constant(factors[exog_vars])\nrols = RollingOLS(endog, exog, window=60)\nrres = rols.fit()\nfig = rres.plot_recursive_coefficient(variables=exog_vars, figsize=(14, 18))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\"/>",
            "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\n()\n\n\n<img alt=\"eigenface 0, eigenface 1, eigenface 2, eigenface 3, eigenface 4, eigenface 5, eigenface 6, eigenface 7, eigenface 8, eigenface 9, eigenface 10, eigenface 11\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_003.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_003.png\"/>",
            "cyclic_cossin_transformer = (\n    transformers=[\n        (\"categorical\", one_hot_encoder, categorical_columns),\n        (\"month_sin\", sin_transformer(12), [\"month\"]),\n        (\"month_cos\", cos_transformer(12), [\"month\"]),\n        (\"weekday_sin\", sin_transformer(7), [\"weekday\"]),\n        (\"weekday_cos\", cos_transformer(7), [\"weekday\"]),\n        (\"hour_sin\", sin_transformer(24), [\"hour\"]),\n        (\"hour_cos\", cos_transformer(24), [\"hour\"]),\n    ],\n    remainder=(),\n)\ncyclic_cossin_linear_pipeline = (\n    cyclic_cossin_transformer,\n    (alphas=alphas),\n)\nevaluate(cyclic_cossin_linear_pipeline, X, y, cv=ts_cv)",
            "pairwise_bayesian = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_post = (\n        df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n    )\n    worse_prob = t_post.cdf(rope_interval[0])\n    better_prob = 1 - t_post.cdf(rope_interval[1])\n    rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n\n    pairwise_bayesian.append([worse_prob, better_prob, rope_prob])\n\npairwise_bayesian_df = (\n    pairwise_bayesian, columns=[\"worse_prob\", \"better_prob\", \"rope_prob\"]\n).round(3)\n\npairwise_comp_df = pairwise_comp_df.join(pairwise_bayesian_df)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Trigonometric features"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: Bayesian approach"
            ]
        ]
    },
    "821955": {
        "jupyter_code_cell": "sales = [{'account': 'Jones LLC', 'Jan': 150, 'Feb': 200, 'Mar': 140},\n         {'account': 'Alpha Co',  'Jan': 200, 'Feb': 210, 'Mar': 215},\n         {'account': 'Blue Inc',  'Jan': 50,  'Feb': 90,  'Mar': 95 }]\npd.DataFrame(sales)",
        "matched_tutorial_code_inds": [
            6068,
            1701,
            6740,
            6022,
            3784
        ],
        "matched_tutorial_codes": [
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "oildata = [\n    111.0091,\n    130.8284,\n    141.2871,\n    154.2278,\n    162.7409,\n    192.1665,\n    240.7997,\n    304.2174,\n    384.0046,\n    429.6622,\n    359.3169,\n    437.2519,\n    468.4008,\n    424.4353,\n    487.9794,\n    509.8284,\n    506.3473,\n    340.1842,\n    240.2589,\n    219.0328,\n    172.0747,\n    252.5901,\n    221.0711,\n    276.5188,\n    271.1480,\n    342.6186,\n    428.3558,\n    442.3946,\n    432.7851,\n    437.2497,\n    437.2092,\n    445.3641,\n    453.1950,\n    454.4096,\n    422.3789,\n    456.0371,\n    440.3866,\n    425.1944,\n    486.2052,\n    500.4291,\n    521.2759,\n    508.9476,\n    488.8889,\n    509.8706,\n    456.7229,\n    473.8166,\n    525.9509,\n    549.8338,\n    542.3405,\n]\noil = pd.Series(oildata, index=pd.date_range(\"1965\", \"2013\", freq=\"AS\"))\noil.plot()\nplt.ylabel(\"Annual oil production in Saudi Arabia (Mt)\")",
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Simple exponential smoothing"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ]
        ]
    },
    "1242776": {
        "jupyter_code_cell": "def get_street_images_url():\n    GENDER_I = 6\n    PRODUCT_ID = 8\n    all_street_urls = []\n    # unique products \n    unique_street_urls = [] #expect: 4181\n    products_added = []\n\n    \n    for key in test_embeddings: \n        if key[GENDER_I] == \"others\": continue \n        all_street_images_urls.append(key)\n        product = key[PRODUCT_ID]\n        if product not in products_added:\n            products_added.append(product)\n            unique_street_urls.append(product)\n            \n    return all_street_urls, unique_street_urls",
        "matched_tutorial_code_inds": [
            3667,
            1707,
            6145,
            6711,
            1763
        ],
        "matched_tutorial_codes": [
            "def get_ids(network):\n    url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\n    r = requests.get(url.format(network))\n    md = pd.io.json.json_normalize(r.json()['features'])\n    md['network'] = network\n    return md",
            "def compute_indices(pol, con):\n    bp = breakpoints[pol]\n    \n    if pol == 'CO':\n        inc = 0.1\n    else:\n        inc = 1\n    \n    if bp[0] &lt;= con &lt; bp[1]:\n        Bl = bp[0]\n        Bh = bp[1] - inc\n        Ih = AQI[1] - inc\n        Il = AQI[0]\n\n    elif bp[1] &lt;= con &lt; bp[2]:\n        Bl = bp[1]\n        Bh = bp[2] - inc\n        Ih = AQI[2] - inc\n        Il = AQI[1]\n\n    elif bp[2] &lt;= con &lt; bp[3]:\n        Bl = bp[2]\n        Bh = bp[3] - inc\n        Ih = AQI[3] - inc\n        Il = AQI[2]\n\n    elif bp[3] &lt;= con &lt; bp[4]:\n        Bl = bp[3]\n        Bh = bp[4] - inc\n        Ih = AQI[4] - inc\n        Il = AQI[3]\n\n    elif bp[4] &lt;= con &lt; bp[5]:\n        Bl = bp[4]\n        Bh = bp[5] - inc\n        Ih = AQI[5] - inc\n        Il = AQI[4]\n\n    elif bp[5] &lt;= con:\n        Bl = bp[5]\n        Bh = bp[5] + bp[4] - (2 * inc)\n        Ih = AQI[6]\n        Il = AQI[5]\n\n    else:\n        print(\"Concentration out of range!\")\n        \n    return ((Ih - Il) / (Bh - Bl)) * (con - Bl) + Il",
            "def gen_outcome(otype, mtime0):\n    if otype == \"full\":\n        lp = 0.5 * mtime0\n    elif otype == \"no\":\n        lp = exp\n    else:\n        lp = exp + mtime0\n    mn = np.exp(-lp)\n    ytime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    ystatus = (ctime = ytime0).astype(int)\n    ytime = np.where(ytime0 &lt;= ctime, ytime0, ctime)\n    return ytime, ystatus",
            "def gen_data_for_model1():\n    nobs = 1000\n\n    rs = np.random.RandomState(seed=93572)\n\n    d = 5\n    var_y = 5\n    var_coeff_x = 0.01\n    var_coeff_w = 0.5\n\n    x_t = rs.uniform(size=nobs)\n    w_t = rs.uniform(size=nobs)\n    eps = rs.normal(scale=var_y ** 0.5, size=nobs)\n\n    beta_x = np.cumsum(rs.normal(size=nobs, scale=var_coeff_x ** 0.5))\n    beta_w = np.cumsum(rs.normal(size=nobs, scale=var_coeff_w ** 0.5))\n\n    y_t = d + beta_x * x_t + beta_w * w_t + eps\n    return y_t, x_t, w_t, beta_x, beta_w\n\n\ny_t, x_t, w_t, beta_x, beta_w = gen_data_for_model1()\n_ = plt.plot(y_t)",
            "def initialise_params(hidden_dim, input_dim):\n    # forget gate\n    Wf = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bf = rng.standard_normal(size=(hidden_dim, 1))\n    # input gate\n    Wi = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bi = rng.standard_normal(size=(hidden_dim, 1))\n    # candidate memory gate\n    Wcm = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bcm = rng.standard_normal(size=(hidden_dim, 1))\n    # output gate\n    Wo = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bo = rng.standard_normal(size=(hidden_dim, 1))\n\n    # fully connected layer for classification\n    W2 = rng.standard_normal(size=(1, hidden_dim))\n    b2 = np.zeros((1, 1))\n\n    parameters = {\n        \"Wf\": Wf,\n        \"bf\": bf,\n        \"Wi\": Wi,\n        \"bi\": bi,\n        \"Wcm\": Wcm,\n        \"bcm\": bcm,\n        \"Wo\": Wo,\n        \"bo\": bo,\n        \"W2\": W2,\n        \"b2\": b2\n    }\n    return parameters"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Sub-indices"
            ],
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Overview of the Model Architecture"
            ]
        ]
    },
    "1349938": {
        "jupyter_code_cell": "data = featureFormat(my_dataset, features_list, sort_keys = True)\nlabels, features = targetFeatureSplit(data)\n\nk=4\nk_best = SelectKBest(k=k)\nk_best.fit(features, labels)\nscores = k_best.scores_\nprint(scores)\n\nfeatures_list = ['poi','total_stock_value','fraction_to_poi_email','expenses','shared_receipt_with_poi']",
        "matched_tutorial_code_inds": [
            4682,
            1100,
            6068,
            6165,
            2955
        ],
        "matched_tutorial_codes": [
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "774917": {
        "jupyter_code_cell": "from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(binary=True)\nX_train_transformed = vectorizer.fit_transform(X_train)",
        "matched_tutorial_code_inds": [
            3611,
            3328,
            3609,
            6796,
            3448
        ],
        "matched_tutorial_codes": [
            "from sklearn.feature_extraction.text import TfidfTransformer\n tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n X_train_tf = tf_transformer.transform(X_train_counts)\n X_train_tf.shape\n(2257, 35788)",
            "from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "from sklearn.feature_extraction.text import CountVectorizer\n count_vect = CountVectorizer()\n X_train_counts = count_vect.fit_transform(twenty_train.data)\n X_train_counts.shape\n(2257, 35788)",
            "from statsmodels.tsa.forecasting.theta import ThetaModel\n\ntm = ThetaModel(housing)\nres = tm.fit()\nprint(res.summary())",
            "from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies"
            ],
            [
                "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ]
        ]
    },
    "1382150": {
        "jupyter_code_cell": "skill_list = ['excel','vba','python','r','matlab','c#','c++','sas','stata','sql','mysql','php','html','java','javascript',\n    'ios','perl','hadoop','nosql','hive','mapreduce','pip','mongodb','hbase','tableau','spark','scala','d3']",
        "matched_tutorial_code_inds": [
            3602,
            5359,
            2504,
            3812,
            4462
        ],
        "matched_tutorial_codes": [
            "categories = ['alt.atheism', 'soc.religion.christian',\n...               'comp.graphics', 'sci.med']",
            "array(['lme4', 'Matrix', 'tools', 'stats', 'graphics', 'grDevices',\n       'utils', 'datasets', 'methods', 'base'], dtype='&lt;U9')",
            "from sklearn.datasets import \n\ndiabetes = ()\nX, y = diabetes.data, diabetes.target\nprint(diabetes.DESCR)",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "from scipy.spatial import Voronoi\n vor = Voronoi(points)\n vor.vertices\narray([[0.5, 0.5],\n       [0.5, 1.5],\n       [1.5, 0.5],\n       [1.5, 1.5]])"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Tutorials->Working With Text Data->Loading the 20 newsgroups dataset"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Linear Mixed-Effects"
            ],
            [
                "sklearn->Examples->Feature Selection->Model-based and sequential feature selection->Loading the data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "scipy->Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams"
            ]
        ]
    },
    "1141092": {
        "jupyter_code_cell": "data_combine.columns",
        "matched_tutorial_code_inds": [
            5561,
            1453,
            1478,
            5560,
            5316
        ],
        "matched_tutorial_codes": [
            "data_student.dtypes",
            "reconstructed.shape",
            "load_xy.shape",
            "data_student.head(5)",
            "params.iloc[57:62]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Our arrays as a csv file"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ]
        ]
    },
    "1225844": {
        "jupyter_code_cell": "d_male=df1.loc[df1['gender'] == 1]\nd_female=df1.loc[df1['gender'] == 2]\ndf_pie1 = pd.DataFrame([d_male.shape[0], d_female.shape[0]], index=['MALE', 'FEMALE'], columns=['Frequency'])\ndf_pie1.plot(kind='pie', subplots=True, figsize=(10, 10))",
        "matched_tutorial_code_inds": [
            3784,
            5590,
            1634,
            1701,
            6782
        ],
        "matched_tutorial_codes": [
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "modfd_logit = OrderedModel.from_formula(\"apply ~ 1 + pared + public + gpa + C(dummy)\", data_student,\n                                      distr='logit')\nresfd_logit = modfd_logit.fit(method='bfgs')\nprint(resfd_logit.summary())",
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "cpi_apparel = DataReader('CPIAPPNS', 'fred', start='1986')\ncpi_apparel.index = pd.DatetimeIndex(cpi_apparel.index, freq='MS')\ninf_apparel = np.log(cpi_apparel).diff().iloc[1:] * 1200\ninf_apparel.plot(figsize=(15, 5));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_chandrasekhar_10_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_chandrasekhar_10_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Practical example"
            ]
        ]
    },
    "1058006": {
        "jupyter_code_cell": "def rescale_columns(data, colnames, total_samples_preview):\n    \n    scaler = pp.MinMaxScaler()\n    \n    rescaled_data = data[colnames]\n    rescaled_data = scaler.fit_transform(rescaled_data)\n    rescaled_data = pd.DataFrame(rescaled_data, columns=colnames)\n    \n    rescaled_data_sample = rescaled_data.sample(n=5)\n    \n    scatter_plot_two_features(rescaled_data, colnames, total_samples_preview)\n    \n    return rescaled_data_sample\n\n\nrescale_columns(data, cols, 500)",
        "matched_tutorial_code_inds": [
            6382,
            2417,
            3123,
            3126,
            3125
        ],
        "matched_tutorial_codes": [
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>",
            "def get_impute_zero_score(X_missing, y_missing):\n\n    imputer = (\n        missing_values=, add_indicator=True, strategy=\"constant\", fill_value=0\n    )\n    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return zero_impute_scores.mean(), zero_impute_scores.std()\n\n\nmses_california[1], stds_california[1] = get_impute_zero_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Zero imputation\")",
            "def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Replace missing values by 0"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean"
            ]
        ]
    },
    "192534": {
        "jupyter_code_cell": "mpl.rcParams['figure.figsize'] = (8, 6)\nsns.countplot(x=\"edibility\", data=md);",
        "matched_tutorial_code_inds": [
            4147,
            6563,
            4133,
            4132,
            4179
        ],
        "matched_tutorial_codes": [
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\", data=tips);\n",
            "dta.plot(figsize=(12,4));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\"/>",
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_estimator=np.mean);\n",
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_jitter=.05);\n",
            "sns.set_style(\"whitegrid\")\ndata = np.random.normal(size=(20, 6)) + np.arange(6) / 2\nsns.boxplot(data=data);\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Conditioning on other variables",
                "seaborn->Statistical operations->Estimating regression fits->Conditioning on other variables"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models",
                "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models",
                "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Seaborn figure styles",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Seaborn figure styles"
            ]
        ]
    },
    "431172": {
        "jupyter_code_cell": "response = oanda.create_order(account_id,\n                              instrument = \"AUD_USD\",\n                              units=1000,\n                              side=\"buy\",\n                              type=\"limit\",\n                              price=0.7420,\n                              expiry=trade_expire)\nprint(response)",
        "matched_tutorial_code_inds": [
            2817,
            5484,
            6737,
            6007,
            2955
        ],
        "matched_tutorial_codes": [
            "res = []\nfor subset_label, X, df in [\n    (\"train\", X_train, df_train),\n    (\"test\", X_test, df_test),\n]:\n    exposure = df[\"Exposure\"].values\n    res.append(\n        {\n            \"subset\": subset_label,\n            \"observed\": df[\"ClaimAmount\"].values.sum(),\n            \"predicted, frequency*severity model\": (\n                exposure * glm_freq.predict(X) * glm_sev.predict(X)\n            ),\n            \"predicted, tweedie, power=%.2f\"\n            % glm_pure_premium.power: (exposure * glm_pure_premium.predict(X)),\n        }\n    )\n\nprint((res).set_index(\"subset\").T)",
            "resp = dict(\n    zip(\n        range(1, 9),\n        respondent1000[\n            [\n                \"occupation\",\n                \"educ\",\n                \"occupation_husb\",\n                \"rate_marriage\",\n                \"age\",\n                \"yrs_married\",\n                \"children\",\n                \"religious\",\n            ]\n        ].tolist(),\n    )\n)\nresp.update({0: 1})\nprint(resp)",
            "results_dict = {\n    \"intercept\": res_mle.params[0],\n    \"var.e\": res_mle.params[1],\n    \"var.x.coeff\": res_mle.params[2],\n    \"var.w.coeff\": res_mle.params[3],\n}\nplt.tight_layout()\n_ = pm.plot_trace(\n    trace,\n    lines=[(k, {}, [v]) for k, v in dict(results_dict).items()],\n    combined=True,\n    figsize=(12, 12),\n)",
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Pure Premium Modeling via a Product Model vs single TweedieRegressor"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->How does the posterior distribution compare with the MLE estimation?"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "1362139": {
        "jupyter_code_cell": "from datetime import datetime\n\ndates_by_model = []\n\nfor i in range(len(dates_model)):\n    dates_points = datetime.strptime(solve(dates_model[i]), '%B %d %Y')\n    dates_by_model.append(dates_points)\n\ndates_by_points = []\n\nfor i in range(len(maturities)):\n    dates_points = datetime.strptime(solve(maturities[i]), '%B %d %Y')\n    dates_by_points.append(dates_points)\n    \ndates_by_model_graph = matplotlib.dates.date2num(dates_by_model)\ndates_by_points_graph = matplotlib.dates.date2num(dates_by_points)    ",
        "matched_tutorial_code_inds": [
            3265,
            287,
            303,
            5994,
            6380
        ],
        "matched_tutorial_codes": [
            "from itertools import \nfrom math import \n\nn_comparisons = (len(model_scores)) / (\n    (2) * (len(model_scores) - 2)\n)\npairwise_t_test = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\n    p_val *= n_comparisons  # implement Bonferroni correction\n    # Bonferroni can output p-values higher than 1\n    p_val = 1 if p_val  1 else p_val\n    pairwise_t_test.append(\n        [model_scores.index[model_i], model_scores.index[model_k], t_stat, p_val]\n    )\n\npairwise_comp_df = (\n    pairwise_t_test, columns=[\"model_1\", \"model_2\", \"t_stat\", \"p_val\"]\n).round(3)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "from pathlib import Path\nimport requests\n\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)",
            "from IPython.core.debugger import set_trace\n\nlr = 0.5  # learning rate\nepochs = 2  # how many epochs to train for\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n         = [start_i:end_i]\n         = [start_i:end_i]\n         = ()\n         = loss_func(, )\n\n        ()\n        with ():\n             -=  * lr\n             -=  * lr\n            .zero_()\n            .zero_()",
            "from urllib.request import urlopen\nimport numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nimport pandas as pd\n\npd.set_option(\"display.width\", 100)\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.api import interaction_plot, abline_plot\nfrom statsmodels.stats.anova import anova_lm\n\ntry:\n    salary_table = pd.read_csv(\"salary.table\")\nexcept:  # recent pandas can read URL without urlopen\n    url = \"http://stats191.stanford.edu/data/salary.table\"\n    fh = urlopen(url)\n    salary_table = pd.read_table(fh)\n    salary_table.to_csv(\"salary.table\")\n\nE = salary_table.E\nM = salary_table.M\nX = salary_table.X\nS = salary_table.S",
            "from statsmodels.tsa.seasonal import STL\n\nstl = STL(co2, seasonal=13)\nres = stl.fit()\nfig = res.plot()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_6_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_6_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: frequentist approach"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->MNIST data setup"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Neural net from scratch (no torch.nn)"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Atmospheric CO2"
            ]
        ]
    },
    "1006350": {
        "jupyter_code_cell": "report_model1 = classification_report(y_true_test, y_pred_test)\nprint (report_model1)",
        "matched_tutorial_code_inds": [
            2722,
            6029,
            6713,
            6033,
            6971
        ],
        "matched_tutorial_codes": [
            "reg_ols = ()\ny_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\nr2_score_ols = (y_test, y_pred_ols)\nprint(\"OLS R2 score\", r2_score_ols)",
            "min_lm = ols(\"JPERF ~ TEST\", data=jobtest_table).fit()\nprint(min_lm.summary())",
            "mod = TVRegression(y_t, x_t, w_t)\nres = mod.fit()\n\nprint(res.summary())",
            "min_lm2 = ols(\"JPERF ~ TEST + TEST:MINORITY\", data=jobtest_table).fit()\n\nprint(min_lm2.summary())",
            "sm_probit_manual = MyProbit(endog, exog).fit()\nprint(sm_probit_manual.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients->And then estimate it with our custom model class"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ]
        ]
    },
    "285581": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, kurtosis\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nfrom collections import OrderedDict",
        "matched_tutorial_code_inds": [
            4728,
            3142,
            3409,
            3128,
            3308
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom collections import \n\npopulations = (list)\ncommon_params = {\n    \"n_samples\": 10_000,\n    \"n_features\": 2,\n    \"n_informative\": 2,\n    \"n_redundant\": 0,\n    \"random_state\": 0,\n}\nweights = (0.1, 0.8, 6)\nweights = weights[::-1]\n\n# fit and evaluate base model on balanced classes\nX, y = (**common_params, weights=[0.5, 0.5])\nestimator = ().fit(X, y)\nlr_base = extract_score((estimator, X, y, scoring=scoring, cv=10))\npos_lr_base, pos_lr_base_std = lr_base[\"positive\"].values\nneg_lr_base, neg_lr_base_std = lr_base[\"negative\"].values",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import , \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\npipe = (\n    [\n        (\"scaling\", ()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", (dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [(iterated_power=7), (max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [()],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = (pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# To use this experimental feature, we need to explicitly ask for it:\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.datasets import \nfrom sklearn.impute import \nfrom sklearn.impute import \nfrom sklearn.linear_model import , \nfrom sklearn.kernel_approximation import \nfrom sklearn.ensemble import \nfrom sklearn.neighbors import \nfrom sklearn.pipeline import \nfrom sklearn.model_selection import \n\nN_SPLITS = 5\n\nrng = (0)\n\nX_full, y_full = (return_X_y=True)\n# ~2k samples is enough for the purpose of the example.\n# Remove the following two lines for a slower run with different error bars.\nX_full = X_full[::10]\ny_full = y_full[::10]\nn_samples, n_features = X_full.shape\n\n# Estimate the score on the entire dataset, with no missing values\nbr_estimator = ()\nscore_full_data = (\n    (\n        br_estimator, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    ),\n    columns=[\"Full Data\"],\n)\n\n# Add a single missing value to each row\nX_missing = X_full.copy()\ny_missing = y_full\nmissing_samples = (n_samples)\nmissing_features = rng.choice(n_features, n_samples, replace=True)\nX_missing[missing_samples, missing_features] = \n\n# Estimate the score after imputation (mean and median strategies)\nscore_simple_imputer = ()\nfor strategy in (\"mean\", \"median\"):\n    estimator = (\n        (missing_values=, strategy=strategy), br_estimator\n    )\n    score_simple_imputer[strategy] = (\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\n# Estimate the score after iterative imputation of the missing values\n# with different estimators\nestimators = [\n    (),\n    (\n        # We tuned the hyperparameters of the RandomForestRegressor to get a good\n        # enough predictive performance for a restricted execution time.\n        n_estimators=4,\n        max_depth=10,\n        bootstrap=True,\n        max_samples=0.5,\n        n_jobs=2,\n        random_state=0,\n    ),\n    (\n        (kernel=\"polynomial\", degree=2, random_state=0), (alpha=1e3)\n    ),\n    (n_neighbors=15),\n]\nscore_iterative_imputer = ()\n# iterative imputer is sensible to the tolerance and\n# dependent on the estimator used internally.\n# we tuned the tolerance to keep this example run with limited computational\n# resources while not changing the results too much compared to keeping the\n# stricter default value for the tolerance parameter.\ntolerances = (1e-3, 1e-1, 1e-1, 1e-2)\nfor impute_estimator, tol in zip(estimators, tolerances):\n    estimator = (\n        (\n            random_state=0, estimator=impute_estimator, max_iter=25, tol=tol\n        ),\n        br_estimator,\n    )\n    score_iterative_imputer[impute_estimator.__class__.__name__] = (\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\nscores = (\n    [score_full_data, score_simple_imputer, score_iterative_imputer],\n    keys=[\"Original\", \"SimpleImputer\", \"IterativeImputer\"],\n    axis=1,\n)\n\n# plot california housing results\nfig, ax = (figsize=(13, 6))\nmeans = -scores.mean()\nerrors = scores.std()\nmeans.plot.barh(xerr=errors, ax=ax)\nax.set_title(\"California Housing Regression with Different Imputation Methods\")\nax.set_xlabel(\"MSE (smaller is better)\")\nax.set_yticks((means.shape[0]))\nax.set_yticklabels([\" w/ \".join(label) for label in means.index.tolist()])\n(pad=1)\n()",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn import datasets\nfrom sklearn.neighbors import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nn_neighbors = 15\n\n# import some data to play with\niris = ()\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# Create color maps\ncmap_light = ([\"orange\", \"cyan\", \"cornflowerblue\"])\ncmap_bold = ([\"darkorange\", \"c\", \"darkblue\"])\n\nfor shrinkage in [None, 0.2]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = (shrink_threshold=shrinkage)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    print(shrinkage, (y == y_pred))\n\n    _, ax = ()\n    (\n        clf, X, cmap=cmap_light, ax=ax, response_method=\"predict\"\n    )\n\n    # Plot also the training points\n    (X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n    (\"3-Class classification (shrink_threshold=%r)\" % shrinkage)\n    (\"tight\")\n\n()"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values with variants of IterativeImputer"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Nearest Centroid Classification"
            ]
        ]
    },
    "1415485": {
        "jupyter_code_cell": "max_foreing = complete_fo_sw['foreign_unemployment_rate'].max()\nmin_foreign = complete_fo_sw['foreign_unemployment_rate'].min()\nmax_swiss = complete_fo_sw['swiss_unemployment_rate'].max()\nmin_swiss = complete_fo_sw['swiss_unemployment_rate'].min()",
        "matched_tutorial_code_inds": [
            403,
            3880,
            5457,
            6765,
            5454
        ],
        "matched_tutorial_codes": [
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ]
        ]
    },
    "775310": {
        "jupyter_code_cell": "import math\nimport numpy as np\nfrom numpy import mean\nimport pandas\nimport csv\nimport seaborn\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import classification_report\n",
        "matched_tutorial_code_inds": [
            1258,
            1341,
            1223,
            5312,
            2557
        ],
        "matched_tutorial_codes": [
            "import os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, GPT2TokenizerFast\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport functools\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n checkpoint_wrapper,\n CheckpointImpl,\n apply_activation_checkpointing_wrapper)\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    BackwardPrefetch,\n    ShardingStrategy,\n    FullStateDictConfig,\n    StateDictType,\n)\nfrom torch.distributed.fsdp.wrap import (\n    transformer_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom summarization_dataset import *\nfrom transformers.models.t5.modeling_t5 import T5Block\nfrom typing import Type\nimport time\nimport tqdm\nfrom datetime import datetime",
            "import sys\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, \n\nclass PositionalEncoding():\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = (p=dropout)\n\n        pe = (max_len, d_model)\n        position = (0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = ((0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = (position * div_term)\n        pe[:, 1::2] = (position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.pe = nn.Parameter(pe, requires_grad=False)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)",
            "import os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# On Windows platform, the torch.distributed package only\n# supports Gloo backend, FileStore and TcpStore.\n# For FileStore, set init_method parameter in init_process_group\n# to a local file. Example as follow:\n# init_method=\"file:///f:/libtmp/some_file\"\n# dist.init_process_group(\n#    \"gloo\",\n#    rank=rank,\n#    init_method=init_method,\n#    world_size=world_size)\n# For TcpStore, same way as on Linux.\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "import time\nfrom sklearn.gaussian_process.kernels import \nfrom sklearn.kernel_ridge import \n\nkernel_ridge = (kernel=())\n\nstart_time = ()\nkernel_ridge.fit(training_data, training_noisy_target)\nprint(\n    f\"Fitting KernelRidge with default kernel: {() - start_time:.3f} seconds\"\n)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5"
            ],
            [
                "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Define the model"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Basic Use Case"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Kernel ridge"
            ]
        ]
    },
    "245702": {
        "jupyter_code_cell": "sns.pairplot(features[hum_col])",
        "matched_tutorial_code_inds": [
            3966,
            3941,
            4093,
            4173,
            3963
        ],
        "matched_tutorial_codes": [
            "sns.catplot(data=flights_wide, kind=\"box\")\n",
            "sns.pairplot(data=penguins, hue=\"species\")\n",
            "sns.pairplot(penguins)\n",
            "sns.pairplot(iris, hue=\"species\", height=2.5)\n",
            "sns.relplot(data=flights_wide, kind=\"line\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Multivariate views on complex datasets",
                "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data",
                "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ]
        ]
    },
    "1308287": {
        "jupyter_code_cell": "# Function convert long id to integer\n# Read \n#    ds - a data series with long index\n#    dic - empty dictionary for store pair \n#    max_val - counter for updating integer\ndef dummy_encoding(ds, dic, max_val):\n    for i, count in zip(ds, range(len(ds))):\n        if dic.has_key(i): # if exist, use integer\n            ds.iloc[count] = dic.get(i)\n        else: # otherwise, add new integer pairs\n            max_val += 1\n            dic[i] = max_val\n            ds.iloc[count] = max_val\n    return ds, dic, max_val",
        "matched_tutorial_code_inds": [
            1776,
            383,
            1121,
            1777,
            5987
        ],
        "matched_tutorial_codes": [
            "# initialise the moving averages\ndef initialise_mav(hidden_dim, input_dim, params):\n    v = {}\n    s = {}\n    # Initialize dictionaries v, s\n    for key in params:\n        v['d' + key] = np.zeros(params[key].shape)\n        s['d' + key] = np.zeros(params[key].shape)\n    # Return initialised moving averages\n    return v, s",
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "# Utility to profile the workload\ndef profile_workload(forward_func, , iteration_count=100, label=\"\"):\n    # Perform warm-up iterations\n    for _ in range(3):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before starting the timer\n    ()\n    start = time.perf_counter()\n    for _ in range(iteration_count):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before stopping the timer\n    ()\n    stop = time.perf_counter()\n    iters_per_second = iteration_count / (stop - start)\n    if label:\n        print(label)\n    print(\"Average iterations per second: {:.2f}\".format(iters_per_second))",
            "# Update the parameters using Adam optimization\ndef update_parameters(parameters, gradients, v, s,\n                      learning_rate=0.01, beta1=0.9, beta2=0.999):\n    for key in parameters:\n        # Moving average of the gradients\n        v['d' + key] = (beta1 * v['d' + key]\n                        + (1 - beta1) * gradients['d' + key])\n\n        # Moving average of the squared gradients\n        s['d' + key] = (beta2 * s['d' + key]\n                        + (1 - beta2) * (gradients['d' + key] ** 2))\n\n        # Update parameters\n        parameters[key] = (parameters[key] - learning_rate\n                           * v['d' + key] / np.sqrt(s['d' + key] + 1e-8))\n    # Return updated parameters and moving averages\n    return parameters, v, s",
            "# hyp = 0 is the null hypothesis, hyp = 1 is the alternative hypothesis.\n# cov_struct is a statsmodels covariance structure\ndef dosim(hyp, cov_struct=None, mcrep=500):\n\n    # Storage for the simulation results\n    scales = [[], []]\n\n    # P-values from the score test\n    pv = []\n\n    # Monte Carlo loop\n    for k in range(mcrep):\n\n        # Generate random \"probability points\" u  that are uniformly\n        # distributed, and correlated within clusters\n        z = np.random.normal(size=n)\n        u = np.random.normal(size=n//m)\n        u = np.kron(u, np.ones(m))\n        z = r*z +np.sqrt(1-r**2)*u\n        u = norm.cdf(z)\n\n        # Generate the observed responses\n        y = negbinom(u, mu=mu[hyp], scale=scale)\n\n        # Fit the null model\n        m0 = sm.GEE(y, x0, groups=grp, cov_struct=cov_struct, family=sm.families.Poisson())\n        r0 = m0.fit(scale='X2')\n        scales[0].append(r0.scale)\n\n        # Fit the alternative model\n        m1 = sm.GEE(y, x, groups=grp, cov_struct=cov_struct, family=sm.families.Poisson())\n        r1 = m1.fit(scale='X2')\n        scales[1].append(r1.scale)\n\n        # Carry out the score test\n        st = m1.compare_score_test(r0)\n        pv.append(st[\"p-value\"])\n\n    pv = np.asarray(pv)\n    rslt = [np.mean(pv), np.mean(pv &lt; 0.1)]\n\n    return rslt, scales"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Updating the Parameters"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Setup and Performance Metrics"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Updating the Parameters"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ]
        ]
    },
    "233409": {
        "jupyter_code_cell": "data=customers.join(churns,customers['ID']==churns['ID']).select(customers['*'],churns['CHURN'])\ndata.show(5)",
        "matched_tutorial_code_inds": [
            6170,
            3889,
            6181,
            6174,
            5887
        ],
        "matched_tutorial_codes": [
            "sel = ar_select_order(housing, 13, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "sel = ar_select_order(yoy_housing, 13, glob=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "sel = ar_select_order(housing, 13, seasonal=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "sidak = ols_model.outlier_test(\"sidak\")\nsidak.sort_values(\"unadj_p\", inplace=True)\nprint(sidak)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dynamics"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dummies"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ]
        ]
    },
    "1487660": {
        "jupyter_code_cell": "# Rebounds over time\nrebound_time_since_post = []\nfor rebound in all_rebounds:\n    shot_id_for_rebound = int(rebound['rebound_source_url'].replace('https://api.dribbble.com/v1/shots/', ''))\n    rebound_time = pd.to_datetime(rebound['created_at'])\n    \n    for shot in all_shots: \n        if shot['id'] == shot_id_for_rebound:\n            time_diff = get_days_between(pd.to_datetime(shot['created_at']), rebound_time)\n            rebound_time_since_post.append(time_diff)\n            \n# simple stats\npd.Series(rebound_time_since_post).describe()",
        "matched_tutorial_code_inds": [
            473,
            6437,
            4775,
            406,
            370
        ],
        "matched_tutorial_codes": [
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "# Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "# plt.figure creates a matplotlib.figure.Figure instance\nfig = plt.figure()\nrect = fig.patch  # a rectangle instance\nrect.set_facecolor('lightgoldenrodyellow')\n\nax1 = fig.add_axes([0.1, 0.3, 0.4, 0.4])\nrect = ax1.patch\nrect.set_facecolor('lightslategray')\n\n\nfor label in ax1.xaxis.get_ticklabels():\n    # label is a Text instance\n    label.set_color('red')\n    label.set_rotation(45)\n    label.set_fontsize(16)\n\nfor line in ax1.yaxis.get_ticklines():\n    # line is a Line2D instance\n    line.set_color('green')\n    line.set_markersize(25)\n    line.set_markeredgewidth(3)\n\nplt.show()\n\n\n<img alt=\"artists\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_artists_004.png\" srcset=\"../../_images/sphx_glr_artists_004.png, ../../_images/sphx_glr_artists_004_2_0x.png 2.0x\"/>",
            "# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
            "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Axis containers"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard"
            ]
        ]
    },
    "1188623": {
        "jupyter_code_cell": "primes = [2, 3, 5, 7, 11, 13]     # A list of primes\nmore_primes = primes + [17, 19]   # List concatentation\nprint('First few primes are: {primes}'.format(primes=primes))\nprint('Here are the primes up to the number 20: {}'.format(more_primes))",
        "matched_tutorial_code_inds": [
            1634,
            6725,
            987,
            4599,
            2124
        ],
        "matched_tutorial_codes": [
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "true_values = {\n    \"var_e1\": 0.01,\n    \"var_e2\": 0.01,\n    \"var_w1\": 0.01,\n    \"var_w2\": 0.01,\n    \"delta1\": 0.8,\n    \"delta2\": 0.5,\n    \"delta3\": 0.7,\n}\n\n\ndef gen_data_for_model3():\n    # Starting values\n    alpha1_0 = 2.1\n    alpha2_0 = 1.1\n\n    t_max = 500\n\n    def gen_i(alpha1, s):\n        return alpha1 * s + np.sqrt(true_values[\"var_e1\"]) * np.random.randn()\n\n    def gen_m_hat(alpha2):\n        return 1 * alpha2 + np.sqrt(true_values[\"var_e2\"]) * np.random.randn()\n\n    def gen_alpha1(alpha1, alpha2):\n        w1 = np.sqrt(true_values[\"var_w1\"]) * np.random.randn()\n        return true_values[\"delta1\"] * alpha1 + true_values[\"delta2\"] * alpha2 + w1\n\n    def gen_alpha2(alpha2):\n        w2 = np.sqrt(true_values[\"var_w2\"]) * np.random.randn()\n        return true_values[\"delta3\"] * alpha2 + w2\n\n    s_t = 0.3 + np.sqrt(1.4) * np.random.randn(t_max)\n    i_hat = np.empty(t_max)\n    m_hat = np.empty(t_max)\n\n    current_alpha1 = alpha1_0\n    current_alpha2 = alpha2_0\n    for t in range(t_max):\n        # Obs eqns\n        i_hat[t] = gen_i(current_alpha1, s_t[t])\n        m_hat[t] = gen_m_hat(current_alpha2)\n\n        # state eqns\n        new_alpha1 = gen_alpha1(current_alpha1, current_alpha2)\n        new_alpha2 = gen_alpha2(current_alpha2)\n\n        # Update states for next period\n        current_alpha1 = new_alpha1\n        current_alpha2 = new_alpha2\n\n    return i_hat, m_hat, s_t\n\n\ni_hat, m_hat, s_t = gen_data_for_model3()",
            "gpus_per_trial = 2\n# ...\nresult = tune.run(\n    partial(train_cifar, data_dir=data_dir),\n    resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n    config=config,\n    num_samples=num_samples,\n    scheduler=scheduler,\n    progress_reporter=reporter,\n    checkpoint_at_end=True)",
            "a = np.arange(12).reshape(3,4)\n\n class fnc1d_class:\n...     def __init__(self, shape, axis = -1):\n...         # store the filter axis:\n...         self.axis = axis\n...         # store the shape:\n...         self.shape = shape\n...         # initialize the coordinates:\n...         self.coordinates = [0] * len(shape)\n...\n...     def filter(self, iline, oline):\n...         oline[...] = iline[:-2] + 2 * iline[1:-1] + 3 * iline[2:]\n...         print(self.coordinates)\n...         # calculate the next coordinates:\n...         axes = list(range(len(self.shape)))\n...         # skip the filter axis:\n...         del axes[self.axis]\n...         axes.reverse()\n...         for jj in axes:\n...             if self.coordinates[jj] &lt; self.shape[jj] - 1:\n...                 self.coordinates[jj] += 1\n...                 break\n...             else:\n...                 self.coordinates[jj] = 0\n...\n fnc = fnc1d_class(shape = (3,4))\n generic_filter1d(a, fnc.filter, 3)\n[0, 0]\n[1, 0]\n[2, 0]\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "n_row, n_col = 2, 3\nn_components = n_row * n_col\nimage_shape = (64, 64)\n\n\ndef plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\n    fig, axs = (\n        nrows=n_row,\n        ncols=n_col,\n        figsize=(2.0 * n_col, 2.3 * n_row),\n        facecolor=\"white\",\n        constrained_layout=True,\n    )\n    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\n    fig.set_edgecolor(\"black\")\n    fig.suptitle(title, size=16)\n    for ax, vec in zip(axs.flat, images):\n        vmax = max(vec.max(), -vec.min())\n        im = ax.imshow(\n            vec.reshape(image_shape),\n            cmap=cmap,\n            interpolation=\"nearest\",\n            vmin=-vmax,\n            vmax=vmax,\n        )\n        ax.axis(\"off\")\n\n    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\n    ()"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->Matrix notation for the state space model"
            ],
            [
                "torch->Model Optimization->Hyperparameter tuning with Ray Tune->Configuring the search space"
            ],
            [
                "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Generic filter functions"
            ],
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Dataset preparation"
            ]
        ]
    },
    "1068412": {
        "jupyter_code_cell": "from accessory_functions import preprocess_series_text, nltk_path\n\ndata = pd.read_csv('../data/spam.csv', sep='\\t')\ndata['text'] = preprocess_series_text(data.text, nltk_path=nltk_path)\n\ndata.head()",
        "matched_tutorial_code_inds": [
            1154,
            2977,
            5063,
            5238,
            2590
        ],
        "matched_tutorial_codes": [
            "from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
            "from sklearn.datasets import \n\nX, y = (return_X_y=True)\n\ny[y != 2] = 0\ny[y == 2] = 1  # We will try to separate class 2 from the other 6 classes.",
            "import mpl_toolkits.axisartist as AA\nfrom mpl_toolkits.axes_grid1 import host_subplot\n\nhost = host_subplot(111, axes_class=AA.Axes)",
            "from statsmodels.datasets.longley import load_pandas\n\ny = load_pandas().endog\nX = load_pandas().exog\nX = sm.add_constant(X)",
            "from sklearn.datasets import \n\nco2 = (data_id=41187, as_frame=True, parser=\"pandas\")\nco2.frame.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results"
            ],
            [
                "sklearn->Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Preparing the data"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist->axisartist with ParasiteAxes"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ]
        ]
    },
    "915120": {
        "jupyter_code_cell": "household_population = df2.groupby(['County_name'])[['Households_estimate_total', 'Families_estimate_total', \n                                                     'Married_couple_families_estimate_total', \n                                                     'Nonfamily_households_estimate_total']].sum()\nhousehold_population",
        "matched_tutorial_code_inds": [
            1701,
            5706,
            5702,
            2974,
            6805
        ],
        "matched_tutorial_codes": [
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + age + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e = glm.fit()\nprint(res_e.summary())",
            "glm = smf.glm(\n    \"affairs ~ rate_marriage + age + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    var_weights=np.asarray(dc[\"freq\"]),\n)\nres_fv = glm.fit()\nprint(res_fv.summary())",
            "cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "forecasts = pd.DataFrame(\n    {\n        \"ln PCE\": np.log(pce.PCE),\n        \"theta=1.2\": res.forecast(12, theta=1.2),\n        \"theta=2\": res.forecast(12),\n        \"theta=3\": res.forecast(12, theta=3),\n        \"No damping\": res.forecast(12, theta=np.inf),\n    }\n)\n_ = forecasts.tail(36).plot()\nplt.title(\"Forecasts of ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->aggregated or averaged data (unique values of explanatory variables)->using exposure"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->condensed using var_weights instead of freq_weights"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ]
        ]
    },
    "1449845": {
        "jupyter_code_cell": "plt.scatter(y, x_RM, s=5, label = 'RM')\nplt.scatter(y, x_LSTAT, s=5, label = 'LSTAT')\nplt.legend(fontsize=15)\nplt.xlabel('Average number of rooms & Low status population', fontsize=15)\nplt.ylabel('Price', fontsize=15)\nplt.legend()\nplt.show()",
        "matched_tutorial_code_inds": [
            4404,
            4392,
            4397,
            1514,
            4680
        ],
        "matched_tutorial_codes": [
            "plt.plot(t, y, '-rx')\n plt.plot(t, yconst, '-bo')\n plt.plot(t, ylin, '-k+')\n plt.grid()\n plt.legend(['signal', 'const. detrend', 'linear detrend'])\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with no units. A red trace corresponding to the original signal curves from the bottom left to the top right. A blue trace has the constant detrend applied and is below the red trace with zero Y offset. The last black trace has the linear detrend applied and is almost flat from left to right highlighting the curve of the original signal. This last trace has an average slope of zero and looks very different.\"' class=\"plot-directive\" src=\"../_images/signal-10.png\"/>\n</figure>",
            "plt.title('Digital filter frequency response')\n plt.plot(w, 20*np.log10(np.abs(h)))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response [dB]')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with amplitude response on the Y axis vs Frequency on the X axis. A single trace shows a smooth low-pass filter with the left third passband near 0 dB. The right two-thirds are about 60 dB down with two sharp narrow valleys dipping down to -100 dB.\"' class=\"plot-directive\" src=\"../_images/signal-6.png\"/>\n</figure>",
            "plt.title('Pole / Zero Plot')\n plt.xlabel('Real')\n plt.ylabel('Imaginary')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is an IIR filter response as an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. The low-pass filter shown has a passband from 0 to 100 Hz with 0 dB response and a stop-band from about 175 Hz to 1 KHz about 40 dB down. There are two sharp discontinuities in the filter near 175 Hz and 300 Hz. The second plot is an X-Y showing the transfer function in the complex plane. The Y axis is real-valued an the X axis is complex-valued. The filter has four zeros near [300+0j, 175+0j, -175+0j, -300+0j] shown as blue X markers. The filter also has four poles near [50-30j, -50-30j, 100-8j, -100-8j] shown as red dots.\"' class=\"plot-directive\" src=\"../_images/signal-7_01_00.png\"/>\n</figure>",
            "plt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)",
            "plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')\nplt.axis([0, 6, 0, 20])\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_003.png\" srcset=\"../../_images/sphx_glr_pyplot_003.png, ../../_images/sphx_glr_pyplot_003_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Signal Processing (scipy.signal)->Detrend"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Analog Filter Design"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Fitting Data"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot->Formatting the style of your plot"
            ]
        ]
    },
    "396334": {
        "jupyter_code_cell": "Total_mass = m",
        "matched_tutorial_code_inds": [
            1335,
            5970,
            6119,
            1453,
            2804
        ],
        "matched_tutorial_codes": [
            "Total parameters in model: 1,444,261,998",
            "n_groups = 100",
            "dta_c.T[0]",
            "reconstructed.shape",
            "6237"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism->Model scale and Pipe initialization"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Frequency model \u2013 Poisson distribution"
            ]
        ]
    },
    "48669": {
        "jupyter_code_cell": "for layer in model_tune.layers:\n    layer.trainable = False\n    \n# compile the model (should be done *after* setting layers to non-trainable)\nmodel_tune.compile(optimizer='rmsprop', loss='binary_crossentropy',  metrics=[eval_metric])\n\nmodel_tune.layers",
        "matched_tutorial_code_inds": [
            1093,
            2178,
            83,
            80,
            2968
        ],
        "matched_tutorial_codes": [
            "for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # Unscales the gradients of optimizer's assigned params in-place\n        scaler.unscale_(opt)\n\n        # Since the gradients of optimizer's assigned params are now unscaled, clips as usual.\n        # You may use the same value for max_norm here as you would without gradient scaling.\n        (net.parameters(), max_norm=0.1)\n\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
            "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        # Runs the forward pass under autocast.\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            # output is float16 because linear layers autocast to float16.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss is float32 because mse_loss layers autocast to float32.\n            assert loss.dtype is torch.float32\n\n        # Exits autocast before backward().\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
            "for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ],
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->Inspecting/modifying gradients (e.g., clipping)"
            ],
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->Adding autocast"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "1194669": {
        "jupyter_code_cell": "import pandas as pd\nimport numpy as np\nimport gensim\nfrom matplotlib import pyplot as plt\nfrom sklearn.manifold import TSNE\n",
        "matched_tutorial_code_inds": [
            4021,
            6140,
            5191,
            6222,
            4962
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom colorspacious import cspace_converter"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "statsmodels->Examples->Statistics->Mediation analysis with duration data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps"
            ]
        ]
    },
    "145564": {
        "jupyter_code_cell": "mpg_data.horsepower = mpg_data.horsepower.astype('float')\nmpg_data.dtypes",
        "matched_tutorial_code_inds": [
            6014,
            1447,
            1502,
            5570,
            4633
        ],
        "matched_tutorial_codes": [
            "interM_lm.model.exog\ninterM_lm.model.exog_names",
            "img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape",
            "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "sio.loadmat\nsio.savemat\nsio.whosmat"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Applying to all colors"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "scipy->File IO (scipy.io)->MATLAB files->The basic functions"
            ]
        ]
    },
    "297475": {
        "jupyter_code_cell": "# BS4 can quickly parse our text, make sure to tell it that you're giving html\nhtml = bs4(rsp.text, 'html.parser')\n\n# BS makes it easy to look through a document\nprint(html.prettify()[:1000])",
        "matched_tutorial_code_inds": [
            6964,
            165,
            1754,
            1202,
            12
        ],
        "matched_tutorial_codes": [
            "# Here we'll catch the exception to prevent printing too much of\n# the exception trace output in this notebook\ntry:\n    res.forecast('2000-01-03')\nexcept KeyError as e:\n    print(e)",
            "# And just to show that we can round trip all of the results from earlier:\nround_tripped_results = pickle.loads(pickle.dumps(results))\nassert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))",
            "# Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()",
            "try:\n    torch._dynamo.export(bar, (10), (10))\nexcept:\n    tb.print_exc()\n\nmodel_exp = torch._dynamo.export(init_model(), generate_data(16)[0])\nprint(model_exp[0](generate_data(16)[0]))",
            "# Equates to one random 28x28 image\nrandom_data = ((1, 1, 28, 28))\n\nmy_nn = Net()\nresult = my_nn(random_data)\nprint (result)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->6. Saving/Loading benchmark results"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->TorchDynamo and FX Graphs"
            ],
            [
                "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps->4. [Optional] Pass data through your model to test"
            ]
        ]
    },
    "1139357": {
        "jupyter_code_cell": "loans_2007.isnull().sum()",
        "matched_tutorial_code_inds": [
            3734,
            3720,
            1500,
            5600,
            3723
        ],
        "matched_tutorial_codes": [
            "delays.nsmallest(5).sort_values()",
            "flights.dep_time.unique()",
            "china_mask.nonzero()",
            "resf_logit.predict()",
            "flights.dep_time.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ]
        ]
    },
    "661865": {
        "jupyter_code_cell": "populations = pd.Series({\"China\":1357000000, \"India\":1252000000, \"United States\":321068000, \"Indonesia\":249900000, \n                         \"Brazil\":200400000, \"Pakistan\":191854000})\nprint(populations)",
        "matched_tutorial_code_inds": [
            3979,
            4030,
            6670,
            1918,
            5410
        ],
        "matched_tutorial_codes": [
            "flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "dowjones = sns.load_dataset(\"dowjones\")\nsns.relplot(data=dowjones, x=\"Date\", y=\"Price\", kind=\"line\")\n",
            "# Retrieve the posterior means\nparams = pm.summary(trace_uc)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_uc_bayes = mod_uc.smooth(params)",
            "Accuracy (train) for L1 logistic: 83.3%\nAccuracy (train) for L2 logistic (Multinomial): 82.7%\nAccuracy (train) for L2 logistic (OvR): 79.3%\nAccuracy (train) for Linear SVC: 82.0%\nAccuracy (train) for GPC: 82.7%\n\n\n\n<br/>",
            "# rob_crime_model = rlm(\"murder ~ pctmetro + poverty + pcths + single\", data=dta, M=sm.robust.norms.TukeyBiweight()).fit(conv=\"weights\")\n# print(rob_crime_model.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "sklearn->Examples->Classification->Plot classification probability"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ]
        ]
    },
    "1164436": {
        "jupyter_code_cell": "# Number of Unique Items\n\ntotal_unique_items = heroes_df['Item Name'].nunique()\n\n# total puchases\n\ntotal_purchases = heroes_df['Price'].count()\n\n#total Revenue\n\ntotal_revenue = round(heroes_df['Price'].sum(),2)\n\n#avg price\n\navg_price = round(total_revenue /total_purchases, 2)\n\n#create dataframe for values\n\npur_analysis = pd.DataFrame([{\n    \n    \"Number of Unique Items\": total_unique_items,\n    'Average Purchase Price': avg_price,\n    'Total Purchases': total_purchases,\n    'Total Revenue': total_revenue\n}])\n\npur_analysis.style.format({'Average Purchase Price': '${:.2f}', 'Total Revenue': '${:,.2f}'})",
        "matched_tutorial_code_inds": [
            3104,
            473,
            2762,
            370,
            5133
        ],
        "matched_tutorial_codes": [
            "# range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "# some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard"
            ],
            [
                "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting"
            ]
        ]
    },
    "1283162": {
        "jupyter_code_cell": "twitter_archive_master.to_csv(\"/home/workspace/twitter_archive_master.csv\", encoding='utf-8')",
        "matched_tutorial_code_inds": [
            4031,
            3960,
            3676,
            6194,
            4153
        ],
        "matched_tutorial_codes": [
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "weather = pd.read_hdf(\"weather.h5\", \"weather\").sort_index()\n\nweather.head()",
            "fig = res_glob.plot_predict(start=714, end=732)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\"/>",
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ]
        ]
    },
    "546488": {
        "jupyter_code_cell": "data[\"Do you celebrate Thanksgiving?\"].value_counts()",
        "matched_tutorial_code_inds": [
            2138,
            5560,
            5681,
            5679,
            5561
        ],
        "matched_tutorial_codes": [
            "data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]",
            "data_student.head(5)",
            "data[:3]",
            "data = sm.datasets.fair.load_pandas().data",
            "data_student.dtypes"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ]
        ]
    },
    "968489": {
        "jupyter_code_cell": "ggplot(data=sug, mapping=aes(x='datetime')) + \\\n    scale_x_date(breaks=['2014-05-20', '2014-06-01','2014-06-15','2014-07-01'],\n                 labels=['20 may','1 june','15 june','1 july']) + \\\n    geom_line(mapping=aes(y='weight'), color='red') + \\\n    geom_line(mapping=aes(y='sugar'), color='blue') + \\\n    ylab('-')",
        "matched_tutorial_code_inds": [
            3957,
            5383,
            3823,
            4542,
            5393
        ],
        "matched_tutorial_codes": [
            "g = sns.FacetGrid(penguins, col=\"sex\", height=3.5, aspect=.75)\n",
            "fig = sm.graphics.influence_plot(prestige_model, criterion=\"cooks\")\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_12_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_12_0.png\"/>",
            "g = sns.pairplot(df, hue='cut')",
            "ax.plot(x1, np.zeros(x1.shape), 'b+', ms=20)  # rug plot\n x_eval = np.linspace(-10, 10, num=200)\n ax.plot(x_eval, kde1(x_eval), 'k-', label=\"Scott's Rule\")\n ax.plot(x_eval, kde2(x_eval), 'r-', label=\"Silverman's Rule\")",
            "fig = sm.graphics.plot_ccpr_grid(prestige_model)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Specifying figure sizes",
                "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Specifying figure sizes"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Influence plots"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Component-Component plus Residual (CCPR) Plots"
            ]
        ]
    },
    "118313": {
        "jupyter_code_cell": "s.describe()   # descritptive statistics",
        "matched_tutorial_code_inds": [
            4194,
            4093,
            4189,
            4187,
            4207
        ],
        "matched_tutorial_codes": [
            "sns.color_palette()\n",
            "sns.pairplot(penguins)\n",
            "sns.set_theme()\n",
            "sns.axes_style()\n",
            "sns.color_palette(\"rocket\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Choosing color palettes->Qualitative color palettes",
                "seaborn->Figure aesthetics->Choosing color palettes->Qualitative color palettes"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Scaling plot elements",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Scaling plot elements"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles"
            ],
            [
                "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Discrete vs. continuous mapping",
                "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Discrete vs. continuous mapping"
            ]
        ]
    },
    "1168946": {
        "jupyter_code_cell": "len(entries), len(cchmmpred_data)",
        "matched_tutorial_code_inds": [
            3568,
            2801,
            717,
            707,
            2803
        ],
        "matched_tutorial_codes": [
            "len(vectorizer.vocabulary_)",
            "len(df_test)",
            "get_perf(, \"jacrev\", , \"jacfwd\")",
            "get_perf(, \"without vmap\",  , \"vmap\")",
            "len(df_test[df_test[\"ClaimAmount\"]  0])"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->DictVectorizer"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Frequency model \u2013 Poisson distribution"
            ],
            [
                "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->reverse-mode Jacobian (jacrev) vs forward-mode Jacobian (jacfwd)"
            ],
            [
                "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->Computing the Jacobian"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Frequency model \u2013 Poisson distribution"
            ]
        ]
    },
    "78187": {
        "jupyter_code_cell": "print(len(train_shift_left))",
        "matched_tutorial_code_inds": [
            6296,
            5232,
            1000,
            5875,
            5863
        ],
        "matched_tutorial_codes": [
            "print(dta.head(10))",
            "print(res3.f_test(R))",
            "print(layer.parametrizations.weight[0])",
            "print(prestige.head(10))",
            "print(stats.norm.fit(fat_tails))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Small group effects"
            ],
            [
                "torch->Model Optimization->Parametrizations Tutorial->Inspecting a parametrized module"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ]
        ]
    },
    "1097968": {
        "jupyter_code_cell": "import matplotlib.pyplot as plt\nimport random\n# Visualizations will be shown in the notebook.\n\n# Plot sign of each class\nplt.figure(figsize=(20, 10))\nfor i in range(n_classes):\n    plt.subplot(4, 11, i+1)\n    img = X_train[y_train == i][0]\n    plt.axis('off')\n    plt.title(\"ClassId: \" + str(i))\n    plt.imshow(img)\nplt.show()",
        "matched_tutorial_code_inds": [
            4842,
            4974,
            4728,
            3506,
            2056
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.rcParams['savefig.facecolor'] = \"0.8\"\n\n\ndef example_plot(ax, fontsize=12):\n    ax.plot([1, 2])\n\n    ax.locator_params(nbins=3)\n    ax.set_xlabel('x-label', fontsize=fontsize)\n    ax.set_ylabel('y-label', fontsize=fontsize)\n    ax.set_title('Title', fontsize=fontsize)\n\nplt.close('all')\nfig, ax = plt.subplots()\nexample_plot(ax, fontsize=24)\n\n\n<img alt=\"Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_001.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_001.png, ../../_images/sphx_glr_tight_layout_guide_001_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nx1 = np.linspace(0.0, 5.0, 100)\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]')\nax.set_ylabel('Damped oscillation [V]')\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_002.png\" srcset=\"../../_images/sphx_glr_text_intro_002.png, ../../_images/sphx_glr_text_intro_002_2_0x.png 2.0x\"/>",
            "import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nX, y = (return_X_y=True)\nX = X[:150]\ny = y[:150]\n\nlasso = (random_state=0, max_iter=10000)\nalphas = (-4, -0.5, 30)\n\ntuned_parameters = [{\"alpha\": alphas}]\nn_folds = 5\n\nclf = (lasso, tuned_parameters, cv=n_folds, refit=False)\nclf.fit(X, y)\nscores = clf.cv_results_[\"mean_test_score\"]\nscores_std = clf.cv_results_[\"std_test_score\"]",
            "import matplotlib.pyplot as plt\nfrom sklearn.covariance import , \n\n# fit a MCD robust estimator to data\nrobust_cov = ().fit(X)\n# fit a MLE estimator to data\nemp_cov = ().fit(X)\nprint(\n    \"Estimated covariance matrix:\\nMCD (Robust):\\n{}\\nMLE:\\n{}\".format(\n        robust_cov.covariance_, emp_cov.covariance_\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Simple Example"
            ],
            [
                "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings"
            ],
            [
                "sklearn->Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Load dataset and apply GridSearchCV"
            ],
            [
                "sklearn->Examples->Covariance estimation->Robust covariance estimation and Mahalanobis distances relevance->Comparison of results"
            ]
        ]
    },
    "2582": {
        "jupyter_code_cell": "Girl=df[df['Gender']=='Girl']\nGirl_performance=Girl.groupby(['State'])['Total'].mean()\nprint(Girl_performance.plot(kind='bar',figsize=(20,7)))\nplt.xlabel(\"Girls performance across States\")",
        "matched_tutorial_code_inds": [
            6811,
            6423,
            3730,
            5554,
            6892
        ],
        "matched_tutorial_codes": [
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "f = \"Lottery ~ Literacy * Wealth\"\ny, X = patsy.dmatrices(f, df, return_type=\"dataframe\")\nprint(y[:5])\nprint(X[:5])"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->Using formulas with models that do not (yet) support them"
            ]
        ]
    },
    "995746": {
        "jupyter_code_cell": "#correlation matrix\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(10, 9))\nsns.heatmap(abs(corrmat), vmax=0.6, square=True,annot=True)\nplt.savefig('./figures/corrmat.png')",
        "matched_tutorial_code_inds": [
            6423,
            3730,
            5554,
            5368,
            6264
        ],
        "matched_tutorial_codes": [
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ]
        ]
    },
    "1292954": {
        "jupyter_code_cell": "# Number of posts / tweets to retrieve.\n# Small value for development, then increase to collect final data.\nn = 20  # 4000",
        "matched_tutorial_code_inds": [
            6735,
            6654,
            6558,
            5636,
            530
        ],
        "matched_tutorial_codes": [
            "# Set sampling params\nndraws = 3000  #  3000 number of draws from the distribution\nnburn = 600  # 600 number of \"burn-in points\" (which will be discarded)",
            "# Set sampling params\nndraws = 3000  # number of draws from the distribution\nnburn = 600  # number of \"burn-in points\" (which will be discarded)",
            "# Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "# Seed for consistency\nnp.random.seed(1)",
            "frame_skip = 1\nframes_per_batch = 1000 // frame_skip\n# For a complete training, bring the number of frames up to 1M\ntotal_frames = 50_000 // frame_skip"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->Bayesian estimation"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->5. Bayesian estimation with NUTS",
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define Hyperparameters->Data collection parameters"
            ]
        ]
    },
    "1188870": {
        "jupyter_code_cell": "data[4995:]",
        "matched_tutorial_code_inds": [
            5681,
            5560,
            5316,
            5561,
            5324
        ],
        "matched_tutorial_codes": [
            "data[:3]",
            "data_student.head(5)",
            "params.iloc[57:62]",
            "data_student.dtypes",
            "res.nobs[10:15]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->Expanding Sample"
            ]
        ]
    },
    "129968": {
        "jupyter_code_cell": "ggplot(airports) + aes(x = 'altitude') + geom_histogram(bins = 40)",
        "matched_tutorial_code_inds": [
            4174,
            6194,
            4063,
            4070,
            4153
        ],
        "matched_tutorial_codes": [
            "g = sns.pairplot(iris, hue=\"species\", palette=\"Set2\", diag_kind=\"kde\", height=2.5)\n",
            "fig = res_glob.plot_predict(start=714, end=732)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\"/>",
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"kde\", fill=True)\n",
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Conditioning on other variables",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Conditioning on other variables"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ]
        ]
    },
    "1515095": {
        "jupyter_code_cell": "#dateArgument\ntemp_dataset['lawMinorPresent'] = numpy.where(temp_dataset.lawMinor.isnull(), 0, 1)\ntemp_dataset['lawMinor'] = temp_dataset['lawMinor'].fillna(0)",
        "matched_tutorial_code_inds": [
            6953,
            6951,
            6765,
            3857,
            6949
        ],
        "matched_tutorial_codes": [
            "# Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "# Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "# Annual frequency, using a PeriodIndex\nindex = pd.period_range(start='2000', periods=4, freq='A')\nendog1 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog1.index)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ]
        ]
    },
    "50668": {
        "jupyter_code_cell": "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
        "matched_tutorial_code_inds": [
            5796,
            6505,
            6561,
            1516,
            5967
        ],
        "matched_tutorial_codes": [
            "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->What you\u2019ll need",
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure",
                "statsmodels->Examples->User Notes->Least squares fitting of models to data"
            ]
        ]
    },
    "124443": {
        "jupyter_code_cell": "biz_df.hist(column='stars')",
        "matched_tutorial_code_inds": [
            5631,
            5152,
            6059,
            5150,
            5522
        ],
        "matched_tutorial_codes": [
            "kde.evaluate(-1)",
            "sf.plot()",
            "kidney_table.head(10)",
            "sf.summary().head()",
            "glm_mod.model.data.orig_endog.sum(1)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution"
            ],
            [
                "statsmodels->User Guide->Other Models->Methods for Survival and Duration Analysis->Survival function estimation and inference"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA"
            ],
            [
                "statsmodels->User Guide->Other Models->Methods for Survival and Duration Analysis->Survival function estimation and inference"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ]
        ]
    },
    "570824": {
        "jupyter_code_cell": "feature_df.describe()",
        "matched_tutorial_code_inds": [
            5600,
            5560,
            6211,
            6458,
            6335
        ],
        "matched_tutorial_codes": [
            "resf_logit.predict()",
            "data_student.head(5)",
            "det_proc.in_sample().head()",
            "arima_res.predict(0, 2)",
            "res_areturns.summary()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Custom Deterministic Terms"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Switching variances"
            ]
        ]
    },
    "273867": {
        "jupyter_code_cell": "#split training data labels into new labels for each axis\ntrain_labels_IE = [entry[0] for entry in train_labels]\ntrain_labels_NS = [entry[1] for entry in train_labels]\ntrain_labels_TF = [entry[2] for entry in train_labels]\ntrain_labels_JP = [entry[3] for entry in train_labels]",
        "matched_tutorial_code_inds": [
            4432,
            442,
            5449,
            5799,
            2106
        ],
        "matched_tutorial_codes": [
            "N = 21\n D = np.diag(0.5*np.ones(N-1), k=1) - np.diag(0.5*np.ones(N-1), k=-1)\n D[0] = D[-1] = 0 # take away edge effects\n Dop = FirstDerivative(N, dtype=np.float64)",
            "test_loss = evaluate(model, )\ntest_ppl = math.exp(test_loss)\nprint('=' * 89)\nprint(f'| End of training | test loss {test_loss:5.2f} | '\n      f'test ppl {test_ppl:8.2f}')\nprint('=' * 89)",
            "probit_mod = sm.Probit(spector_data.endog, spector_data.exog)\nprobit_res = probit_mod.fit()\nprobit_margeff = probit_res.get_margeff()\nprint(\"Parameters: \", probit_res.params)\nprint(\"Marginal effects: \")\nprint(probit_margeff.summary())",
            "model1 = sm.GLM.from_formula(\n    \"blotch ~ 0 + C(variety) + C(site)\", family=sm.families.Binomial(), data=df\n)\nresult1 = model1.fit(scale=\"X2\")\nprint(result1.summary())",
            "iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Use of LinearOperator"
            ],
            [
                "torch->Text->Language Modeling with nn.Transformer and TorchText->Evaluate the best model on the test dataset"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Probit Model"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier"
            ]
        ]
    },
    "1438729": {
        "jupyter_code_cell": "data.plot_product('product_1842')",
        "matched_tutorial_code_inds": [
            5680,
            5560,
            5561,
            7000,
            5294
        ],
        "matched_tutorial_codes": [
            "data.describe()",
            "data_student.head(5)",
            "data_student.dtypes",
            "data = sm.datasets.sunspots.load()",
            "res.plot_cusum_squares()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models->Getting started"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper",
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money"
            ]
        ]
    },
    "1026224": {
        "jupyter_code_cell": "lats = np.array(lat)\nlons = np.array(lon)",
        "matched_tutorial_code_inds": [
            1757,
            993,
            6466,
            663,
            6449
        ],
        "matched_tutorial_codes": [
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "layer = LinearSymmetric(3)\nout = layer((8, 3))",
            "delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]",
            "traced_rn18 = (rn18)\nprint()",
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "torch->Model Optimization->Parametrizations Tutorial->Implementing parametrizations by hand"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA"
            ]
        ]
    },
    "96976": {
        "jupyter_code_cell": "import pandas as pd\n\ncountry_data = pd.read_csv('datasets/WDI_Data.csv', encoding = \"ISO-8859-1\")",
        "matched_tutorial_code_inds": [
            4021,
            1612,
            1154,
            6824,
            2952
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
            "import pandas as pd\n\nurl = \"https://stats.idre.ucla.edu/stat/data/hsb2.csv\"\nhsb2 = pd.read_table(url, delimiter=\",\")",
            "import pandas as pd\n\nfeature_names = rf[:-1].get_feature_names_out()\n\nmdi_importances = (\n    rf[-1].feature_importances_, index=feature_names\n).sort_values(ascending=True)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio"
            ],
            [
                "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Example Data"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ]
        ]
    },
    "1111978": {
        "jupyter_code_cell": "ETA = 0.005",
        "matched_tutorial_code_inds": [
            1837,
            3538,
            2379,
            2886,
            1835
        ],
        "matched_tutorial_codes": [
            "0.7137",
            "0.007",
            "0.796",
            "0.001",
            "0.7344"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function"
            ],
            [
                "sklearn->Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Feature Extraction using TfidfVectorizer"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function"
            ]
        ]
    },
    "523898": {
        "jupyter_code_cell": "out = dist_TS.fit_normal()",
        "matched_tutorial_code_inds": [
            5252,
            5904,
            5600,
            6985,
            6813
        ],
        "matched_tutorial_codes": [
            "infl = ols_results.get_influence()",
            "infl = ols_model.get_influence()",
            "resf_logit.predict()",
            "mod = NBin(y, X)\nres = mod.fit()",
            "pca_model = PCA(dta.T, standardize=False, demean=True)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Dropping an observation"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "1423447": {
        "jupyter_code_cell": "df = pd.read_csv('/home/nbangs/Notebooks/data/mcnulty/sahie_2008.csv')",
        "matched_tutorial_code_inds": [
            3812,
            3638,
            4152,
            4022,
            4167
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ]
        ]
    },
    "1496843": {
        "jupyter_code_cell": "y = df_25_70.target\n\nsubset_care_categories = ['physiotherapy', 'obstetrics', 'geriatrics', 'pharmaceuticals']\nX = df_25_70[subset_care_categories]",
        "matched_tutorial_code_inds": [
            5352,
            5270,
            2369,
            2328,
            5586
        ],
        "matched_tutorial_codes": [
            "pred_ols = res_ols.get_prediction()\niv_l_ols = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u_ols = pred_ols.summary_frame()[\"obs_ci_upper\"]",
            "sigma = rho ** order\ngls_model = sm.GLS(data.endog, data.exog, sigma=sigma)\ngls_results = gls_model.fit()",
            "coverage_fraction(\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n)",
            "gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Generalized Least Squares"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ]
        ]
    },
    "1047259": {
        "jupyter_code_cell": "def smartdownsample(img,n):\n    img_2D = img.reshape(28,28)\n    no_samples = len(img_2D)//n\n    img_binned = np.zeros([no_samples,no_samples])\n    for i in range(no_samples):\n        for j in range(no_samples):\n            img_binned[i,j] = img_2D[i*n:(i+1)*n, j*n:(j+1)*n].sum()\n    return img_binned            ",
        "matched_tutorial_code_inds": [
            384,
            1727,
            2395,
            5810,
            1672
        ],
        "matched_tutorial_codes": [
            "def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\n,  = next(iter(dataloaders['train']))\n\n# Make a grid from batch\n = ()\n\nimshow(, title=[class_names[x] for x in ])\n\n\n<img alt=\"['bees', 'bees', 'ants', 'bees']\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\"/>",
            "def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    (figsize=(1.8 * n_col, 2.4 * n_row))\n    (bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n    for i in range(n_row * n_col):\n        (n_row, n_col, i + 1)\n        (images[i].reshape((h, w)), cmap=plt.cm.gray)\n        (titles[i], size=12)\n        (())\n        (())",
            "def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax",
            "def julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data->Visualize a few images"
            ],
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Julia set"
            ]
        ]
    },
    "560286": {
        "jupyter_code_cell": "heights = [] # barplot heights\nfor i in categories:\n    temp_df = corp_it.loc[corp_it['category'] == i]\n    heights.append((i,sum(temp_df['amount'])))\ntemp_df = None",
        "matched_tutorial_code_inds": [
            403,
            6703,
            3257,
            4894,
            5990
        ],
        "matched_tutorial_codes": [
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "# histogram our data with numpy\ndata = np.random.randn(1000)\nn, bins = np.histogram(data, 100)",
            "_ = plt.boxplot([scales[0][0], scales[0][1], scales[1][0], scales[1][1]])\nplt.ylabel(\"Estimated scale\")"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach"
            ],
            [
                "matplotlib->Tutorials->Advanced->Path Tutorial->Compound paths"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ]
        ]
    },
    "1385211": {
        "jupyter_code_cell": "rf = RandomForestClassifier(class_weight='balanced', random_state= 42)\nrf.fit(X_train_1, y_train)",
        "matched_tutorial_code_inds": [
            2106,
            6713,
            5287,
            5197,
            5401
        ],
        "matched_tutorial_codes": [
            "iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)",
            "mod = TVRegression(y_t, x_t, w_t)\nres = mod.fit()\n\nprint(res.summary())",
            "mod = sm.RecursiveLS(endog, exog)\nres = mod.fit()\n\nprint(res.summary())",
            "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())",
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients->And then estimate it with our custom model class"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ]
        ]
    },
    "1517204": {
        "jupyter_code_cell": "# also, in order to have exact bottleneck features to the pre-computed ones, I needed to remove dividing the images by 255 \ntrain_tensors_a = paths_to_tensor(train_files).astype('float32')\nvalid_tensors_a = paths_to_tensor(valid_files).astype('float32')\ntest_tensors_a = paths_to_tensor(test_files).astype('float32')",
        "matched_tutorial_code_inds": [
            1945,
            2025,
            639,
            1636,
            32
        ],
        "matched_tutorial_codes": [
            "# The following bandwidth can be automatically detected using\nbandwidth = (X, quantile=0.2, n_samples=500)\n\nms = (bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\n\nlabels_unique = (labels)\nn_clusters_ = len(labels_unique)\n\nprint(\"number of estimated clusters : %d\" % n_clusters_)",
            "# Computing a few extra eigenvectors may speed up the eigen_solver.\n# The spectral clustering quality may also benetif from requesting\n# extra regions for segmentation.\nn_regions_plus = 3\n\n# Apply spectral clustering using the default eigen_solver='arpack'.\n# Any implemented solver can be used: eigen_solver='arpack', 'lobpcg', or 'amg'.\n# Choosing eigen_solver='amg' requires an extra package called 'pyamg'.\n# The quality of segmentation and the speed of calculations is mostly determined\n# by the choice of the solver and the value of the tolerance 'eigen_tol'.\n# TODO: varying eigen_tol seems to have no effect for 'lobpcg' and 'amg' #21243.\nfor assign_labels in (\"kmeans\", \"discretize\", \"cluster_qr\"):\n    t0 = ()\n    labels = (\n        graph,\n        n_clusters=(n_regions + n_regions_plus),\n        eigen_tol=1e-7,\n        assign_labels=assign_labels,\n        random_state=42,\n    )\n\n    t1 = ()\n    labels = labels.reshape(rescaled_coins.shape)\n    (figsize=(5, 5))\n    (rescaled_coins, cmap=plt.cm.gray)\n\n    (())\n    (())\n    title = \"Spectral clustering: %s, %.2fs\" % (assign_labels, (t1 - t0))\n    print(title)\n    (title)\n    for l in range(n_regions):\n        colors = [plt.cm.nipy_spectral((l + 4) / float(n_regions + 4))]\n        (labels == l, colors=colors)\n        # To view individual segments as appear comment in plt.pause(0.5)\n()\n\n# TODO: After #21194 is merged and #21243 is fixed, check which eigen_solver\n# is the best and set eigen_solver='arpack', 'lobpcg', or 'amg' and eigen_tol\n# explicitly in this example.\n\n\n\n<img alt=\"Spectral clustering: kmeans, 2.04s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\"/>\n<img alt=\"Spectral clustering: discretize, 1.83s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\"/>\n<img alt=\"Spectral clustering: cluster_qr, 1.82s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\"/>",
            "# This enables the extended features such as the camera.\nstart_x=1\n\n# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.\ngpu_mem=128\n\n# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.\n#camera_auto_detect=1",
            "# The threshold is \"greater than 150\"\n# Return `1` if true, `0` otherwise\nxray_image_mask_less_noisy = np.where(xray_image &gt; 150, 1, 0)\n\nplt.imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\" src=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\"/>",
            "# import the modules used here in this recipe\nimport torch\nimport torch.quantization\nimport torch.nn as nn\nimport copy\nimport os\nimport time\n\n# define a very, very simple LSTM for demonstration purposes\n# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n# inspired by\n# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\nclass lstm_for_demonstration():\n  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n     Not to be used for anything other than demonstration.\n  \"\"\"\n  def __init__(self,in_dim,out_dim,depth):\n     super(lstm_for_demonstration,self).__init__()\n     self.lstm = (in_dim,out_dim,depth)\n\n  def forward(self,inputs,hidden):\n     out,hidden = self.lstm(inputs,hidden)\n     return out, hidden\n\n\n(29592)  # set the seed for reproducibility\n\n#shape parameters\nmodel_dimension=8\nsequence_length=20\nbatch_size=1\nlstm_depth=1\n\n# random data for input\ninputs = (sequence_length,batch_size,model_dimension)\n# hidden is actually is a tuple of the initial hidden state and the initial cell state\nhidden = ((lstm_depth,batch_size,model_dimension), (lstm_depth,batch_size,model_dimension))"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->A demo of the mean-shift clustering algorithm->Compute clustering with MeanShift"
            ],
            [
                "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Raspberry Pi 4 Setup"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->1: Set Up"
            ]
        ]
    },
    "122427": {
        "jupyter_code_cell": "k.info()",
        "matched_tutorial_code_inds": [
            5631,
            6402,
            5629,
            3690,
            5317
        ],
        "matched_tutorial_codes": [
            "kde.evaluate(-1)",
            "kpss_test(sunspots[\"SUNACTIVITY\"])",
            "kde.entropy",
            "sped.head()",
            "params.tail()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)->KPSS test"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ]
        ]
    },
    "1149662": {
        "jupyter_code_cell": "from plotly.tools import FigureFactory as FF\niplot(FF.create_scatterplotmatrix(transit_graph,width=1200, size=10, height=1200, diag='box'));",
        "matched_tutorial_code_inds": [
            5053,
            5035,
            3794,
            1157,
            4179
        ],
        "matched_tutorial_codes": [
            "from mpl_toolkits.axes_grid1.axes_size import Fixed, Scaled\nvert = [Fixed(2), Scaled(2), Scaled(3)]",
            "from matplotlib.backends.backend_pgf import FigureCanvasPgf\nmatplotlib.backend_bases.register_backend('pdf', FigureCanvasPgf)",
            "g = sns.FacetGrid(wins.reset_index(), hue='team', size=7, aspect=.5, palette=['k'])\ng.map(sns.pointplot, 'is_home', 'win_pct').set(ylim=(0, 1));",
            "from ax.plot.contour import interact_contour_plotly\n\ninteract_contour_plotly(model=gs.model, metric_name=\"val_acc\")",
            "sns.set_style(\"whitegrid\")\ndata = np.random.normal(size=(20, 6)) + np.arange(6) / 2\nsns.boxplot(data=data);\n"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Toolkits->The axes_grid1 toolkit->AxesDivider"
            ],
            [
                "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ],
            [
                "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Seaborn figure styles",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Seaborn figure styles"
            ]
        ]
    },
    "1125729": {
        "jupyter_code_cell": "data.boxplot('happiness_rating', by='trade_number', figsize=(12, 8))",
        "matched_tutorial_code_inds": [
            3936,
            4083,
            4082,
            3938,
            4026
        ],
        "matched_tutorial_codes": [
            "sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5))\n",
            "sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n",
            "sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\", hue=\"size\",\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ]
        ]
    },
    "103124": {
        "jupyter_code_cell": "df[(df == 0).astype(int).sum(axis=1)==10].groupby(['target']).agg(['count'])",
        "matched_tutorial_code_inds": [
            3706,
            3712,
            3700,
            3633,
            3639
        ],
        "matched_tutorial_codes": [
            "m.groupby('skyc1').dep_delay.agg(['mean', 'count']).sort_values(by='mean')",
            "(weather.reset_index(level='station')\n .query('station in @locs')\n .groupby(['station', pd.TimeGrouper('H')])).mean()",
            "pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True).head()",
            "df.ix[10:15, ['fl_date', 'tail_num']]",
            "first.loc[['AA', 'AS', 'DL'], ['fl_date', 'tail_num']]"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->Merge Version"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ]
        ]
    },
    "63594": {
        "jupyter_code_cell": "from keras.preprocessing import text as ktxt, sequence\n\nvocab_max = 100000\n\nhint(\"Fitting the tokenizer...\")\ntokenizer = ktxt.Tokenizer(num_words=vocab_max)\ntokenizer.fit_on_texts(X)\n\nhint(\"Tokenizing...\")\nX = tokenizer.texts_to_sequences(X)\nX_ = tokenizer.texts_to_sequences(X_)\n\nhint(\"Padding the sequences...\")\nmax_comment_length = 200  # padded/cropped comment length\nX = sequence.pad_sequences(X, maxlen=max_comment_length)\nX_ = sequence.pad_sequences(X_, maxlen=max_comment_length)\n\nhint(\"Done\")",
        "matched_tutorial_code_inds": [
            3512,
            3028,
            2074,
            2808,
            2150
        ],
        "matched_tutorial_codes": [
            "from sklearn import datasets, neighbors, linear_model\n\nX_digits, y_digits = (return_X_y=True)\nX_digits = X_digits / X_digits.max()\n\nn_samples = len(X_digits)\n\nX_train = X_digits[: int(0.9 * n_samples)]\ny_train = y_digits[: int(0.9 * n_samples)]\nX_test = X_digits[int(0.9 * n_samples) :]\ny_test = y_digits[int(0.9 * n_samples) :]\n\nknn = ()\nlogistic = (max_iter=1000)\n\nprint(\"KNN score: %f\" % knn.fit(X_train, y_train).score(X_test, y_test))\nprint(\n    \"LogisticRegression score: %f\"\n    % logistic.fit(X_train, y_train).score(X_test, y_test)\n)",
            "from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>",
            "from sklearn.cross_decomposition import \n\nn = 1000\nq = 3\np = 10\nX = (size=n * p).reshape((n, p))\nB = ([[1, 2] + [0] * (p - 2)] * q).T\n# each Yj = 1*X1 + 2*X2 + noize\nY = (X, B) + (size=n * q).reshape((n, q)) + 5\n\npls2 = (n_components=3)\npls2.fit(X, Y)\nprint(\"True B (such that: Y = XB + Err)\")\nprint(B)\n# compare pls2.coef_ with B\nprint(\"Estimated B\")\nprint(np.round(pls2.coef_, 1))\npls2.predict(X)",
            "from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "from sklearn.decomposition import \n\nprint(\"Learning the dictionary...\")\nt0 = ()\ndico = (\n    # increase to 300 for higher quality results at the cost of slower\n    # training times.\n    n_components=50,\n    batch_size=200,\n    alpha=1.0,\n    max_iter=10,\n)\nV = dico.fit(data).components_\ndt = () - t0\nprint(f\"{dico.n_iter_} iterations / {dico.n_steps_} steps in {dt:.2f}.\")\n\n(figsize=(4.2, 4))\nfor i, comp in enumerate(V[:100]):\n    (10, 10, i + 1)\n    (comp.reshape(patch_size), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    (())\n    (())\n(\n    \"Dictionary learned from face patches\\n\"\n    + \"Train time %.1fs on %d patches\" % (dt, len(data)),\n    fontsize=16,\n)\n(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n\n<img alt=\"Dictionary learned from face patches Train time 16.0s on 22692 patches\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_image_denoising_002.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Tutorial exercises->Digits Classification Exercise"
            ],
            [
                "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves"
            ],
            [
                "sklearn->Examples->Cross decomposition->Compare cross decomposition methods->PLS regression, with multivariate response, a.k.a. PLS2"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution"
            ],
            [
                "sklearn->Examples->Decomposition->Image denoising using dictionary learning->Learn the dictionary from reference patches"
            ]
        ]
    },
    "563488": {
        "jupyter_code_cell": "# To find individual cells\nl = 14\ntemp=markers.copy()\ntemp[markers!=l]=-1\nplt.imshow(temp)",
        "matched_tutorial_code_inds": [
            6703,
            3730,
            184,
            3685,
            6423
        ],
        "matched_tutorial_codes": [
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "# With indecies\ntemp = weather['tmpf']\n\nc = (temp - 32) * 5 / 9\nc.to_frame()",
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Easier Arithmetic, Analysis"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ]
        ]
    },
    "128770": {
        "jupyter_code_cell": "param_grid = dict(gamma=gamma_range, C=C_range)\n\ngs_svc = GridSearchCV(SVC(), param_grid, scoring='accuracy', \n                      cv=5, n_jobs=-1).fit(X, y)\n\ngs_svc.best_params_",
        "matched_tutorial_code_inds": [
            3377,
            3290,
            23,
            3197,
            2896
        ],
        "matched_tutorial_codes": [
            "param_grid = {\n    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n    \"classifier__C\": [0.1, 1.0, 10, 100],\n}\n\nsearch_cv = (clf, param_grid, n_iter=10, random_state=0)\nsearch_cv",
            "cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations"
            ]
        ]
    },
    "1358369": {
        "jupyter_code_cell": "#Leer un archivo de texto\nwith open('advs.txt', 'r') as a:\n    texto = a.read()",
        "matched_tutorial_code_inds": [
            3812,
            634,
            587,
            1612,
            1175
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)",
            "import requests\n\nresp = requests.post(\"http://localhost:5000/predict\",\n                     files={\"file\": open('&lt;PATH/TO/.jpg/FILE&gt;/cat.jpg','rb')})",
            "import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "import traceback as tb\ntry:\n    (f1)\nexcept:\n    tb.print_exc()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Integrating the model in our API Server"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing"
            ]
        ]
    },
    "29495": {
        "jupyter_code_cell": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.float_format', '{:.4f}'.format)\npd.options.display.max_columns = 999\npd.options.display.max_rows = 999\nsns.set()",
        "matched_tutorial_code_inds": [
            5635,
            6280,
            6154,
            6794,
            6376
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn as sns\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=15)\nplt.rc(\"lines\", linewidth=3)\nsns.set_style(\"darkgrid\")",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data"
            ],
            [
                "statsmodels->Examples->Statistics->Copulas"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Imports"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ]
        ]
    },
    "1219618": {
        "jupyter_code_cell": "#Get a list of current columns\ncols_list = list(df.columns.values)\ncols_list",
        "matched_tutorial_code_inds": [
            3812,
            3635,
            1531,
            1082,
            2443
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "# filter the warning for now on\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)",
            "AB = results.params\nA = AB[0]\nB = AB[1]",
            "# Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()",
            "all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation"
            ]
        ]
    },
    "85609": {
        "jupyter_code_cell": "pvalue",
        "matched_tutorial_code_inds": [
            3902,
            3351,
            5629,
            5943,
            1470
        ],
        "matched_tutorial_codes": [
            "total_by_employee",
            "clf",
            "kde.entropy",
            "beta_true",
            "whos"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s"
            ]
        ]
    },
    "694743": {
        "jupyter_code_cell": "example.loc['Name']",
        "matched_tutorial_code_inds": [
            3714,
            6119,
            2183,
            2309,
            5940
        ],
        "matched_tutorial_codes": [
            "weather.loc['DSM']",
            "dta_c.T[0]",
            "num_selector(X)",
            "diabetes = ()\nX, y = diabetes.data, diabetes.target",
            "all_betas.mean(0)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables"
            ],
            [
                "sklearn->Examples->Ensemble methods->Combine predictors using stacking->Make pipeline to preprocess the data"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Load the data"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss"
            ]
        ]
    },
    "1325610": {
        "jupyter_code_cell": "\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.ensemble.partial_dependence import partial_dependence\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import train_test_split\nimport datetime as dt\n\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc",
        "matched_tutorial_code_inds": [
            4562,
            662,
            3296,
            5464,
            1616
        ],
        "matched_tutorial_codes": [
            "import numpy as np\n import matplotlib.pyplot as plt; plt.style.use('classic')\n from scipy.stats import multiscale_graphcorr",
            "import statistics, tabulate, time\nfrom typing import Any, Dict, List\nfrom torch.fx import ",
            "import time\n\nfrom sklearn.manifold import \nfrom sklearn.neighbors import \nfrom sklearn.pipeline import \n\ndatasets = [\n    (\"MNIST_10000\", load_mnist(n_samples=10_000)),\n    (\"MNIST_20000\", load_mnist(n_samples=20_000)),\n]\n\nn_iter = 500\nperplexity = 30\nmetric = \"euclidean\"\n# TSNE requires a certain number of neighbors which depends on the\n# perplexity parameter.\n# Add one since we include each sample as its own neighbor.\nn_neighbors = int(3.0 * perplexity + 1) + 1\n\ntsne_params = dict(\n    init=\"random\",  # pca not supported for sparse matrices\n    perplexity=perplexity,\n    method=\"barnes_hut\",\n    random_state=42,\n    n_iter=n_iter,\n    learning_rate=\"auto\",\n)\n\ntransformers = [\n    (\n        \"KNeighborsTransformer\",\n        (n_neighbors=n_neighbors, mode=\"distance\", metric=metric),\n    ),\n    (\n        \"NMSlibTransformer\",\n        NMSlibTransformer(n_neighbors=n_neighbors, metric=metric),\n    ),\n    (\n        \"PyNNDescentTransformer\",\n        PyNNDescentTransformer(\n            n_neighbors=n_neighbors, metric=metric, parallel_batch_queries=True\n        ),\n    ),\n]\n\nfor dataset_name, (X, y) in datasets:\n\n    msg = f\"Benchmarking on {dataset_name}:\"\n    print(f\"\\n{msg}\\n\" + str(\"-\" * len(msg)))\n\n    for transformer_name, transformer in transformers:\n        longest = np.max([len(name) for name, model in transformers])\n        start = ()\n        transformer.fit(X)\n        fit_duration = () - start\n        print(f\"{transformer_name:&lt;{longest}} {fit_duration:.3f} sec (fit)\")\n        start = ()\n        Xt = transformer.transform(X)\n        transform_duration = () - start\n        print(f\"{transformer_name:&lt;{longest}} {transform_duration:.3f} sec (transform)\")\n        if transformer_name == \"PyNNDescentTransformer\":\n            start = ()\n            Xt = transformer.transform(X)\n            transform_duration = () - start\n            print(\n                f\"{transformer_name:&lt;{longest}} {transform_duration:.3f} sec\"\n                \" (transform)\"\n            )",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "import numpy as np\nnum_imgs = 9\n\ncombined_xray_images_1 = np.array(\n    [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)]\n)"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Statistics (scipy.stats)->Kernel density estimation->Multiscale Graph Correlation (MGC)"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Approximate nearest neighbors in TSNE"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Combine images into a multidimensional array to demonstrate progression"
            ]
        ]
    },
    "941578": {
        "jupyter_code_cell": "#Combining features and CGPA(Target)\nvis_dataset = pd.concat([features, target], axis = 1)",
        "matched_tutorial_code_inds": [
            6898,
            6765,
            5240,
            6770,
            1260
        ],
        "matched_tutorial_codes": [
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "def setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ],
            [
                "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5"
            ]
        ]
    },
    "286998": {
        "jupyter_code_cell": "df = pd.read_pickle(\"Electronics_meta.pickle\")\ndf.head(1)",
        "matched_tutorial_code_inds": [
            4152,
            4031,
            4022,
            3812,
            4053
        ],
        "matched_tutorial_codes": [
            "tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ]
        ]
    },
    "1242186": {
        "jupyter_code_cell": "full_PCA = sklearn.decomposition.PCA()",
        "matched_tutorial_code_inds": [
            6813,
            5600,
            1256,
            319,
            6985
        ],
        "matched_tutorial_codes": [
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "resf_logit.predict()",
            "model = Net().to(rank)\nmodel = DDP(model)",
            "fit()\n\nprint(loss_func((), ))",
            "mod = NBin(y, X)\nres = mod.fit()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Refactor using nn.Linear"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example"
            ]
        ]
    },
    "1241726": {
        "jupyter_code_cell": "mix = quandl.get(['NSE/OIL.1', 'WIKI/AAPL.4'])\nmix.plot(subplots=True)",
        "matched_tutorial_code_inds": [
            5456,
            4094,
            6166,
            6898,
            6765
        ],
        "matched_tutorial_codes": [
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ]
        ]
    },
    "567587": {
        "jupyter_code_cell": "df[df.index.duplicated()]",
        "matched_tutorial_code_inds": [
            2431,
            3633,
            6006,
            3637,
            3640
        ],
        "matched_tutorial_codes": [
            "df[\"count\"].max()",
            "df.ix[10:15, ['fl_date', 'tail_num']]",
            "df_infl[:5]",
            "first.ix[10:15, ['fl_date', 'tail_num']]",
            "first.iloc[[0, 1, 3], [0, 1]]"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ]
        ]
    },
    "1225934": {
        "jupyter_code_cell": "xls2Hotcodeclean.columns",
        "matched_tutorial_code_inds": [
            6002,
            5999,
            6119,
            5522,
            6801
        ],
        "matched_tutorial_codes": [
            "lm.model.data.frame[:5]",
            "lm.model.exog[:5]",
            "dta_c.T[0]",
            "glm_mod.model.data.orig_endog.sum(1)",
            "res.forecast_components(12)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data"
            ]
        ]
    },
    "1179458": {
        "jupyter_code_cell": "0.234",
        "matched_tutorial_code_inds": [
            3538,
            2379,
            2886,
            1837,
            2370
        ],
        "matched_tutorial_codes": [
            "0.007",
            "0.796",
            "0.001",
            "0.7137",
            "0.868"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Feature Extraction using TfidfVectorizer"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval"
            ]
        ]
    },
    "1121415": {
        "jupyter_code_cell": "print('The outputs are:')\nfor out in outputs.keys():\n    print('- {}'.format(out))\n\nif 'sub_volume_boundary' in outputs.keys():\n    print('There are {} sub_volume_boundary outputs'.format(outputs['sub_volume_boundary'].shape[0]))",
        "matched_tutorial_code_inds": [
            506,
            3730,
            6779,
            5240,
            3155
        ],
        "matched_tutorial_codes": [
            "print('Checking the results of test dataset.')\naccu_test = evaluate()\nprint('test accuracy {:8.3f}'.format(accu_test))",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "print('Optimizer iterations')\nprint('- Original model:     %d' % res_ar.mle_retvals['iterations'])\nprint('- Concentrated model: %d' % res_ar_conc.mle_retvals['iterations'])",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "n_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target == 8\nprint(\n    f\"The number of images is {X.shape[0]} and each image contains {X.shape[1]} pixels\"\n)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->Text classification with the torchtext library->Evaluate the model with test dataset"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: SARIMAX"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ],
            [
                "sklearn->Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->The dataset"
            ]
        ]
    },
    "170297": {
        "jupyter_code_cell": "adult_dropna_df = dropna_titanic_df[(dropna_titanic_df.IsAdult == 'Adult')]\ntotal_adult = adult_dropna_df['PassengerId'].count()\ntotal_adult",
        "matched_tutorial_code_inds": [
            6251,
            3731,
            3857,
            6898,
            6765
        ],
        "matched_tutorial_codes": [
            "table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ]
        ]
    },
    "624962": {
        "jupyter_code_cell": "new_index = midnights.union(data.index)\nnew_index",
        "matched_tutorial_code_inds": [
            6217,
            1502,
            3450,
            1497,
            3892
        ],
        "matched_tutorial_codes": [
            "auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "y_train = (y)\ny_train[unlabeled_set] = -1",
            "china_total = china_masked.data\nchina_total",
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ]
        ]
    },
    "97209": {
        "jupyter_code_cell": "plt.scatter(area, width*height)\nplt.xlabel('area')\nplt.ylabel('width*height')\nplt.xlim(0.1,.25)\nplt.ylim(0.1,.25)",
        "matched_tutorial_code_inds": [
            3730,
            6264,
            5801,
            6811,
            3799
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "plt.clf()\nplt.grid(True)\nplt.plot(result1.predict(linear=True), result1.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team"
            ]
        ]
    },
    "906330": {
        "jupyter_code_cell": "import seaborn as sns\ncmap = sns.diverging_palette(220, 10, as_cmap=True)",
        "matched_tutorial_code_inds": [
            4748,
            4021,
            5089,
            5035,
            5063
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(2, 1, 1) # two rows, one column, first plot",
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')",
            "from matplotlib.backends.backend_pgf import FigureCanvasPgf\nmatplotlib.backend_bases.register_backend('pdf', FigureCanvasPgf)",
            "import mpl_toolkits.axisartist as AA\nfrom mpl_toolkits.axes_grid1 import host_subplot\n\nhost = host_subplot(111, axes_class=AA.Axes)"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Artist tutorial"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The mplot3d toolkit"
            ],
            [
                "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist->axisartist with ParasiteAxes"
            ]
        ]
    },
    "158789": {
        "jupyter_code_cell": "ax = sns.barplot(x=\"WWID\", y=\"Pre_Q30\", hue=\"MatchSequence\",data=df,palette=\"husl\")\nsns.plt.ylim(0, 10)\nplt.legend(bbox_to_anchor=(1.04, 1),loc=1, borderaxespad=0.) ",
        "matched_tutorial_code_inds": [
            2655,
            6509,
            5318,
            3881,
            3883
        ],
        "matched_tutorial_codes": [
            "ax = (\n    data=full_data, x=\"input_feature\", y=\"target\", color=\"black\", alpha=0.75\n)\nax.plot(X_plot, y_plot, color=\"black\", label=\"Ground Truth\")\nax.plot(X_plot, y_brr, color=\"red\", label=\"BayesianRidge with polynomial features\")\nax.plot(X_plot, y_ard, color=\"navy\", label=\"ARD with polynomial features\")\nax.fill_between(\n    X_plot.ravel(),\n    y_ard - y_ard_std,\n    y_ard + y_ard_std,\n    color=\"navy\",\n    alpha=0.3,\n)\nax.fill_between(\n    X_plot.ravel(),\n    y_brr - y_brr_std,\n    y_brr + y_brr_std,\n    color=\"red\",\n    alpha=0.3,\n)\nax.legend()\n_ = ax.set_title(\"Polynomial fit of a non-linear feature\")\n\n\n<img alt=\"Polynomial fit of a non-linear feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ard_003.png\" srcset=\"../../_images/sphx_glr_plot_ard_003.png\"/>",
            "ax = res.impulse_responses(10, orthogonalized=True, impulse=[1, 0]).plot(figsize=(13,3))\nax.set(xlabel='t', title='Responses to a shock to `dln_inv`');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\"/>",
            "fig = rres.plot_recursive_coefficient(variables=[\"Mkt-RF\"], figsize=(14, 6))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_10_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_10_0.png\"/>",
            "ax = y.plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_ylabel(\"Monthly Flights\")\nplt.legend()\nsns.despine()",
            "ax = y.plot(label='observed')\npred_dy.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_dy_ci.index,\n                pred_dy_ci.iloc[:, 0],\n                pred_dy_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_ylabel(\"Monthly Flights\")\n\n# Highlight the forecast area\nax.fill_betweenx(ax.get_ylim(), pd.Timestamp('2013-01-01'), y.index[-1],\n                 alpha=.1, zorder=-1)\nax.annotate('Dynamic $\\\\longrightarrow$', (pd.Timestamp('2013-02-01'), 550))\n\nplt.legend()\nsns.despine()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Bayesian regressions with polynomial feature expansion->Plotting polynomial regressions with std errors of the scores"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Forecasting"
            ]
        ]
    },
    "1001878": {
        "jupyter_code_cell": "years = xrange(2009,2015)\npages = xrange(1,9)\nyear_pagetxt = {}\nfor year in years: \n    pagestext = {}\n    for page in pages: \n        r = requests.get(\"http://www.boxofficemojo.com/yearly/chart/?page=%s&view=releasedate&view2=domestic&yr=%s&p=.htm\"%(page, year))\n        pagestext[page] = r.text\n        time.sleep(1)\n    year_pagetxt[year] = pagestext\n    ",
        "matched_tutorial_code_inds": [
            1778,
            2773,
            3726,
            94,
            4683
        ],
        "matched_tutorial_codes": [
            "hidden_dim = 64\ninput_dim = emb_matrix['memory'].shape[0]\nlearning_rate = 0.001\nepochs = 10\nparameters = initialise_params(hidden_dim,\n                               input_dim)\nv, s = initialise_mav(hidden_dim,\n                      input_dim,\n                      parameters)",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "names = ['group_a', 'group_b', 'group_c']\nvalues = [1, 10, 100]\n\nplt.figure(figsize=(9, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\n\n\n<img alt=\"Categorical Plotting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_006.png\" srcset=\"../../_images/sphx_glr_pyplot_006.png, ../../_images/sphx_glr_pyplot_006_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Training the Network"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with categorical variables"
            ]
        ]
    },
    "985676": {
        "jupyter_code_cell": "kk.line_tidy(close_px['AAPL'])",
        "matched_tutorial_code_inds": [
            6402,
            4187,
            4194,
            3781,
            4061
        ],
        "matched_tutorial_codes": [
            "kpss_test(sunspots[\"SUNACTIVITY\"])",
            "sns.axes_style()\n",
            "sns.color_palette()\n",
            "sns.set(style='ticks', context='paper')",
            "sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)->KPSS test"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles"
            ],
            [
                "seaborn->User guide and tutorial->Choosing color palettes->Qualitative color palettes",
                "seaborn->Figure aesthetics->Choosing color palettes->Qualitative color palettes"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables"
            ]
        ]
    },
    "15264": {
        "jupyter_code_cell": "sns.distplot(df['temperature'], bins=20, fit=norm)",
        "matched_tutorial_code_inds": [
            4173,
            4072,
            4056,
            3966,
            4075
        ],
        "matched_tutorial_codes": [
            "sns.pairplot(iris, hue=\"species\", height=2.5)\n",
            "sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "sns.displot(tips, x=\"day\", shrink=.8)\n",
            "sns.catplot(data=flights_wide, kind=\"box\")\n",
            "sns.displot(diamonds, x=\"carat\", kde=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size",
                "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls",
                "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls"
            ]
        ]
    },
    "1425563": {
        "jupyter_code_cell": "# Creating boxplots of fare paid by passenger survival status\n\nsns.boxplot(x='Survived',y='Fare',data=titanic_df_clean,fliersize=0,palette=['r','g'])\nplt.xticks([0,1],['Died','Survived'])\nplt.ylim(0,150)\nplt.xlabel('Survival status')\nplt.ylabel('Fare paid (USD)')\nplt.title(\"Chart 3: Boxplots of fare paid by survival status (excluding outliers)\")",
        "matched_tutorial_code_inds": [
            5639,
            6538,
            1966,
            3679,
            383
        ],
        "matched_tutorial_codes": [
            "# Now create a bootstrap confidence interval around the a LOWESS fit\n\n\ndef lowess_with_confidence_bounds(\n    x, y, eval_x, N=200, conf_interval=0.95, lowess_kw=None\n):\n    \"\"\"\n    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n    \"\"\"\n    # Lowess smoothing\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, xvals=eval_x, **lowess_kw)\n\n    # Perform bootstrap resamplings of the data\n    # and  evaluate the smoothing at a fixed set of points\n    smoothed_values = np.empty((N, len(eval_x)))\n    for i in range(N):\n        sample = np.random.choice(len(x), len(x), replace=True)\n        sampled_x = x[sample]\n        sampled_y = y[sample]\n\n        smoothed_values[i] = sm.nonparametric.lowess(\n            exog=sampled_x, endog=sampled_y, xvals=eval_x, **lowess_kw\n        )\n\n    # Get the confidence interval\n    sorted_values = np.sort(smoothed_values, axis=0)\n    bound = int(N * (1 - conf_interval) / 2)\n    bottom = sorted_values[bound - 1]\n    top = sorted_values[-bound]\n\n    return smoothed, bottom, top\n\n\n# Compute the 95% confidence interval\neval_x = np.linspace(0, 4 * np.pi, 31)\nsmoothed, bottom, top = lowess_with_confidence_bounds(\n    x, y, eval_x, lowess_kw={\"frac\": 0.1}\n)",
            "# Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>",
            "# Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "# Bring in the flights data\n\nflights = pd.read_hdf('flights.h5', 'flights')\n\nweather_locs = weather.index.levels[0]\n# The `categories` attribute of a Categorical is an Index\norigin_locs = flights.origin.cat.categories\ndest_locs = flights.dest.cat.categories\n\nairports = weather_locs &amp; origin_locs &amp; dest_locs\nairports",
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Data"
            ],
            [
                "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets"
            ],
            [
                "pandas_toms_blog->Indexes->Set Operations"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ]
        ]
    },
    "543699": {
        "jupyter_code_cell": "pred = knn.predict(X_test)\nprint(confusion_matrix(y_test,pred))",
        "matched_tutorial_code_inds": [
            2328,
            4167,
            1981,
            5205,
            2369
        ],
        "matched_tutorial_codes": [
            "gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "centers = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = (\n    n_samples=300, centers=centers, cluster_std=0.5, random_state=0\n)",
            "res = sm.OLS(y, X).fit()\nprint(res.summary())",
            "coverage_fraction(\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "sklearn->Examples->Clustering->Demo of affinity propagation clustering algorithm->Generate sample data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval"
            ]
        ]
    },
    "1402926": {
        "jupyter_code_cell": "#Loading the unique Hotel's information to plot them on the map\ntemp_df = df.drop_duplicates(['Hotel_Name'])\nlen(temp_df)",
        "matched_tutorial_code_inds": [
            3857,
            3731,
            6951,
            3730,
            6811
        ],
        "matched_tutorial_codes": [
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "# Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "1144631": {
        "jupyter_code_cell": "yahoo['day_range'] = yahoo['High'] - yahoo['Low']\nyahoo['intraday_return'] = (yahoo['Close'] - yahoo['Open'])/yahoo['Open']",
        "matched_tutorial_code_inds": [
            3688,
            3973,
            3684,
            3779,
            6546
        ],
        "matched_tutorial_codes": [
            "dsm = weather.loc['DSM']\n\nhourly = dsm.resample('H').mean()\n\ntemp = hourly['tmpf'].sample(frac=.5, random_state=1).sort_index()\nsped = hourly['sped'].sample(frac=.5, random_state=2).sort_index()",
            "year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "from pandas_datareader.data import DataReader\nendog = DataReader('UNRATE', 'fred', start='1954-01-01')\nendog.index.freq = endog.index.inferred_freq"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing long-form data"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ]
        ]
    },
    "313536": {
        "jupyter_code_cell": "import gmt\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors as mcolors\n\nimport collections",
        "matched_tutorial_code_inds": [
            4021,
            4793,
            6197,
            4962,
            5191
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "from cycler import cycler\nimport numpy as np\nimport matplotlib.pyplot as plt",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nplt.rc(\"figure\", figsize=(16, 9))\nplt.rc(\"font\", size=16)",
            "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom colorspacious import cspace_converter",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Styling with cycler"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms"
            ],
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares"
            ]
        ]
    },
    "58274": {
        "jupyter_code_cell": "import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nimport operator",
        "matched_tutorial_code_inds": [
            5191,
            5464,
            6222,
            4021,
            4696
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "matplotlib->Tutorials->Introductory->Image tutorial->Startup commands"
            ]
        ]
    },
    "50548": {
        "jupyter_code_cell": "import nltk\n#list to store the stop words\nstopwords_list_570 = []\n# all the stopwords are stored in a text file\nwith open('./stopwords_en.txt') as f:\n    stopwords_list_570 = f.read().splitlines()",
        "matched_tutorial_code_inds": [
            1964,
            86,
            1061,
            2590,
            2504
        ],
        "matched_tutorial_codes": [
            "import time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, \nfrom sklearn.preprocessing import \nfrom itertools import , \n\n(0)",
            "import torch\n\nUSE_CUDA = False\n\n = (20, 30)\nif USE_CUDA:\n    ()\n\ndevice = 'cpu'\nif USE_CUDA:\n    device = 'cuda'\n = (128, 20, device=device)\nprint(().device)",
            "import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {, }, dtype=\n)\nprint(quantized_model)",
            "from sklearn.datasets import \n\nco2 = (data_id=41187, as_frame=True, parser=\"pandas\")\nco2.frame.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "from sklearn.datasets import \n\ndiabetes = ()\nX, y = diabetes.data, diabetes.target\nprint(diabetes.DESCR)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets"
            ],
            [
                "torch->PyTorch Recipes->Changing default device"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->4. Test dynamic quantization"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "sklearn->Examples->Feature Selection->Model-based and sequential feature selection->Loading the data"
            ]
        ]
    },
    "967198": {
        "jupyter_code_cell": "a_preds = model1.predict_proba(a_test)\n\nb_preds = model2.predict_proba(b_test)\n\nc_preds = model3.predict_proba(c_test)",
        "matched_tutorial_code_inds": [
            1579,
            6449,
            47,
            1757,
            2104
        ],
        "matched_tutorial_codes": [
            "training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "optimizerA = (netA.parameters(), lr=0.001, momentum=0.9)\noptimizerB = (netB.parameters(), lr=0.001, momentum=0.9)",
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA"
            ],
            [
                "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Steps->3. Initialize the optimizer"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets"
            ]
        ]
    },
    "1438081": {
        "jupyter_code_cell": "df1 = pd.DataFrame ({\n        'Age' : customers['Field age'],\n        'Customer id' : customers['Field customer id']\n    })",
        "matched_tutorial_code_inds": [
            5789,
            3731,
            3644,
            6703,
            2591
        ],
        "matched_tutorial_codes": [
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "import pandas as pd\n\nco2_data = co2.frame\nco2_data[\"date\"] = (co2_data[[\"year\", \"month\", \"day\"]])\nco2_data = co2_data[[\"date\", \"co2\"]].set_index(\"date\")\nco2_data.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ]
        ]
    },
    "402972": {
        "jupyter_code_cell": "print(\"Size before dropping dubs =\", df.size)\ndf = df[~df.index.duplicated(keep='first')]\nprint(\"Size after dropping dubs =\", df.size)",
        "matched_tutorial_code_inds": [
            3684,
            2822,
            5533,
            4153,
            3704
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ]
        ]
    },
    "334960": {
        "jupyter_code_cell": "#build the least squares model\ntoyregr_skl = linear_model.LinearRegression()\n#save regression info (parameters, etc) in results_skl\nresults_skl = toyregr_skl.fit(x_train,y_train)\n#pull the beta parameters out from results_skl\nbeta0_skl = toyregr_skl.intercept_\nbeta1_skl = toyregr_skl.coef_[0]\n\nprint(\"(beta0, beta1) = (%f, %f)\" %(beta0_skl, beta1_skl))",
        "matched_tutorial_code_inds": [
            2705,
            4891,
            3104,
            2762,
            364
        ],
        "matched_tutorial_codes": [
            "# make a copy of the previous data\nXs = X.copy()\n# make Xs sparse by replacing the values lower than 2.5 with 0s\nXs[Xs &lt; 2.5] = 0.0\n# create a copy of Xs in sparse format\nXs_sp = (Xs)\nXs_sp = Xs_sp.tocsc()\n\n# compute the proportion of non-zero coefficient in the data matrix\nprint(f\"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%\")\n\nalpha = 0.1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = ()\nsparse_lasso.fit(Xs_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(Xs, y)\nprint(f\"Dense Lasso done in  {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")",
            "# make a new figure\nfig, ax = plt.subplots()\n# add a line\n(ln,) = ax.plot(x, np.sin(x), animated=True)\n# add a frame number\nfr_number = ax.annotate(\n    \"0\",\n    (0, 1),\n    xycoords=\"axes fraction\",\n    xytext=(10, -10),\n    textcoords=\"offset points\",\n    ha=\"left\",\n    va=\"top\",\n    animated=True,\n)\nbm = BlitManager(fig.canvas, [ln, fr_number])\n# make sure our window is on the screen and drawn\nplt.show(block=False)\nplt.pause(.1)\n\nfor j in range(100):\n    # update the artists\n    ln.set_ydata(np.sin(x + (j / 100) * np.pi))\n    fr_number.set_text(\"frame: {j}\".format(j=j))\n    # tell the blitting manager to do its thing\n    bm.update()\n\n\n<img alt=\"blitting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_blitting_002.png\" srcset=\"../../_images/sphx_glr_blitting_002.png, ../../_images/sphx_glr_blitting_002_2_0x.png 2.0x\"/>",
            "# range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# create grid of images\nimg_grid = torchvision.utils.make_grid(images)\n\n# show images\nmatplotlib_imshow(img_grid, one_channel=True)\n\n# write to tensorboard\nwriter.add_image('four_fashion_mnist_images', img_grid)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Sparse data"
            ],
            [
                "matplotlib->Tutorials->Advanced->Faster rendering by using blitting->Class-based example"
            ],
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->2. Writing to TensorBoard"
            ]
        ]
    },
    "1421575": {
        "jupyter_code_cell": "dfweekdaytot = dftot\ndfweekdaytot['Day'] = dftot.index.weekday",
        "matched_tutorial_code_inds": [
            6911,
            7002,
            3641,
            2309,
            5544
        ],
        "matched_tutorial_codes": [
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "data.endog.index = dates\nendog = data.endog\nendog",
            "f = pd.DataFrame({'a':[1,2,3,4,5], 'b':[10,20,30,40,50]})\nf",
            "diabetes = ()\nX, y = diabetes.data, diabetes.target",
            "nobs = glm_mod.nobs\ny = glm_mod.model.endog\nyhat = glm_mod.mu"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->SettingWithCopy"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Load the data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ]
        ]
    },
    "287224": {
        "jupyter_code_cell": "fig , ax = plt.subplots(figsize=(12,5))\nax.bar(range(x.shape[0]), absolute_deviation)\nax.set_title(\"Absolute deviation in trip time from tracks & trackspoints\")\n#ax.set_xlim((min(absolute_deviation), max(absolute_deviation)))\nplt.show();",
        "matched_tutorial_code_inds": [
            5494,
            5298,
            5496,
            4778,
            3730
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.cdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.cdf(support), label=\"Probit\")\nax.legend()",
            "fig, ax = plt.subplots(figsize=(13, 3))\n\nax.plot(m2_ewma, label=\"M2 Growth (EWMA)\")\nax.plot(cpi_ewma, label=\"CPI Inflation (EWMA)\")\nax.legend()",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.pdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.pdf(support), label=\"Probit\")\nax.legend()",
            "fig, ax = plt.subplots()\nline_up, = ax.plot([1, 2, 3], label='Line 2')\nline_down, = ax.plot([3, 2, 1], label='Line 1')\nax.legend(handles=[line_up, line_down])",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Exercise: Logit vs Probit"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Exercise: Logit vs Probit"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Legend guide->Controlling the legend entries"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ]
        ]
    },
    "389299": {
        "jupyter_code_cell": "dayMonth = df.groupby(by=['Dayofweek','Month']).count()['Reason'].unstack()\ndayMonth.head()",
        "matched_tutorial_code_inds": [
            6877,
            3855,
            3638,
            3644,
            3636
        ],
        "matched_tutorial_codes": [
            "df = dta.data[[\"Lottery\", \"Literacy\", \"Wealth\", \"Region\"]].dropna()\ndf.head()",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Formulas->OLS regression using formulas"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ]
        ]
    },
    "1226615": {
        "jupyter_code_cell": "theta = np.radians(150)\nc, s = np.cos(theta), np.sin(theta)\nr = np.array([[c, -s], [s, c]])\npos = r.dot(pos.T).T",
        "matched_tutorial_code_inds": [
            6266,
            4794,
            4403,
            5821,
            4540
        ],
        "matched_tutorial_codes": [
            "arparams = np.array([1, 0.35, -0.15, 0.55, 0.1])\nmaparams = np.array([1, 0.65])\narma_t = ArmaProcess(arparams, maparams)\narma_t.isstationary",
            "x = np.linspace(0, 2 * np.pi, 50)\noffsets = np.linspace(0, 2 * np.pi, 4, endpoint=False)\nyy = np.transpose([np.sin(x + phi) for phi in offsets])",
            "t = np.linspace(-10, 10, 20)\n y = 1 + t + 0.01*t**2\n yconst = signal.detrend(y, type='constant')\n ylin = signal.detrend(y, type='linear')",
            "t = 1.345\nsupport = np.linspace(-3 * t, 3 * t, 1000)\nhuber = norms.HuberT(t=t)\nplot_weights(support, huber.weights, [\"-3*t\", \"0\", \"3*t\"], [-3 * t, 0, 3 * t])",
            "x1 = np.array([-7, -5, 1, 4, 5], dtype=np.float64)\n kde1 = stats.gaussian_kde(x1)\n kde2 = stats.gaussian_kde(x1, bw_method='silverman')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Styling with cycler"
            ],
            [
                "scipy->Signal Processing (scipy.signal)->Detrend"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Huber\u2019s t"
            ],
            [
                "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation"
            ]
        ]
    },
    "548289": {
        "jupyter_code_cell": "scipy.stats.ttest_1samp(temp, 98.6, axis=0)",
        "matched_tutorial_code_inds": [
            5871,
            5492,
            1464,
            4211,
            4210
        ],
        "matched_tutorial_codes": [
            "sm.robust.mad(fat_tails, c=stats.t(6).ppf(0.75))",
            "affair_mod.model.cdf(affair_mod.fittedvalues[1000])",
            "np.savez(\"x_y-squared.npz\", x_axis=x, y_axis=y)",
            "sns.cubehelix_palette(start=.5, rot=-.75, as_cmap=True)\n",
            "sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Save your arrays with NumPy\u2019s"
            ],
            [
                "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes",
                "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes"
            ],
            [
                "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes",
                "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes"
            ]
        ]
    },
    "991195": {
        "jupyter_code_cell": "release_dates.head()\npd.Series(pd.DatetimeIndex(pd.merge(\\\n    cast.loc[cast.name == 'Tom Cruise'].loc[:,['title']],\\\n    release_dates.loc[release_dates.country == 'USA'],\\\n    on='title').date).month).value_counts().sort_index().plot.bar()",
        "matched_tutorial_code_inds": [
            929,
            6821,
            3589,
            2112,
            3932
        ],
        "matched_tutorial_codes": [
            "find_package(OpenCV REQUIRED)\nadd_library(warp_perspective SHARED op.cpp)\ntarget_compile_features(warp_perspective PRIVATE cxx_range_for)\ntarget_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)",
            "make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "digits.images[0]\narray([[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.],\n       [  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.],\n       [  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.],\n       [  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.],\n       [  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.],\n       [  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.],\n       [  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.],\n       [  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]])",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in C++"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "sklearn->Tutorials->An introduction to machine learning with scikit-learn->Loading an example dataset"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ]
        ]
    },
    "225031": {
        "jupyter_code_cell": "sec_api = project_helper.SecAPI()",
        "matched_tutorial_code_inds": [
            1758,
            4189,
            3967,
            6813,
            3723
        ],
        "matched_tutorial_codes": [
            "textproc = TextPreprocess()",
            "sns.set_theme()\n",
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "flights.dep_time.head()"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Scaling plot elements",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Scaling plot elements"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ]
        ]
    },
    "836642": {
        "jupyter_code_cell": "import pandas as pd #Python's data manipulation library\nimport matplotlib.pyplot as plt #Python's graphing library\nfrom mpl_toolkits.basemap import Basemap #The geomapping library built ontop of matplotlib",
        "matched_tutorial_code_inds": [
            5058,
            6376,
            7,
            2053,
            6896
        ],
        "matched_tutorial_codes": [
            "import mpl_toolkits.axisartist as AA\nfig = plt.figure()\nfig.add_axes([0.1, 0.1, 0.8, 0.8], axes_class=AA.Axes)",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "torch->PyTorch Recipes->Loading data in PyTorch->5. [Optional] Visualize the data"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction"
            ]
        ]
    },
    "391995": {
        "jupyter_code_cell": "plt.figure(figsize=(10,6))\ndf[df['credit.policy'] == 0]['fico'].hist(bins=30, alpha=0.5,label='Credit.Policy=0', color='blue')\ndf[df['credit.policy'] == 1]['fico'].hist(bins=30, alpha=0.5, label='Credit.Policy=1', color='red')\nplt.legend()\nplt.xlabel('FICO')",
        "matched_tutorial_code_inds": [
            5995,
            405,
            556,
            4719,
            4823
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(6, 6))\nsymbols = [\"D\", \"^\"]\ncolors = [\"r\", \"g\", \"blue\"]\nfactor_groups = salary_table.groupby([\"E\", \"M\"])\nfor values, group in factor_groups:\n    i, j = values\n    plt.scatter(group[\"X\"], group[\"S\"], marker=symbols[j], color=colors[i - 1], s=144)\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
            "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
            "plt.rcParams.update({'figure.autolayout': True})\n\nfig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_006.png\" srcset=\"../../_images/sphx_glr_lifecycle_006.png, ../../_images/sphx_glr_lifecycle_006_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon"
            ],
            [
                "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Results"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ]
        ]
    },
    "1492338": {
        "jupyter_code_cell": "# Apply PCA with two components\npca = PCA(n_components=2, random_state=2)\n\n# Fit and transform the scaled data\nreduced_data = pca.fit_transform(X_train_scaled)\n# Fit and transform the sample data\nreduced_samples = pca.transform(samples_scaled)\n\n# Create a DataFrame for the reduced data\nreduced_data_df = pd.DataFrame(reduced_data, columns = [\"PCA 1\", \"PCA 2\"])",
        "matched_tutorial_code_inds": [
            6625,
            6628,
            5622,
            5637,
            633
        ],
        "matched_tutorial_codes": [
            "# Construct a local level model for inflation\nmod = sm.tsa.UnobservedComponents(dta.infl, 'llevel')\n\n# Fit the model's parameters (sigma2_varepsilon and sigma2_eta)\n# via maximum likelihood\nres = mod.fit()\nprint(res.params)\n\n# Create simulation smoother objects\nsim_kfs = mod.simulation_smoother()              # default method is KFS\nsim_cfa = mod.simulation_smoother(method='cfa')  # can specify CFA method",
            "# Plot the inflation data along with simulated trends\nfig, axes = plt.subplots(2, figsize=(15, 6))\n\n# Plot data and KFS simulations\ndta.infl.plot(ax=axes[0], color='k')\naxes[0].set_title('Simulations based on KFS approach, MLE parameters')\nsimulated_state_kfs.plot(ax=axes[0], color='C0', alpha=0.25, legend=False)\n\n# Plot data and CFA simulations\ndta.infl.plot(ax=axes[1], color='k')\naxes[1].set_title('Simulations based on CFA approach, MLE parameters')\nsimulated_state_cfa.plot(ax=axes[1], color='C0', alpha=0.25, legend=False)\n\n# Add a legend, clean up layout\nhandles, labels = axes[0].get_legend_handles_labels()\naxes[0].legend(handles[:2], ['Data', 'Simulated state'])\nfig.tight_layout();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\"/>",
            "# Create three equidistant points\ndata = np.linspace(-1, 1, 3)\nkde = sm.nonparametric.KDEUnivariate(data)\n\n# Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, kernel in enumerate(kernel_switch.keys()):\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(kernel))\n\n    # Fit the model (estimate densities)\n    kde.fit(kernel=kernel, fft=False, gridsize=2 ** 10)\n\n    # Create the plot\n    ax.plot(kde.support, kde.density, lw=3, label=\"KDE from samples\", zorder=10)\n    ax.scatter(data, np.zeros_like(data), marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim([-3, 3])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\"/>",
            "# Generate data looking like cosine\nx = np.random.uniform(0, 4 * np.pi, size=200)\ny = np.cos(x) + np.random.random(size=len(x))\n\n# Compute a lowess smoothing of the data\nsmoothed = sm.nonparametric.lowess(exog=x, endog=y, frac=0.2)",
            "# Input to the model\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\ntorch_out = torch_model(x)\n\n# Export the model\n(torch_model,               # model being run\n                  x,                         # model input (or a tuple for multiple inputs)\n                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=10,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                                'output' : {0 : 'batch_size'}})"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Local level model"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Local level model"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions on three data points"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ]
        ]
    },
    "413568": {
        "jupyter_code_cell": "time_to_fund = (loans_df.funded_time - loans_df.posted_time)\ntime_to_fund_in_days = (time_to_fund.astype('timedelta64[s]')/(3600 * 24))\nloans_df = loans_df.assign(time_to_fund=time_to_fund)\nloans_df = loans_df.assign(time_to_fund_in_days=time_to_fund_in_days)\n",
        "matched_tutorial_code_inds": [
            1634,
            3255,
            1701,
            2462,
            1103
        ],
        "matched_tutorial_codes": [
            "pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "t_stat_uncorrected = (differences) / ((differences, ddof=1) / n)\np_val_uncorrected = .sf(np.abs(t_stat_uncorrected), df)\n\nprint(\n    f\"Uncorrected t-value: {t_stat_uncorrected:.3f}\\n\"\n    f\"Uncorrected p-value: {p_val_uncorrected:.3f}\"\n)",
            "pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "hour_df = (\n    (0, 26, 1000).reshape(-1, 1),\n    columns=[\"hour\"],\n)\nsplines = periodic_spline_transformer(24, n_splines=12).fit_transform(hour_df)\nsplines_df = (\n    splines,\n    columns=[f\"spline_{i}\" for i in range(splines.shape[1])],\n)\n([hour_df, splines_df], axis=\"columns\").plot(x=\"hour\", cmap=plt.cm.tab20b)\n_ = (\"Periodic spline-based encoding for the 'hour' feature\")\n\n\n<img alt=\"Periodic spline-based encoding for the 'hour' feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\"/>",
            "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\nper_channel_quantized_model.eval()\nper_channel_quantized_model.fuse_model()\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nprint(per_channel_quantized_model.qconfig)\n\ntorch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\ntorch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: frequentist approach"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Periodic spline features"
            ],
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->4. Post-training static quantization"
            ]
        ]
    },
    "197186": {
        "jupyter_code_cell": "from sklearn.cluster import spectral_clustering\n\nobj = spectral_clustering(np.matrix(A.data), 5)",
        "matched_tutorial_code_inds": [
            3204,
            2826,
            2719,
            5517,
            3457
        ],
        "matched_tutorial_codes": [
            "from sklearn.model_selection import \n\ny_pred = (lr, X, y, cv=10)",
            "from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, random_state=42)",
            "from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5)",
            "from statsmodels.formula.api import glm\n\nglm_mod = glm(formula, dta, family=sm.families.Binomial()).fit()",
            "from scipy import stats\n\npred_entropies = (lp_model.label_distributions_.T)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Plotting Cross-Validated Predictions",
                "sklearn->Examples->Model Selection->Plotting Learning Curves and Checking Models' Scalability"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Non-negative least squares"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Plot the most uncertain predictions"
            ]
        ]
    },
    "304587": {
        "jupyter_code_cell": "monthlyrain.to_csv('sample_data/rotterdam_monthly_rainfall_2012.csv')\nmonthlyrain.to_csv('sample_data/rotterdam_monthly_rainfall_2012.txt')\n\n\nmonthlyrain.to_excel('sample_data/rotterdam_monthly_rainfall_2012.xls')\n",
        "matched_tutorial_code_inds": [
            3704,
            162,
            1714,
            2822,
            3684
        ],
        "matched_tutorial_codes": [
            "m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ]
        ]
    },
    "343435": {
        "jupyter_code_cell": "plt.subplots(figsize=(10,7))\nsns.heatmap(df1.corr(), annot=True, cmap='viridis')",
        "matched_tutorial_code_inds": [
            6311,
            6303,
            3822,
            4761,
            5401
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "ax = fig.add_subplot()\nrect = ax.patch  # a Rectangle instance\nrect.set_facecolor('green')",
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical."
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Axes container"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ]
        ]
    },
    "1282212": {
        "jupyter_code_cell": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.style.use('ggplot')",
        "matched_tutorial_code_inds": [
            5191,
            2105,
            5464,
            4021,
            6222
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.tree import \nfrom sklearn import tree",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data"
            ]
        ]
    },
    "934165": {
        "jupyter_code_cell": "del movies['color']\ndel movies['num_critic_for_reviews']\ndel movies['director_facebook_likes']\ndel movies['actor_3_facebook_likes']\ndel movies['actor_1_facebook_likes']\ndel movies['genres']\ndel movies['num_user_for_reviews']\ndel movies['language']\ndel movies['country']\ndel movies['actor_2_facebook_likes']\ndel movies['aspect_ratio']\ndel movies['movie_facebook_likes']\ndel movies['num_voted_users']\ndel movies['cast_total_facebook_likes']\ndel movies['plot_keywords']\ndel movies['movie_imdb_link']\nmovies.head()",
        "matched_tutorial_code_inds": [
            1169,
            1171,
            6361,
            6359,
            1047
        ],
        "matched_tutorial_codes": [
            "eager eval time 0: 0.009543840408325194\neager eval time 1: 0.008958080291748046\neager eval time 2: 0.0089717435836792\neager eval time 3: 0.008945695877075195\neager eval time 4: 0.008947936058044434\neager eval time 5: 0.008927264213562013\neager eval time 6: 0.008937503814697266\neager eval time 7: 0.008948351860046387\neager eval time 8: 0.008941663742065429\neager eval time 9: 0.00894159984588623\n~~~~~~~~~~\ncompile eval time 0: 0.009375519752502441\ncompile eval time 1: 0.009378111839294434\ncompile eval time 2: 0.009247039794921875\ncompile eval time 3: 0.009245856285095215\ncompile eval time 4: 0.008823455810546875\ncompile eval time 5: 0.00714246416091919\ncompile eval time 6: 0.007155712127685547\ncompile eval time 7: 0.007184415817260742\ncompile eval time 8: 0.007147552013397217\ncompile eval time 9: 0.007140128135681152\n~~~~~~~~~~\n(eval) eager median: 0.008946815967559814, compile median: 0.00800393581390381, speedup: 1.1178020633321555x\n~~~~~~~~~~",
            "eager train time 0: 0.4737531127929687\neager train time 1: 0.02575574493408203\neager train time 2: 0.025766271591186524\neager train time 3: 0.02569219207763672\neager train time 4: 0.02566761589050293\neager train time 5: 0.026013599395751954\neager train time 6: 0.025726463317871092\neager train time 7: 0.025708160400390624\neager train time 8: 0.025737247467041015\neager train time 9: 0.02180371284484863\n~~~~~~~~~~\ncompile train time 0: 22.97644140625\ncompile train time 1: 0.02164531135559082\ncompile train time 2: 0.02067158317565918\ncompile train time 3: 0.02125984001159668\ncompile train time 4: 0.021174720764160156\ncompile train time 5: 0.020945056915283203\ncompile train time 6: 0.021309440612792968\ncompile train time 7: 0.021041824340820314\ncompile train time 8: 0.020995264053344728\ncompile train time 9: 0.021283327102661134\n~~~~~~~~~~\n(train) eager median: 0.02573185539245605, compile median: 0.02121728038787842, speedup: 1.2127782129493299x\n~~~~~~~~~~",
            "fit1 = Holt(air, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast1 = fit1.forecast(5).rename(\"Holt's linear trend\")\nfit2 = Holt(air, exponential=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast2 = fit2.forecast(5).rename(\"Exponential trend\")\nfit3 = Holt(air, damped_trend=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2\n)\nfcast3 = fit3.forecast(5).rename(\"Additive damped trend\")\n\nplt.figure(figsize=(12, 8))\nplt.plot(air, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n        )\n        / float(\n            .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n        )\n    )\n)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "torch->Model Optimization->Pruning Tutorial->Global pruning"
            ]
        ]
    },
    "1212139": {
        "jupyter_code_cell": "wellness_seg.head()",
        "matched_tutorial_code_inds": [
            3723,
            5600,
            1500,
            6458,
            4183
        ],
        "matched_tutorial_codes": [
            "flights.dep_time.head()",
            "resf_logit.predict()",
            "china_mask.nonzero()",
            "arima_res.predict(0, 2)",
            "sinplot()\nsns.despine()\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ]
        ]
    },
    "1363790": {
        "jupyter_code_cell": "scatter_compare(subdf[good]);",
        "matched_tutorial_code_inds": [
            3861,
            3865,
            4131,
            4132,
            4145
        ],
        "matched_tutorial_codes": [
            "sns.heatmap(X.corr());",
            "tsplot(res_trend.resid, lags=36);",
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips);\n",
            "sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_jitter=.05);\n",
            "sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips);\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->Autocorrelation"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models",
                "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models",
                "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Conditioning on other variables",
                "seaborn->Statistical operations->Estimating regression fits->Conditioning on other variables"
            ]
        ]
    },
    "439350": {
        "jupyter_code_cell": "dataFile = \"AdultCensusIncome.csv\"\nimport os, urllib\nif not os.path.isfile(dataFile):\n    urllib.request.urlretrieve(\"https://mmlspark.azureedge.net/datasets/\"+dataFile, dataFile)\ndata = spark.createDataFrame(pd.read_csv(dataFile, dtype={\" hours-per-week\": np.float64}))\ndata = data.select([\" education\", \" marital-status\", \" hours-per-week\", \" income\"])\ntrain, test = data.randomSplit([0.75, 0.25], seed=123)\ntrain.limit(10).toPandas()",
        "matched_tutorial_code_inds": [
            1100,
            6998,
            6165,
            3932,
            5319
        ],
        "matched_tutorial_codes": [
            "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "exog_vars = [\"Mkt-RF\", \"SMB\", \"HML\"]\nexog = sm.add_constant(factors[exog_vars])\nrols = RollingOLS(endog, exog, window=60)\nrres = rols.fit()\nfig = rres.plot_recursive_coefficient(variables=exog_vars, figsize=(14, 18))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ]
        ]
    },
    "289985": {
        "jupyter_code_cell": "uber_pred = model.predict(X_test) # Predicting the features of the dataset",
        "matched_tutorial_code_inds": [
            6458,
            5570,
            6464,
            5600,
            6973
        ],
        "matched_tutorial_codes": [
            "arima_res.predict(0, 2)",
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "sarima_res.predict(0, 2)",
            "resf_logit.predict()",
            "sm_probit_canned = sm.Probit(endog, exog).fit()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ]
        ]
    },
    "931479": {
        "jupyter_code_cell": "# Train classifier 1\ntrain_forest = forest.fit(X_1, y_1_train)\n# Obtain predictions\npredict_1 = train_forest.predict(X_test)\n\n# Train classifier 2\ntrain_forest = forest.fit(X_2, y_2_train)\n# Obtain predictions\npredict_2 = train_forest.predict(X_test)\n\n# Train classifier 3\ntrain_forest = forest.fit(X_3, y_3_train)\n# Obtain predictions\npredict_3 = train_forest.predict(X_test)\n\n# Train classifier 4\ntrain_forest = forest.fit(X_4, y_4_train)\n# Obtain predictions\npredict_4 = train_forest.predict(X_test)\n\n# Train classifier 5\ntrain_forest = forest.fit(X_5, y_5_train)\n# Obtain predictions\npredict_5 = train_forest.predict(X_test)",
        "matched_tutorial_code_inds": [
            1611,
            6930,
            6540,
            6934,
            37
        ],
        "matched_tutorial_codes": [
            "# The training set metrics.\ny_training_error = [\n    store_training_loss[i] / float(len(training_images))\n    for i in range(len(store_training_loss))\n]\nx_training_error = range(1, len(store_training_loss) + 1)\ny_training_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_training_accuracy = range(1, len(store_training_accurate_pred) + 1)\n\n# The test set metrics.\ny_test_error = [\n    store_test_loss[i] / float(len(test_images)) for i in range(len(store_test_loss))\n]\nx_test_error = range(1, len(store_test_loss) + 1)\ny_test_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_test_accuracy = range(1, len(store_test_accurate_pred) + 1)\n\n# Display the plots.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\naxes[0].set_title(\"Training set error, accuracy\")\naxes[0].plot(x_training_accuracy, y_training_accuracy, label=\"Training set accuracy\")\naxes[0].plot(x_training_error, y_training_error, label=\"Training set error\")\naxes[0].set_xlabel(\"Epochs\")\naxes[1].set_title(\"Test set error, accuracy\")\naxes[1].plot(x_test_accuracy, y_test_accuracy, label=\"Test set accuracy\")\naxes[1].plot(x_test_error, y_test_error, label=\"Test set error\")\naxes[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\" src=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\"/>",
            "# Step 2: produce one-step-ahead forecasts\nfcast = training_res.forecast()\n\n# Step 3: compute root mean square forecasting error\ntrue = endog.reindex(fcast.index)\nerror = true - fcast\n\n# Print out the results\nprint(pd.concat([true.rename('true'),\n                 fcast.rename('forecast'),\n                 error.rename('error')], axis=1))",
            "# Output\noutput_mod = sm.tsa.UnobservedComponents(dta['US GNP'], **unrestricted_model)\noutput_res = output_mod.fit(method='powell', disp=False)\n\n# Prices\nprices_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **unrestricted_model)\nprices_res = prices_mod.fit(method='powell', disp=False)\n\nprices_restricted_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **restricted_model)\nprices_restricted_res = prices_restricted_mod.fit(method='powell', disp=False)\n\n# Money\nmoney_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **unrestricted_model)\nmoney_res = money_mod.fit(method='powell', disp=False)\n\nmoney_restricted_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **restricted_model)\nmoney_restricted_res = money_restricted_mod.fit(method='powell', disp=False)",
            "# Step 2: produce one-step-ahead forecasts\nfcast = append_res.forecast()\n\n# Step 3: compute root mean square forecasting error\ntrue = endog.reindex(fcast.index)\nerror = true - fcast\n\n# Print out the results\nprint(pd.concat([true.rename('true'),\n                 fcast.rename('forecast'),\n                 error.rename('error')], axis=1))",
            "# run the float model\nout1, hidden1 = float_lstm(inputs, hidden)\nmag1 = (abs(out1)).item()\nprint('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n\n# run the quantized model\nout2, hidden2 = quantized_lstm(inputs, hidden)\nmag2 = (abs(out2)).item()\nprint('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n\n# compare them\nmag3 = (abs(out1-out2)).item()\nprint('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Compose the model and begin training and testing it"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Model"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->5: Look at Accuracy"
            ]
        ]
    },
    "1272029": {
        "jupyter_code_cell": "data = pd.read_csv('../datasets/santander.csv')\n\ndata.shape",
        "matched_tutorial_code_inds": [
            3967,
            7002,
            3960,
            3830,
            6911
        ],
        "matched_tutorial_codes": [
            "anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "data.endog.index = dates\nendog = data.endog\nendog",
            "flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ]
        ]
    },
    "928393": {
        "jupyter_code_cell": "US_coeff = 0.0408\nUK_coeff = 0.0506\n\nprint('Exponentiating the US coefficient {} yields {:.3f}'.\\\n     format(US_coeff, np.exp(US_coeff)))\nprint('Exponentiating the UK coefficient {} yields {:.3f}'.\\\n     format(UK_coeff, np.exp(UK_coeff)))",
        "matched_tutorial_code_inds": [
            3478,
            4955,
            4956,
            6016,
            1760
        ],
        "matched_tutorial_codes": [
            "C_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = (C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))",
            "N = 100\nX, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (Z1 - Z2) * 2\n\nfig, ax = plt.subplots(2, 1)\n\npcm = ax[0].pcolormesh(X, Y, Z,\n                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                       cmap='RdBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='both')\n\npcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z), shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='both')\nplt.show()\n\n\n<img alt=\"colormapnorms\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_003.png\" srcset=\"../../_images/sphx_glr_colormapnorms_003.png, ../../_images/sphx_glr_colormapnorms_003_2_0x.png 2.0x\"/>",
            "N = 100\nX, Y = np.mgrid[0:3:complex(0, N), 0:2:complex(0, N)]\nZ1 = (1 + np.sin(Y * 10.)) * X**2\n\nfig, ax = plt.subplots(2, 1, constrained_layout=True)\n\npcm = ax[0].pcolormesh(X, Y, Z1, norm=colors.PowerNorm(gamma=0.5),\n                       cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='max')\nax[0].set_title('PowerNorm()')\n\npcm = ax[1].pcolormesh(X, Y, Z1, cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='max')\nax[1].set_title('Normalize()')\nplt.show()\n\n\n<img alt=\"PowerNorm(), Normalize()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_004.png\" srcset=\"../../_images/sphx_glr_colormapnorms_004.png, ../../_images/sphx_glr_colormapnorms_004_2_0x.png 2.0x\"/>",
            "infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "X_train = textproc.cleantext(train_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:2000]\n\nX_test = textproc.cleantext(test_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:1000]\n\ny_train = train_df['sentiment'].to_numpy()[0:2000]\ny_test = test_df['sentiment'].to_numpy()[0:1000]"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Support Vector Machines->RBF SVM parameters->Train classifiers"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Symmetric logarithmic"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Power-law"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ]
        ]
    },
    "1418017": {
        "jupyter_code_cell": "hom_var, hom_lags = \"CASES\", [1, 2, 6]\nhom_ts = pd.read_csv(\"./DBs/Hom/HomCDMX4RM.csv\", parse_dates=[\"TMS\"], index_col=\"TMS\")[[hom_var]]\nhom_gt = gt_ins.load(\"./DBs/Hom/HomCDMXHorariosSeM2004-2018.csv\", var=\"GI\")\nhom_dr = DataRegularizer(data=hom_ts, gt=hom_gt, lags=hom_lags, var_list=[], tf=(\"2009-01\", \"2017-01\"))\nhom_dr.proceed()\nhom_lm = GTModelComparison(data=hom_dr.df, train=hom_dr.train, gt_vars=hom_dr.gt_vars)\nhom_lm.run_all()",
        "matched_tutorial_code_inds": [
            4690,
            4636,
            5222,
            4666,
            1761
        ],
        "matched_tutorial_codes": [
            "mu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, facecolor='g', alpha=0.75)\n\n\nplt.xlabel('Smarts')\nplt.ylabel('Probability')\nplt.title('Histogram of IQ')\nplt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\nplt.axis([40, 160, 0, 0.03])\nplt.grid(True)\nplt.show()\n\n\n<img alt=\"Histogram of IQ\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_008.png\" srcset=\"../../_images/sphx_glr_pyplot_008.png, ../../_images/sphx_glr_pyplot_008_2_0x.png 2.0x\"/>",
            "mat_contents = sio.loadmat('octave_a.mat')\n mat_contents\n{'a': array([[[  1.,   4.,   7.,  10.],\n        [  2.,   5.,   8.,  11.],\n        [  3.,   6.,   9.,  12.]]]),\n '__version__': '1.0',\n '__header__': 'MATLAB 5.0 MAT-file, written by\n Octave 3.6.3, 2013-02-17 21:02:11 UTC',\n '__globals__': []}\n oct_a = mat_contents['a']\n oct_a\narray([[[  1.,   4.,   7.,  10.],\n        [  2.,   5.,   8.,  11.],\n        [  3.,   6.,   9.,  12.]]])\n oct_a.shape\n(1, 3, 4)",
            "pred_ols2 = res2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y, \"o\", label=\"Data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\nax.plot(x, res2.fittedvalues, \"r--.\", label=\"Predicted\")\nax.plot(x, iv_u, \"r--\")\nax.plot(x, iv_l, \"r--\")\nlegend = ax.legend(loc=\"best\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ols_26_0.png\" src=\"../../../_images/examples_notebooks_generated_ols_26_0.png\"/>",
            "mu, sigma = 115, 15\nx = mu + sigma * np.random.randn(10000)\nfig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\n# the histogram of the data\nn, bins, patches = ax.hist(x, 50, density=True, facecolor='C0', alpha=0.75)\n\nax.set_xlabel('Length [cm]')\nax.set_ylabel('Probability')\nax.set_title('Aardvark lengths\\n (not really)')\nax.text(75, .025, r'$\\mu=115,\\ \\sigma=15$')\nax.axis([55, 175, 0, 0.03])\nax.grid(True)\n\n\n<img alt=\"Aardvark lengths  (not really)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_009.png\" srcset=\"../../_images/sphx_glr_quick_start_009.png, ../../_images/sphx_glr_quick_start_009_2_0x.png 2.0x\"/>",
            "speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with text"
            ],
            [
                "scipy->File IO (scipy.io)->MATLAB files->How do I start?"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Labelling plots->Axes labels and text"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ]
        ]
    },
    "286983": {
        "jupyter_code_cell": "Image(filename=\"geomagia_refs.png\")",
        "matched_tutorial_code_inds": [
            6028,
            4744,
            2547,
            4134,
            1414
        ],
        "matched_tutorial_codes": [
            "Text(0, 0.5, 'JPERF')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_39_2.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_39_2.png\"/>",
            "ani.save(filename=\"/tmp/pillow_example.gif\", writer=\"pillow\")\nani.save(filename=\"/tmp/pillow_example.apng\", writer=\"pillow\")",
            "GaussianMixture()\n\n<br/>\n<br/>",
            "anscombe = sns.load_dataset(\"anscombe\")\n",
            "plt.imshow(img)\nplt.show()\n\n\n\n\n<img alt=\"../_images/0802995ab45723028c194f181d4e07f2e90f6c49c7f49d85ee3de0f218782122.png\" src=\"../_images/0802995ab45723028c194f181d4e07f2e90f6c49c7f49d85ee3de0f218782122.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data"
            ],
            [
                "matplotlib->Tutorials->Introductory->Animations using Matplotlib->Animation Writers->Saving Animations"
            ],
            [
                "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Model training and selection"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Fitting different kinds of models",
                "seaborn->Statistical operations->Estimating regression fits->Fitting different kinds of models"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content"
            ]
        ]
    },
    "452562": {
        "jupyter_code_cell": "# The correlations among the actual, predicted and residual is as follows.\n\ncomb[['gen_tot','losses2', 'losses2_pred_gen_total', 'losses2_resid_gen_total']].corr()",
        "matched_tutorial_code_inds": [
            6942,
            1567,
            6884,
            6735,
            2845
        ],
        "matched_tutorial_codes": [
            "# Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "# Display the label of the 60,000th image (indexed at 59,999) from the training set.\ny_train[59999]",
            "res1 = ols(formula=\"Lottery ~ Literacy : Wealth - 1\", data=df).fit()\nres2 = ols(formula=\"Lottery ~ Literacy * Wealth - 1\", data=df).fit()\nprint(res1.params, \"\\n\")\nprint(res2.params)",
            "# Set sampling params\nndraws = 3000  #  3000 number of draws from the distribution\nnburn = 600  # 600 number of \"burn-in points\" (which will be discarded)",
            "feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example",
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->Multiplicative interactions"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->Bayesian estimation"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "1408963": {
        "jupyter_code_cell": "cm = confusion_matrix(y_deploy, y_pred_HG)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix, without normalization')\nprint(cm)\nplt.figure(1)\nplt.subplot(2,2,1)\nplot_confusion_matrix(cm)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nprint('Normalized confusion matrix')\nprint(cm_normalized)\n#plt.figure()\nplt.subplot(2,2,2)\nplot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\nplt.show()",
        "matched_tutorial_code_inds": [
            2132,
            6007,
            4682,
            2888,
            5372
        ],
        "matched_tutorial_codes": [
            "fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Lasso model, optimum regularization, normalized variables\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Lasso model, optimum regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\"/>",
            "vc = {\"g1\": \"0 + C(group1)\", \"g2\": \"0 + C(group2)\"}\noo = np.ones(df.shape[0])\nmodel3 = sm.MixedLM.from_formula(\"y ~ 1\", groups=oo, vc_formula=vc, data=df)\nresult3 = model3.fit()\nprint(result3.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ]
        ]
    },
    "842247": {
        "jupyter_code_cell": "# build a dataframe of cases in which this is not null\ndf_ineq = df_states[~df_states['incshare_top1'].isnull()]\n# check that it worked\nany(df_ineq['incshare_top1'].isnull())",
        "matched_tutorial_code_inds": [
            6703,
            6434,
            6597,
            6321,
            6892
        ],
        "matched_tutorial_codes": [
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "# Assign better names for our seasonal terms\ntrue_seasonal_10_3 = terms[0]\ntrue_seasonal_100_2 = terms[1]\ntrue_sum = true_seasonal_10_3 + true_seasonal_100_2\n<br/>",
            "# Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()",
            "f = \"Lottery ~ Literacy * Wealth\"\ny, X = patsy.dmatrices(f, df, return_type=\"dataframe\")\nprint(y[:5])\nprint(X[:5])"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable"
            ],
            [
                "statsmodels->Examples->User Notes->Formulas->Using formulas with models that do not (yet) support them"
            ]
        ]
    },
    "852877": {
        "jupyter_code_cell": "null_rows = wrangle_clean[pd.isnull(wrangle_clean).any(axis=1)]\nnull_rows",
        "matched_tutorial_code_inds": [
            1495,
            6911,
            653,
            5570,
            3892
        ],
        "matched_tutorial_codes": [
            "china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "traced_model = (model)\nprint(traced_model.graph)",
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ]
        ]
    },
    "307032": {
        "jupyter_code_cell": "# Create a DataFrame from a dictionary.\nd = pd.DataFrame({'capital':['Montgomery', 'Juneau', 'Phoenix'], 'state':['AL', 'AK', 'AZ']})\nd.head(2)",
        "matched_tutorial_code_inds": [
            6423,
            6951,
            6828,
            3730,
            6357
        ],
        "matched_tutorial_codes": [
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "# Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ]
        ]
    },
    "42295": {
        "jupyter_code_cell": "slact = ROOT.TChain('slacAnalyzer/eventTree')\nfor run_num in range(1930, 1939):\n    slact.Add('../rootOutputs/gm2slac_run0{}.root'.format(run_num))\n\n# we are only going to use the CrystalHits\nslact.SetBranchStatus(\"*\",0)\nfor used_branch in ['EventNum', 'IslandNum', 'XtalNum', 'Energy', 'Time']:\n    slact.SetBranchStatus('XtalHit_' + used_branch)\n\nslact.GetEntries()",
        "matched_tutorial_code_inds": [
            6022,
            3266,
            3002,
            3004,
            23
        ],
        "matched_tutorial_codes": [
            "lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "pairwise_bayesian = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_post = (\n        df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n    )\n    worse_prob = t_post.cdf(rope_interval[0])\n    better_prob = 1 - t_post.cdf(rope_interval[1])\n    rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n\n    pairwise_bayesian.append([worse_prob, better_prob, rope_prob])\n\npairwise_bayesian_df = (\n    pairwise_bayesian, columns=[\"worse_prob\", \"better_prob\", \"rope_prob\"]\n).round(3)\n\npairwise_comp_df = pairwise_comp_df.join(pairwise_bayesian_df)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "sr_lle, sr_err = (\n    sr_points, n_neighbors=12, n_components=2\n)\n\nsr_tsne = (n_components=2, perplexity=40, random_state=0).fit_transform(\n    sr_points\n)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sr_lle[:, 0], sr_lle[:, 1], c=sr_color)\naxs[0].set_title(\"LLE Embedding of Swiss Roll\")\naxs[1].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss Roll\")\n\n\n<img alt=\"LLE Embedding of Swiss Roll, t-SNE Embedding of Swiss Roll\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_002.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_002.png\"/>",
            "sh_lle, sh_err = (\n    sh_points, n_neighbors=12, n_components=2\n)\n\nsh_tsne = (\n    n_components=2, perplexity=40, init=\"random\", random_state=0\n).fit_transform(sh_points)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\naxs[0].set_title(\"LLE Embedding of Swiss-Hole\")\naxs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\n\n\n<img alt=\"LLE Embedding of Swiss-Hole, t-SNE Embedding of Swiss-Hole\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_004.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_004.png\"/>",
            "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: Bayesian approach"
            ],
            [
                "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss Roll"
            ],
            [
                "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss-Hole"
            ],
            [
                "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples"
            ]
        ]
    },
    "595116": {
        "jupyter_code_cell": "frame.to_csv('order_segmentation_0.0.csv')",
        "matched_tutorial_code_inds": [
            3891,
            3813,
            6005,
            5679,
            3639
        ],
        "matched_tutorial_codes": [
            "df.visualize(rankdir='LR')",
            "df.info()",
            "df_infl = infl.summary_frame()",
            "data = sm.datasets.fair.load_pandas().data",
            "first.loc[['AA', 'AS', 'DL'], ['fl_date', 'tail_num']]"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ]
        ]
    },
    "796741": {
        "jupyter_code_cell": "temp['From'] = temp['From'].str.capitalize()\ntemp['To'] = temp['To'].str.capitalize()",
        "matched_tutorial_code_inds": [
            156,
            1757,
            1584,
            6449,
            1579
        ],
        "matched_tutorial_codes": [
            "m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "print(y_train[0])\nprint(y_train[1])\nprint(y_train[2])",
            "sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ]
        ]
    },
    "1329174": {
        "jupyter_code_cell": "# Kernel Density Plot\nfig = plt.figure(figsize=(15,4),)\nax=sns.kdeplot(df.loc[(df['turnover'] == 0),'evaluation'] , color='b',shade=True,label='no turnover')\nax=sns.kdeplot(df.loc[(df['turnover'] == 1),'evaluation'] , color='r',shade=True, label='turnover')\nplt.title('Last evaluation')",
        "matched_tutorial_code_inds": [
            6671,
            6538,
            5640,
            6545,
            6436
        ],
        "matched_tutorial_codes": [
            "# Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n\n# Plot estimate of the level term\nres_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\nres_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n\nax.legend(loc=\"lower left\");\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\"/>",
            "# Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>",
            "# Plot the confidence interval and fit\nfig, ax = pylab.subplots()\nax.scatter(x, y)\nax.plot(eval_x, smoothed, c=\"k\")\nax.fill_between(eval_x, bottom, top, alpha=0.5, color=\"b\")\npylab.autoscale(enable=True, axis=\"x\", tight=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\" src=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\"/>",
            "# Create Table I\ntable_i = np.zeros((5,6))\n\nstart = dta.index[0]\nend = dta.index[-1]\ntime_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\nmodels = [\n    ('US GNP', time_range, 'None'),\n    ('US Prices', time_range, 'None'),\n    ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n    ('US monetary base', time_range, 'None'),\n    ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n]\nindex = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\nparameter_symbols = [\n    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n]\n\ni = 0\nfor res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n    if res.model.stochastic_level:\n        (sigma_irregular, sigma_level, sigma_trend,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n    else:\n        (sigma_irregular, sigma_level,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n        sigma_trend = np.nan\n    period_cycle = 2 * np.pi / frequency_cycle\n\n    table_i[i, :] = [\n        sigma_level*1e7, sigma_trend*1e7,\n        sigma_cycle*1e7, damping_cycle, period_cycle,\n        sigma_irregular*1e7\n    ]\n    i += 1\n\npd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\ntable_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\ntable_i",
            "# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Personal consumption', xlabel='Date', ylabel='Billions of dollars')\n\n# Plot data points\ndata.loc['1977-07-01':, 'consump'].plot(ax=ax, style='o', label='Observed')\n\n# Plot predictions\npredict.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='r--', label='One-step-ahead forecast')\nci = predict_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\npredict_dy.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='g', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='g', alpha=0.1)\n\nlegend = ax.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Data"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Model"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ]
        ]
    },
    "715128": {
        "jupyter_code_cell": "BBB_Train[\"Rank_Clicks\"] = 0\nDDD_Train[\"Rank_Clicks\"] = 0\nFFF_Train[\"Rank_Clicks\"] = 0\n\nBBB_Test[\"Rank_Clicks\"] = 0\nDDD_Test[\"Rank_Clicks\"] = 0\nFFF_Test[\"Rank_Clicks\"] = 0",
        "matched_tutorial_code_inds": [
            3731,
            6811,
            6357,
            844,
            6770
        ],
        "matched_tutorial_codes": [
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")",
            "out = SinhBad.apply(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
            "mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "torch->Extending PyTorch->Double Backward with Custom Functions->Saving Intermediate Results: What not to do"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale"
            ]
        ]
    },
    "226104": {
        "jupyter_code_cell": "accuracy = np.mean(positive_rating[row_idx, col_idx] == validation[:, 2])\naccuracy",
        "matched_tutorial_code_inds": [
            5570,
            3892,
            1502,
            6217,
            1447
        ],
        "matched_tutorial_codes": [
            "predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Applying to all colors"
            ]
        ]
    },
    "1076191": {
        "jupyter_code_cell": "k = 10 #number of variables for heatmap\ncols = Truth.corr().nlargest(k, 'RenovatablePriceFactor')['RenovatablePriceFactor'].index # Basically just show the K-largest correlations\ncm = np.corrcoef(Truth[cols].values.T)\nsns.set(font_scale=1.25)\nplt.subplots(figsize=[12,12])\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)",
        "matched_tutorial_code_inds": [
            2054,
            6270,
            2140,
            5955,
            2621
        ],
        "matched_tutorial_codes": [
            "n_features = 100\n# simulation covariance matrix (AR(1) process)\nr = 0.1\nreal_cov = (r ** (n_features))\ncoloring_matrix = (real_cov)\n\nn_samples_range = (6, 31, 1)\nrepeat = 100\nlw_mse = ((n_samples_range.size, repeat))\noa_mse = ((n_samples_range.size, repeat))\nlw_shrinkage = ((n_samples_range.size, repeat))\noa_shrinkage = ((n_samples_range.size, repeat))\nfor i, n_samples in enumerate(n_samples_range):\n    for j in range(repeat):\n        X = ((size=(n_samples, n_features)), coloring_matrix.T)\n\n        lw = (store_precision=False, assume_centered=True)\n        lw.fit(X)\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\n        lw_shrinkage[i, j] = lw.shrinkage_\n\n        oa = (store_precision=False, assume_centered=True)\n        oa.fit(X)\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\n        oa_shrinkage[i, j] = oa.shrinkage_\n\n# plot MSE\n(2, 1, 1)\n(\n    n_samples_range,\n    lw_mse.mean(1),\n    yerr=lw_mse.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_mse.mean(1),\n    yerr=oa_mse.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"Squared error\")\n(loc=\"upper right\")\n(\"Comparison of covariance estimators\")\n(5, 31)\n\n# plot shrinkage coefficient\n(2, 1, 2)\n(\n    n_samples_range,\n    lw_shrinkage.mean(1),\n    yerr=lw_shrinkage.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_shrinkage.mean(1),\n    yerr=oa_shrinkage.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"n_samples\")\n(\"Shrinkage\")\n(loc=\"lower right\")\n(()[0], 1.0 + (()[1] - ()[0]) / 10.0)\n(5, 31)\n\n()\n\n\n<img alt=\"Comparison of covariance estimators\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\" srcset=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\"/>",
            "lags = int(10 * np.log10(arma_rvs.shape[0]))\narma11 = ARIMA(arma_rvs, order=(1, 0, 1)).fit()\nresid = arma11.resid\nr, q, p = sm.tsa.acf(resid, nlags=lags, fft=True, qstat=True)\ndata = np.c_[range(1, lags + 1), r[1:], q, p]\ntable = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>",
            "nsample = 50\nx1 = np.linspace(0, 20, nsample)\nX = np.column_stack((x1, (x1 - 5) ** 2))\nX = sm.add_constant(X)\nsig = 0.3  # smaller error variance makes OLS&lt;-RLM contrast bigger\nbeta = [5, 0.5, -0.0]\ny_true2 = np.dot(X, beta)\ny2 = y_true2 + sig * 1.0 * np.random.normal(size=nsample)\ny2[[39, 41, 43, 45, 48]] -= 5  # add some outliers (10% of nsample)",
            "vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR"
            ]
        ]
    },
    "54427": {
        "jupyter_code_cell": "cols = ['B19083_001E']\ncols.extend(['NAME', 'GEOID'])",
        "matched_tutorial_code_inds": [
            4167,
            1531,
            3812,
            3975,
            3684
        ],
        "matched_tutorial_codes": [
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "AB = results.params\nA = AB[0]\nB = AB[1]",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ]
        ]
    },
    "944408": {
        "jupyter_code_cell": "DepDelay = flights.loc[lambda df: flights['DepDelay'] < 29, :]\nArrDelay = flights.loc[lambda df: flights['ArrDelay'] < 27, :]\ndelayDep = DepDelay['DepDelay']\ndelayArr = ArrDelay['ArrDelay']",
        "matched_tutorial_code_inds": [
            1639,
            2656,
            1992,
            3688,
            4921
        ],
        "matched_tutorial_codes": [
            "forceA = np.array([1, 0, 0])\nforceB = np.array([0, 1, 0])\nprint(\"Force A =\", forceA)\nprint(\"Force B =\", forceB)",
            "training SGD\ntraining ASGD\ntraining Perceptron\ntraining Passive-Aggressive I\ntraining Passive-Aggressive II\ntraining SAG\n\n\n\n<br/>",
            "Evaluation of KMeans with k-means++ init\nEvaluation of KMeans with random init\nEvaluation of MiniBatchKMeans with k-means++ init\nEvaluation of MiniBatchKMeans with random init\n\n\n\n<br/>",
            "dsm = weather.loc['DSM']\n\nhourly = dsm.resample('H').mean()\n\ntemp = hourly['tmpf'].sample(frac=.5, random_state=1).sort_index()\nsped = hourly['sped'].sample(frac=.5, random_state=2).sort_index()",
            "self.transData = (\n    self.transScale + self.transShift + self.transProjection +\n    (self.transProjectionAffine + self.transWedge + self.transAxes))"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Solving equilibrium with Newton\u2019s second law"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Comparing various online solvers"
            ],
            [
                "sklearn->Examples->Clustering->Empirical evaluation of the impact of k-means initialization"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment"
            ],
            [
                "matplotlib->Tutorials->Advanced->Transformations Tutorial->The transformation pipeline"
            ]
        ]
    },
    "921933": {
        "jupyter_code_cell": "centroids = []\nfor i in range(10):\n    tmp = np.where((np.where(mnist.train.labels == 1)[1]) == i)[0]\n    tmp = np.average(mnist.train.images[tmp], axis=0)\n    centroids.append(tmp)",
        "matched_tutorial_code_inds": [
            5368,
            6423,
            6811,
            5375,
            3730
        ],
        "matched_tutorial_codes": [
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ]
        ]
    },
    "640743": {
        "jupyter_code_cell": "# Creating Decision Tree model\nfrom sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()",
        "matched_tutorial_code_inds": [
            51,
            6632,
            3226,
            45,
            3177
        ],
        "matched_tutorial_codes": [
            "# Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)",
            "# Create an instance of our TVPVAR class with our observed dataset y\nmod = TVPVAR(y)",
            "from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "# Specify a path\nPATH = \"entire_model.pt\"\n\n# Save\n(net, PATH)\n\n# Load\nmodel = (PATH)\nmodel.eval()",
            "from sklearn.linear_model import \n\nclassifier = ()\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->3. Save model A"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->TVP-VAR model in Statsmodels"
            ],
            [
                "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning"
            ],
            [
                "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps->5. Save and load entire model"
            ],
            [
                "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->Load and prepare data"
            ]
        ]
    },
    "9180": {
        "jupyter_code_cell": "x = tf.placeholder(tf.float32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\nkeep_prob = tf.placeholder(tf.float32)",
        "matched_tutorial_code_inds": [
            2611,
            1981,
            4167,
            2328,
            1091
        ],
        "matched_tutorial_codes": [
            "X = (0, 5, num=30).reshape(-1, 1)\ny = target_generator(X, add_noise=False)",
            "centers = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = (\n    n_samples=300, centers=centers, cluster_std=0.5, random_state=0\n)",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=25, device='cpu')\n\nvisualize_model(new_model)\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Data generation"
            ],
            [
                "sklearn->Examples->Clustering->Demo of affinity propagation clustering algorithm->Generate sample data"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 1. Training a Custom Classifier based on a Quantized Feature Extractor->Train and evaluate"
            ]
        ]
    },
    "1421037": {
        "jupyter_code_cell": "df[df.salt > 60]",
        "matched_tutorial_code_inds": [
            6006,
            2431,
            3633,
            2592,
            3792
        ],
        "matched_tutorial_codes": [
            "df_infl[:5]",
            "df[\"count\"].max()",
            "df.ix[10:15, ['fl_date', 'tail_num']]",
            "co2_data.index.min(), co2_data.index.max()",
            "df['home_win'] = df.home_points &gt; df.away_points"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 1: Create an outcome variable"
            ]
        ]
    },
    "566972": {
        "jupyter_code_cell": "process_pclass()",
        "matched_tutorial_code_inds": [
            663,
            1758,
            5600,
            6351,
            5607
        ],
        "matched_tutorial_codes": [
            "traced_rn18 = (rn18)\nprint()",
            "textproc = TextPreprocess()",
            "resf_logit.predict()",
            "res_filardo.summary()",
            "res_logit.summary()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Filardo (1994) Time-Varying Transition Probabilities"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit"
            ]
        ]
    },
    "714393": {
        "jupyter_code_cell": "laplace = np.random.laplace(10, 2, 100)\n# Print the histogram\nplt.hist(laplace)\nplt.show()",
        "matched_tutorial_code_inds": [
            6803,
            6157,
            5246,
            4166,
            6166
        ],
        "matched_tutorial_codes": [
            "mod = ThetaModel(np.log(pce))\nres = mod.fit()\nprint(res.summary())",
            "sample = copula.rvs(10000)\nh = sns.jointplot(x=sample[:, 0], y=sample[:, 1], kind=\"hex\")\n_ = h.set_axis_labels(\"X1\", \"X2\", fontsize=16)",
            "eigs = np.linalg.eigvals(norm_xtx)\ncondition_number = np.sqrt(eigs.max() / eigs.min())\nprint(condition_number)",
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "statsmodels->Examples->Statistics->Copulas->Sampling from a copula"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Condition number"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ]
        ]
    },
    "200955": {
        "jupyter_code_cell": "gmail_regex = r'(([a-zA-Z0-9]+)@(gmail)\\.(com))'\ntext  = 'email1@gmail.com, email2@yahoo.com, email3@gmail.com'\nre.findall(gmail_regex, text)",
        "matched_tutorial_code_inds": [
            3833,
            4800,
            5456,
            3730,
            4802
        ],
        "matched_tutorial_codes": [
            "gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "from cycler import cycler\ncc = (cycler(color=list('rgb')) +\n      cycler(linestyle=['-', '--', '-.']))\nfor d in cc:\n    print(d)",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "from cycler import cycler\ncc = (cycler(color=list('rgb')) *\n      cycler(linestyle=['-', '--', '-.']))\nfor d in cc:\n    print(d)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Styling with cycler->Cycling through multiple properties"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Styling with cycler->Cycling through multiple properties"
            ]
        ]
    },
    "987800": {
        "jupyter_code_cell": "model_gb = ensemble.GradientBoostingRegressor(random_state=42)\nn_estimators = [20, 50, 100, 150, 200, 250]\nmax_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nlearning_rate = [ 0.1, 0.05, 0.01, 0.001]\n\nparam_grid = {'n_estimators': n_estimators, \n              'max_depth': max_depth, \n              'learning_rate': learning_rate}\n\ngs = GridSearchCV(estimator = model_gb, \n                  param_grid = param_grid, \n                  scoring = 'neg_mean_squared_error', \n                  cv = 10)\ngs = gs.fit(X_treino, y_treino)\n\nprint(gs.best_params_)",
        "matched_tutorial_code_inds": [
            3499,
            43,
            3281,
            960,
            3008
        ],
        "matched_tutorial_codes": [
            "model_l2 = (penalty=\"l2\", loss=\"squared_hinge\", dual=True)\nCs = (-4.5, -2, 10)\n\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = (train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = (\n        model_l2, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = (results)",
            "model = Net()\noptimizer = (net.parameters(), lr=0.001, momentum=0.9)\n\ncheckpoint = (PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nmodel.eval()\n# - or -\nmodel.train()",
            "alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "model = MyModule(500, 10).cuda()\ninput = (128, 500).cuda()\nmask = ((500, 500, 500), dtype=torch.double).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith (with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)",
            "diabetes = ()\nX = (diabetes.data, columns=diabetes.feature_names)\ny = diabetes.target\n\ntree = ()\nmlp = (\n    (),\n    (hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0),\n)\ntree.fit(X, y)\nmlp.fit(X, y)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Support Vector Machines->Scaling the regularization parameter for SVCs->L2-penalty case"
            ],
            [
                "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->5. Load the general checkpoint"
            ],
            [
                "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors"
            ],
            [
                "torch->Model Optimization->Profiling your PyTorch Module->Profile the forward pass"
            ],
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Train models on the diabetes dataset"
            ]
        ]
    },
    "507966": {
        "jupyter_code_cell": "#function that extracts title\ndef title(passenger):\n    name = passenger['Name']\n    start = name.find(',') + 1\n    end = name.find('.')\n    \n    return name[start:end].strip()\n\n#extracting title\ndf_train['title'] = df_train.apply(title, axis=1)\ndf_test['title'] = df_test.apply(title, axis=1)\n\n#showing unique values and count\nprint(df_train.groupby('title').size(), '\\n\\n',\n      df_test.groupby('title').size())",
        "matched_tutorial_code_inds": [
            1782,
            3104,
            5133,
            483,
            2762
        ],
        "matched_tutorial_codes": [
            "# To store predicted sentiments\npredictions = {}\n\n# define the length of a paragraph\npara_len = 100\n\n# Retrieve trained values of the parameters\nif os.path.isfile('tutorial-nlp-from-scratch/parameters.npy'):\n    parameters = np.load('tutorial-nlp-from-scratch/parameters.npy', allow_pickle=True).item()\n\n# This is the prediction loop.\nfor index, text in enumerate(X_pred):\n    # split each speech into paragraphs\n    paras = textproc.text_to_paras(text, para_len)\n    # To store the network outputs\n    preds = []\n\n    for para in paras:\n        # split text sample into words/tokens\n        para_tokens = textproc.word_tokeniser(para)\n        # Forward Propagation\n        caches = forward_prop(para_tokens,\n                              parameters,\n                              input_dim)\n\n        # Retrieve the output of the fully connected layer\n        sent_prob = caches['fc_values'][0]['a2'][0][0]\n        preds.append(sent_prob)\n\n    threshold = 0.5\n    preds = np.array(preds)\n    # Mark all predictions &gt; threshold as positive and &lt; threshold as negative\n    pos_indices = np.where(preds &gt; threshold)  # indices where output &gt; 0.5\n    neg_indices = np.where(preds &lt; threshold)  # indices where output &lt; 0.5\n    # Store predictions and corresponding piece of text\n    predictions[speakers[index]] = {'pos_paras': paras[pos_indices[0]],\n                                    'neg_paras': paras[neg_indices[0]]}",
            "# range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "# some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)",
            "# One-hot vector for category\ndef categoryTensor(category):\n    li = all_categories.index(category)\n    tensor = (1, n_categories)\n    tensor[0][li] = 1\n    return tensor\n\n# One-hot matrix of first to last letters (not including EOS) for input\ndef inputTensor(line):\n    tensor = (len(line), 1, n_letters)\n    for li in range(len(line)):\n        letter = line[li]\n        tensor[li][0][all_letters.find(letter)] = 1\n    return tensor\n\n# LongTensor of second letter to end (EOS) for target\ndef targetTensor(line):\n    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n    letter_indexes.append(n_letters - 1) # EOS\n    return torch.LongTensor(letter_indexes)",
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data"
            ],
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds"
            ],
            [
                "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting"
            ],
            [
                "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Training->Preparing for Training"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ]
        ]
    },
    "1242106": {
        "jupyter_code_cell": "awards2016.info()",
        "matched_tutorial_code_inds": [
            3720,
            2828,
            3723,
            3788,
            5607
        ],
        "matched_tutorial_codes": [
            "flights.dep_time.unique()",
            "survey.data.info()",
            "flights.dep_time.head()",
            "rest.unstack().head()",
            "res_logit.summary()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Stack / Unstack"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit"
            ]
        ]
    },
    "1312124": {
        "jupyter_code_cell": "x_c = copy.deepcopy(x)\ny_c = copy.deepcopy(y)\n\nx_c = np.append(x_c, 65)\ny_c = np.append(y_c, 65*SLOPE + INTERCEPT + 20)",
        "matched_tutorial_code_inds": [
            5438,
            5456,
            4094,
            2581,
            5451
        ],
        "matched_tutorial_codes": [
            "spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "rng = (1)\ntraining_indices = rng.choice((y.size), size=6, replace=False)\nX_train, y_train = X[training_indices], y[training_indices]",
            "anes_data = sm.datasets.anes96.load()\nanes_exog = anes_data.exog\nanes_exog = sm.add_constant(anes_exog)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noise-free target"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ]
        ]
    },
    "1232588": {
        "jupyter_code_cell": "plt.rcParams['figure.figsize'] = (15, 15)\nc1 = np.concatenate([np.concatenate([conv1[:, :, :, j*16+i].squeeze() for i in range(16)], axis=1) for j in range(16)])\nc1img = Image.fromarray((c1 + 1) * 127.5)\nplt.imshow(c1img)",
        "matched_tutorial_code_inds": [
            4823,
            4824,
            4719,
            4848,
            4855
        ],
        "matched_tutorial_codes": [
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>",
            "plt.rcParams.update({'figure.autolayout': True})\n\nfig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_006.png\" srcset=\"../../_images/sphx_glr_lifecycle_006.png, ../../_images/sphx_glr_lifecycle_006_2_0x.png 2.0x\"/>",
            "plt.close('all')\nfig = plt.figure()\n\nax1 = plt.subplot2grid((3, 3), (0, 0))\nax2 = plt.subplot2grid((3, 3), (0, 1), colspan=2)\nax3 = plt.subplot2grid((3, 3), (1, 0), colspan=2, rowspan=2)\nax4 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\nexample_plot(ax1)\nexample_plot(ax2)\nexample_plot(ax3)\nexample_plot(ax4)\n\nplt.tight_layout()\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_007.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_007.png, ../../_images/sphx_glr_tight_layout_guide_007_2_0x.png 2.0x\"/>",
            "plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Simple Example"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar"
            ]
        ]
    },
    "455986": {
        "jupyter_code_cell": "modeldata['Sex'].drop_duplicates()",
        "matched_tutorial_code_inds": [
            5600,
            5561,
            4134,
            6813,
            5304
        ],
        "matched_tutorial_codes": [
            "resf_logit.predict()",
            "data_student.dtypes",
            "anscombe = sns.load_dataset(\"anscombe\")\n",
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "res.plot_cusum()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "seaborn->User guide and tutorial->Estimating regression fits->Fitting different kinds of models",
                "seaborn->Statistical operations->Estimating regression fits->Fitting different kinds of models"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money"
            ]
        ]
    },
    "413255": {
        "jupyter_code_cell": "countries_json = json.loads(countries_raw.text)\ncountries_json",
        "matched_tutorial_code_inds": [
            6217,
            3892,
            1502,
            1497,
            6014
        ],
        "matched_tutorial_codes": [
            "auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "china_total = china_masked.data\nchina_total",
            "interM_lm.model.exog\ninterM_lm.model.exog_names"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "869915": {
        "jupyter_code_cell": "figure = plt.figure(figsize = (12, 8))\nfigure = sm.graphics.plot_regress_exog(model, 'amb_iRateMe_exp', fig = figure)",
        "matched_tutorial_code_inds": [
            6264,
            5401,
            6276,
            6177,
            5390
        ],
        "matched_tutorial_codes": [
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax = cpi.plot(ax=ax)\nax.legend()",
            "fig = plt.figure(figsize=(16, 9))\nfig = res.plot_diagnostics(lags=30, fig=fig)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_21_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_21_0.png\"/>",
            "fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Exercise: How good of in-sample prediction can you do for another series, say, CPI->Hint:"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dummies"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)"
            ]
        ]
    },
    "196279": {
        "jupyter_code_cell": "# Access the data file from the FBI: UCR \ndataset = pd.read_excel(\"NYCCrime.xls\", header=4)",
        "matched_tutorial_code_inds": [
            6915,
            12,
            6703,
            6434,
            6558
        ],
        "matched_tutorial_codes": [
            "# The default is to get a one-step-ahead forecast:\nprint(res.forecast())",
            "# Equates to one random 28x28 image\nrandom_data = ((1, 1, 28, 28))\n\nmy_nn = Net()\nresult = my_nn(random_data)\nprint (result)",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "# Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Forecasting"
            ],
            [
                "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps->4. [Optional] Pass data through your model to test"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ]
        ]
    },
    "1094427": {
        "jupyter_code_cell": "# Get connections to the databases\ndb_a = sqlite3.connect('livetweets.db')\ndb_b = sqlite3.connect('histtweets.db')\n\n# Get the contents of a table\nb_cursor = db_b.cursor()\nb_cursor.execute('SELECT * FROM tweets')\noutput = b_cursor.fetchall()   # Returns the results as a list.\n\n# Insert those contents into another table.\na_cursor = db_a.cursor()\nfor row in output:\n    try:\n        a_cursor.execute('INSERT INTO tweets VALUES (?,?,?,?,?,?,?,?,?,?,?)', row)\n    except sqlite3.IntegrityError: # skip duplicate tweet ids\n        pass\n\n# Cleanup\ndb_a.commit()\na_cursor.close()\nb_cursor.close()\n\n# Rename the merged db, and delete the other\nos.rename('livetweets.db', 'tweets.db')\nos.remove('histtweets.db')",
        "matched_tutorial_code_inds": [
            6936,
            6944,
            6345,
            6348,
            5122
        ],
        "matched_tutorial_codes": [
            "# Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.append(updated_endog, refit=False)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "# Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.extend(updated_endog)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "# Get the dataset\new_excs = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/ew_excs.prn\").content\nraw = pd.read_table(BytesIO(ew_excs), header=None, skipfooter=1, engine=\"python\")\nraw.index = pd.date_range(\"1926-01-01\", \"1995-12-01\", freq=\"MS\")\n\ndta_kns = raw.loc[:\"1986\"] - raw.loc[:\"1986\"].mean()\n\n# Plot the dataset\ndta_kns[0].plot(title=\"Excess returns\", figsize=(12, 3))\n\n# Fit the model\nmod_kns = sm.tsa.MarkovRegression(\n    dta_kns, k_regimes=3, trend=\"n\", switching_variance=True\n)\nres_kns = mod_kns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\"/>",
            "# Get the dataset\nfilardo = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/filardo.prn\").content\ndta_filardo = pd.read_table(\n    BytesIO(filardo), sep=\" +\", header=None, skipfooter=1, engine=\"python\"\n)\ndta_filardo.columns = [\"month\", \"ip\", \"leading\"]\ndta_filardo.index = pd.date_range(\"1948-01-01\", \"1991-04-01\", freq=\"MS\")\n\ndta_filardo[\"dlip\"] = np.log(dta_filardo[\"ip\"]).diff() * 100\n# Deflated pre-1960 observations by ratio of std. devs.\n# See hmt_tvp.opt or Filardo (1994) p. 302\nstd_ratio = (\n    dta_filardo[\"dlip\"][\"1960-01-01\":].std() / dta_filardo[\"dlip\"][:\"1959-12-01\"].std()\n)\ndta_filardo[\"dlip\"][:\"1959-12-01\"] = dta_filardo[\"dlip\"][:\"1959-12-01\"] * std_ratio\n\ndta_filardo[\"dlleading\"] = np.log(dta_filardo[\"leading\"]).diff() * 100\ndta_filardo[\"dmdlleading\"] = dta_filardo[\"dlleading\"] - dta_filardo[\"dlleading\"].mean()\n\n# Plot the data\ndta_filardo[\"dlip\"].plot(\n    title=\"Standardized growth rate of industrial production\", figsize=(13, 3)\n)\nplt.figure()\ndta_filardo[\"dmdlleading\"].plot(title=\"Leading indicator\", figsize=(13, 3))",
            "# Load the statsmodels api\nimport statsmodels.api as sm\n\n# Load your dataset\nendog = pd.read_csv('your/dataset/here.csv')\n\n# Fit a local level model\nmod_ll = sm.tsa.UnobservedComponents(endog, 'local level')\n# Note that mod_ll is an instance of the UnobservedComponents class\n\n# Fit the model via maximum likelihood\nres_ll = mod_ll.fit()\n# Note that res_ll is an instance of the UnobservedComponentsResults class\n\n# Show the summary of results\nprint(res_ll.summary())\n\n# Show a plot of the estimated level and trend component series\nfig_ll = res_ll.plot_components()\n\n# We could further add a damped stochastic cycle as follows\nmod_cycle = sm.tsa.UnobservedComponents(endog, 'local level', cycle=True,\n                                        damped_cycle=True,\n                                        stochastic_cycle=True)\nres_cycle = mod_cycle.fit()\n\n# Show the summary of results\nprint(res_cycle.summary())\n\n# Show a plot of the estimated level, trend, and cycle component series\nfig_cycle = res_cycle.plot_components()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Kim, Nelson, and Startz (1998) Three-state Variance Switching"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Filardo (1994) Time-Varying Transition Probabilities"
            ],
            [
                "statsmodels->User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Models and Estimation->Unobserved Components"
            ]
        ]
    },
    "126499": {
        "jupyter_code_cell": "\"\"\"\n1/16-degree Gridded cell centroids\n\"\"\"\n# List of available data\nhs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\nNAmer = hs.content['NAmer_dem_list.shp']\n\n\"\"\"\nSauk\n\"\"\"\n# Watershed extent\nhs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\nsauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n\n\"\"\"\nElwha\n\"\"\"\n# Watershed extent\nhs.getResourceFromHydroShare('4aff8b10bc424250b3d7bac2188391e8', )\nelwha = hs.content[\"elwha_ws_bnd_wgs84.shp\"]\n\n\"\"\"\nRio Salado\n\"\"\"\n# Watershed extent\nhs.getResourceFromHydroShare('5c041d95ceb64dce8eb85d2a7db88ed7')\nriosalado = hs.content['UpperRioSalado_delineatedBoundary.shp']",
        "matched_tutorial_code_inds": [
            1235,
            6554,
            5041,
            1237,
            1236
        ],
        "matched_tutorial_codes": [
            "\"\"\" Dataset partitioning helper \"\"\"\nclass Partition(object):\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])",
            "\"\"\"\nUnivariate Local Linear Trend Model\n\"\"\"\nclass LocalLinearTrend(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Model order\n        k_states = k_posdef = 2\n\n        # Initialize the statespace\n        super(LocalLinearTrend, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1, 0])\n        self.ssm['transition'] = np.array([[1, 1],\n                                       [0, 1]])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['sigma2.measurement', 'sigma2.level', 'sigma2.trend']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*3\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(LocalLinearTrend, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm[self._state_cov_idx] = params[1:]",
            "\"\"\"\n=============\nPGF texsystem\n=============\n\"\"\"\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({\n    \"pgf.texsystem\": \"pdflatex\",\n    \"pgf.preamble\": \"\\n\".join([\n         r\"\\usepackage[utf8x]{inputenc}\",\n         r\"\\usepackage[T1]{fontenc}\",\n         r\"\\usepackage{cmbright}\",\n    ]),\n})\n\nfig, ax = plt.subplots(figsize=(4.5, 2.5))\n\nax.plot(range(5))\n\nax.text(0.5, 3., \"serif\", family=\"serif\")\nax.text(0.5, 2., \"monospace\", family=\"monospace\")\nax.text(2.5, 2., \"sans-serif\", family=\"sans-serif\")\nax.set_xlabel(r\"\u00b5 is not $\\mu$\")\n\nfig.tight_layout(pad=.5)",
            "\"\"\" Distributed Synchronous SGD Example \"\"\"\ndef run(rank, size):\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n    optimizer = optim.SGD(model.parameters(),\n                          lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        print('Rank ', dist.get_rank(), ', epoch ',\n              epoch, ': ', epoch_loss / num_batches)",
            "\"\"\" Partitioning MNIST \"\"\"\ndef partition_dataset():\n    dataset = datasets.MNIST('./data', train=True, download=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))\n                             ]))\n    size = dist.get_world_size()\n    bsz = 128 / float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(partition,\n                                         batch_size=bsz,\n                                         shuffle=True)\n    return train_set, bsz"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Distributed Training"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend->Choosing the TeX system"
            ],
            [
                "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Distributed Training"
            ],
            [
                "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Distributed Training"
            ]
        ]
    },
    "1175442": {
        "jupyter_code_cell": "#Data handling for Labview output. Seperate different line segments, folder in dev15_vsd needs to be deleted first.\n\ndef dataparsing(input_file, output_file, data_width):\n    j = 0\n    with open(input_file, 'r') as f_i:    \n        for line, data in enumerate(f_i):\n            if line % data_width == 0:\n                j = j + 1\n            else:\n                with open(output_file + str(j) + '.txt','a+') as f_o:\n                    f_o.write(data)\n\ninput_file = 'Data\\\\4_uc_new device\\\\dev16\\\\Dev16_Vgs-sweep -2V to 1V -- Vds 0V to 1V.txt'\noutput_file = 'Data\\\\4_uc_new device\\\\dev16\\\\Dev16_RoomT_Vgs_sweep\\\\dev16_dataset'\ndata_width = 606\n\ndataparsing(input_file, output_file, data_width)",
        "matched_tutorial_code_inds": [
            6437,
            383,
            1966,
            473,
            6416
        ],
        "matched_tutorial_codes": [
            "# Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "# Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "# Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ],
            [
                "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ]
        ]
    },
    "988965": {
        "jupyter_code_cell": "x = np.log(dbars.price).cumsum()\ncprint(x)\n\nx.plot()",
        "matched_tutorial_code_inds": [
            6803,
            5504,
            4095,
            5948,
            5536
        ],
        "matched_tutorial_codes": [
            "mod = ThetaModel(np.log(pce))\nres = mod.fit()\nprint(res.summary())",
            "dta = sm.datasets.star98.load_pandas().data\nprint(dta.columns)",
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "data = sm.datasets.stackloss.load()\ndata.exog = sm.add_constant(data.exog)",
            "means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots",
                "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Estimation"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ]
        ]
    },
    "9974": {
        "jupyter_code_cell": "plot_learning_curves(ridge_reg4, X, y,141)\nplot_learning_curves(ridge_reg5, X, y,142)\nplot_learning_curves(ridge_reg6, X, y,143)",
        "matched_tutorial_code_inds": [
            5401,
            5390,
            2369,
            5454,
            5964
        ],
        "matched_tutorial_codes": [
            "fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)",
            "coverage_fraction(\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n)",
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "resrlm2 = sm.RLM(y2, X2).fit()\nprint(resrlm2.params)\nprint(resrlm2.bse)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM->Example 2: linear function with linear truth"
            ]
        ]
    },
    "314889": {
        "jupyter_code_cell": "rf_all_features = pd.DataFrame(fit_all_rf.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)",
        "matched_tutorial_code_inds": [
            1709,
            3101,
            6607,
            2966,
            2996
        ],
        "matched_tutorial_codes": [
            "sub_indices = np.stack((vcompute_indices('PM2.5', pollutants[..., 0]),\n                        vcompute_indices('PM10', pollutants[..., 1]),\n                        vcompute_indices('NO2', pollutants[..., 2]),\n                        vcompute_indices('NH3', pollutants[..., 3]),\n                        vcompute_indices('SO2', pollutants[..., 4]),\n                        vcompute_indices('CO', pollutants[..., 5]),\n                        vcompute_indices('O3', pollutants[..., 6])), axis=1)",
            "svc_disp = (svc, X_test, y_test)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\"/>",
            "ucm_trend = pd.Series(res.level.smoothed, index=endog.index)",
            "train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()",
            "t_sne = (\n    n_components=n_components,\n    perplexity=30,\n    init=\"random\",\n    n_iter=250,\n    random_state=0,\n)\nS_t_sne = t_sne.fit_transform(S_points)\n\nplot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")\n\n\n<img alt=\"T-distributed Stochastic    Neighbor Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_006.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Sub-indices"
            ],
            [
                "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Plotting the ROC Curve"
            ],
            [
                "statsmodels->Examples->State space models->Fixed / constrained parameters in state space models"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->T-distributed Stochastic Neighbor Embedding"
            ]
        ]
    },
    "1195980": {
        "jupyter_code_cell": "plt.scatter(bos.LSTAT, bos.PRICE)\nplt.xlabel(\"% lower status of the population\")\nplt.ylabel(\"Housing Price\")\nplt.title(\"Relationship between LSTAT and Price\")",
        "matched_tutorial_code_inds": [
            6811,
            5801,
            6357,
            3730,
            5806
        ],
        "matched_tutorial_codes": [
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "plt.clf()\nplt.grid(True)\nplt.plot(result1.predict(linear=True), result1.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "plt.clf()\nplt.grid(True)\nplt.plot(result2.predict(linear=True), result2.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ]
        ]
    },
    "612806": {
        "jupyter_code_cell": "cast.shape",
        "matched_tutorial_code_inds": [
            1478,
            1415,
            1445,
            1431,
            1453
        ],
        "matched_tutorial_codes": [
            "load_xy.shape",
            "img.shape",
            "img_array.shape",
            "img_gray.shape",
            "reconstructed.shape"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Our arrays as a csv file"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Shape, axis and array properties"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Applying to all colors"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Operations on an axis"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays"
            ]
        ]
    },
    "1125698": {
        "jupyter_code_cell": "SOLID_FUEL_COLUMN_INDEX = 2\na = sns.jointplot(iran_csv.ix[:,SOLID_FUEL_COLUMN_INDEX],\n              turkey_csv.ix[:,SOLID_FUEL_COLUMN_INDEX]).set_axis_labels(\n    \"IRAN: \" + iran_csv.columns[SOLID_FUEL_COLUMN_INDEX], \n    \"TURKEY: \" + turkey_csv.columns[SOLID_FUEL_COLUMN_INDEX])\na.savefig(\"output.png\")",
        "matched_tutorial_code_inds": [
            1126,
            4955,
            4956,
            4958,
            4954
        ],
        "matched_tutorial_codes": [
            "SHAPE_COUNT = 20\ndynamic_sizes = deepcopy(input_size)\n\ninputs1: List[] = []\ninputs2: List[] = []\ngrad_outputs: List[] = []\n\n\n# Create some random shapes\nfor _ in range(SHAPE_COUNT):\n    dynamic_sizes[0] = input_size[0] + random.randrange(-2, 3)\n    dynamic_sizes[1] = input_size[1] + random.randrange(-2, 3)\n    input = (*dynamic_sizes, device=device, =, requires_grad=True)\n    inputs1.append(input)\n    inputs2.append((input))\n    grad_outputs.append((input))",
            "N = 100\nX, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (Z1 - Z2) * 2\n\nfig, ax = plt.subplots(2, 1)\n\npcm = ax[0].pcolormesh(X, Y, Z,\n                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                       cmap='RdBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='both')\n\npcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z), shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='both')\nplt.show()\n\n\n<img alt=\"colormapnorms\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_003.png\" srcset=\"../../_images/sphx_glr_colormapnorms_003.png, ../../_images/sphx_glr_colormapnorms_003_2_0x.png 2.0x\"/>",
            "N = 100\nX, Y = np.mgrid[0:3:complex(0, N), 0:2:complex(0, N)]\nZ1 = (1 + np.sin(Y * 10.)) * X**2\n\nfig, ax = plt.subplots(2, 1, constrained_layout=True)\n\npcm = ax[0].pcolormesh(X, Y, Z1, norm=colors.PowerNorm(gamma=0.5),\n                       cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='max')\nax[0].set_title('PowerNorm()')\n\npcm = ax[1].pcolormesh(X, Y, Z1, cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='max')\nax[1].set_title('Normalize()')\nplt.show()\n\n\n<img alt=\"PowerNorm(), Normalize()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_004.png\" srcset=\"../../_images/sphx_glr_colormapnorms_004.png, ../../_images/sphx_glr_colormapnorms_004_2_0x.png 2.0x\"/>",
            "N = 100\nX, Y = np.meshgrid(np.linspace(-3, 3, N), np.linspace(-2, 2, N))\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = ((Z1 - Z2) * 2)[:-1, :-1]\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 6), constrained_layout=True)\nax = ax.flatten()\n\n# Default norm:\npcm = ax[0].pcolormesh(X, Y, Z, cmap='RdBu_r')\nfig.colorbar(pcm, ax=ax[0], orientation='vertical')\nax[0].set_title('Default norm')\n\n# Even bounds give a contour-like effect:\nbounds = np.linspace(-1.5, 1.5, 7)\nnorm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)\npcm = ax[1].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')\nfig.colorbar(pcm, ax=ax[1], extend='both', orientation='vertical')\nax[1].set_title('BoundaryNorm: 7 boundaries')\n\n# Bounds may be unevenly spaced:\nbounds = np.array([-0.2, -0.1, 0, 0.5, 1])\nnorm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)\npcm = ax[2].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')\nfig.colorbar(pcm, ax=ax[2], extend='both', orientation='vertical')\nax[2].set_title('BoundaryNorm: nonuniform')\n\n# With out-of-bounds colors:\nbounds = np.linspace(-1.5, 1.5, 7)\nnorm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend='both')\npcm = ax[3].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')\n# The colorbar inherits the \"extend\" argument from BoundaryNorm.\nfig.colorbar(pcm, ax=ax[3], orientation='vertical')\nax[3].set_title('BoundaryNorm: extend=\"both\"')\nplt.show()\n\n\n<img alt=\"Default norm, BoundaryNorm: 7 boundaries, BoundaryNorm: nonuniform, BoundaryNorm: extend=\" both=\"\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_005.png\" srcset=\"../../_images/sphx_glr_colormapnorms_005.png, ../../_images/sphx_glr_colormapnorms_005_2_0x.png 2.0x\"/>",
            "delta = 0.1\nx = np.arange(-3.0, 4.001, delta)\ny = np.arange(-4.0, 3.001, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (0.9*Z1 - 0.5*Z2) * 2\n\n# select a divergent colormap\ncmap = cm.coolwarm\n\nfig, (ax1, ax2) = plt.subplots(ncols=2)\npc = ax1.pcolormesh(Z, cmap=cmap)\nfig.colorbar(pc, ax=ax1)\nax1.set_title('Normalize()')\n\npc = ax2.pcolormesh(Z, norm=colors.CenteredNorm(), cmap=cmap)\nfig.colorbar(pc, ax=ax2)\nax2.set_title('CenteredNorm()')\n\nplt.show()\n\n\n<img alt=\"Normalize(), CenteredNorm()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_002.png\" srcset=\"../../_images/sphx_glr_colormapnorms_002.png, ../../_images/sphx_glr_colormapnorms_002_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->nvFuser & Dynamic Shapes"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Symmetric logarithmic"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Power-law"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Discrete bounds"
            ],
            [
                "matplotlib->Tutorials->Colors->Colormap Normalization->Centered"
            ]
        ]
    },
    "236926": {
        "jupyter_code_cell": "# Read data into memory\ndf = pd.read_excel('../data/fig6.24.xls', sheet_name='tidy')\n\ndf.head()",
        "matched_tutorial_code_inds": [
            3812,
            6703,
            3889,
            3769,
            634
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "import string\n\ns = pd.Series(np.random.choice(list(string.ascii_letters), 100000))\nprint('{:0.2f} KB'.format(s.memory_usage(index=False) / 1000))",
            "import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "pandas_toms_blog->Scaling->Using Dask"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Categoricals"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ]
        ]
    },
    "1239988": {
        "jupyter_code_cell": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.layers import Dense\nfrom keras.layers import Softmax\nfrom keras.layers import Dropout\nfrom keras.models import Sequential",
        "matched_tutorial_code_inds": [
            1883,
            2324,
            2843,
            2063,
            1832
        ],
        "matched_tutorial_codes": [
            "from sklearn.calibration import CalibrationDisplay\nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.naive_bayes import \n\n# Create classifiers\nlr = ()\ngnb = ()\nsvc = NaivelyCalibratedLinearSVC(C=1.0)\nrfc = ()\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (gnb, \"Naive Bayes\"),\n    (svc, \"SVC\"),\n    (rfc, \"Random forest\"),\n]",
            "from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "from sklearn.metrics import \nfrom sklearn.metrics import PredictionErrorDisplay\n\nmae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}",
            "from sklearn.model_selection import \nfrom sklearn.covariance import , \n\n# GridSearch for an optimal shrinkage coefficient\ntuned_parameters = [{\"shrinkage\": shrinkages}]\ncv = ((), tuned_parameters)\ncv.fit(X_train)\n\n# Ledoit-Wolf optimal shrinkage coefficient estimate\nlw = ()\nloglik_lw = lw.fit(X_train).score(X_test)\n\n# OAS coefficient estimate\noa = ()\nloglik_oa = oa.fit(X_train).score(X_test)",
            "from sklearn.feature_selection import \nfrom sklearn.neighbors import \nfrom sklearn.datasets import \n\nX, y = (return_X_y=True, as_frame=True)\nfeature_names = X.columns\nknn = (n_neighbors=3)\nsfs = (knn, n_features_to_select=2)\nsfs.fit(X, y)\nprint(\n    \"Features selected by forward sequential selection: \"\n    f\"{feature_names[sfs.get_support()].tolist()}\"\n)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Calibration->Comparison of Calibration of Classifiers->Calibration curves"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Covariance estimation->Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood->Compare different approaches to setting the regularization parameter"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New SequentialFeatureSelector transformer"
            ]
        ]
    },
    "1157290": {
        "jupyter_code_cell": "def scotch_recommender(user_scotches, n_recs=5):\n    user_preferences = (scotch_props.loc[scotch_props.index.isin(user_scotches)]\n                                    .mean(axis=0))\n    distances = cdist(scotch_props, user_preferences.to_frame().T).squeeze()\n    distances = pd.Series(data=distances,\n                      index=scotch_props.index)\n    distances = distances[~distances.index.isin(user_scotches)]\n    \n    return distances.sort_values()[:n_recs].index.tolist()",
        "matched_tutorial_code_inds": [
            3864,
            1953,
            5297,
            1672,
            5366
        ],
        "matched_tutorial_codes": [
            "def tsplot(y, lags=None, figsize=(10, 8)):\n    fig = plt.figure(figsize=figsize)\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n    \n    y.plot(ax=ts_ax)\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    [ax.set_xlim(1.5) for ax in [acf_ax, pacf_ax]]\n    sns.despine()\n    plt.tight_layout()\n    return ts_ax, acf_ax, pacf_ax",
            "def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "def ewma(series, beta, n_window):\n    nobs = len(series)\n    scalar = (1 - beta) / (1 + beta)\n    ma = []\n    k = np.arange(n_window, 0, -1)\n    weights = np.r_[beta ** k, 1, beta ** k[::-1]]\n    for t in range(n_window, nobs - n_window):\n        window = series.iloc[t - n_window : t + n_window + 1].values\n        ma.append(scalar * np.sum(weights * window))\n    return pd.Series(ma, name=series.name, index=series.iloc[n_window:-n_window].index)\n\n\nm2_ewma = ewma(np.log(m2[\"M2SL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4)\ncpi_ewma = ewma(\n    np.log(cpi[\"CPIAUCSL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4\n)",
            "def julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
            "def f(x):\n    n = x.shape[0]\n    g2 = x.group2\n    u = g2.unique()\n    u.sort()\n    uv = {v: k for k, v in enumerate(u)}\n    mat = np.zeros((n, len(u)))\n    for i in range(n):\n        mat[i, uv[g2.iloc[i]]] = 1\n    colnames = [\"%d\" % z for z in u]\n    return mat, colnames"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->Autocorrelation"
            ],
            [
                "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money"
            ],
            [
                "numpy->NumPy Applications->Plotting Fractals->Julia set"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ]
        ]
    },
    "14967": {
        "jupyter_code_cell": "import datashader as ds\nfrom datashader.bokeh_ext import InteractiveImage\nfrom bokeh.models import Range1d\n\np = base_plot()\np.xaxis.axis_label = \"Passengers\"\np.yaxis.axis_label = \"Tip, $\"\np.x_range = Range1d(-0.5, 6.5)\np.y_range = Range1d(0, 60)\n\npipeline = ds.Pipeline(df, ds.Point(\"passenger_count\", \"tip_amount\"), width_scale=0.035)\nInteractiveImage(p, pipeline)",
        "matched_tutorial_code_inds": [
            5177,
            4780,
            5671,
            3557,
            3435
        ],
        "matched_tutorial_codes": [
            "import pyarrow as pa\nimport pyarrow.parquet as pq\nimport statsmodels.formula.api as smf\n\nclass DataSet(dict):\n    def __init__(self, path):\n        self.parquet = pq.ParquetFile(path)\n\n    def __getitem__(self, key):\n        try:\n            return self.parquet.read([key]).to_pandas()[key]\n        except:\n            raise KeyError\n\nLargeData = DataSet('LargeData.parquet')\n\nres = smf.ols('Profit ~ Sugar + Power + Women', data=LargeData).fit()",
            "import matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nred_patch = mpatches.Patch(color='red', label='The red data')\nax.legend(handles=[red_patch])\n\nplt.show()\n\n\n<img alt=\"legend guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_legend_guide_001.png\" srcset=\"../../_images/sphx_glr_legend_guide_001.png, ../../_images/sphx_glr_legend_guide_001_2_0x.png 2.0x\"/>",
            "import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nstar98 = sm.datasets.star98.load_pandas().data\nformula = \"SUCCESS ~ LOWINC + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \\\n           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF\"\ndta = star98[\n    [\n        \"NABOVE\",\n        \"NBELOW\",\n        \"LOWINC\",\n        \"PERASIAN\",\n        \"PERBLACK\",\n        \"PERHISP\",\n        \"PCTCHRT\",\n        \"PCTYRRND\",\n        \"PERMINTE\",\n        \"AVYRSEXP\",\n        \"AVSALK\",\n        \"PERSPENK\",\n        \"PTRATIO\",\n        \"PCTAF\",\n    ]\n].copy()\nendog = dta[\"NABOVE\"] / (dta[\"NABOVE\"] + dta.pop(\"NBELOW\"))\ndel dta[\"NABOVE\"]\ndta[\"SUCCESS\"] = endog",
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, (ax0, ax1) = (ncols=2, figsize=(16, 6), sharey=True)\n\ndf = (evaluations[::-1]).set_index(\"estimator\")\ndf_std = (evaluations_std[::-1]).set_index(\"estimator\")\n\ndf.drop(\n    [\"train_time\"],\n    axis=\"columns\",\n).plot.barh(ax=ax0, xerr=df_std)\nax0.set_xlabel(\"Clustering scores\")\nax0.set_ylabel(\"\")\n\ndf[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\nax1.set_xlabel(\"Clustering time (s)\")\n()\n\n\n<img alt=\"plot document clustering\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_clustering_001.png\" srcset=\"../../_images/sphx_glr_plot_document_clustering_001.png\"/>",
            "import pandas as pd\nfrom sklearn.decomposition import \n\npca = (n_components=2).fit(X_train)\nscaled_pca = (n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\nfirst_pca_component = (\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n\n_ = ()\n\n\n<img alt=\"Weights of the first principal component\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->User Guide->Statistics and Tools->Working with Large Data Sets->Subsetting your data"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Legend guide->Creating artists specifically for adding to the legend (aka. Proxy artists)"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Using Formulas with GLMs"
            ],
            [
                "sklearn->Examples->Working with text documents->Clustering text documents using k-means->Clustering evaluation summary"
            ],
            [
                "sklearn->Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on a PCA dimensional reduction"
            ]
        ]
    },
    "723290": {
        "jupyter_code_cell": "if ('applForm' in locals()): del applForm",
        "matched_tutorial_code_inds": [
            675,
            1342,
            1758,
            5854,
            663
        ],
        "matched_tutorial_codes": [
            "print(.is_contiguous(memory_format=))  # Ouputs: True",
            "if sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif () &lt; 4:\n    print('Need at least four GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder():\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        self.encoder = (ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # Need (S, N) format for encoder.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder():\n    def __init__(self, ntoken, ninp):\n        super(Decoder, self).__init__()\n        self.decoder = (ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # Need batch dimension first for output of pipeline.\n        return self.decoder(inp).permute(1, 0, 2)",
            "textproc = TextPreprocess()",
            "sm.robust.scale.iqr(x)",
            "traced_rn18 = (rn18)\nprint()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->Memory Format API"
            ],
            [
                "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Define the model"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ],
            [
                "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing"
            ]
        ]
    },
    "714784": {
        "jupyter_code_cell": "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\npivoted.T[labels == 0].T.plot(legend=False, alpha=0.1, ax=ax[0]);\npivoted.T[labels == 1].T.plot(legend=False, alpha=0.1, ax=ax[1]);\n\nax[0].set_title('Purple Cluster')\nax[1].set_title('Red Cluster');",
        "matched_tutorial_code_inds": [
            4670,
            4665,
            4879,
            4663,
            4669
        ],
        "matched_tutorial_codes": [
            "fig, ax = plt.subplots(figsize=(5, 2.7))\nax.plot(np.arange(len(data1)), data1, label='data1')\nax.plot(np.arange(len(data2)), data2, label='data2')\nax.plot(np.arange(len(data3)), data3, 'd', label='data3')\nax.legend()\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_011.png\" srcset=\"../../_images/sphx_glr_quick_start_011.png, ../../_images/sphx_glr_quick_start_011_2_0x.png 2.0x\"/>",
            "fig, ax = plt.subplots(figsize=(5, 2.7))\nax.plot(data1, 'o', label='data1')\nax.plot(data2, 'd', label='data2')\nax.plot(data3, 'v', label='data3')\nax.plot(data4, 's', label='data4')\nax.legend()\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_008.png\" srcset=\"../../_images/sphx_glr_quick_start_008.png, ../../_images/sphx_glr_quick_start_008_2_0x.png 2.0x\"/>",
            "fig, ax = plt.subplots(ncols=2, figsize=(12, 8))\nax[0].plot(x, y)\nax[0].set_xlim(left=-1, right=1)\nax[0].plot(x + np.pi * 0.5, y)\nax[0].set_title(\"set_xlim(left=-1, right=1)\\n\")\nax[1].plot(x, y)\nax[1].set_xlim(left=-1, right=1)\nax[1].plot(x + np.pi * 0.5, y)\nax[1].autoscale()\nax[1].set_title(\"set_xlim(left=-1, right=1)\\nautoscale()\")\n\n\n<img alt=\"set_xlim(left=-1, right=1) , set_xlim(left=-1, right=1) autoscale()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_autoscale_007.png\" srcset=\"../../_images/sphx_glr_autoscale_007.png, ../../_images/sphx_glr_autoscale_007_2_0x.png 2.0x\"/>",
            "fig, ax = plt.subplots(figsize=(5, 2.7))\nx = np.arange(len(data1))\nax.plot(x, np.cumsum(data1), color='blue', linewidth=3, linestyle='--')\nl, = ax.plot(x, np.cumsum(data2), color='orange', linewidth=2)\nl.set_linestyle(':')\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_006.png\" srcset=\"../../_images/sphx_glr_quick_start_006.png, ../../_images/sphx_glr_quick_start_006_2_0x.png 2.0x\"/>",
            "fig, ax = plt.subplots(figsize=(5, 2.7))\n\nt = np.arange(0.0, 5.0, 0.01)\ns = np.cos(2 * np.pi * t)\nline, = ax.plot(t, s, lw=2)\n\nax.annotate('local max', xy=(2, 1), xytext=(3, 1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nax.set_ylim(-2, 2)\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_010.png\" srcset=\"../../_images/sphx_glr_quick_start_010.png, ../../_images/sphx_glr_quick_start_010_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Labelling plots->Legends"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Styling Artists->Linewidths, linestyles, and markersizes"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Autoscaling->Controlling autoscale"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Styling Artists"
            ],
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Labelling plots->Annotations"
            ]
        ]
    },
    "815500": {
        "jupyter_code_cell": "# Chicago domestic violence rate\nym = [i.strftime('%Y') for i in df['Date'][df['Domestic']]]\ncounts = Counter(ym)\nax = plot_counter(counts, sample_frac=1, last_year=True)\nax.set_title('Chicago Domestic Violence Rate')",
        "matched_tutorial_code_inds": [
            6423,
            6811,
            6951,
            6953,
            6949
        ],
        "matched_tutorial_codes": [
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "# Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "# Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "# Annual frequency, using a PeriodIndex\nindex = pd.period_range(start='2000', periods=4, freq='A')\nendog1 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog1.index)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ]
        ]
    },
    "1157750": {
        "jupyter_code_cell": "# Packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport scipy.stats as stats\nfrom scipy.stats import chi2\nfrom scipy.stats import shapiro\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols",
        "matched_tutorial_code_inds": [
            2969,
            630,
            6416,
            3929,
            1082
        ],
        "matched_tutorial_codes": [
            "from collections import \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import \nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import \n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import \nfrom sklearn.model_selection import",
            "# Some standard imports\nimport io\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx",
            "# Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "# Apply the default theme\nsns.set_theme()\n",
            "# Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites"
            ]
        ]
    },
    "1515110": {
        "jupyter_code_cell": "# Freshman\nalg = LogisticRegression(random_state=1)\nscores = cross_validation.cross_val_score(alg, frs[predictors], frs[\"finalmajor_full\"])\n\nprint(scores.mean())",
        "matched_tutorial_code_inds": [
            3257,
            6321,
            5454,
            6703,
            5457
        ],
        "matched_tutorial_codes": [
            "# initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "# Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()",
            "mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ]
        ]
    },
    "964580": {
        "jupyter_code_cell": "Data = pd.read_csv('Salary_Data.csv')\nX = Data.iloc[:, :1].values\ny = Data.iloc[:, 1].values",
        "matched_tutorial_code_inds": [
            3731,
            3857,
            6898,
            6251,
            5153
        ],
        "matched_tutorial_codes": [
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "fig = sf.plot()\nax = fig.get_axes()[0]\npt = ax.get_lines()[1]\npt.set_visible(False)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data"
            ],
            [
                "statsmodels->User Guide->Other Models->Methods for Survival and Duration Analysis->Survival function estimation and inference"
            ]
        ]
    },
    "1221941": {
        "jupyter_code_cell": "def second_most(s):\n    ugds, s = s.iloc[0], s.iloc[1:]\n    s = s.sort_values(ascending=False)\n    second_pct = s.iloc[1]\n    second_pop = (second_pct * ugds).astype(int)\n    return second_pop",
        "matched_tutorial_code_inds": [
            5297,
            1705,
            1953,
            5366,
            5810
        ],
        "matched_tutorial_codes": [
            "def ewma(series, beta, n_window):\n    nobs = len(series)\n    scalar = (1 - beta) / (1 + beta)\n    ma = []\n    k = np.arange(n_window, 0, -1)\n    weights = np.r_[beta ** k, 1, beta ** k[::-1]]\n    for t in range(n_window, nobs - n_window):\n        window = series.iloc[t - n_window : t + n_window + 1].values\n        ma.append(scalar * np.sum(weights * window))\n    return pd.Series(ma, name=series.name, index=series.iloc[n_window:-n_window].index)\n\n\nm2_ewma = ewma(np.log(m2[\"M2SL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4)\ncpi_ewma = ewma(\n    np.log(cpi[\"CPIAUCSL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4\n)",
            "def moving_mean(a, n):\n    ret = np.cumsum(a, dtype=float, axis=0)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\npollutants_A_24hr_avg = moving_mean(pollutants_A, 24)\npollutants_B_8hr_avg = moving_mean(pollutants_B, 8)[-(pollutants_A_24hr_avg.shape[0]):]",
            "def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "def f(x):\n    n = x.shape[0]\n    g2 = x.group2\n    u = g2.unique()\n    u.sort()\n    uv = {v: k for k, v in enumerate(u)}\n    mat = np.zeros((n, len(u)))\n    for i in range(n):\n        mat[i, uv[g2.iloc[i]]] = 1\n    colnames = [\"%d\" % z for z in u]\n    return mat, colnames",
            "def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Moving averages"
            ],
            [
                "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression"
            ]
        ]
    },
    "131740": {
        "jupyter_code_cell": "y_pred = gscv.best_estimator_.predict(x_test)",
        "matched_tutorial_code_inds": [
            5600,
            6973,
            6813,
            6900,
            6464
        ],
        "matched_tutorial_codes": [
            "resf_logit.predict()",
            "sm_probit_canned = sm.Probit(endog, exog).fit()",
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "ypred = olsres.predict(X)\nprint(ypred)",
            "sarima_res.predict(0, 2)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->In-sample prediction"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ]
        ]
    },
    "1046703": {
        "jupyter_code_cell": "mlt.subplots(figsize=(10,6))\nump=pd.concat([matches['umpire1'],matches['umpire2']]) \nax=ump.value_counts().head(10).plot.bar(width=0.8,color=sns.color_palette('summer',10))\nfor p in ax.patches:\n    ax.annotate(format(p.get_height()), (p.get_x()+0.15, p.get_height()+0.25))\nmlt.show()",
        "matched_tutorial_code_inds": [
            4972,
            4824,
            4823,
            4855,
            3944
        ],
        "matched_tutorial_codes": [
            "mpl.rcParams.update({'font.size': 14})\n\n# Indices to step through colormap.\nx = np.linspace(0.0, 1.0, 100)\n\ngradient = np.linspace(0, 1, 256)\ngradient = np.vstack((gradient, gradient))\n\n\ndef plot_color_gradients(cmap_category, cmap_list):\n    fig, axs = plt.subplots(nrows=len(cmap_list), ncols=2)\n    fig.subplots_adjust(top=0.95, bottom=0.01, left=0.2, right=0.99,\n                        wspace=0.05)\n    fig.suptitle(cmap_category + ' colormaps', fontsize=14, y=1.0, x=0.6)\n\n    for ax, name in zip(axs, cmap_list):\n\n        # Get RGB values for colormap.\n        rgb = mpl.colormaps[name](x)[np.newaxis, :, :3]\n\n        # Get colormap in CAM02-UCS colorspace. We want the lightness.\n        lab = cspace_converter(\"sRGB1\", \"CAM02-UCS\")(rgb)\n        L = lab[0, :, 0]\n        L = np.float32(np.vstack((L, L, L)))\n\n        ax[0].imshow(gradient, aspect='auto', cmap=mpl.colormaps[name])\n        ax[1].imshow(L, aspect='auto', cmap='binary_r', vmin=0., vmax=100.)\n        pos = list(ax[0].get_position().bounds)\n        x_text = pos[0] - 0.01\n        y_text = pos[1] + pos[3]/2.\n        fig.text(x_text, y_text, name, va='center', ha='right', fontsize=10)\n\n    # Turn off *all* ticks & spines, not just the ones with colormaps.\n    for ax in axs.flat:\n        ax.set_axis_off()\n\n    plt.show()\n\n\nfor cmap_category, cmap_list in cmaps.items():\n\n    plot_color_gradients(cmap_category, cmap_list)\n\n\n\n<img alt=\"Perceptually Uniform Sequential colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_015.png\" srcset=\"../../_images/sphx_glr_colormaps_015.png, ../../_images/sphx_glr_colormaps_015_2_0x.png 2.0x\"/>\n<img alt=\"Sequential colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_016.png\" srcset=\"../../_images/sphx_glr_colormaps_016.png, ../../_images/sphx_glr_colormaps_016_2_0x.png 2.0x\"/>\n<img alt=\"Sequential (2) colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_017.png\" srcset=\"../../_images/sphx_glr_colormaps_017.png, ../../_images/sphx_glr_colormaps_017_2_0x.png 2.0x\"/>\n<img alt=\"Diverging colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_018.png\" srcset=\"../../_images/sphx_glr_colormaps_018.png, ../../_images/sphx_glr_colormaps_018_2_0x.png 2.0x\"/>\n<img alt=\"Cyclic colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_019.png\" srcset=\"../../_images/sphx_glr_colormaps_019.png, ../../_images/sphx_glr_colormaps_019_2_0x.png 2.0x\"/>\n<img alt=\"Qualitative colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_020.png\" srcset=\"../../_images/sphx_glr_colormaps_020.png, ../../_images/sphx_glr_colormaps_020_2_0x.png 2.0x\"/>\n<img alt=\"Miscellaneous colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_021.png\" srcset=\"../../_images/sphx_glr_colormaps_021.png, ../../_images/sphx_glr_colormaps_021_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>",
            "plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>",
            "sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Grayscale conversion"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization"
            ]
        ]
    },
    "74527": {
        "jupyter_code_cell": "from encoder import encoder",
        "matched_tutorial_code_inds": [
            4473,
            1429,
            1719,
            2018,
            4548
        ],
        "matched_tutorial_codes": [
            "from scipy import stats",
            "from numpy import linalg",
            "from gym import wrappers\nfrom gym.wrappers import Monitor",
            "from sklearn import datasets\n\nfaces = ()",
            "from functools import partial"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Statistics (scipy.stats)->Random variables"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Operations on an axis"
            ],
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Set up Pong"
            ],
            [
                "sklearn->Examples->Clustering->Online learning of a dictionary of parts of faces->Load the data"
            ],
            [
                "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation"
            ]
        ]
    },
    "156597": {
        "jupyter_code_cell": "# Import the dataset\nraw = pd.read_csv(\"../datasets/data_indonesia.csv\", sep=',', \\\n                  dtype = None, error_bad_lines=False, \\\n                  encoding='utf-8', keep_default_na=False)                  \n                  \n# Extract the data into pandas dataframe\ndf = pd.DataFrame(raw)\n\n# Drop rows with missing values and drop duplicate (optional)\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)",
        "matched_tutorial_code_inds": [
            6345,
            6340,
            6314,
            6334,
            5622
        ],
        "matched_tutorial_codes": [
            "# Get the dataset\new_excs = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/ew_excs.prn\").content\nraw = pd.read_table(BytesIO(ew_excs), header=None, skipfooter=1, engine=\"python\")\nraw.index = pd.date_range(\"1926-01-01\", \"1995-12-01\", freq=\"MS\")\n\ndta_kns = raw.loc[:\"1986\"] - raw.loc[:\"1986\"].mean()\n\n# Plot the dataset\ndta_kns[0].plot(title=\"Excess returns\", figsize=(12, 3))\n\n# Fit the model\nmod_kns = sm.tsa.MarkovRegression(\n    dta_kns, k_regimes=3, trend=\"n\", switching_variance=True\n)\nres_kns = mod_kns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\"/>",
            "# Get the RGNP data to replicate Hamilton\ndta = pd.read_stata(\"https://www.stata-press.com/data/r14/rgnp.dta\").iloc[1:]\ndta.index = pd.DatetimeIndex(dta.date, freq=\"QS\")\ndta_hamilton = dta.rgnp\n\n# Plot the data\ndta_hamilton.plot(title=\"Growth rate of Real GNP\", figsize=(12, 3))\n\n# Fit the model\nmod_hamilton = sm.tsa.MarkovAutoregression(\n    dta_hamilton, k_regimes=2, order=4, switching_ar=False\n)\nres_hamilton = mod_hamilton.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\"/>",
            "# Get the federal funds rate data\nfrom statsmodels.tsa.regime_switching.tests.test_markov_regression import fedfunds\n\ndta_fedfunds = pd.Series(\n    fedfunds, index=pd.date_range(\"1954-07-01\", \"2010-10-01\", freq=\"QS\")\n)\n\n# Plot the data\ndta_fedfunds.plot(title=\"Federal funds rate\", figsize=(12, 3))\n\n# Fit the model\n# (a switching mean is the default of the MarkovRegession model)\nmod_fedfunds = sm.tsa.MarkovRegression(dta_fedfunds, k_regimes=2)\nres_fedfunds = mod_fedfunds.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\"/>",
            "# Get the federal funds rate data\nfrom statsmodels.tsa.regime_switching.tests.test_markov_regression import areturns\n\ndta_areturns = pd.Series(\n    areturns, index=pd.date_range(\"2004-05-04\", \"2014-5-03\", freq=\"W\")\n)\n\n# Plot the data\ndta_areturns.plot(title=\"Absolute returns, S&P500\", figsize=(12, 3))\n\n# Fit the model\nmod_areturns = sm.tsa.MarkovRegression(\n    dta_areturns.iloc[1:],\n    k_regimes=2,\n    exog=dta_areturns.iloc[:-1],\n    switching_variance=True,\n)\nres_areturns = mod_areturns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_25_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_25_0.png\"/>",
            "# Create three equidistant points\ndata = np.linspace(-1, 1, 3)\nkde = sm.nonparametric.KDEUnivariate(data)\n\n# Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, kernel in enumerate(kernel_switch.keys()):\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(kernel))\n\n    # Fit the model (estimate densities)\n    kde.fit(kernel=kernel, fft=False, gridsize=2 ** 10)\n\n    # Create the plot\n    ax.plot(kde.support, kde.density, lw=3, label=\"KDE from samples\", zorder=10)\n    ax.scatter(data, np.zeros_like(data), marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim([-3, 3])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Kim, Nelson, and Startz (1998) Three-state Variance Switching"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Hamilton (1989) switching model of GNP"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Switching variances"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions on three data points"
            ]
        ]
    },
    "870398": {
        "jupyter_code_cell": "# It seems that there are a lot of extra null columns; we'll drop those\ndf3.dropna(axis=1, how='all', inplace=True)",
        "matched_tutorial_code_inds": [
            6597,
            6964,
            165,
            4894,
            6953
        ],
        "matched_tutorial_codes": [
            "# Assign better names for our seasonal terms\ntrue_seasonal_10_3 = terms[0]\ntrue_seasonal_100_2 = terms[1]\ntrue_sum = true_seasonal_10_3 + true_seasonal_100_2\n<br/>",
            "# Here we'll catch the exception to prevent printing too much of\n# the exception trace output in this notebook\ntry:\n    res.forecast('2000-01-03')\nexcept KeyError as e:\n    print(e)",
            "# And just to show that we can round trip all of the results from earlier:\nround_tripped_results = pickle.loads(pickle.dumps(results))\nassert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))",
            "# histogram our data with numpy\ndata = np.random.randn(1000)\nn, bins = np.histogram(data, 100)",
            "# Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Benchmark->Steps->6. Saving/Loading benchmark results"
            ],
            [
                "matplotlib->Tutorials->Advanced->Path Tutorial->Compound paths"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ]
        ]
    },
    "1245411": {
        "jupyter_code_cell": "# Steps to normalize these results:\n# Convert data to a single column array.\n# Rehape the array to a single sample\n# Normalize the results\n\nhandgun_array = np.array(wyoming_permit_dat['handgun'])\n\nlong_gun_array = np.array(wyoming_permit_dat['long_gun'])\n\nz, p = wtests.ztest(handgun_array, long_gun_array)\n\nprint(\"The z-value comes out to be {}\".format(z))\n\nprint(\"The p-value comes out to be {}\".format(p))",
        "matched_tutorial_code_inds": [
            383,
            3679,
            3509,
            85,
            7011
        ],
        "matched_tutorial_codes": [
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "# Bring in the flights data\n\nflights = pd.read_hdf('flights.h5', 'flights')\n\nweather_locs = weather.index.levels[0]\n# The `categories` attribute of a Categorical is an Index\norigin_locs = flights.origin.cat.categories\ndest_locs = flights.dest.cat.categories\n\nairports = weather_locs &amp; origin_locs &amp; dest_locs\nairports",
            "# To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\n\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nlasso_cv = (alphas=alphas, random_state=0, max_iter=10000)\nk_fold = (3)\n\nprint(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\nprint()\nprint(\"Alpha parameters maximising the generalization score on different\")\nprint(\"subsets of the data:\")\nfor k, (train, test) in enumerate(k_fold.split(X, y)):\n    lasso_cv.fit(X[train], y[train])\n    print(\n        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\n            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\n        )\n    )\nprint()\nprint(\"Answer: Not very much since we obtained different alphas for different\")\nprint(\"subsets of the data and moreover, the scores for these alphas differ\")\nprint(\"quite substantially.\")\n\n()",
            "# Read checkpoint as desired, e.g.,\n# dev = torch.cuda.current_device()\n# checkpoint = torch.load(\"filename\",\n#                         map_location = lambda storage, loc: storage.cuda(dev))\nnet.load_state_dict(checkpoint[\"model\"])\nopt.load_state_dict(checkpoint[\"optimizer\"])\nscaler.load_state_dict(checkpoint[\"scaler\"])",
            "# You can use `scipy.optimize.curve_fit` to get the best-fit parameters and parameter errors.\nfrom scipy.optimize import curve_fit\n\n\ndef f(x, a, b):\n    return a * x + b\n\n\nxdata = data[\"x\"]\nydata = data[\"y\"]\np0 = [0, 0]  # initial parameter estimate\nsigma = data[\"y_err\"]\npopt, pcov = curve_fit(f, xdata, ydata, p0, sigma, absolute_sigma=True)\nperr = np.sqrt(np.diag(pcov))\nprint(\"a = {0:10.3f} +- {1:10.3f}\".format(popt[0], perr[0]))\nprint(\"b = {0:10.3f} +- {1:10.3f}\".format(popt[1], perr[1]))"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ],
            [
                "pandas_toms_blog->Indexes->Set Operations"
            ],
            [
                "sklearn->Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Bonus: how much can you trust the selection of alpha?"
            ],
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->Saving/Resuming"
            ],
            [
                "statsmodels->Examples->User Notes->Least squares fitting of models to data->Linear models->Check against scipy.optimize.curve_fit"
            ]
        ]
    },
    "360337": {
        "jupyter_code_cell": "# Support Vector Classifier (SVM/SVC)\nfrom sklearn.svm import SVC\nsvc = SVC(gamma=0.22)\nsvc.fit(X_train, y_train)\n#y_pred = logreg.predict(X_test)\nscore_svc = svc.score(X_test,y_test)\nprint('The accuracy of SVC is', score_svc)",
        "matched_tutorial_code_inds": [
            42,
            1966,
            6671,
            5640,
            383
        ],
        "matched_tutorial_codes": [
            "# Additional information\nEPOCH = 5\nPATH = \"model.pt\"\nLOSS = 0.4\n\n({\n            'epoch': EPOCH,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': LOSS,\n            }, PATH)",
            "# Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "# Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n\n# Plot estimate of the level term\nres_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\nres_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n\nax.legend(loc=\"lower left\");\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\"/>",
            "# Plot the confidence interval and fit\nfig, ax = pylab.subplots()\nax.scatter(x, y)\nax.plot(eval_x, smoothed, c=\"k\")\nax.fill_between(eval_x, bottom, top, alpha=0.5, color=\"b\")\npylab.autoscale(enable=True, axis=\"x\", tight=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\" src=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\"/>",
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")"
        ],
        "matched_tutorial_paths": [
            [
                "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->4. Save the general checkpoint"
            ],
            [
                "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ]
        ]
    },
    "921659": {
        "jupyter_code_cell": "np.ptp(data)",
        "matched_tutorial_code_inds": [
            5842,
            5685,
            5335,
            6071,
            5937
        ],
        "matched_tutorial_codes": [
            "np.median(x)",
            "np.bincount(data[\"affairs\"].astype(int))",
            "np.linalg.cond(results.model.exog)",
            "np.array(nobs1 + nobs2)",
            "np.array(se_beta).mean()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Regression Diagnostics->Multicollinearity"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss"
            ]
        ]
    },
    "628170": {
        "jupyter_code_cell": "col4.validators",
        "matched_tutorial_code_inds": [
            3718,
            5629,
            6907,
            6119,
            1453
        ],
        "matched_tutorial_codes": [
            "flights.dep_time",
            "kde.entropy",
            "res.params",
            "dta_c.T[0]",
            "reconstructed.shape"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Predicting with Formulas"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays"
            ]
        ]
    },
    "835435": {
        "jupyter_code_cell": "beadtypes = set([a.name for a in traj.topology.atoms])",
        "matched_tutorial_code_inds": [
            6969,
            6813,
            6302,
            717,
            5895
        ],
        "matched_tutorial_codes": [
            "exog = sm.add_constant(exog, prepend=True)",
            "pca_model = PCA(dta.T, standardize=False, demean=True)",
            "bk_cycles = sm.tsa.filters.bkfilter(dta[[\"infl\", \"unemp\"]])",
            "get_perf(, \"jacrev\", , \"jacfwd\")",
            "dta = sm.datasets.get_rdataset(\"starsCYG\", \"robustbase\", cache=True).data"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical."
            ],
            [
                "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->reverse-mode Jacobian (jacrev) vs forward-mode Jacobian (jacfwd)"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ]
        ]
    },
    "836066": {
        "jupyter_code_cell": "IRIS_FILE_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\nfile = requests.get(IRIS_FILE_URL).text",
        "matched_tutorial_code_inds": [
            5559,
            3741,
            5589,
            2443,
            587
        ],
        "matched_tutorial_codes": [
            "url = \"https://stats.idre.ucla.edu/stat/data/ologit.dta\"\ndata_student = pd.read_stata(url)",
            "if not os.path.exists(\"data/airports.csv.zip\"):\n    download_airports()",
            "nobs = len(data_student)\ndata_student[\"dummy\"] = (np.arange(nobs) &lt; (nobs / 2)).astype(float)",
            "all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "import requests\n\nresp = requests.post(\"http://localhost:5000/predict\",\n                     files={\"file\": open('&lt;PATH/TO/.jpg/FILE&gt;/cat.jpg','rb')})"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Integrating the model in our API Server"
            ]
        ]
    },
    "521639": {
        "jupyter_code_cell": "file=\"Transfers_Ultra.tree\"\n\ntry:\n    f=open(file, 'r')\nexcept IOError:\n    print (\"Unknown file: \"+file)\n    sys.exit()\n\nline = \"\"\nfor l in f:\n    line += l.strip()\n    \nf.close()\n\ntreeToAnnotate = Tree( line )\n\n\nrenumberNodes(treeToAnnotate, leafList2NodeIdRef)\n#print (treeToAnnotate.write(features=[\"support\"], format=2) )\nprint (treeToAnnotate.write( format=2) )",
        "matched_tutorial_code_inds": [
            6998,
            138,
            82,
            2773,
            2140
        ],
        "matched_tutorial_codes": [
            "url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "model = ()\ninputs = (5, 3, 224, 224)\n\nwith (activities=[ProfilerActivity.CPU],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n#                       aten::addmm      19.53 Kb      19.53 Kb             1\n#               aten::empty_strided         572 b         572 b            25\n#                     aten::resize_         240 b         240 b             6\n#                         aten::abs         480 b         240 b             4\n#                         aten::add         160 b         160 b            20\n#               aten::masked_select         120 b         112 b             1\n#                          aten::ne         122 b          53 b             6\n#                          aten::eq          60 b          30 b             2\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms\n\nprint(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#                  aten::batch_norm      47.41 Mb           0 b            20\n#      aten::_batch_norm_impl_index      47.41 Mb           0 b            20\n#           aten::native_batch_norm      47.41 Mb           0 b            20\n#                      aten::conv2d      47.37 Mb           0 b            20\n#                 aten::convolution      47.37 Mb           0 b            20\n#                aten::_convolution      47.37 Mb           0 b            20\n#          aten::mkldnn_convolution      47.37 Mb           0 b            20\n#                  aten::max_pool2d      11.48 Mb           0 b             1\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms",
            "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\nscaler = (enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Profiler->Steps->4. Using profiler to analyze memory consumption"
            ],
            [
                "torch->PyTorch Recipes->Automatic Mixed Precision->All together: \u201cAutomatic Mixed Precision\u201d"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns"
            ]
        ]
    },
    "1234230": {
        "jupyter_code_cell": "men_bot.shape[0], men_bot.sum(), fem_bot.shape[0], fem_bot.sum()",
        "matched_tutorial_code_inds": [
            1424,
            5751,
            1454,
            5735,
            5727
        ],
        "matched_tutorial_codes": [
            "img_array.max(), img_array.min()",
            "res_a2.pearson_chi2, res_a.pearson_chi2, res_a2.resid_pearson.sum(), res_a.resid_pearson.sum()",
            "reconstructed.min(), reconstructed.max()",
            "(res_e2._results.resid_response ** 2).sum(), (res_e._results.resid_response ** 2).sum()",
            "res_e._results.resid_response.mean(), res_e.model.family.variance(res_e.mu)[\n    :5\n], res_e.mu[:5]"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Shape, axis and array properties"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder"
            ],
            [
                "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Investigating Pearson chi-square statistic"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Investigating Pearson chi-square statistic"
            ]
        ]
    },
    "1468231": {
        "jupyter_code_cell": "# get items in api raw data\ndf_pv_mw = pd.DataFrame(pageview_mobileweb)\ndf_pv_ma = pd.DataFrame(pageview_mobileapp)\ndf_pv_dk = pd.DataFrame(pageview_desktop)\ndf_pc_ms = pd.DataFrame(pagecounts_mobile_site)\ndf_pc_ds = pd.DataFrame(pagecounts_desktop_site)\ndf_pv_mw_items = df_pv_mw['items']\ndf_pv_ma_items = df_pv_ma['items']\ndf_pv_dk_items = df_pv_dk['items']\ndf_pc_ms_items = df_pc_ms['items']\ndf_pc_ds_items = df_pc_ds['items']",
        "matched_tutorial_code_inds": [
            4694,
            1966,
            383,
            5639,
            406
        ],
        "matched_tutorial_codes": [
            "# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n# make up some data in the open interval (0, 1)\ny = np.random.normal(loc=0.5, scale=0.4, size=1000)\ny = y[(y  0) & (y &lt; 1)]\ny.sort()\nx = np.arange(len(y))\n\n# plot with various axes scales\nplt.figure()\n\n# linear\nplt.subplot(221)\nplt.plot(x, y)\nplt.yscale('linear')\nplt.title('linear')\nplt.grid(True)\n\n# log\nplt.subplot(222)\nplt.plot(x, y)\nplt.yscale('log')\nplt.title('log')\nplt.grid(True)\n\n# symmetric log\nplt.subplot(223)\nplt.plot(x, y - y.mean())\nplt.yscale('symlog', linthresh=0.01)\nplt.title('symlog')\nplt.grid(True)\n\n# logit\nplt.subplot(224)\nplt.plot(x, y)\nplt.yscale('logit')\nplt.title('logit')\nplt.grid(True)\n# Adjust the subplot layout, because the logit one may take more space\n# than usual, due to y-tick labels like \"1 - 10^{-3}\"\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()\n\n\n<img alt=\"linear, log, symlog, logit\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_010.png\" srcset=\"../../_images/sphx_glr_pyplot_010.png, ../../_images/sphx_glr_pyplot_010_2_0x.png 2.0x\"/>",
            "# Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "# Now create a bootstrap confidence interval around the a LOWESS fit\n\n\ndef lowess_with_confidence_bounds(\n    x, y, eval_x, N=200, conf_interval=0.95, lowess_kw=None\n):\n    \"\"\"\n    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n    \"\"\"\n    # Lowess smoothing\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, xvals=eval_x, **lowess_kw)\n\n    # Perform bootstrap resamplings of the data\n    # and  evaluate the smoothing at a fixed set of points\n    smoothed_values = np.empty((N, len(eval_x)))\n    for i in range(N):\n        sample = np.random.choice(len(x), len(x), replace=True)\n        sampled_x = x[sample]\n        sampled_y = y[sample]\n\n        smoothed_values[i] = sm.nonparametric.lowess(\n            exog=sampled_x, endog=sampled_y, xvals=eval_x, **lowess_kw\n        )\n\n    # Get the confidence interval\n    sorted_values = np.sort(smoothed_values, axis=0)\n    bound = int(N * (1 - conf_interval) / 2)\n    bottom = sorted_values[bound - 1]\n    top = sorted_values[-bound]\n\n    return smoothed, bottom, top\n\n\n# Compute the 95% confidence interval\neval_x = np.linspace(0, 4 * np.pi, 31)\nsmoothed, bottom, top = lowess_with_confidence_bounds(\n    x, y, eval_x, lowess_kw={\"frac\": 0.1}\n)",
            "# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Logarithmic and other nonlinear axes"
            ],
            [
                "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples"
            ]
        ]
    },
    "333615": {
        "jupyter_code_cell": "def bootstrap_statistic(stat_function, y_test, y_hat):\n    #Setting random seed ensures paired samples\n    np.random.seed(3743)\n    N = 10**3\n    result = np.empty(N)\n    for n in xrange(N):\n        I = np.random.choice(len(y_test), len(y_test))\n        result[n] = stat_function(y_test[I], y_hat[I])\n        \n    return result",
        "matched_tutorial_code_inds": [
            1953,
            5810,
            3144,
            1727,
            3123
        ],
        "matched_tutorial_codes": [
            "def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax",
            "def scoring_on_bootstrap(estimator, X, y, rng, n_bootstrap=100):\n    results_for_prevalence = (list)\n    for _ in range(n_bootstrap):\n        bootstrap_indices = rng.choice(\n            (X.shape[0]), size=X.shape[0], replace=True\n        )\n        for key, value in scoring(\n            estimator, X[bootstrap_indices], y[bootstrap_indices]\n        ).items():\n            results_for_prevalence[key].append(value)\n    return (results_for_prevalence)",
            "def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "def get_impute_zero_score(X_missing, y_missing):\n\n    imputer = (\n        missing_values=, add_indicator=True, strategy=\"constant\", fill_value=0\n    )\n    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return zero_impute_scores.mean(), zero_impute_scores.std()\n\n\nmses_california[1], stds_california[1] = get_impute_zero_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Zero imputation\")"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ],
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Replace missing values by 0"
            ]
        ]
    },
    "683966": {
        "jupyter_code_cell": "cols_to_visualize = [\"GPA\", \"weight\", \"life_rewarding\", \"healthy_feeling\"]\n\n# Plot histograms\nfor col in cols_to_visualize:\n    plt.title(col + \" histogram\")\n    plt.hist(QOFL_df[col])\n    plt.show()\n\n# Plot scatter matrix\npd.plotting.scatter_matrix(QOFL_df[cols_to_visualize])\nplt.suptitle(\"Scatter matrix of \" + ', '.join(cols_to_visualize))\nplt.show()",
        "matched_tutorial_code_inds": [
            2896,
            2852,
            3291,
            3290,
            5319
        ],
        "matched_tutorial_codes": [
            "features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")",
            "column_to_drop = [\"AGE\"]\n\ncv_model = (\n    model,\n    X.drop(columns=column_to_drop),\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\n\ncoefs = (\n    [\n        est[-1].regressor_.coef_\n        * est[:-1].transform(X.drop(columns=column_to_drop).iloc[train_idx]).std(axis=0)\n        for est, (train_idx, _) in zip(cv_model[\"estimator\"], cv.split(X, y))\n    ],\n    columns=feature_names[:-1],\n)",
            "cvs = [\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n()\n\n\n\n<img alt=\"KFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_006.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_006.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_007.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_007.png\"/>\n<img alt=\"ShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_008.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_008.png\"/>\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_009.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_009.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_010.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_010.png\"/>\n<img alt=\"GroupShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_011.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_011.png\"/>\n<img alt=\"StratifiedShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_012.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_012.png\"/>\n<img alt=\"TimeSeriesSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_013.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_013.png\"/>",
            "cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "exog_vars = [\"Mkt-RF\", \"SMB\", \"HML\"]\nexog = sm.add_constant(factors[exog_vars])\nrols = RollingOLS(endog, exog, window=60)\nrres = rols.fit()\nfig = rres.plot_recursive_coefficient(variables=exog_vars, figsize=(14, 18))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize cross-validation indices for many CV objects"
            ],
            [
                "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ]
        ]
    },
    "82504": {
        "jupyter_code_cell": "stats.binom_test(x=12,n=12+35,p=24/(24.+35.))",
        "matched_tutorial_code_inds": [
            4499,
            4538,
            4537,
            4488,
            6245
        ],
        "matched_tutorial_codes": [
            "stats.t.isf([0.1, 0.05, 0.01], [10, 11, 12])\narray([ 1.37218364,  1.79588482,  2.68099799])",
            "stats.ks_2samp(rvs1, rvs3)\nKstestResult(statistic=0.114, pvalue=0.00299005061044668)  # random",
            "stats.ks_2samp(rvs1, rvs2)\nKstestResult(statistic=0.026, pvalue=0.9959527565364388)  # random",
            "norm.stats(loc=3, scale=4, moments=\"mv\")\n(array(3.0), array(16.0))",
            "stats.normaltest(resid)"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Statistics (scipy.stats)->Random variables->Broadcasting"
            ],
            [
                "scipy->Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp"
            ],
            [
                "scipy->Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp"
            ],
            [
                "scipy->Statistics (scipy.stats)->Random variables->Shifting and scaling"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data",
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data"
            ]
        ]
    },
    "581134": {
        "jupyter_code_cell": "#remember only execute it once!\ndf_AAII = pd.read_csv('AAII.csv')\n\"\"\"convert_to_float(df_AAII) # --> df_AAII with \"Bullish\", \"Neutral\", \"Bearish\" set to float \n:param df_AAII: the dataframe to be processed\n:type df_AAII: pandas.core.frame.DataFrame\n:return: df_AAII DataFrame with float type column for the sentiment type\n\"\"\"\ndef convert_to_float(df_AAII):\n    sentimentList = [\"Bullish\", \"Neutral\", \"Bearish\"]\n    for i in range(len(sentimentList)):\n        sentimentType = sentimentList[i]\n        df_AAII[sentimentType] = df_AAII[sentimentType].replace('%','',regex=True).astype('float')/100\n    return df_AAII\ndf_AAII_float = convert_to_float(df_AAII)\n",
        "matched_tutorial_code_inds": [
            473,
            4775,
            137,
            5575,
            1092
        ],
        "matched_tutorial_codes": [
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "# plt.figure creates a matplotlib.figure.Figure instance\nfig = plt.figure()\nrect = fig.patch  # a rectangle instance\nrect.set_facecolor('lightgoldenrodyellow')\n\nax1 = fig.add_axes([0.1, 0.3, 0.4, 0.4])\nrect = ax1.patch\nrect.set_facecolor('lightslategray')\n\n\nfor label in ax1.xaxis.get_ticklabels():\n    # label is a Text instance\n    label.set_color('red')\n    label.set_rotation(45)\n    label.set_fontsize(16)\n\nfor line in ax1.yaxis.get_ticklines():\n    # line is a Line2D instance\n    line.set_color('green')\n    line.set_markersize(25)\n    line.set_markeredgewidth(3)\n\nplt.show()\n\n\n<img alt=\"artists\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_artists_004.png\" srcset=\"../../_images/sphx_glr_artists_004.png, ../../_images/sphx_glr_artists_004_2_0x.png 2.0x\"/>",
            "# (omitting some columns)\n# -------------------------------------------------------  ------------  ------------\n#                                                    Name     Self CUDA    CUDA total\n# -------------------------------------------------------  ------------  ------------\n#                                         model_inference       0.000us      11.666ms\n#                                            aten::conv2d       0.000us      10.484ms\n#                                       aten::convolution       0.000us      10.484ms\n#                                      aten::_convolution       0.000us      10.484ms\n#                              aten::_convolution_nogroup       0.000us      10.484ms\n#                                       aten::thnn_conv2d       0.000us      10.484ms\n#                               aten::thnn_conv2d_forward      10.484ms      10.484ms\n# void at::native::im2col_kernel&lt;float&gt;(long, float co...       3.844ms       3.844ms\n#                                       sgemm_32x32x32_NN       3.206ms       3.206ms\n#                                   sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n# -------------------------------------------------------  ------------  ------------\n# Self CPU time total: 23.015ms\n# Self CUDA time total: 11.666ms",
            "# using a SciPy distribution\nres_exp = OrderedModel(data_student['apply'],\n                           data_student[['pared', 'public', 'gpa']],\n                           distr=stats.expon).fit(method='bfgs', disp=False)\nres_exp.summary()",
            "# notice `quantize=False`\nmodel = models.resnet18(pretrained=True, progress=True, quantize=False)\nnum_ftrs = model.fc.in_features\n\n# Step 1\nmodel.train()\nmodel.fuse_model()\n# Step 2\nmodel_ft = create_combined_model(model)\nmodel_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n# Step 3\nmodel_ft = torch.quantization.prepare_qat(model_ft, inplace=True)"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Axis containers"
            ],
            [
                "torch->PyTorch Recipes->PyTorch Profiler->Steps->3. Using profiler to analyze execution time"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Ordinal regression with a custom cumulative cLogLog distribution:"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model"
            ]
        ]
    },
    "183655": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntips = sns.load_dataset('tips')\n\nsns.lmplot(x = 'total_bill', y = 'tip',hue='sex', data = tips, palette='Set1')\nplt.show()",
        "matched_tutorial_code_inds": [
            4021,
            6376,
            7,
            5089,
            1082
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())",
            "import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')",
            "# Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships",
                "seaborn->Plotting functions->Visualizing statistical relationships"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "torch->PyTorch Recipes->Loading data in PyTorch->5. [Optional] Visualize the data"
            ],
            [
                "matplotlib->Tutorials->Toolkits->The mplot3d toolkit"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites"
            ]
        ]
    },
    "189019": {
        "jupyter_code_cell": "survival_by_age_sex = df.groupby(['Age bins', 'Sex'])['Survived'].mean()",
        "matched_tutorial_code_inds": [
            3684,
            5533,
            3638,
            5380,
            5539
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "prestige_model = ols(\"prestige ~ income + education\", data=prestige).fit()",
            "resp25 = glm_mod.predict(pd.DataFrame(means25).T)\nresp75 = glm_mod.predict(pd.DataFrame(means75).T)\ndiff = resp75 - resp25"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Load the Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ]
        ]
    },
    "1227801": {
        "jupyter_code_cell": "from sklearn.preprocessing import LabelEncoder\nnumber = LabelEncoder()\ndf['Platform'] = number.fit_transform(df['Platform'].astype('str'))\ndf['Genre'] = number.fit_transform(df['Genre'].astype('str'))\ndf['Publisher'] = number.fit_transform(df['Publisher'].astype('str'))",
        "matched_tutorial_code_inds": [
            3328,
            6828,
            3609,
            6198,
            6796
        ],
        "matched_tutorial_codes": [
            "from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "from sklearn.feature_extraction.text import CountVectorizer\n count_vect = CountVectorizer()\n X_train_counts = count_vect.fit_transform(twenty_train.data)\n X_train_counts.shape\n(2257, 35788)",
            "from statsmodels.tsa.deterministic import DeterministicProcess\n\nindex = pd.RangeIndex(0, 100)\ndet_proc = DeterministicProcess(index, constant=True, order=1, seasonal=True, period=5)\ndet_proc.in_sample()",
            "from statsmodels.tsa.forecasting.theta import ThetaModel\n\ntm = ThetaModel(housing)\nres = tm.fit()\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Basic Use"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data"
            ]
        ]
    },
    "779342": {
        "jupyter_code_cell": "player_age.mean()",
        "matched_tutorial_code_inds": [
            5600,
            6341,
            6458,
            3723,
            6211
        ],
        "matched_tutorial_codes": [
            "resf_logit.predict()",
            "res_hamilton.summary()",
            "arima_res.predict(0, 2)",
            "flights.dep_time.head()",
            "det_proc.in_sample().head()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Hamilton (1989) switching model of GNP"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Custom Deterministic Terms"
            ]
        ]
    },
    "955497": {
        "jupyter_code_cell": "plt.figure(figsize=(30,10))\nplt.hist(business_df['categories'].head(350))\nplt.show()",
        "matched_tutorial_code_inds": [
            3730,
            6276,
            6264,
            6311,
            6303
        ],
        "matched_tutorial_codes": [
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax = cpi.plot(ax=ax)\nax.legend()",
            "fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Exercise: How good of in-sample prediction can you do for another series, say, CPI->Hint:"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical."
            ]
        ]
    },
    "1406337": {
        "jupyter_code_cell": "#Xtrain, Xtest = X[:int(len(X) * 0.50)], X[int(len(X) * 0.50):] \n#ytrain, ytest = y[:int(len(y) * 0.50)], y[int(len(y) * 0.50):] \nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.50)\nprint(Xtrain.shape)\nprint(Xtest.shape)",
        "matched_tutorial_code_inds": [
            473,
            370,
            2762,
            6754,
            2705
        ],
        "matched_tutorial_codes": [
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "# fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "# make a copy of the previous data\nXs = X.copy()\n# make Xs sparse by replacing the values lower than 2.5 with 0s\nXs[Xs &lt; 2.5] = 0.0\n# create a copy of Xs in sparse format\nXs_sp = (Xs)\nXs_sp = Xs_sp.tocsc()\n\n# compute the proportion of non-zero coefficient in the data matrix\nprint(f\"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%\")\n\nalpha = 0.1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = ()\nsparse_lasso.fit(Xs_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(Xs, y)\nprint(f\"Dense Lasso done in  {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Sparse data"
            ]
        ]
    },
    "492969": {
        "jupyter_code_cell": "obs = (obs\n       .resample('D')\n       .mean()\n       .interpolate('linear'))\n\nobs.head(10)",
        "matched_tutorial_code_inds": [
            3859,
            3644,
            3857,
            3917,
            5457
        ],
        "matched_tutorial_codes": [
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "daily = (\n    indiv[['transaction_dt', 'transaction_amt']].dropna()\n        .set_index('transaction_dt')['transaction_amt']\n        .resample(\"D\")\n        .sum()\n).compute()\ndaily",
            "poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson"
            ]
        ]
    },
    "111722": {
        "jupyter_code_cell": "from sqlalchemy import create_engine\nimport pandas as pd\nengine = create_engine('mysql+pymysql://python:python@localhost:3306/smarth2o')\ndf = pd.read_sql_query(\"\"\"\n SELECT 'xml' as source ,min(a.reading_date_time) as min_date_reading,max(a.reading_date_time) as max_date_reading ,count(*) as count FROM smarth2o.meter_reading_reimported a\n union all\n SELECT 'production' as source ,min(a.reading_date_time) as min_date_reading,max(a.reading_date_time) as max_date_reading,count(*) as count FROM smarth2o.meter_reading a \"\"\",engine)\ndf",
        "matched_tutorial_code_inds": [
            6516,
            5014,
            5038,
            3884,
            957
        ],
        "matched_tutorial_codes": [
            "from pandas_datareader.data import DataReader\n\n# Get the datasets from FRED\nstart = '1979-01-01'\nend = '2014-12-01'\nindprod = DataReader('IPMAN', 'fred', start=start, end=end)\nincome = DataReader('W875RX1', 'fred', start=start, end=end)\nsales = DataReader('CMRMTSPL', 'fred', start=start, end=end)\nemp = DataReader('PAYEMS', 'fred', start=start, end=end)\n# dta = pd.concat((indprod, income, sales, emp), axis=1)\n# dta.columns = ['indprod', 'income', 'sales', 'emp']",
            "from matplotlib.text import OffsetFrom\n\nfig, ax = plt.subplots(figsize=(3, 3))\nan1 = ax.annotate(\"Test 1\", xy=(0.5, 0.5), xycoords=\"data\",\n                  va=\"center\", ha=\"center\",\n                  bbox=dict(boxstyle=\"round\", fc=\"w\"))\n\noffset_from = OffsetFrom(an1, (0.5, 0))\nan2 = ax.annotate(\"Test 2\", xy=(0.1, 0.1), xycoords=\"data\",\n                  xytext=(0, -10), textcoords=offset_from,\n                  # xytext is offset points from \"xy=(0.5, 0), xycoords=an1\"\n                  va=\"top\", ha=\"center\",\n                  bbox=dict(boxstyle=\"round\", fc=\"w\"),\n                  arrowprops=dict(arrowstyle=\"-\"))\n\n\n<img alt=\"annotations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_annotations_021.png\" srcset=\"../../_images/sphx_glr_annotations_021.png, ../../_images/sphx_glr_annotations_021_2_0x.png 2.0x\"/>",
            "from matplotlib.backends.backend_pgf import PdfPages\nimport matplotlib.pyplot as plt\n\nwith PdfPages('multipage.pdf', metadata={'author': 'Me'}) as pdf:\n\n    fig1, ax1 = plt.subplots()\n    ax1.plot([1, 5, 3])\n    pdf.savefig(fig1)\n\n    fig2, ax2 = plt.subplots()\n    ax2.plot([1, 5, 3])\n    pdf.savefig(fig2)",
            "from pathlib import Path\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_parquet(\"data/indiv-10.parq\", columns=['occupation'], engine='pyarrow')\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name='torch_xla',\n    ext_modules=[\n        CppExtension(\n            '_XLAC',\n            torch_xla_sources,\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            library_dirs=library_dirs,\n            extra_link_args=extra_link_args + \\\n                [make_relative_rpath('torch_xla/lib')],\n        ),\n    ],\n    cmdclass={\n        'build_ext': Build,  # Build is a derived class of BuildExtension\n    }\n    # more configs...\n)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Macroeconomic data"
            ],
            [
                "matplotlib->Tutorials->Text->Annotations->Coordinate systems for annotations"
            ],
            [
                "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend->Multi-Page PDF Files"
            ],
            [
                "pandas_toms_blog->Scaling"
            ],
            [
                "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Build an extension"
            ]
        ]
    },
    "1223118": {
        "jupyter_code_cell": "#edit log formatting\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)\n\n\nfrom gensim.models import Word2Vec",
        "matched_tutorial_code_inds": [
            3635,
            3812,
            1082,
            3294,
            3448
        ],
        "matched_tutorial_codes": [
            "# filter the warning for now on\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "# Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()",
            "import sys\n\ntry:\n    import nmslib\nexcept ImportError:\n    print(\"The package 'nmslib' is required to run this example.\")\n    ()\n\ntry:\n    from pynndescent import PyNNDescentTransformer\nexcept ImportError:\n    print(\"The package 'pynndescent' is required to run this example.\")\n    ()",
            "from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Approximate nearest neighbors in TSNE"
            ],
            [
                "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation"
            ]
        ]
    },
    "934727": {
        "jupyter_code_cell": "from sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, \n                                                    random_state=123)",
        "matched_tutorial_code_inds": [
            3028,
            6011,
            2524,
            2177,
            1794
        ],
        "matched_tutorial_codes": [
            "from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>",
            "from statsmodels.stats.api import anova_lm\n\ntable1 = anova_lm(lm, interX_lm)\nprint(table1)\n\ninterM_lm = ols(\"S ~ X + C(E)*C(M)\", data=salary_table).fit()\nprint(interM_lm.summary())\n\ntable2 = anova_lm(lm, interM_lm)\nprint(table2)",
            "from sklearn.datasets import \n\nX, y = (\n    n_samples=500,\n    n_features=15,\n    n_informative=3,\n    n_redundant=2,\n    n_repeated=0,\n    n_classes=8,\n    n_clusters_per_class=1,\n    class_sep=0.8,\n    random_state=0,\n)",
            "from sklearn.model_selection import \nimport matplotlib.pyplot as plt\n\nscoring = \"neg_mean_absolute_percentage_error\"\nn_cv_folds = 3\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\n\ndef plot_results(figure_title):\n    fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\n    plot_info = [\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n    ]\n\n    x, width = (4), 0.9\n    for key, title, ax, y_limit in plot_info:\n        items = [\n            dropped_result[key],\n            one_hot_result[key],\n            ordinal_result[key],\n            native_result[key],\n        ]\n\n        mape_cv_mean = [(np.abs(item)) for item in items]\n        mape_cv_std = [(item) for item in items]\n\n        ax.bar(\n            x=x,\n            height=mape_cv_mean,\n            width=width,\n            yerr=mape_cv_std,\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n        )\n        ax.set(\n            xlabel=\"Model\",\n            title=title,\n            xticks=x,\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n            ylim=y_limit,\n        )\n    fig.suptitle(figure_title)\n\n\nplot_results(\"Gradient Boosting on Ames Housing\")\n\n\n<img alt=\"Gradient Boosting on Ames Housing, Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\"/>",
            "from sklearn.inspection import PartialDependenceDisplay\n\nfig, ax = (figsize=(14, 4), constrained_layout=True)\n_ = (\n    model,\n    X,\n    features=[\"age\", \"sex\", (\"pclass\", \"sex\")],\n    categorical_features=categorical_features,\n    ax=ax,\n)\n\n\n<img alt=\"plot release highlights 1 2 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_003.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Feature Selection->Recursive feature elimination with cross-validation->Data generation"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Model comparison"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.2->New and enhanced displays"
            ]
        ]
    },
    "858120": {
        "jupyter_code_cell": "x1 = np.linspace(start = chip_data[\"Microchip Test 1\"].min(), \n                 stop = chip_data[\"Microchip Test 1\"].max(), num = 100)\nx2 = np.linspace(start = chip_data[\"Microchip Test 2\"].min(), \n                 stop = chip_data[\"Microchip Test 2\"].max(), num = 100)",
        "matched_tutorial_code_inds": [
            6375,
            1625,
            2967,
            6370,
            2104
        ],
        "matched_tutorial_codes": [
            "states1 = pd.DataFrame(\n    np.c_[fit1.level, fit1.trend, fit1.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nstates2 = pd.DataFrame(\n    np.c_[fit2.level, fit2.trend, fit2.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nfig, [[ax1, ax4], [ax2, ax5], [ax3, ax6]] = plt.subplots(3, 2, figsize=(12, 8))\nstates1[[\"level\"]].plot(ax=ax1)\nstates1[[\"slope\"]].plot(ax=ax2)\nstates1[[\"seasonal\"]].plot(ax=ax3)\nstates2[[\"level\"]].plot(ax=ax4)\nstates2[[\"slope\"]].plot(ax=ax5)\nstates2[[\"seasonal\"]].plot(ax=ax6)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\"/>",
            "x_sobel = ndimage.sobel(xray_image, axis=0)\ny_sobel = ndimage.sobel(xray_image, axis=1)\n\nxray_image_sobel = np.hypot(x_sobel, y_sobel)\n\nxray_image_sobel *= 255.0 / np.max(xray_image_sobel)",
            "train_importances = (\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = (\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)",
            "fit1 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"add\",\n    initialization_method=\"estimated\",\n).fit()\nfit2 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"mul\",\n    initialization_method=\"estimated\",\n).fit()",
            "train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Sobel-Feldman operator (the Sobel filter)"
            ],
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets"
            ]
        ]
    },
    "1218605": {
        "jupyter_code_cell": "dataframe = dataframe.reset_index()\ndataframe['y'] = np.log(dataframe['y'])\ndataframe.head()",
        "matched_tutorial_code_inds": [
            3684,
            2822,
            3704,
            3978,
            4128
        ],
        "matched_tutorial_codes": [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "two_arrays_dict = {s.name: s.to_numpy() for s in two_series}\nsns.relplot(data=two_arrays_dict, kind=\"line\")\n",
            "x = np.random.normal(0, 1, 50)\ny = x * 2 + np.random.normal(0, 2, size=x.size)\nsns.regplot(x=x, y=y)\n"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data"
            ],
            [
                "seaborn->User guide and tutorial->Statistical estimation and error bars->Error bars on regression fits",
                "seaborn->Statistical operations->Statistical estimation and error bars->Error bars on regression fits"
            ]
        ]
    },
    "1469607": {
        "jupyter_code_cell": "import matplotlib.pyplot as plt\nplt.hist(train['is_iceberg'])",
        "matched_tutorial_code_inds": [
            4737,
            4736,
            5464,
            4741,
            4735
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\n plt.style.use(['dark_background', 'presentation'])",
            "import matplotlib.pyplot as plt\n plt.style.use(&lt;style-name)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np",
            "import matplotlib.pyplot as plt\n plt.style.use('./images/presentation.mplstyle')"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Composing styles"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Distributing styles"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "matplotlib->Tutorials->Introductory->Animations using Matplotlib"
            ],
            [
                "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Defining your own style"
            ]
        ]
    },
    "1415295": {
        "jupyter_code_cell": "model_lasso = Lasso(alpha=5e-4, max_iter=1150000).fit(X_train, y)",
        "matched_tutorial_code_inds": [
            2832,
            1051,
            5599,
            5583,
            6957
        ],
        "matched_tutorial_codes": [
            "model.fit(X_train, y_train)",
            "model = ()\nfoobar_unstructured(, name='bias')\n\nprint()",
            "resfd2_logit.predict(data_student.iloc[:5])",
            "resf_logit.predict(data_student.iloc[:5])",
            "mod = sm.tsa.SARIMAX(endog4)\nres = mod.fit()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "torch->Model Optimization->Pruning Tutorial->Extending torch.nn.utils.prune with custom pruning functions"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ]
        ]
    },
    "1134174": {
        "jupyter_code_cell": "# Count how many people responded to the promotion\nprint(clothingFile['RESP'] != 0).sum()",
        "matched_tutorial_code_inds": [
            3777,
            5573,
            6434,
            3857,
            6765
        ],
        "matched_tutorial_codes": [
            "# For each team... get number of days between games\ntidy.groupby('team')['date'].diff().dt.days - 1",
            "pred_choice = predicted.argmax(1)\nprint('Fraction of correct choice predictions')\nprint((np.asarray(data_student['apply'].values.codes) == pred_choice).mean())",
            "# In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach"
            ]
        ]
    },
    "1116941": {
        "jupyter_code_cell": "color_spec = {'var' : 'g', 'vartas' : '#ee55aa', 'varrotated' : '#55aaee', 'varclosest' : '#aa55ee'}\ncontext_data = all_data[all_data.k_type.isin(color_spec.keys())]\nwith sns.axes_style('darkgrid'):\n    g = sns.FacetGrid(context_data,col=\"pooltype\", size=5,hue=\"k_type\",palette=color_spec, sharey=True)\n    g.map(sns.pointplot, \"num_clusters\", \"purities\")\n    g.add_legend()\n    plt.savefig('purities.png')",
        "matched_tutorial_code_inds": [
            6007,
            2773,
            4682,
            3674,
            6598
        ],
        "matched_tutorial_codes": [
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))",
            "time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings"
            ],
            [
                "pandas_toms_blog->Indexes"
            ],
            [
                "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates"
            ]
        ]
    },
    "526388": {
        "jupyter_code_cell": "##Import pandas, numpy, matplotlib, seaborn, patches\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as mpatches",
        "matched_tutorial_code_inds": [
            1754,
            1484,
            5635,
            6376,
            1061
        ],
        "matched_tutorial_codes": [
            "# Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()",
            "import matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {, }, dtype=\n)\nprint(quantized_model)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Exploring the data"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->4. Test dynamic quantization"
            ]
        ]
    },
    "432589": {
        "jupyter_code_cell": "def sgd(model, X_train, y_train, minibatch_size):\n    for iter in range(n_iter):\n        print('Iteration {}'.format(iter))\n\n        # Randomize data point\n        X_train, y_train = shuffle(X_train, y_train)\n\n        for i in range(0, X_train.shape[0], minibatch_size):\n            # Get pair of (X, y) of the current minibatch/chunk\n            X_train_mini = X_train[i:i + minibatch_size]\n            y_train_mini = y_train[i:i + minibatch_size]\n\n            model = sgd_step(model, X_train_mini, y_train_mini)\n\n    return model",
        "matched_tutorial_code_inds": [
            385,
            1246,
            3144,
            1953,
            5810
        ],
        "matched_tutorial_codes": [
            "def train_model(model, , optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for , labels in dataloaders[phase]:\n                 = .to()\n                labels = labels.to()\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with (phase == 'train'):\n                    outputs = model()\n                    _, preds = (outputs, 1)\n                    loss = (outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * .size(0)\n                running_corrects += (preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model",
            "def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    ddp_loss = torch.zeros(2).to(rank)\n    if sampler:\n        sampler.set_epoch(epoch)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(rank), target.to(rank)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target, reduction='sum')\n        loss.backward()\n        optimizer.step()\n        ddp_loss[0] += loss.item()\n        ddp_loss[1] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n    if rank == 0:\n        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))",
            "def scoring_on_bootstrap(estimator, X, y, rng, n_bootstrap=100):\n    results_for_prevalence = (list)\n    for _ in range(n_bootstrap):\n        bootstrap_indices = rng.choice(\n            (X.shape[0]), size=X.shape[0], replace=True\n        )\n        for key, value in scoring(\n            estimator, X[bootstrap_indices], y[bootstrap_indices]\n        ).items():\n            results_for_prevalence[key].append(value)\n    return (results_for_prevalence)",
            "def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Training the model"
            ],
            [
                "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP"
            ],
            [
                "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence"
            ],
            [
                "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression"
            ]
        ]
    },
    "104880": {
        "jupyter_code_cell": "m4=train['KitchenQual'].mode()[0]\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(m4)\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(train['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(train['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(train['SaleType'].mode()[0])",
        "matched_tutorial_code_inds": [
            4424,
            4426,
            4599,
            6507,
            4529
        ],
        "matched_tutorial_codes": [
            "evals_small, evecs_small = eigsh(X, 3, which='SM', tol=1E-2)\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 0.99999999  0.00000024 -0.00000049],    # may vary (signs)\n       [-0.00000023  0.99999999  0.00000056],\n       [ 0.00000031 -0.00000037  0.99999852]])",
            "evals_small, evecs_small = eigsh(X, 3, sigma=0, which='LM')\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 1.  0.  0.],    # may vary (signs)\n       [ 0. -1. -0.],\n       [-0. -0.  1.]])",
            "a = np.arange(12).reshape(3,4)\n\n class fnc1d_class:\n...     def __init__(self, shape, axis = -1):\n...         # store the filter axis:\n...         self.axis = axis\n...         # store the shape:\n...         self.shape = shape\n...         # initialize the coordinates:\n...         self.coordinates = [0] * len(shape)\n...\n...     def filter(self, iline, oline):\n...         oline[...] = iline[:-2] + 2 * iline[1:-1] + 3 * iline[2:]\n...         print(self.coordinates)\n...         # calculate the next coordinates:\n...         axes = list(range(len(self.shape)))\n...         # skip the filter axis:\n...         del axes[self.axis]\n...         axes.reverse()\n...         for jj in axes:\n...             if self.coordinates[jj] &lt; self.shape[jj] - 1:\n...                 self.coordinates[jj] += 1\n...                 break\n...             else:\n...                 self.coordinates[jj] = 0\n...\n fnc = fnc1d_class(shape = (3,4))\n generic_filter1d(a, fnc.filter, 3)\n[0, 0]\n[1, 0]\n[2, 0]\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "exog = endog['dln_consump']\nmod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='n', exog=exog)\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "quantiles = [0.0, 0.01, 0.05, 0.1, 1-0.10, 1-0.05, 1-0.01, 1.0]\n crit = stats.t.ppf(quantiles, 10)\n crit\narray([       -inf, -2.76376946, -1.81246112, -1.37218364,  1.37218364,\n        1.81246112,  2.76376946,         inf])\n n_sample = x.size\n freqcount = np.histogram(x, bins=crit)[0]\n tprob = np.diff(quantiles)\n nprob = np.diff(stats.norm.cdf(crit))\n tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  2.30 pvalue = 0.8901  # random\n print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 64.60 pvalue = 0.0000  # random"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Examples"
            ],
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Examples"
            ],
            [
                "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Generic filter functions"
            ],
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR"
            ],
            [
                "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution"
            ]
        ]
    },
    "1298185": {
        "jupyter_code_cell": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n",
        "matched_tutorial_code_inds": [
            1912,
            2711,
            2645,
            2727,
            2017
        ],
        "matched_tutorial_codes": [
            "# Code source: Ga\u00ebl Varoquaux\n#              Andreas M\u00fcller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.datasets import , , \nfrom sklearn.neural_network import \nfrom sklearn.neighbors import \nfrom sklearn.svm import \nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \nfrom sklearn.tree import \nfrom sklearn.ensemble import , \nfrom sklearn.naive_bayes import \nfrom sklearn.discriminant_analysis import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nnames = [\n    \"Nearest Neighbors\",\n    \"Linear SVM\",\n    \"RBF SVM\",\n    \"Gaussian Process\",\n    \"Decision Tree\",\n    \"Random Forest\",\n    \"Neural Net\",\n    \"AdaBoost\",\n    \"Naive Bayes\",\n    \"QDA\",\n]\n\nclassifiers = [\n    (3),\n    (kernel=\"linear\", C=0.025),\n    (gamma=2, C=1),\n    (1.0 * (1.0)),\n    (max_depth=5),\n    (max_depth=5, n_estimators=10, max_features=1),\n    (alpha=1, max_iter=1000),\n    (),\n    (),\n    (),\n]\n\nX, y = (\n    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n)\nrng = (2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [\n    (noise=0.3, random_state=0),\n    (noise=0.2, factor=0.5, random_state=1),\n    linearly_separable,\n]\n\nfigure = (figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X_train, X_test, y_train, y_test = (\n        X, y, test_size=0.4, random_state=42\n    )\n\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ([\"#FF0000\", \"#0000FF\"])\n    ax = (len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n    # Plot the testing points\n    ax.scatter(\n        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n    )\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = (len(datasets), len(classifiers) + 1, i)\n\n        clf = ((), clf)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        (\n            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n        )\n\n        # Plot the training points\n        ax.scatter(\n            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n        )\n        # Plot the testing points\n        ax.scatter(\n            X_test[:, 0],\n            X_test[:, 1],\n            c=y_test,\n            cmap=cm_bright,\n            edgecolors=\"k\",\n            alpha=0.6,\n        )\n\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(\n            x_max - 0.3,\n            y_min + 0.3,\n            (\"%.2f\" % score).lstrip(\"0\"),\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        i += 1\n\n()\n()",
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import \nfrom sklearn import datasets\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# import some data to play with\niris = ()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\n# Create an instance of Logistic Regression Classifier and fit the data.\nlogreg = (C=1e5)\nlogreg.fit(X, Y)\n\n_, ax = (figsize=(4, 3))\n(\n    logreg,\n    X,\n    cmap=plt.cm.Paired,\n    ax=ax,\n    response_method=\"predict\",\n    plot_method=\"pcolormesh\",\n    shading=\"auto\",\n    xlabel=\"Sepal length\",\n    ylabel=\"Sepal width\",\n    eps=0.5,\n)\n\n# Plot also the training points\n(X[:, 0], X[:, 1], c=Y, edgecolors=\"k\", cmap=plt.cm.Paired)\n\n\n(())\n(())\n\n()",
            "# Author: Vincent Dubourg &lt;vincent.dubourg@gmail.com\n# Adapted to GaussianProcessClassifier:\n#         Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import , ConstantKernel as \n\n# A few constants\nlim = 8\n\n\ndef g(x):\n    \"\"\"The function to predict (classification will then consist in predicting\n    whether g(x) &lt;= 0 or not)\"\"\"\n    return 5.0 - x[:, 1] - 0.5 * x[:, 0] ** 2.0\n\n\n# Design of experiments\nX = (\n    [\n        [-4.61611719, -6.00099547],\n        [4.10469096, 5.32782448],\n        [0.00000000, -0.50000000],\n        [-6.17289014, -4.6984743],\n        [1.3109306, -6.93271427],\n        [-5.03823144, 3.10584743],\n        [-2.87600388, 6.74310541],\n        [5.21301203, 4.26386883],\n    ]\n)\n\n# Observations\ny = (g(X)  0, dtype=int)\n\n# Instantiate and fit Gaussian Process Model\nkernel = (0.1, (1e-5, )) * (sigma_0=0.1) ** 2\ngp = (kernel=kernel)\ngp.fit(X, y)\nprint(\"Learned kernel: %s \" % gp.kernel_)\n\n# Evaluate real function and the predicted probability\nres = 50\nx1, x2 = ((-lim, lim, res), (-lim, lim, res))\nxx = ([x1.reshape(x1.size), x2.reshape(x2.size)]).T\n\ny_true = g(xx)\ny_prob = gp.predict_proba(xx)[:, 1]\ny_true = y_true.reshape((res, res))\ny_prob = y_prob.reshape((res, res))\n\n# Plot the probabilistic classification iso-values\nfig = (1)\nax = fig.gca()\nax.axes.set_aspect(\"equal\")\n([])\n([])\nax.set_xticklabels([])\nax.set_yticklabels([])\n(\"$x_1$\")\n(\"$x_2$\")\n\ncax = (y_prob, cmap=cm.gray_r, alpha=0.8, extent=(-lim, lim, -lim, lim))\nnorm = plt.matplotlib.colors.Normalize(vmin=0.0, vmax=0.9)\ncb = (cax, ticks=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], norm=norm)\ncb.set_label(r\"${\\rm \\mathbb{P}}\\left[\\widehat{G}(\\mathbf{x}) \\leq 0\\right]$\")\n(0, 1)\n\n(X[y &lt;= 0, 0], X[y &lt;= 0, 1], \"r.\", markersize=12)\n\n(X[y  0, 0], X[y  0, 1], \"b.\", markersize=12)\n\n(x1, x2, y_true, [0.0], colors=\"k\", linestyles=\"dashdot\")\n\ncs = (x1, x2, y_prob, [0.666], colors=\"b\", linestyles=\"solid\")\n(cs, fontsize=11)\n\ncs = (x1, x2, y_prob, [0.5], colors=\"k\", linestyles=\"dashed\")\n(cs, fontsize=11)\n\ncs = (x1, x2, y_prob, [0.334], colors=\"r\", linestyles=\"solid\")\n(cs, fontsize=11)\n\n()",
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\nX_train = [0.5, 1].T\ny_train = [0.5, 1]\nX_test = [0, 2].T\n\n(0)\n\nclassifiers = dict(\n    ols=(), ridge=(alpha=0.1)\n)\n\nfor name, clf in classifiers.items():\n    fig, ax = (figsize=(4, 3))\n\n    for _ in range(6):\n        this_X = 0.1 * (size=(2, 1)) + X_train\n        clf.fit(this_X, y_train)\n\n        ax.plot(X_test, clf.predict(X_test), color=\"gray\")\n        ax.scatter(this_X, y_train, s=3, c=\"gray\", marker=\"o\", zorder=10)\n\n    clf.fit(X_train, y_train)\n    ax.plot(X_test, clf.predict(X_test), linewidth=2, color=\"blue\")\n    ax.scatter(X_train, y_train, s=30, c=\"red\", marker=\"+\", zorder=10)\n\n    ax.set_title(name)\n    ax.set_xlim(0, 2)\n    ax.set_ylim((0, 1.6))\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"y\")\n\n    fig.tight_layout()\n\n()",
            "# Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Though the following import is not directly being used, it is required\n# for 3D projection to work with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\nfrom sklearn.cluster import \nfrom sklearn import datasets\n\n(5)\n\niris = ()\nX = iris.data\ny = iris.target\n\nestimators = [\n    (\"k_means_iris_8\", (n_clusters=8, n_init=\"auto\")),\n    (\"k_means_iris_3\", (n_clusters=3, n_init=\"auto\")),\n    (\"k_means_iris_bad_init\", (n_clusters=3, n_init=1, init=\"random\")),\n]\n\nfig = (figsize=(10, 8))\ntitles = [\"8 clusters\", \"3 clusters\", \"3 clusters, bad initialization\"]\nfor idx, ((name, est), title) in enumerate(zip(estimators, titles)):\n    ax = fig.add_subplot(2, 2, idx + 1, projection=\"3d\", elev=48, azim=134)\n    est.fit(X)\n    labels = est.labels_\n\n    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(float), edgecolor=\"k\")\n\n    ax.xaxis.set_ticklabels([])\n    ax.yaxis.set_ticklabels([])\n    ax.zaxis.set_ticklabels([])\n    ax.set_xlabel(\"Petal width\")\n    ax.set_ylabel(\"Sepal length\")\n    ax.set_zlabel(\"Petal length\")\n    ax.set_title(title)\n\n# Plot the ground truth\nax = fig.add_subplot(2, 2, 4, projection=\"3d\", elev=48, azim=134)\n\nfor name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n    ax.text3D(\n        X[y == label, 3].mean(),\n        X[y == label, 0].mean(),\n        X[y == label, 2].mean() + 2,\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.2, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\ny = (y, [1, 2, 0]).astype(float)\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\nax.set_xlabel(\"Petal width\")\nax.set_ylabel(\"Sepal length\")\nax.set_zlabel(\"Petal length\")\nax.set_title(\"Ground Truth\")\n\n(wspace=0.25, hspace=0.25)\n()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Classification->Classifier comparison"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Logistic Regression 3-class Classifier"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Iso-probability lines for Gaussian Processes classification (GPC)"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Ordinary Least Squares and Ridge Regression Variance"
            ],
            [
                "sklearn->Examples->Clustering->K-means Clustering"
            ]
        ]
    },
    "1447973": {
        "jupyter_code_cell": "import pandas as pd\nimport csv\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.utils import shuffle",
        "matched_tutorial_code_inds": [
            6505,
            6561,
            5796,
            5558,
            5464
        ],
        "matched_tutorial_codes": [
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO",
            "import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->VARMAX: Introduction",
                "statsmodels->Examples->State space models->Trends and cycles in unemployment"
            ],
            [
                "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ]
        ]
    },
    "1350574": {
        "jupyter_code_cell": "# Writing a query that collates data on purchases from different countries, \n# which includes columns for total number of customers, total value of sales\n# average value of sales per customer, and average order value.  Countries\n# with just one customer are lumped together into an \"Other\" group, and\n# that group is listed last.\n\n\ncountry_sales_query = \"\"\"\n\nWITH \n    total_customers AS (\n        SELECT country nation, COUNT(*) num_of_customers \n        FROM customer \n        GROUP BY country\n     ), \n    total_value AS(\n        SELECT c.country nation, SUM(i.total) total_sales, AVG(i.total) avg_sales\n        FROM invoice i \n        INNER JOIN customer c ON c.customer_id = i.customer_id\n        GROUP BY nation\n      ),\n    multi_customer_countries AS (\n        SELECT \n            tc.nation, \n            tc.num_of_customers, \n            tv.total_sales total_sales_country,\n            CAST(tv.total_sales as float)/CAST(tc.num_of_customers as Float) average_value_per_customer,\n            tv.avg_sales average_order_value \n        FROM total_customers tc\n        INNER join total_value tv ON tv.nation = tc.nation\n        WHERE tc.num_of_customers > 1\n      ), \n    other_countries AS (\n        SELECT \n            CASE\n                WHEN tc.num_of_customers = 1 THEN \"Other\"\n            END\n            as nation,\n            COUNT(*) num_of_customers,\n            SUM(tv.total_sales) total_sales_country,\n            CAST(SUM(tv.total_sales) AS float)/CAST(COUNT(*) as FLOAT) average_value_per_customer,\n            AVG(tv.avg_sales) avg_order_value\n        FROM total_customers tc\n        INNER join total_value tv ON tv.nation = tc.nation\n        WHERE tc.num_of_customers = 1\n      ),\n    final_sales as (\n        SELECT * FROM multi_customer_countries\n        UNION\n        SELECT * FROM other_countries\n      )\n      \nSELECT nation, num_of_customers, total_sales_country, average_order_value, average_value_per_customer\nFROM (\n  SELECT\n      fs.*,\n      CASE\n          WHEN fs.nation = \"Other\" THEN 1\n          ELSE 0\n      end AS sort\n  FROM final_sales fs\n  )\norder by sort ASC, total_sales_country DESC\n\n\"\"\"\n\nrun_query(country_sales_query)",
        "matched_tutorial_code_inds": [
            1966,
            5639,
            383,
            3509,
            6545
        ],
        "matched_tutorial_codes": [
            "# Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "# Now create a bootstrap confidence interval around the a LOWESS fit\n\n\ndef lowess_with_confidence_bounds(\n    x, y, eval_x, N=200, conf_interval=0.95, lowess_kw=None\n):\n    \"\"\"\n    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n    \"\"\"\n    # Lowess smoothing\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, xvals=eval_x, **lowess_kw)\n\n    # Perform bootstrap resamplings of the data\n    # and  evaluate the smoothing at a fixed set of points\n    smoothed_values = np.empty((N, len(eval_x)))\n    for i in range(N):\n        sample = np.random.choice(len(x), len(x), replace=True)\n        sampled_x = x[sample]\n        sampled_y = y[sample]\n\n        smoothed_values[i] = sm.nonparametric.lowess(\n            exog=sampled_x, endog=sampled_y, xvals=eval_x, **lowess_kw\n        )\n\n    # Get the confidence interval\n    sorted_values = np.sort(smoothed_values, axis=0)\n    bound = int(N * (1 - conf_interval) / 2)\n    bottom = sorted_values[bound - 1]\n    top = sorted_values[-bound]\n\n    return smoothed, bottom, top\n\n\n# Compute the 95% confidence interval\neval_x = np.linspace(0, 4 * np.pi, 31)\nsmoothed, bottom, top = lowess_with_confidence_bounds(\n    x, y, eval_x, lowess_kw={\"frac\": 0.1}\n)",
            "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "# To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\n\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nlasso_cv = (alphas=alphas, random_state=0, max_iter=10000)\nk_fold = (3)\n\nprint(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\nprint()\nprint(\"Alpha parameters maximising the generalization score on different\")\nprint(\"subsets of the data:\")\nfor k, (train, test) in enumerate(k_fold.split(X, y)):\n    lasso_cv.fit(X[train], y[train])\n    print(\n        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\n            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\n        )\n    )\nprint()\nprint(\"Answer: Not very much since we obtained different alphas for different\")\nprint(\"subsets of the data and moreover, the scores for these alphas differ\")\nprint(\"quite substantially.\")\n\n()",
            "# Create Table I\ntable_i = np.zeros((5,6))\n\nstart = dta.index[0]\nend = dta.index[-1]\ntime_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\nmodels = [\n    ('US GNP', time_range, 'None'),\n    ('US Prices', time_range, 'None'),\n    ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n    ('US monetary base', time_range, 'None'),\n    ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n]\nindex = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\nparameter_symbols = [\n    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n]\n\ni = 0\nfor res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n    if res.model.stochastic_level:\n        (sigma_irregular, sigma_level, sigma_trend,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n    else:\n        (sigma_irregular, sigma_level,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n        sigma_trend = np.nan\n    period_cycle = 2 * np.pi / frequency_cycle\n\n    table_i[i, :] = [\n        sigma_level*1e7, sigma_trend*1e7,\n        sigma_cycle*1e7, damping_cycle, period_cycle,\n        sigma_irregular*1e7\n    ]\n    i += 1\n\npd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\ntable_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\ntable_i"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data"
            ],
            [
                "sklearn->Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Bonus: how much can you trust the selection of alpha?"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Model"
            ]
        ]
    },
    "799682": {
        "jupyter_code_cell": "y_train = np.log1p(train['price'])\ntrain['category_name'] = train['category_name'].fillna('Other').astype(str)\ntrain['brand_name'] = train['brand_name'].fillna('missing').astype(str)\ntrain['shipping'] = train['shipping'].astype(str)\ntrain['item_condition_id'] = train['item_condition_id'].astype(str)\ntrain['item_description'] = train['item_description'].fillna('None')",
        "matched_tutorial_code_inds": [
            2433,
            6674,
            5913,
            2824,
            5910
        ],
        "matched_tutorial_codes": [
            "y = df[\"count\"] / df[\"count\"].max()",
            "y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>",
            "student_resid   unadj_p  fdr_bh(p)\n16      -2.049393  0.046415   0.764747\n13      -2.035329  0.047868   0.764747\n33       1.905847  0.063216   0.764747\n18      -1.575505  0.122304   0.764747\n1        1.522185  0.135118   0.764747\n3        1.522185  0.135118   0.764747\n21      -1.450418  0.154034   0.764747\n17      -1.426675  0.160731   0.764747\n29       1.388520  0.171969   0.764747\n14      -1.374733  0.176175   0.764747\n35       1.346543  0.185023   0.764747\n34      -1.272159  0.209999   0.764747\n28      -1.186946  0.241618   0.764747\n20      -1.150621  0.256103   0.764747\n44       1.134779  0.262612   0.764747\n39       1.091886  0.280826   0.764747\n19       1.064878  0.292740   0.764747\n6       -1.026873  0.310093   0.764747\n30      -1.009096  0.318446   0.764747\n22      -0.979768  0.332557   0.764747\n8        0.961218  0.341695   0.764747\n5        0.913802  0.365801   0.768599\n11       0.871997  0.387943   0.768599\n12       0.856685  0.396261   0.768599\n46      -0.833923  0.408829   0.768599\n10       0.743920  0.460879   0.770890\n42       0.727179  0.470968   0.770890\n15      -0.689258  0.494280   0.770890\n43       0.688272  0.494895   0.770890\n7        0.655712  0.515424   0.770890\n40      -0.646396  0.521381   0.770890\n26      -0.640978  0.524862   0.770890\n25      -0.545561  0.588123   0.837630\n32       0.472819  0.638680   0.843682\n37       0.472819  0.638680   0.843682\n38       0.462187  0.646225   0.843682\n0        0.430686  0.668799   0.849556\n31       0.341726  0.734184   0.892552\n36       0.318911  0.751303   0.892552\n4        0.307890  0.759619   0.892552\n9        0.235114  0.815211   0.922751\n41       0.187732  0.851950   0.922751\n2       -0.182093  0.856346   0.922751\n23      -0.156014  0.876736   0.922751\n27      -0.147406  0.883485   0.922751\n24       0.065195  0.948314   0.963776\n45       0.045675  0.963776   0.963776",
            "y = survey.target.values.ravel()\nsurvey.target.head()",
            "student_resid   unadj_p  sidak(p)\n16      -2.049393  0.046415  0.892872\n13      -2.035329  0.047868  0.900286\n33       1.905847  0.063216  0.953543\n18      -1.575505  0.122304  0.997826\n1        1.522185  0.135118  0.998911\n3        1.522185  0.135118  0.998911\n21      -1.450418  0.154034  0.999615\n17      -1.426675  0.160731  0.999735\n29       1.388520  0.171969  0.999859\n14      -1.374733  0.176175  0.999889\n35       1.346543  0.185023  0.999933\n34      -1.272159  0.209999  0.999985\n28      -1.186946  0.241618  0.999998\n20      -1.150621  0.256103  0.999999\n44       1.134779  0.262612  0.999999\n39       1.091886  0.280826  1.000000\n19       1.064878  0.292740  1.000000\n6       -1.026873  0.310093  1.000000\n30      -1.009096  0.318446  1.000000\n22      -0.979768  0.332557  1.000000\n8        0.961218  0.341695  1.000000\n5        0.913802  0.365801  1.000000\n11       0.871997  0.387943  1.000000\n12       0.856685  0.396261  1.000000\n46      -0.833923  0.408829  1.000000\n10       0.743920  0.460879  1.000000\n42       0.727179  0.470968  1.000000\n15      -0.689258  0.494280  1.000000\n43       0.688272  0.494895  1.000000\n7        0.655712  0.515424  1.000000\n40      -0.646396  0.521381  1.000000\n26      -0.640978  0.524862  1.000000\n25      -0.545561  0.588123  1.000000\n32       0.472819  0.638680  1.000000\n37       0.472819  0.638680  1.000000\n38       0.462187  0.646225  1.000000\n0        0.430686  0.668799  1.000000\n31       0.341726  0.734184  1.000000\n36       0.318911  0.751303  1.000000\n4        0.307890  0.759619  1.000000\n9        0.235114  0.815211  1.000000\n41       0.187732  0.851950  1.000000\n2       -0.182093  0.856346  1.000000\n23      -0.156014  0.876736  1.000000\n27      -0.147406  0.883485  1.000000\n24       0.065195  0.948314  1.000000\n45       0.045675  0.963776  1.000000"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points"
            ]
        ]
    },
    "447074": {
        "jupyter_code_cell": "q_value_observations",
        "matched_tutorial_code_inds": [
            3902,
            5561,
            5629,
            5316,
            5600
        ],
        "matched_tutorial_codes": [
            "total_by_employee",
            "data_student.dtypes",
            "kde.entropy",
            "params.iloc[57:62]",
            "resf_logit.predict()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Rolling Least Squares"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ]
        ]
    },
    "1191469": {
        "jupyter_code_cell": "grouped = data.groupby('TripType')\ntem_b = grouped['HEALTH AND BEAUTY AIDS', 'SHOES', 'MENS WEAR', 'SWIMWEAR/OUTERWEAR', 'LADIES SOCKS'].agg([np.sum])\n\ntem_b.plot(kind='bar', figsize = (15,12.5))",
        "matched_tutorial_code_inds": [
            2129,
            3784,
            6159,
            5689,
            2866
        ],
        "matched_tutorial_codes": [
            "batch_pca_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=100, batch_size=3, random_state=rng\n)\nbatch_pca_estimator.fit(faces_centered)\nplot_gallery(\n    \"Sparse components - MiniBatchSparsePCA\",\n    batch_pca_estimator.components_[:n_components],\n)\n\n\n<img alt=\"Sparse components - MiniBatchSparsePCA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\"/>",
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "marginals = [stats.gamma(2), stats.norm]\njoint_dist = CopulaDistribution(copula=IndependenceCopula(), marginals=marginals)\nsample = joint_dist.rvs(512, random_state=20210801)\nh = sns.jointplot(x=sample[:, 0], y=sample[:, 1], kind=\"scatter\")\n_ = h.set_axis_labels(\"X1\", \"X2\", fontsize=16)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_copula_9_0.png\" src=\"../../../_images/examples_notebooks_generated_copula_9_0.png\"/>",
            "gr = data[\"affairs rate_marriage age yrs_married\".split()].groupby(\n    \"rate_marriage age yrs_married\".split()\n)\ndf_a = gr.agg([\"mean\", \"sum\", \"count\"])\n\n\ndef merge_tuple(tpl):\n    if isinstance(tpl, tuple) and len(tpl)  1:\n        return \"_\".join(map(str, tpl))\n    else:\n        return tpl\n\n\ndf_a.columns = df_a.columns.map(merge_tuple)\ndf_a.reset_index(inplace=True)\nprint(df_a.shape)\ndf_a.head()",
            "coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, small regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Sparse components - MiniBatchSparsePCA"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "statsmodels->Examples->Statistics->Copulas->Sampling from a copula->Reproducibility"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique explanatory variables (exog)"
            ],
            [
                "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->"
            ]
        ]
    },
    "1152178": {
        "jupyter_code_cell": "beta = np.array([5000, 2.3, 1.3, 10]).reshape(1, 4)\ny = np.dot(beta, X.T).T\n# Adding noise\ny = y + 2000*np.random.random((10, 1))\ny",
        "matched_tutorial_code_inds": [
            5193,
            3502,
            5230,
            2613,
            4429
        ],
        "matched_tutorial_codes": [
            "nsample = 100\nx = np.linspace(0, 10, 100)\nX = np.column_stack((x, x ** 2))\nbeta = np.array([1, 0.1, 10])\ne = np.random.normal(size=nsample)",
            "X = (5 * (40, 1), axis=0)\ny = (X).ravel()\n\n# add noise to targets\ny[::5] += 3 * (0.5 - (8))",
            "beta = [1.0, 0.3, -0.0, 10]\ny_true = np.dot(X, beta)\ny = y_true + np.random.normal(size=nsample)\n\nres3 = sm.OLS(y, X).fit()",
            "rng = (0)\nX_train = rng.uniform(0, 5, size=20).reshape(-1, 1)\ny_train = target_generator(X_train, add_noise=True)",
            "N = 100\n rng = np.random.default_rng()\n d = rng.normal(0, 1, N).astype(np.float64)\n D = np.diag(d)\n Dop = Diagonal(d, dtype=np.float64)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation"
            ],
            [
                "sklearn->Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels->Generate sample data"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Small group effects"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Data generation"
            ],
            [
                "scipy->Sparse eigenvalue problems with ARPACK->Use of LinearOperator"
            ]
        ]
    },
    "31474": {
        "jupyter_code_cell": "print(pokemon_df['Type 1'].value_counts().head())\n\nprint(pokemon_df['Type 1'].map(lambda type1: type1.upper()).head())",
        "matched_tutorial_code_inds": [
            1582,
            5216,
            5782,
            5452,
            1569
        ],
        "matched_tutorial_codes": [
            "print(training_labels[0])\nprint(training_labels[1])\nprint(training_labels[2])",
            "print(X[:5, :])\nprint(y[:5])\nprint(groups)\nprint(dummy[:5, :])",
            "print(res_f2.summary())\nprint(res_f.summary())",
            "print(anes_data.exog.head())\nprint(anes_data.endog.head())",
            "print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format"
            ]
        ]
    },
    "811437": {
        "jupyter_code_cell": "# import optimized pickle written in C for serializing and \n# de-serializing a Python object\nimport _pickle as pkl\n\n# dump to disk\npkl.dump(hashvec, open('output/hashvec.pkl', 'wb'))\npkl.dump(clf, open('output/clf-sgd.pkl', 'wb'))\n\n# load from disk\nhashvec = pkl.load(open('output/hashvec.pkl', 'rb'))\nclf = pkl.load(open('output/clf-sgd.pkl', 'rb'))\n\ndf_test = pd.read_csv('datasets/test.csv')\nprint('test auc: %.3f' % roc_auc_score(df_test['sentiment'], \\\n            clf.predict_proba(hashvec.transform(df_test['review']))[:,1]))",
        "matched_tutorial_code_inds": [
            2176,
            1590,
            6664,
            473,
            5622
        ],
        "matched_tutorial_codes": [
            "# The ordinal encoder will first output the categorical features, and then the\n# continuous (passed-through) features\n\nhist_native = (\n    ordinal_encoder,\n    (\n        random_state=42,\n        categorical_features=categorical_columns,\n    ),\n).set_output(transform=\"pandas\")",
            "# To store training and test set losses and accurate predictions\n# for visualization.\nstore_training_loss = []\nstore_training_accurate_pred = []\nstore_test_loss = []\nstore_test_accurate_pred = []\n\n# This is a training loop.\n# Run the learning experiment for a defined number of epochs (iterations).\nfor j in range(epochs):\n\n    #################\n    # Training step #\n    #################\n\n    # Set the initial loss/error and the number of accurate predictions to zero.\n    training_loss = 0.0\n    training_accurate_predictions = 0\n\n    # For all images in the training set, perform a forward pass\n    # and backpropagation and adjust the weights accordingly.\n    for i in range(len(training_images)):\n        # Forward propagation/forward pass:\n        # 1. The input layer:\n        #    Initialize the training image data as inputs.\n        layer_0 = training_images[i]\n        # 2. The hidden layer:\n        #    Take in the training image data into the middle layer by\n        #    matrix-multiplying it by randomly initialized weights.\n        layer_1 = np.dot(layer_0, weights_1)\n        # 3. Pass the hidden layer's output through the ReLU activation function.\n        layer_1 = relu(layer_1)\n        # 4. Define the dropout function for regularization.\n        dropout_mask = rng.integers(low=0, high=2, size=layer_1.shape)\n        # 5. Apply dropout to the hidden layer's output.\n        layer_1 *= dropout_mask * 2\n        # 6. The output layer:\n        #    Ingest the output of the middle layer into the the final layer\n        #    by matrix-multiplying it by randomly initialized weights.\n        #    Produce a 10-dimension vector with 10 scores.\n        layer_2 = np.dot(layer_1, weights_2)\n\n        # Backpropagation/backward pass:\n        # 1. Measure the training error (loss function) between the actual\n        #    image labels (the truth) and the prediction by the model.\n        training_loss += np.sum((training_labels[i] - layer_2) ** 2)\n        # 2. Increment the accurate prediction count.\n        training_accurate_predictions += int(\n            np.argmax(layer_2) == np.argmax(training_labels[i])\n        )\n        # 3. Differentiate the loss function/error.\n        layer_2_delta = training_labels[i] - layer_2\n        # 4. Propagate the gradients of the loss function back through the hidden layer.\n        layer_1_delta = np.dot(weights_2, layer_2_delta) * relu2deriv(layer_1)\n        # 5. Apply the dropout to the gradients.\n        layer_1_delta *= dropout_mask\n        # 6. Update the weights for the middle and input layers\n        #    by multiplying them by the learning rate and the gradients.\n        weights_1 += learning_rate * np.outer(layer_0, layer_1_delta)\n        weights_2 += learning_rate * np.outer(layer_1, layer_2_delta)\n\n    # Store training set losses and accurate predictions.\n    store_training_loss.append(training_loss)\n    store_training_accurate_pred.append(training_accurate_predictions)\n\n    ###################\n    # Evaluation step #\n    ###################\n\n    # Evaluate model performance on the test set at each epoch.\n\n    # Unlike the training step, the weights are not modified for each image\n    # (or batch). Therefore the model can be applied to the test images in a\n    # vectorized manner, eliminating the need to loop over each image\n    # individually:\n\n    results = relu(test_images @ weights_1) @ weights_2\n\n    # Measure the error between the actual label (truth) and prediction values.\n    test_loss = np.sum((test_labels - results) ** 2)\n\n    # Measure prediction accuracy on test set\n    test_accurate_predictions = np.sum(\n        np.argmax(results, axis=1) == np.argmax(test_labels, axis=1)\n    )\n\n    # Store test set losses and accurate predictions.\n    store_test_loss.append(test_loss)\n    store_test_accurate_pred.append(test_accurate_predictions)\n\n    # Summarize error and accuracy metrics at each epoch\n    print(\n        (\n            f\"Epoch: {j}\\n\"\n            f\"  Training set error: {training_loss / len(training_images):.3f}\\n\"\n            f\"  Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\"\n            f\"  Test set error: {test_loss / len(test_images):.3f}\\n\"\n            f\"  Test set accuracy: {test_accurate_predictions / len(test_images)}\"\n        )\n    )",
            "# Here we follow the same procedure as above, but now we instantiate the\n# Theano wrapper `Loglike` with the UC model instance instead of the\n# SARIMAX model instance\nloglike_uc = Loglike(mod_uc)\n\nwith pm.Model():\n    # Priors\n    sigma2level = pm.InverseGamma(\"sigma2.level\", 1, 1)\n    sigma2ar = pm.InverseGamma(\"sigma2.ar\", 1, 1)\n    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n\n    # convert variables to tensor vectors\n    theta_uc = tt.as_tensor_variable([sigma2level, sigma2ar, arL1])\n\n    # use a DensityDist (use a lamdba function to \"call\" the Op)\n    pm.DensityDist(\"likelihood\", loglike_uc, observed=theta_uc)\n\n    # Draw samples\n    trace_uc = pm.sample(\n        ndraws,\n        tune=nburn,\n        return_inferencedata=True,\n        cores=1,\n        compute_convergence_checks=False,\n    )",
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "# Create three equidistant points\ndata = np.linspace(-1, 1, 3)\nkde = sm.nonparametric.KDEUnivariate(data)\n\n# Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, kernel in enumerate(kernel_switch.keys()):\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(kernel))\n\n    # Fit the model (estimate densities)\n    kde.fit(kernel=kernel, fft=False, gridsize=2 ** 10)\n\n    # Create the plot\n    ax.plot(kde.support, kde.density, lw=3, label=\"KDE from samples\", zorder=10)\n    ax.scatter(data, np.zeros_like(data), marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim([-3, 3])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with native categorical support"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Compose the model and begin training and testing it"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions on three data points"
            ]
        ]
    },
    "271159": {
        "jupyter_code_cell": "# First looking for features with large numbers of nulls (more than 25%)\n# We'll remove these wholesale, assuming nothing too vital jumps out.\n\nnum_val_counts = train.isnull().sum()\nnum_val_counts[num_val_counts > 0.25*train.shape[0]]",
        "matched_tutorial_code_inds": [
            2495,
            2025,
            639,
            6650,
            1636
        ],
        "matched_tutorial_codes": [
            "# Finding a low-dimension embedding for visualization: find the best position of\n# the nodes (the stocks) on a 2D plane\n\nfrom sklearn import manifold\n\nnode_position_model = (\n    n_components=2, eigen_solver=\"dense\", n_neighbors=6\n)\n\nembedding = node_position_model.fit_transform(X.T).T",
            "# Computing a few extra eigenvectors may speed up the eigen_solver.\n# The spectral clustering quality may also benetif from requesting\n# extra regions for segmentation.\nn_regions_plus = 3\n\n# Apply spectral clustering using the default eigen_solver='arpack'.\n# Any implemented solver can be used: eigen_solver='arpack', 'lobpcg', or 'amg'.\n# Choosing eigen_solver='amg' requires an extra package called 'pyamg'.\n# The quality of segmentation and the speed of calculations is mostly determined\n# by the choice of the solver and the value of the tolerance 'eigen_tol'.\n# TODO: varying eigen_tol seems to have no effect for 'lobpcg' and 'amg' #21243.\nfor assign_labels in (\"kmeans\", \"discretize\", \"cluster_qr\"):\n    t0 = ()\n    labels = (\n        graph,\n        n_clusters=(n_regions + n_regions_plus),\n        eigen_tol=1e-7,\n        assign_labels=assign_labels,\n        random_state=42,\n    )\n\n    t1 = ()\n    labels = labels.reshape(rescaled_coins.shape)\n    (figsize=(5, 5))\n    (rescaled_coins, cmap=plt.cm.gray)\n\n    (())\n    (())\n    title = \"Spectral clustering: %s, %.2fs\" % (assign_labels, (t1 - t0))\n    print(title)\n    (title)\n    for l in range(n_regions):\n        colors = [plt.cm.nipy_spectral((l + 4) / float(n_regions + 4))]\n        (labels == l, colors=colors)\n        # To view individual segments as appear comment in plt.pause(0.5)\n()\n\n# TODO: After #21194 is merged and #21243 is fixed, check which eigen_solver\n# is the best and set eigen_solver='arpack', 'lobpcg', or 'amg' and eigen_tol\n# explicitly in this example.\n\n\n\n<img alt=\"Spectral clustering: kmeans, 2.04s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\"/>\n<img alt=\"Spectral clustering: discretize, 1.83s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\"/>\n<img alt=\"Spectral clustering: cluster_qr, 1.82s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\"/>",
            "# This enables the extended features such as the camera.\nstart_x=1\n\n# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.\ngpu_mem=128\n\n# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.\n#camera_auto_detect=1",
            "# Create an SARIMAX model instance - here we use it to estimate\n# the parameters via MLE using the `fit` method, but we can\n# also re-use it below for the Bayesian estimation\nmod = sm.tsa.statespace.SARIMAX(inf, order=(1, 0, 1))\n\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "# The threshold is \"greater than 150\"\n# Return `1` if true, `0` otherwise\nxray_image_mask_less_noisy = np.where(xray_image &gt; 150, 1, 0)\n\nplt.imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\" src=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Visualizing the stock market structure->Embedding in 2D space"
            ],
            [
                "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions"
            ],
            [
                "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Raspberry Pi 4 Setup"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->3. Fit the model with maximum likelihood"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ]
        ]
    },
    "633273": {
        "jupyter_code_cell": "#get some variable lists before formatting\nunit_type_list = df['unit_type'].unique()\nnhood_list_names = df['neighborhood'].unique()\n#apply standard model formatting\ndf = standard_formatting(df)\ndf = df.drop('index',1)",
        "matched_tutorial_code_inds": [
            6703,
            6321,
            6423,
            403,
            6662
        ],
        "matched_tutorial_codes": [
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()",
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "# Construct the model instance\nmod_uc = sm.tsa.UnobservedComponents(inf, \"rwalk\", autoregressive=1)\n\n# Fit the model via maximum likelihood\nres_uc_mle = mod_uc.fit()\nprint(res_uc_mle.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ]
        ]
    },
    "514077": {
        "jupyter_code_cell": "def knn_impute(data, k_neighbors=3):\n    \"\"\"\n    Imputes missing data using the nearest non-null neighbors.\n    Makes changes in place to dataframe\n    \"\"\"\n    # Iterate over rows\n    for i, row in data.iterrows():\n\n        # Find rows that contain nulls\n        if row.isnull().any():\n\n            # Find K nearest neighbors\n            knn = impute_neighbors(row)\n\n            # Find the cell with the null value and fill it\n            for i, v in row.iteritems():\n                if isinstance(v, float) and isnan(v):\n                    # Fill that with the voted upon value\n                    val = knn[i].mode().values[0]\n                    data.set_value(row.name, i, val)\n\nfin = knn_impute(data)",
        "matched_tutorial_code_inds": [
            384,
            3340,
            4938,
            5420,
            5810
        ],
        "matched_tutorial_codes": [
            "def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\n,  = next(iter(dataloaders['train']))\n\n# Make a grid from batch\n = ()\n\nimshow(, title=[class_names[x] for x in ])\n\n\n<img alt=\"['bees', 'bees', 'ants', 'bees']\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\"/>",
            "def subject_body_extractor(posts):\n    # construct object dtype array with two columns\n    # first column = 'subject' and second column = 'body'\n    features = (shape=(len(posts), 2), dtype=object)\n    for i, text in enumerate(posts):\n        # temporary variable `_` stores '\\n\\n'\n        headers, _, body = text.partition(\"\\n\\n\")\n        # store body text in second column\n        features[i, 1] = body\n\n        prefix = \"Subject:\"\n        sub = \"\"\n        # save text after 'Subject:' in first column\n        for line in headers.split(\"\\n\"):\n            if line.startswith(prefix):\n                sub = line[len(prefix) :]\n                break\n        features[i, 0] = sub\n\n    return features\n\n\nsubject_body_transformer = (subject_body_extractor)",
            "def plot_examples(colormaps):\n    \"\"\"\n    Helper function to plot data with associated colormap.\n    \"\"\"\n    np.random.seed(19680801)\n    data = np.random.randn(30, 30)\n    n = len(colormaps)\n    fig, axs = plt.subplots(1, n, figsize=(n * 2 + 2, 3),\n                            constrained_layout=True, squeeze=False)\n    for [ax, cmap] in zip(axs.flat, colormaps):\n        psm = ax.pcolormesh(data, cmap=cmap, rasterized=True, vmin=-4, vmax=4)\n        fig.colorbar(psm, ax=ax)\n    plt.show()",
            "def beanplot(data, plot_opts={}, jitter=False):\n    \"\"\"helper function to try out different plot options\"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plot_opts_ = {\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    }\n    plot_opts_.update(plot_opts)\n    sm.graphics.beanplot(\n        data, ax=ax, labels=labels, jitter=jitter, plot_opts=plot_opts_\n    )\n    ax.set_xlabel(\"Party identification of respondent.\")\n    ax.set_ylabel(\"Age\")",
            "def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data->Visualize a few images"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Heterogeneous Data Sources->Creating transformers"
            ],
            [
                "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Creating listed colormaps"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Bean Plots"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression"
            ]
        ]
    },
    "1271827": {
        "jupyter_code_cell": "pd.set_option('display.float_format', lambda x: '%.2f' % x) #Suppress Scientific Notation from Python Pandas\ndf_mov.describe()",
        "matched_tutorial_code_inds": [
            3812,
            4167,
            4166,
            2106,
            5789
        ],
        "matched_tutorial_codes": [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ]
        ]
    },
    "692862": {
        "jupyter_code_cell": "#import all relevant packages for this analysis\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\nfrom IPython.display import HTML\n\n#This is done to hide the Python code\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n</script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')",
        "matched_tutorial_code_inds": [
            406,
            4694,
            1138,
            360,
            6538
        ],
        "matched_tutorial_codes": [
            "# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
            "# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n# make up some data in the open interval (0, 1)\ny = np.random.normal(loc=0.5, scale=0.4, size=1000)\ny = y[(y  0) & (y &lt; 1)]\ny.sort()\nx = np.arange(len(y))\n\n# plot with various axes scales\nplt.figure()\n\n# linear\nplt.subplot(221)\nplt.plot(x, y)\nplt.yscale('linear')\nplt.title('linear')\nplt.grid(True)\n\n# log\nplt.subplot(222)\nplt.plot(x, y)\nplt.yscale('log')\nplt.title('log')\nplt.grid(True)\n\n# symmetric log\nplt.subplot(223)\nplt.plot(x, y - y.mean())\nplt.yscale('symlog', linthresh=0.01)\nplt.title('symlog')\nplt.grid(True)\n\n# logit\nplt.subplot(224)\nplt.plot(x, y)\nplt.yscale('logit')\nplt.title('logit')\nplt.grid(True)\n# Adjust the subplot layout, because the logit one may take more space\n# than usual, due to y-tick labels like \"1 - 10^{-3}\"\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()\n\n\n<img alt=\"linear, log, symlog, logit\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_010.png\" srcset=\"../../_images/sphx_glr_pyplot_010.png, ../../_images/sphx_glr_pyplot_010_2_0x.png 2.0x\"/>",
            "# Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "# imports\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# transforms\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# datasets\ntrainset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=True,\n    transform=transform)\ntestset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=False,\n    transform=transform)\n\n# dataloaders\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                        shuffle=True, num_workers=2)\n\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                        shuffle=False, num_workers=2)\n\n# constant for classes\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# helper function to show an image\n# (used in the `plot_classes_preds` function below)\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))",
            "# Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Logarithmic and other nonlinear axes"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Data"
            ]
        ]
    },
    "140574": {
        "jupyter_code_cell": "yearly_data.plot(kind=\"bar\")\nplt.axis('off')",
        "matched_tutorial_code_inds": [
            3973,
            4095,
            3821,
            4022,
            4185
        ],
        "matched_tutorial_codes": [
            "year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing long-form data",
                "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing long-form data"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots",
                "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines",
                "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines"
            ]
        ]
    },
    "1192209": {
        "jupyter_code_cell": "# Extract the maximum temperature in August 2010 from df_climate: august_max\naugust_max = df_climate.loc['2010-Aug', 'Temperature'].max()\nprint(august_max)\n\n# Resample the August 2011 temperatures in df_clean by day and aggregate the maximum value: august_2011\naugust_2011 = df_clean.loc['2011-Aug', 'dry_bulb_faren'].resample('D').max()\n\n# Filter out days in august_2011 where the value exceeded august_max: august_2011_high\naugust_2011_high = august_2011[august_2011 > august_max]\n\n# Construct a CDF of august_2011_high\naugust_2011_high.plot(kind = 'hist', bins = 25, normed = True, cumulative = True)\n\n# Display the plot\nplt.show()",
        "matched_tutorial_code_inds": [
            5130,
            6340,
            6556,
            6944,
            6345
        ],
        "matched_tutorial_codes": [
            "# Load in the example macroeconomic dataset\ndta = sm.datasets.macrodata.load_pandas().data\n# Make sure we have an index with an associated frequency, so that\n# we can refer to time periods with date strings or timestamps\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')\n\n# Separate inflation data into a training and test dataset\ntraining_endog = dta['infl'].iloc[:-1]\ntest_endog = dta['infl'].iloc[-1:]\n\n# Fit an SARIMAX model for inflation\ntraining_model = sm.tsa.SARIMAX(training_endog, order=(4, 0, 0))\ntraining_results = training_model.fit()\n\n# Extend the results to the test observations\ntest_results = training_results.extend(test_endog)\n\n# Print the sum of squared errors in the test sample,\n# based on parameters computed using only the training sample\nprint(test_results.sse)",
            "# Get the RGNP data to replicate Hamilton\ndta = pd.read_stata(\"https://www.stata-press.com/data/r14/rgnp.dta\").iloc[1:]\ndta.index = pd.DatetimeIndex(dta.date, freq=\"QS\")\ndta_hamilton = dta.rgnp\n\n# Plot the data\ndta_hamilton.plot(title=\"Growth rate of Real GNP\", figsize=(12, 3))\n\n# Fit the model\nmod_hamilton = sm.tsa.MarkovAutoregression(\n    dta_hamilton, k_regimes=2, order=4, switching_ar=False\n)\nres_hamilton = mod_hamilton.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\"/>",
            "# Load Dataset\ndf.index = pd.date_range(start='%d-01-01' % df.date[0], end='%d-01-01' % df.iloc[-1, 0], freq='AS')\n\n# Log transform\ndf['lff'] = np.log(df['ff'])\n\n# Setup the model\nmod = LocalLinearTrend(df['lff'])\n\n# Fit it using MLE (recall that we are fitting the three variance parameters)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "# Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.extend(updated_endog)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "# Get the dataset\new_excs = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/ew_excs.prn\").content\nraw = pd.read_table(BytesIO(ew_excs), header=None, skipfooter=1, engine=\"python\")\nraw.index = pd.date_range(\"1926-01-01\", \"1995-12-01\", freq=\"MS\")\n\ndta_kns = raw.loc[:\"1986\"] - raw.loc[:\"1986\"].mean()\n\n# Plot the dataset\ndta_kns[0].plot(title=\"Excess returns\", figsize=(12, 3))\n\n# Fit the model\nmod_kns = sm.tsa.MarkovRegression(\n    dta_kns, k_regimes=3, trend=\"n\", switching_variance=True\n)\nres_kns = mod_kns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Output and postestimation methods and attributes->Applying estimated parameters to an updated or different dataset"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Hamilton (1989) switching model of GNP"
            ],
            [
                "statsmodels->Examples->State space models->State space modeling: Local Linear Trends"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Kim, Nelson, and Startz (1998) Three-state Variance Switching"
            ]
        ]
    },
    "453480": {
        "jupyter_code_cell": "team_wins = df.groupby('WINNING_TEAM').count()['YEAR'].to_frame().reset_index()\nteam_wins.columns = ['team', 'wins']\nteam_wins",
        "matched_tutorial_code_inds": [
            3638,
            5789,
            3636,
            3855,
            3779
        ],
        "matched_tutorial_codes": [
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()",
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ]
        ]
    },
    "593940": {
        "jupyter_code_cell": "from pandas.plotting import scatter_matrix\nfig = plt.figure()\nscatter_matrix(train_X, alpha=0.2, figsize=(24, 24), diagonal='kde')\nplt.show()",
        "matched_tutorial_code_inds": [
            4170,
            3328,
            5635,
            6214,
            6848
        ],
        "matched_tutorial_codes": [
            "g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n",
            "from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "from statsmodels.tsa.api import AutoReg\n\nmod = AutoReg(y, 1, trend=\"n\", deterministic=det_proc)\nres = mod.fit()\nprint(res.summary())",
            "from patsy.contrasts import Diff\n\ncontrast = Diff().code_without_intercept(levels)\nprint(contrast.matrix)"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships"
            ],
            [
                "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Backward Difference Coding"
            ]
        ]
    },
    "797747": {
        "jupyter_code_cell": "# Choose your site for this example\n# Here we will select our testing site, so that we can demonstrate the code works without waiting for a full area to run\nnum = 48\nStudysite = names.ix[num]\nprint(Studysite)",
        "matched_tutorial_code_inds": [
            6423,
            3931,
            6913,
            6828,
            6357
        ],
        "matched_tutorial_codes": [
            "# Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n",
            "# Construct the model\nmod = sm.tsa.SARIMAX(endog, order=(1, 0, 0), trend='c')\n# Estimate the parameters\nres = mod.fit()\n\nprint(res.summary())",
            "from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Constructing and estimating the model"
            ],
            [
                "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ]
        ]
    },
    "1151276": {
        "jupyter_code_cell": "### San Francisco County\n\nsf_train_log = np.log(sf_train)\n\nsf_train_resid = decomposing(sf_train_log)\n\nsf_train_p = 2\nsf_train_q = 2\n\nsf_model_fit = arima_summary(sf_train_log, sf_train_resid, sf_train_p, 2, sf_train_q)",
        "matched_tutorial_code_inds": [
            6416,
            1140,
            1138,
            6636,
            1966
        ],
        "matched_tutorial_codes": [
            "# Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "# Profile scripted rms_norm\n = (with_rms_norm)\nfunc = functools.partial(\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"TorchScript - RMS Norm\")",
            "# Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "# Prior hyperparameters\n\n# Prior for obs. cov. is inverse-Wishart(v_1^0=k + 3, S10=I)\nv10 = mod.k_endog + 3\nS10 = np.eye(mod.k_endog)\n\n# Prior for state cov. variances is inverse-Gamma(v_{i2}^0 / 2 = 3, S+{i2}^0 / 2 = 0.005)\nvi20 = 6\nSi20 = 0.01",
            "# Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization"
            ],
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC"
            ],
            [
                "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets"
            ]
        ]
    },
    "621120": {
        "jupyter_code_cell": "salary.rename(columns = {'2016-17':'Salary'}, inplace = True)\nclean_salaries = salary[['Player','Salary']]\nclean_salaries.head()\nclean_salaries.tail()",
        "matched_tutorial_code_inds": [
            3644,
            3731,
            6170,
            1795,
            6810
        ],
        "matched_tutorial_codes": [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "sel = ar_select_order(housing, 13, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "X, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nX.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "columns = list(map(str, range(1960, 2012)))\ndata.set_index(\"Country Name\", inplace=True)\ndta = data[columns]\ndta = dta.dropna()\ndta.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.2->Faster parser in"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ]
        ]
    },
    "1439804": {
        "jupyter_code_cell": "results = net.aggregate(1000, type='sum', decay='linear')\nprint('ok')",
        "matched_tutorial_code_inds": [
            6168,
            6803,
            5287,
            4095,
            1528
        ],
        "matched_tutorial_codes": [
            "res = mod.fit(cov_type=\"HC0\")\nprint(res.summary())",
            "mod = ThetaModel(np.log(pce))\nres = mod.fit()\nprint(res.summary())",
            "mod = sm.RecursiveLS(endog, exog)\nres = mod.fit()\n\nprint(res.summary())",
            "tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "results = model.fit()\nprint(results.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots",
                "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots"
            ],
            [
                "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors"
            ]
        ]
    },
    "995072": {
        "jupyter_code_cell": "tp = np.sum((y_test == 1) & (predicted == 1))\nfp = np.sum((y_test == 0) & (predicted == 1))\ntn = np.sum((y_test == 0) & (predicted == 0))\nfn = np.sum((y_test == 1) & (predicted == 0))\nprint(tn, fp, fn, tp)",
        "matched_tutorial_code_inds": [
            5368,
            5375,
            3257,
            6108,
            3824
        ],
        "matched_tutorial_codes": [
            "oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "# initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "weights = 1 / var_eff\nmod_glm = GLM(eff, np.ones(len(eff)), var_weights=weights)\nres_glm = mod_glm.fit(scale=1.0)\nprint(res_glm.summary().tables[1])",
            "def core(df, \u03b1=.05):\n    mask = (df &gt; df.quantile(\u03b1)).all(1) &amp; (df &lt; df.quantile(1 - \u03b1)).all(1)\n    return df[mask]"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis"
            ],
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Replicate fixed effect analysis using GLM with var_weights"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "1354219": {
        "jupyter_code_cell": "perceptron = Perceptron()\n\nscores = cross_val_score(perceptron, predictors, target, cv=10)\nprint(scores.mean())\n\ny_pred1=cross_val_predict(perceptron,predictors,target,cv=10)",
        "matched_tutorial_code_inds": [
            2443,
            5205,
            6168,
            2328,
            5530
        ],
        "matched_tutorial_codes": [
            "all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "res = sm.OLS(y, X).fit()\nprint(res.summary())",
            "res = mod.fit(cov_type=\"HC0\")\nprint(res.summary())",
            "gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "means25 = exog.mean()\nprint(means25)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Autoregressions"
            ],
            [
                "sklearn->Examples->Ensemble methods->Monotonic Constraints"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution"
            ]
        ]
    },
    "156032": {
        "jupyter_code_cell": "rating_train, rating_test = train_test_split(relevent_ratings_df, test_size = 0.2, random_state = 42) ",
        "matched_tutorial_code_inds": [
            3404,
            2360,
            5712,
            1759,
            750
        ],
        "matched_tutorial_codes": [
            "X_train, X_test, y_train, y_test = (X, y, random_state=1)",
            "X_train, X_test, y_train, y_test = (X, y, random_state=0)",
            "results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, , )"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Real-world data set"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison"
            ],
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "torch->Frontend APIs->Per-sample-gradients->Per-sample-grads, the efficient way, using function transforms"
            ]
        ]
    },
    "1314247": {
        "jupyter_code_cell": "#Becuase Pearson's test and Spearmanr's test need to two variables, and these two variables have the same length.\n#Generate the new variabl with the same length:\nnewM_1 = np.random.choice(newM,len(newF),replace=False,p=None)",
        "matched_tutorial_code_inds": [
            370,
            473,
            1635,
            6680,
            5986
        ],
        "matched_tutorial_codes": [
            "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "# The threshold is \"greater than 150\"\n# Return the original image if true, `0` otherwise\nxray_image_mask_noisy = np.where(xray_image &gt; 150, xray_image, 0)\n\nplt.imshow(xray_image_mask_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/976c52ef7b0c1a1d25ff407f99a98e8290bc74a7eebe2564e88373b58fc3cee1.png\" src=\"../_images/976c52ef7b0c1a1d25ff407f99a98e8290bc74a7eebe2564e88373b58fc3cee1.png\"/>",
            "# Get the next observation after the \"pre\" dataset\ny_update = y.iloc[-5:-4]\n\n# Print the forecast error\nprint('Forecast error: %.2f' % (y_update.iloc[0] - forecasts_pre.iloc[0]))",
            "# The coefficients used to define the linear predictors\ncoeff = [[4, 0.4, -0.2], [4, 0.4, -0.2, 0, -0.04]]\n\n# The linear predictors\nlp = [np.dot(x0, coeff[0]), np.dot(x, coeff[1])]\n\n# The mean values\nmu = [np.exp(lp[0]), np.exp(lp[1])]"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard"
            ],
            [
                "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results"
            ],
            [
                "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 2: computing the \u201cnews\u201d from a new observation"
            ],
            [
                "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests"
            ]
        ]
    },
    "23861": {
        "jupyter_code_cell": "# what long, lat pair should the map be on?\ncenter = [-37, 145]\n\n# what zoom level (rough equiv. to google) should the map be at?\nzoom = 7\n\n# make map\ndemo_map = Map(\n    center=center,\n    zoom=zoom\n)\n\n# print map\ndemo_map",
        "matched_tutorial_code_inds": [
            3105,
            37,
            3104,
            6678,
            6783
        ],
        "matched_tutorial_codes": [
            "# range of admissible distortions\neps_range = (0.01, 0.99, 100)\n\n# range of number of samples (observation) to embed\nn_samples_range = (2, 6, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(n_samples_range)))\n\n()\nfor n_samples, color in zip(n_samples_range, colors):\n    min_n_components = (n_samples, eps=eps_range)\n    (eps_range, min_n_components, color=color)\n\n([f\"n_samples = {n}\" for n in n_samples_range], loc=\"upper right\")\n(\"Distortion eps\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_components vs eps\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_components vs eps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png\"/>",
            "# run the float model\nout1, hidden1 = float_lstm(inputs, hidden)\nmag1 = (abs(out1)).item()\nprint('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n\n# run the quantized model\nout2, hidden2 = quantized_lstm(inputs, hidden)\nmag2 = (abs(out2)).item()\nprint('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n\n# compare them\nmag3 = (abs(out1-out2)).item()\nprint('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))",
            "# range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "# Get the estimated AR(1) coefficient\nphi_hat = res_pre.params[0]\n\n# Get the last observed value of the variable\ny_T = y_pre.iloc[-1]\n\n# Directly compute the forecasts at the horizons h=1,2,3,4\nmanual_forecasts = pd.Series([phi_hat * y_T, phi_hat**2 * y_T,\n                              phi_hat**3 * y_T, phi_hat**4 * y_T],\n                             index=forecasts_pre.index)\n\n# We'll print the two to double-check that they're the same\nprint(pd.concat([forecasts_pre, manual_forecasts], axis=1))",
            "# Model that will apply Kalman filter recursions\nmod_kf = sm.tsa.SARIMAX(inf_apparel, order=(6, 0, 0), seasonal_order=(15, 0, 0, 12), tolerance=0)\nprint(mod_kf.k_states)\n\n# Model that will apply Chandrasekhar recursions\nmod_ch = sm.tsa.SARIMAX(inf_apparel, order=(6, 0, 0), seasonal_order=(15, 0, 0, 12), tolerance=0)\nmod_ch.ssm.filter_chandrasekhar = True"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds"
            ],
            [
                "torch->PyTorch Recipes->Dynamic Quantization->Steps->5: Look at Accuracy"
            ],
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset"
            ],
            [
                "statsmodels->Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Practical example"
            ]
        ]
    },
    "329950": {
        "jupyter_code_cell": "# Use pandas .values operator to load the values of relative humidity (the 'RH' column)\n# form the pandas dataframe into a numpy array\nRH = df['RH'].values\n\n# Compute the mean using numpy\nnp.mean(RH, axis=0)",
        "matched_tutorial_code_inds": [
            6703,
            6938,
            6434,
            6670,
            403
        ],
        "matched_tutorial_codes": [
            "# Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "# Construct the forecast errors\nforecast_errors = forecasts.apply(lambda column: endog - column).reindex(forecasts.index)\n\nprint(forecast_errors.iloc[:5, :5])",
            "# In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "# Retrieve the posterior means\nparams = pm.summary(trace_uc)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_uc_bayes = mod_uc.smooth(params)",
            "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example",
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ],
            [
                "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack"
            ]
        ]
    },
    "646955": {
        "jupyter_code_cell": "from IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nimport pydotplus \n\ndot_data = StringIO()  \ntree.export_graphviz(clf, \n                     out_file=dot_data,  \n                     feature_names=features)  \ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_pdf())  ",
        "matched_tutorial_code_inds": [
            2034,
            2373,
            2177,
            2562,
            1814
        ],
        "matched_tutorial_codes": [
            "from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "from sklearn.base import clone\n\nalpha = 0.95\nneg_mean_pinball_loss_95p_scorer = (\n    ,\n    alpha=alpha,\n    greater_is_better=False,  # maximize the negative loss\n)\nsearch_95p = clone(search_05p).set_params(\n    estimator__alpha=alpha,\n    scoring=neg_mean_pinball_loss_95p_scorer,\n)\nsearch_95p.fit(X_train, y_train)\n(search_95p.best_params_)",
            "from sklearn.model_selection import \nimport matplotlib.pyplot as plt\n\nscoring = \"neg_mean_absolute_percentage_error\"\nn_cv_folds = 3\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\n\ndef plot_results(figure_title):\n    fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\n    plot_info = [\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n    ]\n\n    x, width = (4), 0.9\n    for key, title, ax, y_limit in plot_info:\n        items = [\n            dropped_result[key],\n            one_hot_result[key],\n            ordinal_result[key],\n            native_result[key],\n        ]\n\n        mape_cv_mean = [(np.abs(item)) for item in items]\n        mape_cv_std = [(item) for item in items]\n\n        ax.bar(\n            x=x,\n            height=mape_cv_mean,\n            width=width,\n            yerr=mape_cv_std,\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n        )\n        ax.set(\n            xlabel=\"Model\",\n            title=title,\n            xticks=x,\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n            ylim=y_limit,\n        )\n    fig.suptitle(figure_title)\n\n\nplot_results(\"Gradient Boosting on Ames Housing\")\n\n\n<img alt=\"Gradient Boosting on Ames Housing, Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\"/>",
            "from sklearn.model_selection import \nfrom sklearn.utils.fixes import loguniform\n\nparam_distributions = {\n    \"alpha\": loguniform(1e0, 1e3),\n    \"kernel__length_scale\": loguniform(1e-2, 1e2),\n    \"kernel__periodicity\": loguniform(1e0, 1e1),\n}\nkernel_ridge_tuned = (\n    kernel_ridge,\n    param_distributions=param_distributions,\n    n_iter=500,\n    random_state=0,\n)\nstart_time = ()\nkernel_ridge_tuned.fit(training_data, training_noisy_target)\nprint(f\"Time for KernelRidge fitting: {() - start_time:.3f} seconds\")",
            "from sklearn.datasets import \nfrom sklearn.cluster import , \nimport matplotlib.pyplot as plt\n\nX, _ = (n_samples=1000, centers=2, random_state=0)\n\nkm = (n_clusters=5, random_state=0, n_init=\"auto\").fit(X)\nbisect_km = (n_clusters=5, random_state=0).fit(X)\n\nfig, ax = (1, 2, figsize=(10, 5))\nax[0].scatter(X[:, 0], X[:, 1], s=10, c=km.labels_)\nax[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=20, c=\"r\")\nax[0].set_title(\"KMeans\")\n\nax[1].scatter(X[:, 0], X[:, 1], s=10, c=bisect_km.labels_)\nax[1].scatter(\n    bisect_km.cluster_centers_[:, 0], bisect_km.cluster_centers_[:, 1], s=20, c=\"r\"\n)\n_ = ax[1].set_title(\"BisectingKMeans\")\n\n\n<img alt=\"KMeans, BisectingKMeans\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_003.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_003.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Model comparison"
            ],
            [
                "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Kernel ridge"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->BisectingKMeans: divide and cluster"
            ]
        ]
    },
    "745719": {
        "jupyter_code_cell": "features_sub2 = df_undersampling.iloc[:, 1:].values\ntarget_sub2 = df_undersampling.iloc[:, 0:1]\n\nss = StandardScaler()\nfeaturesS_sub2 = ss.fit_transform(features_sub2)\n\npredictions_RF_model = forest.predict(featuresS_sub2)\n\npredictions_percentage2 = forest.predict_proba(featuresS_sub2)[:, 0]",
        "matched_tutorial_code_inds": [
            4683,
            2896,
            2112,
            2104,
            94
        ],
        "matched_tutorial_codes": [
            "names = ['group_a', 'group_b', 'group_c']\nvalues = [1, 10, 100]\n\nplt.figure(figsize=(9, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\n\n\n<img alt=\"Categorical Plotting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_006.png\" srcset=\"../../_images/sphx_glr_pyplot_006.png, ../../_images/sphx_glr_pyplot_006_2_0x.png 2.0x\"/>",
            "features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")",
            "node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>",
            "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with categorical variables"
            ],
            [
                "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations"
            ],
            [
                "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path"
            ],
            [
                "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets"
            ],
            [
                "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars"
            ]
        ]
    },
    "735119": {
        "jupyter_code_cell": "sample_name",
        "matched_tutorial_code_inds": [
            3910,
            3350,
            1491,
            3718,
            6076
        ],
        "matched_tutorial_codes": [
            "by_occupation",
            "model score: 0.798",
            "nbcases_ma",
            "flights.dep_time",
            "res3.cache_ci"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Scaling->Dask"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types",
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau"
            ]
        ]
    },
    "1036708": {
        "jupyter_code_cell": "def relatives_count(row): return row['SibSp'] + row['Parch']\n\ntrain_df['Relatives'] = train_df.apply(relatives_count, axis=1)\ntest_df['Relatives'] = test_df.apply(relatives_count, axis=1)",
        "matched_tutorial_code_inds": [
            357,
            1715,
            2367,
            3125,
            5674
        ],
        "matched_tutorial_codes": [
            "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "def t_test(x, y):\n    diff = y - x\n    var = np.var(diff, ddof=1)\n    num = np.mean(diff)\n    denom = np.sqrt(var / len(x))\n    return np.divide(num, denom)\n\nt_value = t_test(before_sample, after_sample)",
            "def coverage_fraction(y, y_low, y_high):\n    return ((y = y_low, y &lt;= y_high))\n\n\ncoverage_fraction(\n    y_train,\n    all_models[\"q 0.05\"].predict(X_train),\n    all_models[\"q 0.95\"].predict(X_train),\n)",
            "def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")",
            "def double_it(x):\n    return 2 * x\n\n\nformula = \"SUCCESS ~ double_it(LOWINC) + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \\\n           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF\"\nmod2 = smf.glm(formula=formula, data=dta, family=sm.families.Binomial()).fit()\nprint(mod2.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Learning PyTorch->What is torch.nn really?->Using your GPU"
            ],
            [
                "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Calculating the test statistics"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Using Formulas with GLMs"
            ]
        ]
    },
    "280085": {
        "jupyter_code_cell": "## CODE CELL 24\n\ndf.info()",
        "matched_tutorial_code_inds": [
            3928,
            3929,
            6705,
            41,
            3930
        ],
        "matched_tutorial_codes": [
            "# Import seaborn\nimport seaborn as sns\n",
            "# Apply the default theme\nsns.set_theme()\n",
            "# Show the summary of the news results\nprint(news.summary())",
            "# Save\n(net.module.state_dict(), PATH)\n\n# Load to whatever device you want",
            "# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ],
            [
                "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor"
            ],
            [
                "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Steps->6. Saving torch.nn.DataParallel Models"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn"
            ]
        ]
    },
    "872298": {
        "jupyter_code_cell": "predsTest = lModel.predict(X=dataTest)\nfig,(ax1,ax2)= plt.subplots(ncols=2)\nfig.set_size_inches(20,5)\nsn.distplot(yLabels,ax=ax1,bins=100)\nsn.distplot(np.exp(predsTest),ax=ax2,bins=100)\nax1.set(title=\"Training Set Distribution\")\nax2.set(title=\"Test Set Distribution\")",
        "matched_tutorial_code_inds": [
            5222,
            2314,
            6103,
            6007,
            5411
        ],
        "matched_tutorial_codes": [
            "pred_ols2 = res2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y, \"o\", label=\"Data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\nax.plot(x, res2.fittedvalues, \"r--.\", label=\"Predicted\")\nax.plot(x, iv_u, \"r--\")\nax.plot(x, iv_l, \"r--\")\nlegend = ax.legend(loc=\"best\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ols_26_0.png\" src=\"../../../_images/examples_notebooks_generated_ols_26_0.png\"/>",
            "feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables"
            ],
            [
                "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ],
            [
                "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers."
            ]
        ]
    },
    "1126720": {
        "jupyter_code_cell": "# Remove the unnecessary columns:\nrm_cols = set(['PRIVATE_CUSTOMER','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n#vin_hash_series = df_all['VIN_HASHED'].to_pandas()\n\nfor col in rm_cols:\n    df_all.drop_column(col)",
        "matched_tutorial_code_inds": [
            1127,
            5433,
            1565,
            5429,
            5431
        ],
        "matched_tutorial_codes": [
            "# Perform warm-up iterations\nfor _ in range(3):\n     = inputs1[0]\n     = inputs2[0]\n     = grad_outputs[0]\n    # Run model, forward and backward\n     = (\n        ,\n        ,\n        ,\n        ,\n        ,\n        normalization_axis=2,\n        dropout_prob=0.1,\n    )\n    ()",
            "# Create a jitter plot.\nfig3 = plt.figure()\nax = fig3.add_subplot(111)\n\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n    \"violin_fc\": (0.8, 0.8, 0.8),\n    \"jitter_marker\": \".\",\n    \"jitter_marker_size\": 3,\n    \"bean_color\": \"#FF6F00\",\n    \"bean_mean_color\": \"#009D91\",\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, jitter=True, plot_opts=plot_opts)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")",
            "# Display 5 random images from the training set.\nnum_examples = 5\nseed = 147197952744\nrng = np.random.default_rng(seed)\n\nfig, axes = plt.subplots(1, num_examples)\nfor sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")\n\n\n\n\n<img alt=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\" src=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\"/>",
            "# Create a violin plot.\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nsm.graphics.violinplot(\n    age,\n    ax=ax,\n    labels=labels,\n    plot_opts={\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    },\n)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")",
            "# Create a bean plot.\nfig2 = plt.figure()\nax = fig2.add_subplot(111)\n\nsm.graphics.beanplot(\n    age,\n    ax=ax,\n    labels=labels,\n    plot_opts={\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    },\n)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->nvFuser & Dynamic Shapes"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Advanced Box Plots"
            ],
            [
                "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Advanced Box Plots"
            ],
            [
                "statsmodels->Examples->Plotting->Box Plots->Advanced Box Plots"
            ]
        ]
    },
    "683396": {
        "jupyter_code_cell": "#set seed for random split and decision tree state\nseed = 99\n\n#randomly split the date into 70% training and 30% testing sample\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =\\\ntrain_test_split(df[df.columns[0:df.columns.shape[0]-1]],df['class'],test_size=0.3,random_state=seed)",
        "matched_tutorial_code_inds": [
            3250,
            3104,
            6754,
            2762,
            370
        ],
        "matched_tutorial_codes": [
            "# create df of model scores ordered by performance\nmodel_scores = results_df.filter(regex=r\"split\\d*_test_score\")\n\n# plot 30 examples of dependency between cv fold and AUC scores\nfig, ax = ()\n(\n    data=model_scores.transpose().iloc[:30],\n    dashes=False,\n    palette=\"Set1\",\n    marker=\"o\",\n    alpha=0.5,\n    ax=ax,\n)\nax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\nax.set_ylabel(\"Model AUC\", size=12)\nax.tick_params(bottom=True, labelbottom=False)\n()\n\n# print correlation of AUC scores across folds\nprint(f\"Correlation of models:\\n {model_scores.transpose().corr()}\")\n\n\n<img alt=\"plot grid search stats\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\"/>",
            "# range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "# fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "# plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Model Selection->Statistical comparison of models using grid search"
            ],
            [
                "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method"
            ],
            [
                "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation"
            ],
            [
                "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard"
            ]
        ]
    },
    "656749": {
        "jupyter_code_cell": "# Data analysis and wrangling\nimport numpy as np\nimport pandas as pd\n\n# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
        "matched_tutorial_code_inds": [
            6376,
            2053,
            5635,
            6896,
            6154
        ],
        "matched_tutorial_codes": [
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction"
            ],
            [
                "statsmodels->Examples->Statistics->Copulas"
            ]
        ]
    },
    "1499311": {
        "jupyter_code_cell": "# Import the necessaries libraries for the code\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt",
        "matched_tutorial_code_inds": [
            1754,
            6376,
            5635,
            1964,
            6154
        ],
        "matched_tutorial_codes": [
            "# Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "import time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, \nfrom sklearn.preprocessing import \nfrom itertools import , \n\n(0)",
            "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ],
            [
                "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets"
            ],
            [
                "statsmodels->Examples->Statistics->Copulas"
            ]
        ]
    },
    "1406454": {
        "jupyter_code_cell": "[len(graph.nodes) for graph in subgraphs]",
        "matched_tutorial_code_inds": [
            5620,
            1810,
            3374,
            1469,
            6604
        ],
        "matched_tutorial_codes": [
            "['gau', 'epa', 'uni', 'tri', 'biw', 'triw', 'cos', 'cos2', 'tric']",
            "[array(['dog', 'snake'], dtype=object)]",
            "['pclass', 'age', 'fare']",
            "['x_axis', 'y_axis']",
            "['sigma2.irregular', 'sigma2.level', 'sigma2.trend']"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Grouping infrequent categories in OneHotEncoder"
            ],
            [
                "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types"
            ],
            [
                "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s"
            ],
            [
                "statsmodels->Examples->State space models->Fixed / constrained parameters in state space models"
            ]
        ]
    },
    "1476769": {
        "jupyter_code_cell": "# create a dictonary based on what should be replaced\nreplaced = {1:'1-11',2:'12-49',3:'50-99',4:'100-299',5:'300-365',6:'0'}\ndf_age_filtered.replace({'# Days Used Alcohol Past Year (Range)':replaced,'# Days Used Marijuana Past Year (Range)':replaced,\n                        '# Days Used Cocaine Past Year (Range)':replaced,'# Days Used Hallucinogens Past Year (Range)':replaced},\n                       inplace=True)",
        "matched_tutorial_code_inds": [
            3642,
            3091,
            6545,
            6538,
            4694
        ],
        "matched_tutorial_codes": [
            "# ignore the context manager for now\nwith pd.option_context('mode.chained_assignment', None):\n    f[f['a'] &lt;= 3]['b'] = f[f['a'] &lt;= 3 ]['b'] / 10\nf",
            "# visualize the decision surface, projected down to the first\n# two principal components of the dataset\npca = (n_components=8).fit(data_train)\n\nX = pca.transform(data_train)\n\n# Generate grid along first two principal components\nmultiples = (-2, 2, 0.1)\n# steps along first component\nfirst = multiples[:, ] * pca.components_[0, :]\n# steps along second component\nsecond = multiples[:, ] * pca.components_[1, :]\n# combine\ngrid = first[, :, :] + second[:, , :]\nflat_grid = grid.reshape(-1, data.shape[1])\n\n# title for the plots\ntitles = [\n    \"SVC with rbf kernel\",\n    \"SVC (linear kernel)\\n with Fourier rbf feature map\\nn_components=100\",\n    \"SVC (linear kernel)\\n with Nystroem rbf feature map\\nn_components=100\",\n]\n\n(figsize=(18, 7.5))\nplt.rcParams.update({\"font.size\": 14})\n# predict and plot\nfor i, clf in enumerate((kernel_svm, nystroem_approx_svm, fourier_approx_svm)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    (1, 3, i + 1)\n    Z = clf.predict(flat_grid)\n\n    # Put the result into a color plot\n    Z = Z.reshape(grid.shape[:-1])\n    levels = (10)\n    lv_eps = 0.01  # Adjust a mapping from calculated contour levels to color.\n    (\n        multiples,\n        multiples,\n        Z,\n        levels=levels - lv_eps,\n        cmap=plt.cm.tab10,\n        vmin=0,\n        vmax=10,\n        alpha=0.7,\n    )\n    (\"off\")\n\n    # Plot also the training points\n    (\n        X[:, 0],\n        X[:, 1],\n        c=targets_train,\n        cmap=plt.cm.tab10,\n        edgecolors=(0, 0, 0),\n        vmin=0,\n        vmax=10,\n    )\n\n    (titles[i])\n()\n()\n\n\n<img alt=\"SVC with rbf kernel, SVC (linear kernel)  with Fourier rbf feature map n_components=100, SVC (linear kernel)  with Nystroem rbf feature map n_components=100\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\" srcset=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\"/>",
            "# Create Table I\ntable_i = np.zeros((5,6))\n\nstart = dta.index[0]\nend = dta.index[-1]\ntime_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\nmodels = [\n    ('US GNP', time_range, 'None'),\n    ('US Prices', time_range, 'None'),\n    ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n    ('US monetary base', time_range, 'None'),\n    ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n]\nindex = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\nparameter_symbols = [\n    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n]\n\ni = 0\nfor res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n    if res.model.stochastic_level:\n        (sigma_irregular, sigma_level, sigma_trend,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n    else:\n        (sigma_irregular, sigma_level,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n        sigma_trend = np.nan\n    period_cycle = 2 * np.pi / frequency_cycle\n\n    table_i[i, :] = [\n        sigma_level*1e7, sigma_trend*1e7,\n        sigma_cycle*1e7, damping_cycle, period_cycle,\n        sigma_irregular*1e7\n    ]\n    i += 1\n\npd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\ntable_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\ntable_i",
            "# Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>",
            "# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n# make up some data in the open interval (0, 1)\ny = np.random.normal(loc=0.5, scale=0.4, size=1000)\ny = y[(y  0) & (y &lt; 1)]\ny.sort()\nx = np.arange(len(y))\n\n# plot with various axes scales\nplt.figure()\n\n# linear\nplt.subplot(221)\nplt.plot(x, y)\nplt.yscale('linear')\nplt.title('linear')\nplt.grid(True)\n\n# log\nplt.subplot(222)\nplt.plot(x, y)\nplt.yscale('log')\nplt.title('log')\nplt.grid(True)\n\n# symmetric log\nplt.subplot(223)\nplt.plot(x, y - y.mean())\nplt.yscale('symlog', linthresh=0.01)\nplt.title('symlog')\nplt.grid(True)\n\n# logit\nplt.subplot(224)\nplt.plot(x, y)\nplt.yscale('logit')\nplt.title('logit')\nplt.grid(True)\n# Adjust the subplot layout, because the logit one may take more space\n# than usual, due to y-tick labels like \"1 - 10^{-3}\"\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()\n\n\n<img alt=\"linear, log, symlog, logit\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_010.png\" srcset=\"../../_images/sphx_glr_pyplot_010.png, ../../_images/sphx_glr_pyplot_010_2_0x.png 2.0x\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->SettingWithCopy"
            ],
            [
                "sklearn->Examples->Miscellaneous->Explicit feature map approximation for RBF kernels->Decision Surfaces of RBF Kernel SVM and Linear SVM"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Model"
            ],
            [
                "statsmodels->Examples->State space models->Unobserved Components: Application->Data"
            ],
            [
                "matplotlib->Tutorials->Introductory->Pyplot tutorial->Logarithmic and other nonlinear axes"
            ]
        ]
    },
    "1488721": {
        "jupyter_code_cell": "np.percentile(finalPrice,1)",
        "matched_tutorial_code_inds": [
            5685,
            5842,
            5937,
            5335,
            6071
        ],
        "matched_tutorial_codes": [
            "np.bincount(data[\"affairs\"].astype(int))",
            "np.median(x)",
            "np.array(se_beta).mean()",
            "np.linalg.cond(results.model.exog)",
            "np.array(nobs1 + nobs2)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators"
            ],
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Regression Diagnostics->Multicollinearity"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example"
            ]
        ]
    },
    "176258": {
        "jupyter_code_cell": "tips[\"tip_bracket\"] = pandas.cut(x = tips[\"tip\"],bins=5)",
        "matched_tutorial_code_inds": [
            4092,
            4028,
            4085,
            4083,
            4088
        ],
        "matched_tutorial_codes": [
            "sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "sns.relplot(data=tips, x=\"total_bill\", y=\"tip\", size=\"size\")\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", levels=[.01, .05, .1, .8])\n",
            "sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "sns.jointplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots",
                "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions",
                "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions"
            ]
        ]
    },
    "767468": {
        "jupyter_code_cell": "listings_sales_differences = listings['Listings'] - listings['Sales']\nprint('Index  Difference')\nprint(listings_sales_differences.sort_values(ascending=False).head())",
        "matched_tutorial_code_inds": [
            3857,
            6357,
            3730,
            6811,
            6951
        ],
        "matched_tutorial_codes": [
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")",
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "# Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing"
            ],
            [
                "pandas_toms_blog->Fast Pandas->Constructors"
            ],
            [
                "statsmodels->Examples->Multivariate Methods->Principal Component Analysis"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ]
        ]
    },
    "306288": {
        "jupyter_code_cell": "epoch=2\nbatch=100\nsp_epoch=X_train.shape[0]",
        "matched_tutorial_code_inds": [
            446,
            394,
            389,
            2443,
            1091
        ],
        "matched_tutorial_codes": [
            "input_batch=big_input_batch\n\nmodel_input = to_tensor(transform(input_batch), padding_value=1)\noutput = model(model_input)\noutput.shape",
            "model_conv = train_model(model_conv, , ,\n                         , num_epochs=25)",
            "model_ft = train_model(model_ft, , , ,\n                       num_epochs=25)",
            "all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=25, device='cpu')\n\nvisualize_model(new_model)\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "torch->Text->Fast Transformer Inference with Better Transformer->Additional Information"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->ConvNet as fixed feature extractor->Train and evaluate"
            ],
            [
                "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Finetuning the convnet->Train and evaluate"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation"
            ],
            [
                "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 1. Training a Custom Classifier based on a Quantized Feature Extractor->Train and evaluate"
            ]
        ]
    },
    "1036325": {
        "jupyter_code_cell": "def observe_matching(matching, verbose):\n    sum_differences = 0\n    for (a, b) in matching.items():\n        propensity_score_a = data_propensity.loc[a][\"propensity\"]\n        propensity_score_b = data_propensity.loc[b][\"propensity\"]\n        diff = abs(propensity_score_a - propensity_score_b)\n        if verbose:\n            print(\"Difference between {} and {} is {}\".format(a, b, diff))\n        sum_differences += diff\n    print(\"Mean difference in prop score is {}\".format(sum_differences / len(matching)))\n    \nobserve_matching(matching_propensity, False)",
        "matched_tutorial_code_inds": [
            3126,
            6382,
            3125,
            3124,
            2417
        ],
        "matched_tutorial_codes": [
            "def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")",
            "def get_impute_knn_score(X_missing, y_missing):\n    imputer = (missing_values=, add_indicator=True)\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return knn_impute_scores.mean(), knn_impute_scores.std()\n\n\nmses_california[2], stds_california[2] = get_impute_knn_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"KNN Imputation\")",
            "def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean"
            ],
            [
                "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->kNN-imputation of the missing values"
            ],
            [
                "sklearn->Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results"
            ]
        ]
    },
    "1160265": {
        "jupyter_code_cell": "print(train_label.shape)",
        "matched_tutorial_code_inds": [
            5893,
            4795,
            6947,
            6994,
            1724
        ],
        "matched_tutorial_codes": [
            "print(rlm_model.weights)",
            "print(yy.shape)",
            "print(endog.index)",
            "print(res_nbin.params)",
            "print(env.observation_space)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Styling with cycler"
            ],
            [
                "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes"
            ],
            [
                "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing"
            ],
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)"
            ]
        ]
    },
    "805831": {
        "jupyter_code_cell": "Y = 50",
        "matched_tutorial_code_inds": [
            1730,
            2379,
            2526,
            2368,
            1835
        ],
        "matched_tutorial_codes": [
            "D = 80 * 80",
            "0.796",
            "Optimal number of features: 3",
            "0.9",
            "0.7344"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Create the policy (the neural network) and the forward pass"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors"
            ],
            [
                "sklearn->Examples->Feature Selection->Recursive feature elimination with cross-validation->Model training and selection"
            ],
            [
                "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval"
            ],
            [
                "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function"
            ]
        ]
    },
    "606503": {
        "jupyter_code_cell": "titanic.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"], axis=1, inplace=True)",
        "matched_tutorial_code_inds": [
            4153,
            5470,
            3958,
            3684,
            3938
        ],
        "matched_tutorial_codes": [
            "g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "dta[\"affair\"] = (dta[\"affairs\"]  0).astype(float)\nprint(dta.head(10))",
            "sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n"
        ],
        "matched_tutorial_paths": [
            [
                "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples",
                "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data"
            ],
            [
                "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data",
                "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data"
            ],
            [
                "pandas_toms_blog->Indexes->Flavors->Row Slicing"
            ],
            [
                "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data"
            ]
        ]
    },
    "476671": {
        "jupyter_code_cell": "header = ['first', 'last', 'gender', 'proportion']\ndf = pd.DataFrame(columns=header)\n\ndf_lt90 = pd.DataFrame(columns=header)",
        "matched_tutorial_code_inds": [
            3638,
            5789,
            3644,
            3779,
            3812
        ],
        "matched_tutorial_codes": [
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures"
            ],
            [
                "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing"
            ],
            [
                "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples"
            ]
        ]
    },
    "1315501": {
        "jupyter_code_cell": "def make_xy(X_col, y_col, vectorizer):\n    X = vectorizer.fit_transform(X_col)\n    y = y_col\n    return X, y",
        "matched_tutorial_code_inds": [
            4661,
            1260,
            3609,
            3611,
            1748
        ],
        "matched_tutorial_codes": [
            "def my_plotter(ax, data1, data2, param_dict):\n    \"\"\"\n    A helper function to make a graph.\n    \"\"\"\n    out = ax.plot(data1, data2, **param_dict)\n    return out",
            "def setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer",
            "from sklearn.feature_extraction.text import CountVectorizer\n count_vect = CountVectorizer()\n X_train_counts = count_vect.fit_transform(twenty_train.data)\n X_train_counts.shape\n(2257, 35788)",
            "from sklearn.feature_extraction.text import TfidfTransformer\n tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n X_train_tf = tf_transformer.transform(X_train_counts)\n X_train_tf.shape\n(2257, 35788)",
            "def update_input(prev_x, cur_x, D):\n    if prev_x is not None:\n        x = cur_x - prev_x\n    else:\n        x = np.zeros(D)\n    return x"
        ],
        "matched_tutorial_paths": [
            [
                "matplotlib->Tutorials->Introductory->Quick start guide->Coding styles->Making a helper functions"
            ],
            [
                "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn"
            ],
            [
                "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies"
            ],
            [
                "numpy->Articles->Deep reinforcement learning with Pong from pixels->Train the agent for a number of episodes"
            ]
        ]
    },
    "324533": {
        "jupyter_code_cell": "elizabethSentences[3]",
        "matched_tutorial_code_inds": [
            3714,
            2146,
            1191,
            6119,
            6006
        ],
        "matched_tutorial_codes": [
            "weather.loc['DSM']",
            "Distorting image...",
            "compile 3: True",
            "dta_c.T[0]",
            "df_infl[:5]"
        ],
        "matched_tutorial_paths": [
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "sklearn->Examples->Decomposition->Image denoising using dictionary learning->Generate distorted image"
            ],
            [
                "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing"
            ],
            [
                "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables"
            ],
            [
                "statsmodels->Examples->Statistics->ANOVA"
            ]
        ]
    },
    "1403304": {
        "jupyter_code_cell": "# 2.6.1.1\n# Set up a wide chart so we can see the separation between offenses\nplt.figure(figsize=(30,10))\n\n# Default font was too small to make out the Offense, so scale it\nsns.set(font_scale=2)\n\n# Create a subset with data from 0 to 24 hours response times\nplt_test = dc[dc.TIME_TO_REPORT < 86400][dc.TIME_TO_REPORT > 0]\n\n# Create the box plot - report the response time in hours instead of seconds.  Group by Offense, and color by Shift\nsns.boxplot(x=plt_test.OFFENSE, y=plt_test.TIME_TO_REPORT/3600.0, hue=plt_test.SHIFT)\n\n# Move the legend out of the way\nplt.legend(loc='upper right')",
        "matched_tutorial_code_inds": [
            6436,
            4626,
            6437,
            3318,
            6671
        ],
        "matched_tutorial_codes": [
            "# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Personal consumption', xlabel='Date', ylabel='Billions of dollars')\n\n# Plot data points\ndata.loc['1977-07-01':, 'consump'].plot(ax=ax, style='o', label='Observed')\n\n# Plot predictions\npredict.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='r--', label='One-step-ahead forecast')\nci = predict_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\npredict_dy.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='g', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='g', alpha=0.1)\n\nlegend = ax.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "# example.py\nimport numpy as np\nimport ctypes\nfrom scipy import ndimage, LowLevelCallable\nfrom numba import cfunc, types, carray\n\n@cfunc(types.intc(types.CPointer(types.intp),\n                  types.CPointer(types.double),\n                  types.intc,\n                  types.intc,\n                  types.voidptr))\ndef transform(output_coordinates_ptr, input_coordinates_ptr,\n              output_rank, input_rank, user_data):\n    input_coordinates = carray(input_coordinates_ptr, (input_rank,))\n    output_coordinates = carray(output_coordinates_ptr, (output_rank,))\n    shift = carray(user_data, (1,), types.double)[0]\n\n    for i in range(input_rank):\n        input_coordinates[i] = output_coordinates[i] - shift\n\n    return 1\n\nshift = 0.5\n\n# Then call the function\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\ncallback = LowLevelCallable(transform.ctypes, ptr)\n\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "# Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "# Author: Jake Vanderplas &lt;jakevdp@cs.washington.edu\n#\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import \nfrom sklearn.neighbors import \n\n# ----------------------------------------------------------------------\n# Plot the progression of histograms to kernels\n(1)\nN = 20\nX = (\n    ((0, 1, int(0.3 * N)), (5, 1, int(0.7 * N)))\n)[:, ]\nX_plot = (-5, 10, 1000)[:, ]\nbins = (-5, 10, 10)\n\nfig, ax = (2, 2, sharex=True, sharey=True)\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\n\n# histogram 1\nax[0, 0].hist(X[:, 0], bins=bins, fc=\"#AAAAFF\", density=True)\nax[0, 0].text(-3.5, 0.31, \"Histogram\")\n\n# histogram 2\nax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc=\"#AAAAFF\", density=True)\nax[0, 1].text(-3.5, 0.31, \"Histogram, bins shifted\")\n\n# tophat KDE\nkde = (kernel=\"tophat\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 0].fill(X_plot[:, 0], (log_dens), fc=\"#AAAAFF\")\nax[1, 0].text(-3.5, 0.31, \"Tophat Kernel Density\")\n\n# Gaussian KDE\nkde = (kernel=\"gaussian\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 1].fill(X_plot[:, 0], (log_dens), fc=\"#AAAAFF\")\nax[1, 1].text(-3.5, 0.31, \"Gaussian Kernel Density\")\n\nfor axi in ax.ravel():\n    axi.plot(X[:, 0], (X.shape[0], -0.01), \"+k\")\n    axi.set_xlim(-4, 9)\n    axi.set_ylim(-0.02, 0.34)\n\nfor axi in ax[:, 0]:\n    axi.set_ylabel(\"Normalized Density\")\n\nfor axi in ax[1, :]:\n    axi.set_xlabel(\"x\")\n\n# ----------------------------------------------------------------------\n# Plot all available kernels\nX_plot = (-6, 6, 1000)[:, None]\nX_src = ((1, 1))\n\nfig, ax = (2, 3, sharex=True, sharey=True)\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n\n\ndef format_func(x, loc):\n    if x == 0:\n        return \"0\"\n    elif x == 1:\n        return \"h\"\n    elif x == -1:\n        return \"-h\"\n    else:\n        return \"%ih\" % x\n\n\nfor i, kernel in enumerate(\n    [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"]\n):\n    axi = ax.ravel()[i]\n    log_dens = (kernel=kernel).fit(X_src).score_samples(X_plot)\n    axi.fill(X_plot[:, 0], (log_dens), \"-k\", fc=\"#AAAAFF\")\n    axi.text(-2.6, 0.95, kernel)\n\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n    axi.yaxis.set_major_locator(plt.NullLocator())\n\n    axi.set_ylim(0, 1.05)\n    axi.set_xlim(-2.9, 2.9)\n\nax[0, 1].set_title(\"Available Kernels\")\n\n# ----------------------------------------------------------------------\n# Plot a 1D density example\nN = 100\n(1)\nX = (\n    ((0, 1, int(0.3 * N)), (5, 1, int(0.7 * N)))\n)[:, ]\n\nX_plot = (-5, 10, 1000)[:, ]\n\ntrue_dens = 0.3 * (0, 1).pdf(X_plot[:, 0]) + 0.7 * (5, 1).pdf(X_plot[:, 0])\n\nfig, ax = ()\nax.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"input distribution\")\ncolors = [\"navy\", \"cornflowerblue\", \"darkorange\"]\nkernels = [\"gaussian\", \"tophat\", \"epanechnikov\"]\nlw = 2\n\nfor color, kernel in zip(colors, kernels):\n    kde = (kernel=kernel, bandwidth=0.5).fit(X)\n    log_dens = kde.score_samples(X_plot)\n    ax.plot(\n        X_plot[:, 0],\n        (log_dens),\n        color=color,\n        lw=lw,\n        linestyle=\"-\",\n        label=\"kernel = '{0}'\".format(kernel),\n    )\n\nax.text(6, 0.38, \"N={0} points\".format(N))\n\nax.legend(loc=\"upper left\")\nax.plot(X[:, 0], -0.005 - 0.01 * (X.shape[0]), \"+k\")\n\nax.set_xlim(-4, 9)\nax.set_ylim(-0.02, 0.4)\n()",
            "# Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n\n# Plot estimate of the level term\nres_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\nres_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n\nax.legend(loc=\"lower left\");\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "scipy->Multidimensional image processing (scipy.ndimage)->Extending scipy.ndimage in C"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting"
            ],
            [
                "sklearn->Examples->Nearest Neighbors->Simple 1D Kernel Density Estimation"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models"
            ]
        ]
    },
    "850846": {
        "jupyter_code_cell": "titanic_ds.isnull().sum()",
        "matched_tutorial_code_inds": [
            1500,
            5600,
            3723,
            302,
            6458
        ],
        "matched_tutorial_codes": [
            "china_mask.nonzero()",
            "resf_logit.predict()",
            "flights.dep_time.head()",
            "tensor(0.1094)",
            "arima_res.predict(0, 2)"
        ],
        "matched_tutorial_paths": [
            [
                "numpy->NumPy Features->Masked Arrays->Missing data"
            ],
            [
                "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model"
            ],
            [
                "pandas_toms_blog->Indexes->Merging->The merge version"
            ],
            [
                "torch->Learning PyTorch->What is torch.nn really?->Neural net from scratch (no torch.nn)"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA"
            ]
        ]
    },
    "603955": {
        "jupyter_code_cell": "air.signup_method.value_counts(ascending=False).plot(kind='pie', autopct='%.2f',figsize=(6,6),title='Distribution of Signup Method',fontsize=12)",
        "matched_tutorial_code_inds": [
            4391,
            3820,
            4031,
            3817,
            3821
        ],
        "matched_tutorial_codes": [
            "b, a = signal.iirfilter(4, Wn=0.2, rp=5, rs=60, btype='lowpass', ftype='ellip')\n w, h = signal.freqz(b, a)",
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "df.plot.scatter(x='carat', y='depth', c='k', alpha=.15)\nplt.tight_layout()",
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()"
        ],
        "matched_tutorial_paths": [
            [
                "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ],
            [
                "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty",
                "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Pandas Built-in Plotting"
            ],
            [
                "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn"
            ]
        ]
    },
    "602423": {
        "jupyter_code_cell": "for row in range(len(df[(df['Week']==2) & (df['Season']==2017)])):\n    if (df['Home_team'][row]=='MIA') | (df['Home_team'][row]=='TAM') | (df['Away_team'][row]=='MIA') | (df['Away_team'][row]=='TAM'):\n        df = df.drop(row)",
        "matched_tutorial_code_inds": [
            2968,
            2178,
            6761,
            6638,
            6364
        ],
        "matched_tutorial_codes": [
            "for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>",
            "for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "for i in range(niter):\n    mod.update_variances(store_obs_cov[i], store_state_cov[i])\n    sim.simulate()\n\n    # 1. Sample states\n    store_states[i + 1] = sim.simulated_state.T\n\n    # 2. Simulate obs cov\n    fitted = np.matmul(mod['design'].transpose(2, 0, 1), store_states[i + 1][..., None])[..., 0]\n    resid = mod.endog - fitted\n    store_obs_cov[i + 1] = invwishart.rvs(v10 + mod.nobs, S10 + resid.T @ resid)\n\n    # 3. Simulate state cov variances\n    resid = store_states[i + 1, 1:] - store_states[i + 1, :-1]\n    sse = np.sum(resid**2, axis=0)\n\n    for j in range(mod.k_states):\n        rv = invgamma.rvs((vi20 + mod.nobs - 1) / 2, scale=(Si20 + sse[j]) / 2)\n        store_state_cov[i + 1, j] = rv",
            "for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)"
            ],
            [
                "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits"
            ],
            [
                "statsmodels->Examples->State space models->ETS models->Predictions"
            ],
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data"
            ]
        ]
    },
    "1006052": {
        "jupyter_code_cell": "# Binarize target labels\nlb = preprocessing.LabelBinarizer()\nSMS_train_target = np.array([number[0] for number in lb.fit_transform(SMS_train_target)])\n\n# Transform data\nsms_count_vect = CountVectorizer(decode_error = 'replace')\nsms_train_counts = sms_count_vect.fit_transform(SMS_train_data)\nsms_tfidf_transformer = TfidfTransformer()\nsms_train_tfidf = sms_tfidf_transformer.fit_transform(sms_train_counts)",
        "matched_tutorial_code_inds": [
            6625,
            632,
            6661,
            5132,
            5637
        ],
        "matched_tutorial_codes": [
            "# Construct a local level model for inflation\nmod = sm.tsa.UnobservedComponents(dta.infl, 'llevel')\n\n# Fit the model's parameters (sigma2_varepsilon and sigma2_eta)\n# via maximum likelihood\nres = mod.fit()\nprint(res.params)\n\n# Create simulation smoother objects\nsim_kfs = mod.simulation_smoother()              # default method is KFS\nsim_cfa = mod.simulation_smoother(method='cfa')  # can specify CFA method",
            "# Load pretrained model weights\nmodel_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\nbatch_size = 1    # just a random number\n\n# Initialize model with the pretrained weights\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict((model_url, map_location=map_location))\n\n# set the model to inference mode\ntorch_model.eval()",
            "# Retrieve the posterior means\nparams = pm.summary(trace)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_bayes = mod.smooth(params)\n\npredict_bayes = res_bayes.get_prediction()\npredict_bayes_ci = predict_bayes.conf_int()\nlower = predict_bayes_ci[\"lower CPIAUCNS\"]\nupper = predict_bayes_ci[\"upper CPIAUCNS\"]\n\n# Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf.plot(ax=ax, style=\"-\", label=\"Observed\")\n\n# Plot predictions\npredict_bayes.predicted_mean.plot(ax=ax, style=\"r.\", label=\"One-step-ahead forecast\")\nax.fill_between(predict_bayes_ci.index, lower, upper, color=\"r\", alpha=0.1)\nax.legend(loc=\"lower left\")\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_24_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_24_0.png\"/>",
            "# Construct a model\nmodel = sm.tsa.SARIMAX(endog, order=(1, 0, 0))\n\n# Fit the model with a fixed value for the AR(1) coefficient using the\n# context manager\nwith model.fix_params({'ar.L1': 0.5}):\n    results = model.fit()",
            "# Generate data looking like cosine\nx = np.random.uniform(0, 4 * np.pi, size=200)\ny = np.cos(x) + np.random.random(size=len(x))\n\n# Compute a lowess smoothing of the data\nsmoothed = sm.nonparametric.lowess(exog=x, endog=y, frac=0.2)"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Local level model"
            ],
            [
                "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->6. Application of Bayesian estimates of parameters"
            ],
            [
                "statsmodels->User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Additional options and tools->Holding some parameters fixed and estimating the rest"
            ],
            [
                "statsmodels->Examples->Nonparametric Statistics->Lowess Regression"
            ]
        ]
    },
    "667164": {
        "jupyter_code_cell": "import pandas as pd\nimport matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.model_selection import KFold",
        "matched_tutorial_code_inds": [
            3007,
            6999,
            6412,
            6376,
            5276
        ],
        "matched_tutorial_codes": [
            "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "import numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport requests\nfrom io import BytesIO",
            "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndata = sm.datasets.engel.load_pandas().data\ndata.head()"
        ],
        "matched_tutorial_paths": [
            [
                "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence"
            ],
            [
                "statsmodels->Examples->User Notes->Dates in Time-Series Models"
            ],
            [
                "statsmodels->Examples->State space models->SARIMAX: Introduction"
            ],
            [
                "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Quantile Regression->Setup"
            ]
        ]
    },
    "501233": {
        "jupyter_code_cell": "X = sm.add_constant(X)\nmodel = sm.OLS(y,X)\nresults = model.fit()\nresults.summary()",
        "matched_tutorial_code_inds": [
            5197,
            6898,
            5205,
            6731,
            5240
        ],
        "matched_tutorial_codes": [
            "model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())",
            "olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "res = sm.OLS(y, X).fit()\nprint(res.summary())",
            "mod = MultipleYsModel(i_hat, s_t, m_hat)\nres = mod.fit()\n\nprint(res.summary())",
            "ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation"
            ],
            [
                "statsmodels->Examples->User Notes->Prediction->Estimation"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters"
            ],
            [
                "statsmodels->Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->What do we need to modify?->2) The update() function"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity"
            ]
        ]
    },
    "1114155": {
        "jupyter_code_cell": "(((x - x.mean()) ** 2).mean()) ** 0.5",
        "matched_tutorial_code_inds": [
            5733,
            5729,
            5683,
            4873,
            5254
        ],
        "matched_tutorial_codes": [
            "(res_e2._results.resid_response ** 2 / res_e2.model.family.variance(res_e2.mu)).sum()",
            "(res_e._results.resid_response ** 2 / res_e.model.family.variance(res_e.mu)).sum()",
            "(data[\"affairs\"] == 0).mean()",
            "(0.05, 0.05)",
            "2.0 / len(X) ** 0.5"
        ],
        "matched_tutorial_paths": [
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Investigating Pearson chi-square statistic"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Investigating Pearson chi-square statistic"
            ],
            [
                "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data"
            ],
            [
                "matplotlib->Tutorials->Intermediate->Autoscaling->Margins"
            ],
            [
                "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Dropping an observation"
            ]
        ]
    }
}