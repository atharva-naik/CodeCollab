{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8560f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simlogs import *\n",
    "from IPython.display import Image\n",
    "\n",
    "def get_pval(df_a, df_b, metric):\n",
    "    set_a = df_a.groupby('user_id')[metric].mean().values\n",
    "    set_b = df_b.groupby('user_id')[metric].mean().values\n",
    "    _,_,_,pval = wald_test(set_a, set_b)\n",
    "    return pval\n",
    "\n",
    "def click_report(df_a, df_b):\n",
    "    print 'Impression level control average: %0.3f' % df_a.click.mean()\n",
    "    print 'Impression level treatment average: %0.3f' % df_b.click.mean()\n",
    "    print 'Lift: %0.2f' % ((df_b.click.mean() - df_a.click.mean()) / df_a.click.mean())\n",
    "    print ''\n",
    "    print 'User level control click average: %0.3f' % df_a.groupby('user_id').click.mean().mean()\n",
    "    print 'User level treatment click average: %0.3f' % df_b.groupby('user_id').click.mean().mean()\n",
    "    print 'Lift: %0.2f' % ((df_b.groupby('user_id').click.mean().mean() - df_a.groupby('user_id').click.mean().mean()) / df_a.groupby('user_id').click.mean().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71655b32",
   "metadata": {},
   "source": [
    "#Simulating Users\n",
    "\n",
    "### Possible Topics\n",
    "* Setup:\n",
    "    * Me (data science type, lots of for loops, clever in python)\n",
    "    * Them (devs, specs, dbs, not inference, tests if you are lucky)\n",
    "    * testing is a place we can all communicating\n",
    "* A/B Testing (notion vs truth)\n",
    "    * Diagram of simple vs Bing\n",
    "* Problem statement (it's one thing to to execute a plan, it's another to achieve robust/trustworthy exp)\n",
    "* Design vs Execution vs Scorecard\n",
    "    * Design can add complexity\n",
    "    * Execution is not analysis\n",
    "    * Scorecard is a \"view\" not the truth\n",
    "    * Parallel views (what someone did versus what we logged)\n",
    "* Execution to Scorecard phase:\n",
    "    * Empirically has had problems\n",
    "    * Many are just incorrect execution\n",
    "    * Some are incorrect aggregation\n",
    "    * Most were fixable with unit testing\n",
    "* Scaling and testing:\n",
    "    * Don't need actual experiments...we can simulate\n",
    "* Simulation\n",
    "    * Humans are suprisingly simple when it comes to log lines\n",
    "    * Artifact of what we log to some extent\n",
    "    * Can produce surprisingly accurate aggregate results\n",
    "* Simple flow:\n",
    "    1. Human visits\n",
    "    2. Human has choice (often influenced by treatment...you hope)\n",
    "    3. Human makes choice\n",
    "    4. (Optional) Human repeats 2 and 3 additional times\n",
    "    * Key point: Logs are *generated* by a process\n",
    "* Abstract the process:\n",
    "    1. Present a choice (probability distribution)\n",
    "    2. Draw from that distribution\n",
    "    3. Given the draw, present a second choice (another probability distribution, possibly different)\n",
    "    4. Draw again\n",
    "    5. Repeat\n",
    "    * Simple process but it captures the essence of the experiment\n",
    "    * The layering of draws and choice of distributions inject flexibility and complexity\n",
    "* Example:\n",
    "    * Simple click stream of an offer\n",
    "    * Provides a way for the devs to interact with the logs in multiple ways\n",
    "    * Easy to write tests for summary stats\n",
    "* Find the \"unknown\" parameters\n",
    "    * Simulated logs are noisy instaniations of your supplied parameters\n",
    "    * In other words, you put a number into the function and it spit out a ton of hand wavey examples\n",
    "    * Your task (well, the dev's task) is to recover that parameter (within reason)\n",
    "    * This is the foundation of the unit test framework\n",
    "* Example test:\n",
    "    * CTR difference between two groups\n",
    "    * How do I deal with \"random\" data in an assert?\n",
    "    * `Almost Equal`?\n",
    "* Dealing with random data:\n",
    "    * Seed (although this will not enable robustness)\n",
    "    * p-value (expensive since I need to test the function ~1000 times)\n",
    "    * Comparisons (balance between the two...i.e. do I always manage to find the more true one)\n",
    "    * I tend to land on the seed method when starting, migrate to p-value for robustness, and do comparisons when devs complain my tests take too long (doesn't happen that often)\n",
    "* End to end test for a conversion\n",
    "    * what do I need to test?\n",
    "        * Log integrity (are there illegal values?) Instrumentation problems (THIS IS A HUGE DEAL)\n",
    "        * Roll ups (you'd be surprised how broken this is)\n",
    "        * Reverse A/A (you are doing this right?)\n",
    "        * Comparisons (This is sort of the important part)\n",
    "* What else can I do?\n",
    "    * Simulate hypotheses\n",
    "    * Be really brute force about sample size calculation\n",
    "    * Practice scaling\n",
    "    * How do bugs or problem \"trickle down\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16089f3",
   "metadata": {},
   "source": [
    "# Who Needs Users? Just Simulate Them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61100f1c",
   "metadata": {},
   "source": [
    "**Chris Harland :: Data Scientist :: Context Relevant :: @cdubhland**\n",
    "\n",
    "(work done while at Microsoft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af551169",
   "metadata": {},
   "source": [
    "### Me (Data Scientist):\n",
    "Not a production programmer..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9792eb8f",
   "metadata": {},
   "source": [
    "### Them (Devs):\n",
    "\"Real\" programmers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d041fdc",
   "metadata": {},
   "source": [
    "### Problem:\n",
    "Experimentation platforms need both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf500b",
   "metadata": {},
   "source": [
    "I claim unit testing is a place we can all \"agree\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eaf0d4",
   "metadata": {},
   "source": [
    "# Who needs experimentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92f01d",
   "metadata": {},
   "source": [
    "Well...we do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f9d13",
   "metadata": {},
   "source": [
    "It is how we (as in humans) establish **causality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you like python 2.7 you can high five me @cdubhland\n",
    "# if you are stunned by my lack of commitment to python 3\n",
    "# you can send complaints to @joelgrus\n",
    "from __future__ import division\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def get_bernoulli_trial(p, n = 1):\n",
    "    \"\"\" return a bernoulli trial of success or failure with probability p \"\"\"\n",
    "    return stats.bernoulli.rvs(p = p, size = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f3f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.5\n",
    "n_trials = 10000\n",
    "print 'Expected p ~ %0.2f and obtained p = %0.2f' % \\\n",
    "(p,np.mean(get_bernoulli_trial(p,n_trials)))\n",
    "# We expect result to be near p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ad931",
   "metadata": {},
   "source": [
    "But all decisions aren't this simple =/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12c42a",
   "metadata": {},
   "source": [
    "### Luckily math can bail us out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make the probability of success a random variable\n",
    "def get_beta_result(a,b, n = 1):\n",
    "    \"\"\" takes a draw from beta(a,b) used to simulate random rates \"\"\"\n",
    "    return stats.beta.rvs(a,b, size = n)\n",
    "\n",
    "# We can model a collection of user behaviors\n",
    "def get_expon_result(mu, _lambda, n = 1):\n",
    "    \"\"\" takes a draw from a exponential(mu, lambda) \"\"\"\n",
    "    return stats.expon.rvs(mu, _lambda, size = n)\n",
    "\n",
    "# We can model the collective results of many choices\n",
    "def get_exp_result(n,p, size = 1):\n",
    "    \"\"\" return the outcome of n bernoulli trials with probability p \"\"\"\n",
    "    return stats.binom.rvs(n = n, p = p, size = size)\n",
    "\n",
    "# Maybe the users visit at different frequencies\n",
    "def gen_user_visit_freq(n_users = 100, _lambda = 2):\n",
    "    \"\"\" return the total number of visits in a set time delta for the number of given users \"\"\"\n",
    "    return stats.poisson.rvs(mu = _lambda, size = n_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4161e818",
   "metadata": {},
   "source": [
    "### Simple user click stream log\n",
    "\n",
    "* Imagine a user comes to your site (this can be a probability)\n",
    "* User executes a bernoulli trial with probability $p$\n",
    "    * (where $p$ is the \"click through rate\")\n",
    "* If the user had a successful trial call another bernoulli trial with probabiliy $q$\n",
    "    * where $q$ is the conditional \"conversion rate\" $P(conv | user, click)$\n",
    "* Log this as `Timestamp, user_id, impression, click, conversion`\n",
    "    * `Timestamp` can be draw from distribution of average gap times or assigned sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bf2739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_impression(p_imp = 1.0, p_click = 0.5, p_convert = 0.5):\n",
    "    \"\"\" This function generates an impression, click, conversion based\n",
    "    on probabilities defined by the input parameters \"\"\"\n",
    "    \n",
    "    impression = get_bernoulli_trial(p_imp)[0]\n",
    "    \n",
    "    # Note: to speed this up would could draw all trials at once\n",
    "    # and post process the results to make the outcomes conditional\n",
    "\n",
    "    if impression == 1:\n",
    "        did_click = get_bernoulli_trial(p_click)[0]\n",
    "        # For now we assume only those that click can convert\n",
    "        if did_click == 1:\n",
    "            did_convert = get_bernoulli_trial(p_convert)[0]\n",
    "        else:\n",
    "            # Optionally this could be a bernoulli with a different p\n",
    "            # (i.e. the base rate)\n",
    "            did_convert = 0\n",
    "        imp_arr = [impression, did_click, did_convert]\n",
    "        return imp_arr\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[gen_impression() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc0132",
   "metadata": {},
   "source": [
    "### Let's get a tiny bit fancy and make this into a real log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ce290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def gen_log_line(uid, t_current = datetime.datetime.now(), p_imp = 1.0, p_click = 0.5, p_convert = 0.5):\n",
    "    \"\"\" Get a log line for the given user and return with timestamp\n",
    "    and impression info \"\"\"\n",
    "    \n",
    "    imp = gen_impression(p_imp, p_click, p_convert)\n",
    "    \n",
    "    if imp is None:\n",
    "        return None\n",
    "    else:\n",
    "        # add a random t_delta\n",
    "        delta_sec = stats.norm.rvs(loc = 300, scale = 100)\n",
    "        t_ = t_current + datetime.timedelta(0,delta_sec)\n",
    "        timestamp = t_.strftime('%Y-%m-%d %I:%M:%S%p')\n",
    "        log_line = [timestamp, uid] + imp\n",
    "        return log_line, t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_log_line('Trey Causey')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7af193",
   "metadata": {},
   "source": [
    "### Heck like a really real log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda682e",
   "metadata": {},
   "source": [
    "Simulated logs are noisy instantiations of your supplied parameters\n",
    "\n",
    "In other words, you put a number into the function and it spit out a ton of hand wavey examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6bb331",
   "metadata": {},
   "source": [
    "Your task (well, the dev's task) is to recover that parameter (within reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f15de0c",
   "metadata": {},
   "source": [
    "### Revisiting the A/A unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='AA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code mock up\n",
    "def test_bucket_split(df_a, df_b, metric):\n",
    "    # Ensure that user bucketing created two equivalent groups\n",
    "    assert get_pval(df_a, df_b, metric) > 0.05\n",
    "    \n",
    "## Want to \"recover\" p > 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f4b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_click_control = 0.1\n",
    "p_convert_control = 0.1\n",
    "n_users = 1000\n",
    "n_rows = 10000\n",
    "\n",
    "# Going to hand wave this function\n",
    "df_a = simulate_log_vectorized(n_users = n_users,\n",
    "                               n_rows=n_rows,\n",
    "                               p_click=p_click_control,\n",
    "                               p_convert=p_convert_control,\n",
    "                               strict = False)\n",
    "\n",
    "df_b = simulate_log_vectorized(n_users = n_users,\n",
    "                               n_rows=n_rows,\n",
    "                               p_click=p_click_control,\n",
    "                               p_convert=p_convert_control,\n",
    "                               strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d2d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65df910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c4bee5",
   "metadata": {},
   "source": [
    "### Sweet no `AssertionError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc93e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bucket_split(df_a, df_b, 'click')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa10b685",
   "metadata": {},
   "source": [
    "Why am I so wary of random number generators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='bloodletting.jpg', width = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164b34f7",
   "metadata": {},
   "source": [
    "     The Burns Archive - Burns Archive via Newsweek, 2.4.2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='stiff_cover.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb432f93",
   "metadata": {},
   "source": [
    "Experimentation helps us find the truth in crazy situations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ee979",
   "metadata": {},
   "source": [
    "### A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fc454",
   "metadata": {},
   "source": [
    "    Homework Machine - A Light in the Attic - Shell Silverstein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076b8da",
   "metadata": {},
   "source": [
    "### I'm confused..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='ab_flame_1.png', width = 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='ab_flame_2.png', width = 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7bb7e1",
   "metadata": {},
   "source": [
    "A/B testing simultaneously:\n",
    "* lifts companies to the pinnacle of optimization\n",
    "* is a complete waste of time and never works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e10cff",
   "metadata": {},
   "source": [
    "### A peek into my bias:\n",
    "\n",
    "Experimentation is the story of three logs:\n",
    "1. Treatment Assignment\n",
    "2. Exp Platform\n",
    "3. Product\n",
    "\n",
    "Together these comprise the **execution** not the **analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8b9f2",
   "metadata": {},
   "source": [
    "### A peek into my bias:\n",
    "Reality : Logs :: Experimental Truth : Scorecard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6de43",
   "metadata": {},
   "source": [
    "### So where should we start?\n",
    "Assume you have a platform (of some kind) and a product (of some kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f3194",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='modified_ab_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59ecda",
   "metadata": {},
   "source": [
    "Common stumbling blocks:\n",
    "* Bucketing (random numbers are hard)\n",
    "* Scorecarding (counting, aggregating, and stats)\n",
    "\n",
    "It's possible to avoid some \"pitfalls\"\n",
    "\n",
    "Critical to know your platform works because users are wacky"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16955fd1",
   "metadata": {},
   "source": [
    "### What might a unit test look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='A-B_testing.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a3e06",
   "metadata": {},
   "source": [
    "### A/B testing (a bit more complicated than you think...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa6030",
   "metadata": {},
   "source": [
    "### So what's the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26667391",
   "metadata": {},
   "source": [
    "Find the \"unknown\" parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code mock up\n",
    "def test_bucket_split(self):\n",
    "    # Ensure that user bucketing created two equivalent groups\n",
    "    for metric in self.important_metrics:\n",
    "        assert abs(self.group_a[metric] - self.group_b[metric]) < self.tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a7105e",
   "metadata": {},
   "source": [
    "But where get these magical groups?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8470d506",
   "metadata": {},
   "source": [
    "### Make fake humans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0394103f",
   "metadata": {},
   "source": [
    "### Users are a collection of log lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c676741",
   "metadata": {},
   "source": [
    "Skip the users and just get to the log lines\n",
    "\n",
    "Anatomy of a log line:\n",
    "    1. Human visits\n",
    "    2. Human has choice (often influenced by treatment...you hope)\n",
    "    3. Human makes choice\n",
    "    4. (Optional) Human repeats 2 and 3 additional times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c3adc6",
   "metadata": {},
   "source": [
    "Logs are **generated** by a **process**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bab58ca",
   "metadata": {},
   "source": [
    "### Abstract the process\n",
    "\n",
    "    1. Present a choice (probability distribution, computer know what these are)\n",
    "    2. Draw from that distribution (nice...a computer can do this)\n",
    "    3. Given the draw, present a second choice (another probability distribution, possibly different)\n",
    "    4. Draw again (hey a computer can do this too)\n",
    "    5. Repeat (oh you bet a computer can do this)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914cb54",
   "metadata": {},
   "source": [
    "Simple process but it captures the essence of the log generation process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c38a0",
   "metadata": {},
   "source": [
    "The layering of draws and choice of distributions inject flexibility and complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0908c94",
   "metadata": {},
   "source": [
    "### Present a choice and then make it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f0753",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='real_testing.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b4835",
   "metadata": {},
   "source": [
    "I see alot of this:\n",
    "\n",
    "`df.click.mean()`\n",
    "\n",
    "Don't do that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd11d1c",
   "metadata": {},
   "source": [
    "### Again...real log stuff looks more like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heavy_users = simulate_log_vectorized(n_users = 10,\n",
    "                                         n_rows= 50000,\n",
    "                                         p_click=0.8,\n",
    "                                         p_convert=0.1,\n",
    "                                         strict=False)\n",
    "\n",
    "df_light_users = simulate_log_vectorized(n_users = 1000,\n",
    "                                         n_rows= 10000,\n",
    "                                         p_click=0.1,\n",
    "                                         p_convert=0.1,\n",
    "                                         strict=False)\n",
    "\n",
    "df_users = pd.concat([df_heavy_users, df_light_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Impression level click average: %0.3f' % df_users.click.mean()\n",
    "print 'User level click average: %0.3f' % df_users.groupby('user_id').click.mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef82d4b",
   "metadata": {},
   "source": [
    "#### Oh but it gets important\n",
    "\n",
    "Let's say your awesome experiment lifts heavy users CTR ~10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heavy_users_moved = simulate_log_vectorized(n_users = 10,\n",
    "                                         n_rows= 50000,\n",
    "                                         p_click=0.88,\n",
    "                                         p_convert=0.1,\n",
    "                                         strict=False)\n",
    "\n",
    "df_users_moved = pd.concat([df_heavy_users_moved, df_light_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c307ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Impression level click average: %0.3f' % df_users_moved.click.mean()\n",
    "print 'User level click average: %0.3f' % df_users_moved.groupby('user_id').click.mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_report(df_users, df_users_moved)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e42dbea",
   "metadata": {},
   "source": [
    "### Works the other way too\n",
    "\n",
    "Let's say 10% of your light users exhibited a 10% lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf6dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heavy_users = simulate_log_vectorized(n_users = 10,\n",
    "                                         n_rows= 50000,\n",
    "                                         p_click=0.8,\n",
    "                                         p_convert=0.1,\n",
    "                                         strict=False)\n",
    "\n",
    "df_stubborn_light_users = simulate_log_vectorized(n_users = 900,\n",
    "                                                  n_rows= 9000,\n",
    "                                                  p_click=0.1,\n",
    "                                                  p_convert=0.1,\n",
    "                                                  strict=False)\n",
    "\n",
    "df_cooperative_light_users = simulate_log_vectorized(n_users = 100,\n",
    "                                                     n_rows= 1000,\n",
    "                                                     p_click=0.11,\n",
    "                                                     p_convert=0.1,\n",
    "                                                     strict=False)\n",
    "\n",
    "\n",
    "df_users_le_sigh = pd.concat([df_heavy_users,\n",
    "                              df_stubborn_light_users,\n",
    "                              df_cooperative_light_users])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec88450",
   "metadata": {},
   "outputs": [],
   "source": [
    "click_report(df_users, df_users_le_sigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7d10db",
   "metadata": {},
   "source": [
    "Impression level rollups aren't sensitive enough =/\n",
    "\n",
    "Unit test for sensitivity (too much or too little)\n",
    "\n",
    "Avoid making ship mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c14d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename = 'reagan.jpg', width = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d7bdf",
   "metadata": {},
   "source": [
    "### But now let's make it \"real\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We often test many metrics\n",
    "def add_metrics(n_metrics, df_a, df_b):\n",
    "    for i in range(n_metrics):\n",
    "        p = np.random.rand()\n",
    "        # same p for both groups...should be equal\n",
    "        df_a.loc[:,'metric_%d'%i] = get_bernoulli_trial(p = p, n = len(df_a))\n",
    "        df_b.loc[:,'metric_%d'%i] = get_bernoulli_trial(p = p, n = len(df_b))\n",
    "    return df_a, df_b\n",
    "\n",
    "# We can make a factory of fails\n",
    "def aa_fail_o_tron(df_a, df_b, n_metrics):\n",
    "    # add some metrics to the pile\n",
    "    df_a_mod, df_b_mod = add_metrics(n_metrics, df_a, df_b)\n",
    "    \n",
    "    # Check that all metrics come back not significant\n",
    "    for i in range(n_metrics):\n",
    "        test_bucket_split(df_a_mod, df_b_mod, 'metric_%d' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f30f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_fail_o_tron(df_a, df_b, 10) # won't always fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41532657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But how often does it fail?\n",
    "def count_dem_fails(df_a, df_b, n_metrics):\n",
    "    pvals = []\n",
    "    # add some metrics to the pile\n",
    "    df_a_mod, df_b_mod = add_metrics(n_metrics, df_a, df_b)\n",
    "    \n",
    "    # Check that all metrics come back not significant\n",
    "    for i in range(n_metrics):\n",
    "        pvals.append(get_pval(df_a_mod, df_b_mod, 'metric_%d' % i))\n",
    "    \n",
    "    return sum([pval < 0.05 for pval in pvals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f7acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print [count_dem_fails(df_a, df_b, 2) for _ in range(10)]\n",
    "print [count_dem_fails(df_a, df_b, 5) for _ in range(10)]\n",
    "print [count_dem_fails(df_a, df_b, 20) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b311d2",
   "metadata": {},
   "source": [
    "### Be a good coder...pass those tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ab6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonferroni_correction(pval, n_metrics):\n",
    "    # Simply make it harder to fail by lowering pvail\n",
    "    return pval / n_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bucket_split(df_a, df_b, metric, n_metrics = 1):\n",
    "    # Ensure that user bucketing created two equivalent groups\n",
    "    assert get_pval(df_a, df_b, metric) > bonferroni_correction(0.05, n_metrics)\n",
    "    \n",
    "def aa_fail_o_tron(df_a, df_b, n_metrics):\n",
    "    # add some metrics to the pile\n",
    "    df_a_mod, df_b_mod = add_metrics(n_metrics, df_a, df_b)\n",
    "    \n",
    "    # Check that all metrics come back not significant\n",
    "    for i in range(n_metrics):\n",
    "        test_bucket_split(df_a_mod, df_b_mod, 'metric_%d' % i, n_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf28e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_fail_o_tron(df_a, df_b, 10) # this passes like a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3996c",
   "metadata": {},
   "source": [
    "### But it will still fail (random imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab49a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "[aa_fail_o_tron(df_a, df_b, 10) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729e786",
   "metadata": {},
   "source": [
    "It's okay though...this is expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7b777",
   "metadata": {},
   "source": [
    "### Quickly on aggregation\n",
    "\n",
    "How should we properly aggregate raw logs before hitting them with stats stick?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e603eb42",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "\n",
    "How do you calculate the **average page click rate per user**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "def create_hash_id(user, salt):\n",
    "    \"\"\" returns a sha1 hash of user string combined with salt string \"\"\"\n",
    "    return hashlib.sha1(salt + '_' + repr(user)).hexdigest()\n",
    "\n",
    "col_names = ['timestamp','user_id','impression','click','conversion']\n",
    "user_hash = create_hash_id('Trey Causey', 'Spurs always let you down')\n",
    "single_log = [gen_log_line(user_hash)[0] for x in range(10)]\n",
    "pd.DataFrame(single_log, columns = col_names).sort('timestamp') \\\n",
    ".reset_index(drop = True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba37c701",
   "metadata": {},
   "source": [
    "### But wait...there's more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2fb71",
   "metadata": {},
   "source": [
    "In general the formula is:\n",
    "* Encapsulate a behavior in a probability distribution\n",
    "    * Poisson for distinct events\n",
    "    * Exponential for time between those events\n",
    "    * Binomial for total wins\n",
    "    * Beta to make random probabilities\n",
    "    * Normal because it's popular\n",
    "* Chain those distributions together to form an impression\n",
    "* Vary the parameters within each chain to generate diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f350a0",
   "metadata": {},
   "source": [
    "The simplicity is deceptive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23030521",
   "metadata": {},
   "source": [
    "(this is bayesian stat testing backwards)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
