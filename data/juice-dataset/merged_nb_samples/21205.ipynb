{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e667b412",
   "metadata": {},
   "source": [
    "# Predicting Zillow Prices\n",
    "### Nick Lind\n",
    "\n",
    "This notebook was created to serve as an illustrative guide to the data science workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bc3913",
   "metadata": {},
   "source": [
    "## Step 1: Framing the problem and looking at the bigger picture\n",
    "1. Define the objective in business terms. What decision are you trying to inform? What problem are you trying to solve?\n",
    "2. How will your solution be used? Who is your user?\n",
    "3. What are the current solutions/workarounds (if any)?\n",
    "4. What type of learning problem are we dealing with (supervised, unsupervised, etc.)?\n",
    "5. How should performance be measured? Is the performance measure aligned with the business objective?\n",
    "7. What would be the minimum performance needed to reach the business objective?\n",
    "8. What are comparable problems? Can you reuse experience or tools?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8bde5",
   "metadata": {},
   "source": [
    "## Step 2: Procure the data\n",
    "1. List the data you need and how much you need.\n",
    "2. Find and document where you can get that data.\n",
    "3. Check how much space it will take.\n",
    "4. Check legal obligations, and get authorization if necessary.\n",
    "5. Get access authorizations.\n",
    "6. Create a workspace (with enough storage space).\n",
    "7. Get the data.\n",
    "8. Double-check for biases.\n",
    "8. Convert the data to a format you can easily manipulate (without changing the data itself).\n",
    "9. Ensure sensitive information is deleted or protected (e.g., anonymized).\n",
    "10. Check the size and type of data (time series, sample, geographical, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97611f5c",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "For most data science problems in Python, we'll use:\n",
    "    * Lists\n",
    "    * Series (NumPy)\n",
    "    * Arrays (NumPy)\n",
    "    * DataFrames (Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c116142",
   "metadata": {},
   "source": [
    "### Import Data & Packages\n",
    "First, we import our dataset as well as the relevant packages we'll need for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a836a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import loadtxt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import ggplot as ggplot\n",
    "color = sns.color_palette()\n",
    "import folium\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "path = \"C:/Users/nthor/Documents/GitHub/Predicting_Property_Value_Zillow/data/\"\n",
    "loc_properties = \"properties_2016.csv\"\n",
    "loc_train = \"train_2016_v2.csv\"\n",
    "trainingsize = .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import files\n",
    "transactions = pd.read_csv(path + loc_train, delimiter=',', parse_dates=[\"transactiondate\"])\n",
    "properties = pd.read_csv(path + loc_properties, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3afa0",
   "metadata": {},
   "source": [
    "Let's take a look at our two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine properties set\n",
    "properties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eafeccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine list of sold homes\n",
    "transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912ca2c",
   "metadata": {},
   "source": [
    "Finally, let's join our datasets together before digging-in to our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca4368",
   "metadata": {},
   "source": [
    "Densely populated at zero, with slightly more spread at either end of the x-axis; studios and large mansions may be harder to accurately predict sales price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d73af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.jointplot(x=merged.finishedsquarefeet12.values, y=merged.bedroomcnt.values, size=10, color=color[4])\n",
    "plt.ylabel('Bedroom Count', fontsize=12)\n",
    "plt.xlabel('Finished Square Feet 12', fontsize=12)\n",
    "plt.title(\"Finished square feet 12 Vs Bedroom Count\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea8ba7",
   "metadata": {},
   "source": [
    "Many of our exogeneous variables are likely to be correlated (as illustrated above), which we'll have to keep in mind as we select our features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ea95b",
   "metadata": {},
   "source": [
    "### Additional steps to consider\n",
    "1. Some inputs should be treated as categorical variables instead of integers\n",
    "2. Identify potential distributions for parametric modeling (Gaussian, etc.)\n",
    "3. Decompose dates / times into the appropriate format(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc41068",
   "metadata": {},
   "source": [
    "## Step 4: Clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b06a8",
   "metadata": {},
   "source": [
    "1. Drop useless features / attributes.\n",
    "2. Fill-in NA's.\n",
    "3. Drop outliers that you can't explain.\n",
    "4. Create binary flags.\n",
    "5. Convert datatypes where necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1001c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and y while filling nulls with 0 and dropping a few categorical variables that we may want to add in later\n",
    "X = merged.drop(['parcelid', 'logerror', 'transactiondate', 'transaction_month', 'accurate_price', 'propertycountylandusecode', 'propertyzoningdesc'], axis=1).fillna(0) # dropped for simplicity; generally you wouldn't want to drop variables without a strong rationale\n",
    "y = merged['logerror']\n",
    "\n",
    "# convert taxdelinquencyflag 'Y's into 1's\n",
    "X.iloc[X[X['taxdelinquencyflag'] == 'Y'].index, X.columns.get_loc('taxdelinquencyflag')] = 1\n",
    "\n",
    "# convert X to float\n",
    "X = X.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293314d",
   "metadata": {},
   "source": [
    "## Step 5: Engineer features for modeling\n",
    "* Discretize continuous features.\n",
    "* Decompose features (e.g., categorical, date/time, etc.).\n",
    "* Add promising transformations of features (e.g., log(x), sqrt(x), x^2, etc.).\n",
    "* Aggregate features into promising new features.\n",
    "* Brainstorm potentially predictive drivers and see what other data you can scrape / procure to improve your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53209707",
   "metadata": {},
   "source": [
    "For the purposes of this demo, we'll create a few simple features using abstractions of data we already have. In the real-world, you'll also want to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636da2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#life of property\n",
    "X['N-life'] = 2018 - X['yearbuilt']\n",
    "\n",
    "#Does property have a garage, pool or hot tub and AC?\n",
    "X['N-GarPoolAC'] = ((X['garagecarcnt']>0) & (X['pooltypeid10']>0) & (X['airconditioningtypeid']!=5))*1 \n",
    "\n",
    "X[\"N-location\"] = X[\"latitude\"] + X[\"longitude\"]\n",
    "X[\"N-location-2\"] = X[\"latitude\"]*X[\"longitude\"]\n",
    "X[\"N-location-2round\"] = X[\"N-location-2\"].round(-4)\n",
    "\n",
    "X[\"N-latitude-round\"] = X[\"latitude\"].round(-4)\n",
    "X[\"N-longitude-round\"] = X[\"longitude\"].round(-4)\n",
    "\n",
    "#Number of properties in the zip\n",
    "zip_count = X['regionidzip'].value_counts().to_dict()\n",
    "X['N-zip_count'] = X['regionidzip'].map(zip_count)\n",
    "\n",
    "#Number of properties in the city\n",
    "city_count = X['regionidcity'].value_counts().to_dict()\n",
    "X['N-city_count'] = X['regionidcity'].map(city_count)\n",
    "\n",
    "#Number of properties in the city\n",
    "region_count = X['regionidcounty'].value_counts().to_dict()\n",
    "X['N-county_count'] = X['regionidcounty'].map(city_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa27e60",
   "metadata": {},
   "source": [
    "TODO: Create new features after exploring initial Random Forest results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0b05d1",
   "metadata": {},
   "source": [
    "## Step 6: Pre-process data for modeling\n",
    "* Split our dataset into test and training sets so we can test how well our model generalizes the future\n",
    "* Standardize / normalize our features to improve processing time and dampen the effect of outliers\n",
    "    * Standardization <- (x - mean)/st.dev\n",
    "        * Rescales features so that they'll have the properties of the standard normal distribution\n",
    "        * Data's mean becomes 0 and st.dev. Becomes 1\n",
    "        * Better performance\n",
    "        * Mostly used for Gaussian distributions\n",
    "    * Normalization <- (X - Xmin)/(Xmax - Xmin)\n",
    "        * Also called Min-Max Scaling\n",
    "        * Causes you to lose some information in the data, especially concerning outliers\n",
    "        * Forces values to fit between the range of 0 and 1\n",
    "        * Good when you do not know the distribution of your data or you know it's not Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd1011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "merged = pd.merge(properties, transactions, on=['parcelid','parcelid'])\n",
    "# Fix latitude and longitude coordinates\n",
    "merged.latitude = merged.latitude / 1e6\n",
    "merged.longitude = merged.longitude / 1e6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78bce3",
   "metadata": {},
   "source": [
    "### Examine Biases\n",
    "\n",
    "Before moving forward, we should address the following questions:\n",
    "    1. Did we introduce bias into the dataset during sampling?\n",
    "    2. Do we have enough observations to establish causal inference?\n",
    "    3. Do we have time-series data extending back far enough to detect regression to the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d8f0f",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data\n",
    "3. Study each attribute and its characteristics:\n",
    "    * Name\n",
    "    * Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "    * Percent of missing values\n",
    "    * Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\n",
    "    * Possibly useful for the task?\n",
    "    * Type of distribution (Gaussian, uniform, logarithmic, etc.)\n",
    "4. For supervised learning tasks, identify the target attribute(s).\n",
    "5. Visualize the data.\n",
    "6. Study the correlations between attributes.\n",
    "7. Study how you would solve the problem manually.\n",
    "8. Identify the promising transformations you may want to apply.\n",
    "9. Identify extra data that would be useful (go back to “Get the Data”).\n",
    "10. Document what you have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b54e60d",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c4301",
   "metadata": {},
   "source": [
    "We notice a few missing values in our .head() calls; let's examine the percent of values in each column that are null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb2415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Extract features\n",
    "pca = PCA(n_components=5)\n",
    "fit = pca.fit(X_s)\n",
    "# Summarize components\n",
    "print(\"Explained Variance: \", fit.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069644a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "pca_viz = PCA().fit(X_s);\n",
    "\n",
    "with plt.style.context('seaborn-whitegrid'):\n",
    "    plt.semilogy(pca_viz.explained_variance_ratio_, '--o');\n",
    "    plt.semilogy(pca_viz.explained_variance_ratio_.cumsum(), '--o');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e0630",
   "metadata": {},
   "source": [
    "We see a sharp drop-off in explained variance around the 45th component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover weights of variables\n",
    "ident = np.identity(X_train.shape[1])\n",
    "coef = pd.DataFrame(pca.transform(ident), columns = ['PC-1', 'PC-2', 'PC-3', 'PC-4', 'PC-5'], index = X.columns)\n",
    "\n",
    "# Print results (first five rows)\n",
    "coef.sort_values(by = 'PC-1', ascending = True, kind = 'mergesort').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1ccc40",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "* L1: technique for feature selection by encouraging sparsity (sum of square of the weights)\n",
    "* L2: penalize large individual weights (sum of weights)\n",
    "![image.png](http://www.chioka.in/wp-content/uploads/2013/12/L1-vs-L2-properties-regularization.png)\n",
    "\n",
    "More detailed explanation [here](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/). Our take-away: advanced, tree-based algorithms use L1 regularization to select features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5b7273",
   "metadata": {},
   "source": [
    "### Tree-Based Feature Importance (LightGBM)\n",
    "Quick and dirty way to identify features before we build a cross-validated model. Here, variable importance is based on the number of times a variable is selected for splitting, weighted by the squared improvement to the model as a result of each split, and averaged over all trees. [Elith et al. 2008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7277ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset\n",
    "d_train = lgb.Dataset(X, label=y.ravel())\n",
    "\n",
    "# Set parameters\n",
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021 # shrinkage_rate\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l1'          # or 'mae'\n",
    "params['sub_feature'] = 0.5      # feature_fraction -- OK, back to .5, but maybe later increase this\n",
    "params['bagging_fraction'] = 0.85 # sub_row\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512        # num_leaf\n",
    "params['min_data'] = 500         # min_data_in_leaf\n",
    "params['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\n",
    "params['verbose'] = 5\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3\n",
    "\n",
    "# Train model\n",
    "# Train model\n",
    "clf = lgb.train(params, \n",
    "               d_train,\n",
    "               500)\n",
    "\n",
    "# Plot importance\n",
    "lgb.plot_importance(clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4dd72e",
   "metadata": {},
   "source": [
    "## Step 8: Build and test multiple models\n",
    "1. If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or Random Forests).\n",
    "2. Train many quick and dirty models from different categories (e.g., linear, naive Bayes, SVM, Random Forests, neural net, etc.) using standard parameters.\n",
    "3. Measure and compare their performance.\n",
    "    * For each model, use N-fold cross-validation and compute the mean and standard deviation of the performance measure on the N folds.\n",
    "4. Analyze the most significant variables for each algorithm.\n",
    "5. Analyze the types of errors the models make.\n",
    "    * What data would a human have used to avoid these errors?\n",
    "6. Have a quick round of feature selection and engineering.\n",
    "7. Have one or two more quick iterations of the five previous steps.\n",
    "8. Short-list the top three to five most promising models, preferring models that make different types of errors.\n",
    "9. Cross-validate each model (a good tutorial for cross-validating neural nets can be found [here](https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/))\n",
    "10. Tune our parameters to improve our CV scores. Note that you should not tune your parameters using your test set; doing so will leak information to your model that will reduce it's ability to deliver accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82190f4b",
   "metadata": {},
   "source": [
    "### Introduction to Decision Trees, Random Forests, and Gradient Boosted Machines (GBMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfa327",
   "metadata": {},
   "source": [
    "Key concepts:\n",
    "    * Decision trees\n",
    "    * Bagging (ensembling multiple trees together to reduce variance)\n",
    "    * Random Forests (randomly selecting features out of your feature set)\n",
    "    * Boosting (using weak learners to learn from misclassified training samples and reduce bias)\n",
    "    \n",
    "Helpful visuals [here](http://xgboost.readthedocs.io/en/latest/model.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f172d0c5",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c6f89",
   "metadata": {},
   "source": [
    "Recently released into the open source community by Microsoft, LightGBM is a fast, distributed, high performance gradient boosting (GBDT, GBRT, GBM or MART) framework. Here, we cross-validate our previously-trained model and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619cc106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine outliers\n",
    "merged.logerror.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d829d3c",
   "metadata": {},
   "source": [
    "We have a few major outliers on either end of the spectrum. In general, the Zestimate seems to overshoot the actual sales price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340bdc5",
   "metadata": {},
   "source": [
    "### Time Variables\n",
    "Next, we explore how our transactions vary across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9cff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create month variable and accurate prediction indicator\n",
    "merged['transaction_month'] = merged['transactiondate'].dt.month\n",
    "merged['accurate_price'] = '0'\n",
    "merged['accurate_price'].loc[(merged.logerror.values == 0)] == '1'\n",
    "\n",
    "\n",
    "\n",
    "# Plot results\n",
    "cnt_srs = merged.transaction_month.value_counts()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xlabel('Month of transaction', fontsize=12)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048cf2c",
   "metadata": {},
   "source": [
    "Unsurprisingly, most real estate activity occurs during the spring and summer (when it's most convenient for the average American to move / relocate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fde5e4",
   "metadata": {},
   "source": [
    "### Geographic Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31be1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "sns.jointplot(x=merged.latitude.values, y=merged.longitude.values, size=10)\n",
    "plt.ylabel('Longitude', fontsize=12)\n",
    "plt.xlabel('Latitude', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543f91f",
   "metadata": {},
   "source": [
    "Sales made across three regions in California. Any pattern by region?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers > 8 std. dev. away and create array for plotting\n",
    "filtered_pred = merged[abs(merged.logerror.values) > 8*np.std(merged.logerror.values)]\n",
    "lat_long = np.transpose(np.array([filtered_pred.latitude, filtered_pred.longitude]))\n",
    "lat = np.array([filtered_pred.latitude])\n",
    "long = np.array([filtered_pred.longitude])\n",
    "\n",
    "from ggplot import * \n",
    "ggplot(aes(x='latitude', y='longitude', color='logerror'), data= filtered_pred) + \\\n",
    "    geom_point() + \\\n",
    "    scale_color_gradient(low = 'aqua', high = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad55993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take average for folium map\n",
    "avg_lat = sum(merged.latitude.values)/len(merged.latitude.values)\n",
    "avg_long = sum(merged.longitude.values)/len(merged.longitude.values)\n",
    "\n",
    "# Plot folium map\n",
    "map_1 = folium.Map(location=[avg_lat, avg_long], zoom_start = 8)\n",
    "\n",
    "for i in range(0, len(filtered_pred)):\n",
    "    folium.CircleMarker([filtered_pred.latitude.iloc[i], filtered_pred.longitude.iloc[i]],\n",
    "                        radius=5,\n",
    "                        color='#3186cc',\n",
    "                        fill_color='#3186cc',).add_to(map_1)\n",
    "map_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b68e06d",
   "metadata": {},
   "source": [
    "There does seem to be some grouping in our errors, paticularly around Huntington Park, Compton, and Mission Viejo. We may be able to improve our predictions by using clustering / dimensionality reduction techniques to generate new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af520e",
   "metadata": {},
   "source": [
    "### Geospatial Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6e7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster by coordinates\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import DBSCAN\n",
    "from geopy.distance import vincenty\n",
    "\n",
    "dbscan = DBSCAN(eps=5/6371., min_samples=5, algorithm='ball_tree', metric='haversine') # 50 meters\n",
    "dbscan.fit(np.radians(lat_long))\n",
    "\n",
    "cluster_labels = dbscan.labels_\n",
    "num_clusters = len(set(cluster_labels))\n",
    "clusters = pd.Series([lat_long[cluster_labels == n] for n in range(num_clusters)])\n",
    "print('Number of clusters: {}'.format(num_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba21a9",
   "metadata": {},
   "source": [
    "When we filter our dataset to only include houses where *logerror* is outside two standard deviations, we seem to have 7 logical clusters using DBSCAN. We could use these clusters to classify new observations and create new features for our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052df885",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf2a30",
   "metadata": {},
   "source": [
    "We use correlation analysis to explore the relationships between the various features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad47445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define heat map\n",
    "corrmat = merged.corr(method='spearman')\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "sns.heatmap(corrmat, vmax=1., square=True, annot = False)\n",
    "plt.title(\"Important variables correlation map\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11aa34",
   "metadata": {},
   "source": [
    "Which x-variables are highly correlated with our predicted values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4831c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat.logerror[corrmat.logerror.values > .025]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d8c55",
   "metadata": {},
   "source": [
    "In plain English, the following variables appear to be highly correlated with mistaken Zestimates:\n",
    "* House / basement size\n",
    "* Number of bathrooms\n",
    "* Number of bedrooms\n",
    "* Number of fireplaces\n",
    "* Number of garage spaces\n",
    "* Number of stories\n",
    "* Newer apartments\n",
    "* Taxes paid\n",
    "\n",
    "In short, Zillow's estimates seem to inappropriately price large / lavish / new homes. Basement size appears to be particularly correlated to logerror; perhaps the algorithm doesn't accurately value large basements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x=\"basementsqft\", y=\"logerror\", data = merged)\n",
    "plt.ylabel('Log Error', fontsize=12)\n",
    "plt.xlabel('Basement Sq. Ft.', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Log Error vs. Basement Size\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad365841",
   "metadata": {},
   "source": [
    "We see a slight positive trend, with a greater degree of logerror variance near the end of the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672be784",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['bedroomcnt'].loc[merged['bedroomcnt']>7] = 7 # Set maximum number of bedrooms to seven\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.violinplot(x='bedroomcnt', y='logerror', data=merged)\n",
    "plt.xlabel('Bedroom count', fontsize=12)\n",
    "plt.ylabel('Log Error', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find percent of missing values in each column\n",
    "merged.isnull().sum()/len(merged)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf921c",
   "metadata": {},
   "source": [
    "Visualized differently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = merged.isnull().sum(axis=0).reset_index()\n",
    "missing_df.columns = ['column_name', 'missing_count']\n",
    "missing_df = missing_df.loc[missing_df['missing_count']>0]\n",
    "missing_df['missing_ratio'] = missing_df['missing_count'] / merged.shape[0]\n",
    "missing_df = missing_df.sort_values(by='missing_count')\n",
    "\n",
    "ind = np.arange(missing_df.shape[0])\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "rects = ax.barh(ind, missing_df.missing_ratio.values, color=color[2])\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Percentage of missing values\")\n",
    "ax.set_title(\"Percentage of missing values in each column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023bdb5",
   "metadata": {},
   "source": [
    "Unsurpisingly, many of our more obscure features (e.g., *poolcnt*) are blank for the vast majority of homes. Thankfully, our dependent variable, *logerror*, is never null. \n",
    "\n",
    "How should we handle these nulls? Generally, we do one of the following:\n",
    "* Use a model that can deal with nulls (random forest)\n",
    "* Drop the nulls\n",
    "* Predict the nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2550ba7",
   "metadata": {},
   "source": [
    "### Logerror"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645345a",
   "metadata": {},
   "source": [
    "Let's start by examining the value that we're responsible for predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.distplot(merged.logerror.values, bins=50, kde=False)\n",
    "plt.xlabel('logerror', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d326461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X = X.fillna(0)\n",
    "X_s = scaler.fit_transform(X)\n",
    "y_s = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split into test / training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_s, y_s, test_size=0.2, random_state=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0532e",
   "metadata": {},
   "source": [
    "## Step 7: Select key features\n",
    "\n",
    "**Curse of Dimensionality**: Many problems that do not exist in low-dimensional space arise in high-dimensional space. As we add additional features into our model, the number of observations necessary to cover the potential state landscape scales exponentially.\n",
    "\n",
    "Potential solutions include:\n",
    "* Variable selection via LASSO, ElasticNet, Recursive Feature Elimination, etc.\n",
    "* Dimensionality reduction via PCA, autoencoders, etc.\n",
    "* Importance scores from gradient-boosted tree algorithms\n",
    "* Using commen sense and correlation tables to eliminate nonsensical features\n",
    "* Using regularization to select features\n",
    "\n",
    "For linear models, we'd prefer to use Recursive Feature Selection, ElasticNet, and LASSO to identify the variables we care about (all available through glmnet in R or sk-learn in Python). When we use a boosted tree approach, there are more effective, built-in tools we can use to identify the features we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e65bd5",
   "metadata": {},
   "source": [
    "### Principle Component Analysis\n",
    "In our final output table below, the variables with the highest weights should be considered most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc52b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with 10-fold CV\n",
    "d_train2 = lgb.Dataset(X_train, label=y_train.ravel())\n",
    "\n",
    "clf_cv = lgb.cv(params, \n",
    "                d_train, \n",
    "                500, \n",
    "                nfold = 10, \n",
    "                early_stopping_rounds = 100,\n",
    "                stratified = True)\n",
    "\n",
    "# Generate predictions\n",
    "lgbm_pred = clf.predict(X_test, num_iteration=clf.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e9cbe4",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation of XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c53131f",
   "metadata": {},
   "source": [
    "XGBoost was originally created to detect the Higgs boson particle. Let's see how it performs for our dataset."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
