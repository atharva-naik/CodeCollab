{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3940c1",
   "metadata": {},
   "source": [
    "## Part 3 - Word Vectors\n",
    "\n",
    "So far we've seen simple feature selection methods, a statistical feature selection approach, dimensionality reduction techniques such as PCA and SVD, but in the last few years, with the rise in popularity of Neural Networks, a new technique has become the state of the art for representing words in NLP tasks.\n",
    "\n",
    "This technique is commonly referred to as word vectors or word embeddings, and its inner workings are really simple. It consists on defining a vocabulary and a vector for each word in it with a maximum number of dimensions. Then all the vectors' weights are found through the use of neural networks. This not only allows us to reduce significantly the number of features inputed to our models, but it also allows meaningful and easy representations across the data, that are transferrable among tasks. \n",
    "\n",
    "Pretty cool, huh?\n",
    "\n",
    "<img src=\"../media/what-year-is-this.jpg\" width=\"400\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5684bb",
   "metadata": {},
   "source": [
    "## 1 - Word Vectors Explained\n",
    "\n",
    "First of all, by now you could be thinking: \"But wait Doc, didn't I get a bunch of vectors before also?\". Why yes, yes you did Marty. You could consider the matrix with document-term counts to contain in their columns a possible word vector representation. You could even construct a simpler matrix. \n",
    "\n",
    "If you assume your vocabulary with size V, and each word having an index in it, a natural representation would be what it is called a 1-hot encoding, where each word can be represented by a vector of size V - the vocabulary size - with the single component corresponding to it's word set to 1, and the remaining zeroed out.\n",
    "\n",
    "<img src=\"../media/one-hot-vec.png\" width=\"300\">\n",
    "\n",
    "\n",
    "We are going in the right direction! But keep in mind that this representation fits in a very large space and we suddenly fall into the pitfalls of high-dimensionality. You could think of applying PCA or SVD to these 1-hot vectors but as for most tasks nowadays, neural networks have proven to be better at the task. To simply put it, here is a more elegant way. \n",
    "\n",
    "<img src=\"../media/but-how-doc.jpg\" width=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1335764",
   "metadata": {},
   "source": [
    "### Training Word Vectors\n",
    "\n",
    "So you know the data - a bunch of words. You know the goal - a vector with an arbitrary number of K features. And you know the means - neural networks. So how does it all work? \"You shall know a word by the company it keeps\". These are the words of John Rupert Firth (at least according to wikipedia), and they are the basis of the following method - Word2vec. \n",
    "\n",
    "**Word2vec** is a generic name for the technique of using neural models to produce word embeddings. There are two main approaches - Continuous Bag Of Words (CBOW) and skipgram - that we will describe here.\n",
    "\n",
    "Initially, we prepare the dataset to consider for each sentence several windows of length n,  centered around each word. Each of these will be a training example that we will plugin on our neural network, in one of two ways:\n",
    "\n",
    "1 - **CBOW**: the input words are the context words, and we predict the center word, this is, our model output. \n",
    "\n",
    "2 - **Skip-gram**: simmetrically to the previous method, the input is the center word, and the predictions are the output words\n",
    "\n",
    "The weights of the network are shared in both cases for the side that has more than one word, and there are a few more details on how setup these models, but the basic intuition can be seen on the following image:\n",
    "\n",
    "<img src=\"../media/word2vec.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf6b28",
   "metadata": {},
   "source": [
    "### Pretrained word vectors\n",
    "\n",
    "The best thing about these vectors, however, is that we can transfer them among tasks. What this means is that we don't need to go through the painful task of training them, and we can rely on pretrained vectors. Most of these pretrained vectors were trained on a huge amount of data in the same language, that would take time to gather, process and iterate over to train the network.\n",
    "\n",
    "One of such pretrained vectors offered are the ones in **spacy**. [Spacy](https://spacy.io) is a toolkit similar to NLTK, but it contains embedded deep learning models for NLP and it typically has better performance for industrial applications. The pretrained word vectors are easy to use out of the box by importing the spacy library. At this point, if you did not go through the README carefully, you should run this command to download the required models:\n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "Spacy has different versions with different sizes, and the one we are downloading is the medium one. You can try to switch between versions to see the impact it gets in the following experiments. Different sizes are related to different vocabulary sizes and feature size. Load the medium pretrained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bfb03",
   "metadata": {},
   "source": [
    "There are several available libraries of word vectors out there, such as [FastText](https://fasttext.cc) and [Glove](https://nlp.stanford.edu/projects/glove/). These all provide good quality embeddings for your NLP tasks. Their training methods are usually based on the Word2vec, but they normally have a few difference in details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45836fe",
   "metadata": {},
   "source": [
    "## 2 - Word Representations in Spacy\n",
    "\n",
    "Now let's dig into the vectors and see what we can get from them. We can start by seing the representation for a particular word, for example *house*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('house').vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d9635e",
   "metadata": {},
   "source": [
    "We can define a simple function just to make it easier and avoid rewriting the same thing over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6c2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6a7f71",
   "metadata": {},
   "source": [
    "## 3 - Cosine similarity\n",
    "\n",
    "We can check similarities between words using cosine similarity. The cosine similarity is a measure of distance between to vectors. It is defined by the following equation:\n",
    "\n",
    "$$\\text{cos-similarity} = \\frac{A \\cdot B}{\\| A \\| \\| B \\|}$$\n",
    "\n",
    "And it's computation is very intuitive in the 2D plane. \n",
    "\n",
    "<img src=\"../media/cosine.png\" width=\"400\">\n",
    "\n",
    "In this example, there are three animals that have two features that represent them - if the animal lives in the woods and how much it hunts. The vectors represent where each animal is in this feature space and so if the vectors are more close together, they are more similar. This can be measured by the cosine of the angle between them - if the angle between two vectors is low (similar vectors), the cosine of that angle is greater and thus the similarity between the words in this feature space is greater!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e0bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce2f8c2",
   "metadata": {},
   "source": [
    "Let's test it out. Using cosine similarity, closer words - like *house* and *home* - should have higher scores. On the other hand words with different meanings, even if they are close in terms of characters - like *house* and *mouse* - should produce a low score, if our word vectors really capture meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5532b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec('house'), vec('mouse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de94a0a",
   "metadata": {},
   "source": [
    "As expected, *house* is closer to *home* than it is to *mouse*. Makes sense!\n",
    "\n",
    "<img src=\"../media/future.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Once again, to simplify our next examples, let's create a function that gets us the closest words to the vector that we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10, dont_include_list=[]):\n",
    "    return sorted([(x, cosine(vec_to_check, vec(x))) for x in token_list if x not in dont_include_list],\n",
    "                  key=lambda x: x[1],\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23eea82",
   "metadata": {},
   "source": [
    "We are going to apply this function in further examples. To simplify a bit, let's limit the vocabulary to the one from our previous example. We can then find the closest words to the word *house*.  We start by reading the dataset and getting its vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca5e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/twitter_rep_dem_data_small.csv')\n",
    "\n",
    "handle_remotion = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_remotion)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(df.Tweet)\n",
    "\n",
    "tweet_vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89601b59",
   "metadata": {},
   "source": [
    "We can also obtain the 10 closest words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a2d002",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tweet_vocab.keys(),\n",
    "              vec('house'),\n",
    "              dont_include_list=['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbda918",
   "metadata": {},
   "source": [
    "## 4 - Word relations\n",
    "\n",
    "There are much more that we can do to show you that these vectors capture the meaning, or at least some semantic information, of our vocabulary. Hopefully, if you haven't believed it yet, this will help. For example, what do you think it will happen if we subtract man from king and add woman?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tweet_vocab.keys(), \n",
    "              vec('king') - vec('man') + vec('woman'),\n",
    "              dont_include_list=['king', 'man', 'woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aa27c2",
   "metadata": {},
   "source": [
    "<img src=\"../media/mind-blown-2.png\" width=\"300\">\n",
    "\n",
    "\n",
    "And what is the mean between morning and evening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0776ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tweet_vocab.keys(),\n",
    "              np.mean(np.array([vec('morning'), vec('evening')]), axis=0),\n",
    "              dont_include_list=['morning', 'evening'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6281f028",
   "metadata": {},
   "source": [
    "<img src=\"../media/mind-blown-3.png\" width=\"300\">\n",
    "\n",
    "\n",
    "If sky is to blue, grass is to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d666c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tweet_vocab.keys(), \n",
    "              vec('blue') - vec('sky') + vec('grass'),\n",
    "              dont_include_list=['blue', 'sky', 'grass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5724a32",
   "metadata": {},
   "source": [
    "<img src=\"../media/mind-blown-4.png\" width=\"300\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## 5 - Applying word vectors to sentences\n",
    "\n",
    "There are several ways you could think of to construct a sentence representation from these vectors, such as:\n",
    "\n",
    "* sum\n",
    "* average \n",
    "* concatenation\n",
    "\n",
    "The average is a good enough approach to start with, so let's implement a function to get the sentence vector representation from the average of its words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a844a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return np.mean(np.array([w.vector for w in sent]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec27bd6",
   "metadata": {},
   "source": [
    "We can then use the same logic to get the closest sentence according to the sentence representation we chose. Below you have the implementation of the previous function that used cosine similarity, but for sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(sentvec(x), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15732f3f",
   "metadata": {},
   "source": [
    "Let's try it out with a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19165587",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec('house'), vec('home'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in spacy_closest_sent(df.Tweet.values[:2000], \"i am against the trump administration .\"):\n",
    "    print(sent)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6bb2e5",
   "metadata": {},
   "source": [
    "It seems to have worked quite well, wouldn't you agree, Marty?\n",
    "\n",
    "If you are still not convinced about this, you can try to project all your vectors into a 2D space (by applying PCA, for example) and convince yourself that words are somewhat organized by meaning, and we can extract word relations from its distances. If you project your vectors, you should get something similar to this:\n",
    "\n",
    "<img src=\"../media/word-vectors-projection.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd506aa5",
   "metadata": {},
   "source": [
    "## 6 - NLP practical example\n",
    "\n",
    "All that is left is to try and use these new representations as the features to our models. We start by defining a function to build our vectors for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6260b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_vecs(docs):\n",
    "    num_examples = len(docs)\n",
    "    word_vector_shape = nlp.vocab.vectors.shape[-1]\n",
    "    vectors = np.zeros([num_examples, word_vector_shape])\n",
    "    for ii, doc in enumerate(docs):\n",
    "        vector = sentvec(doc)\n",
    "        vectors[ii] = vector\n",
    "    \n",
    "    # in case we get any NaN's or Inf, replace them with 0s\n",
    "    return np.nan_to_num(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6312546a",
   "metadata": {},
   "source": [
    "First let's get a baseline as we did before (it should match the one from the previous notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d403ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_remotion = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_remotion)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=seed)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train, train_data.Party)\n",
    "pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(pred, test_data.Party)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64865ab",
   "metadata": {},
   "source": [
    "Let's also get baselines for our previous methods - SVD and PCA. We'll use 300 as the number of components to keep so we can compare with the new technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e0b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=seed)\n",
    "svd.fit(X_train)\n",
    "X_train_svd = svd.transform(X_train)\n",
    "X_test_svd =  svd.transform(X_test)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train_svd, train_data.Party)\n",
    "pred = clf.predict(X_test_svd)\n",
    "print('Truncated SVD Accuracy: {}'.format(accuracy_score(pred, test_data.Party)))\n",
    "\n",
    "pca = PCA(n_components=300, random_state=seed)\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "pca.fit(X_train_dense)\n",
    "X_train_pca = pca.transform(X_train_dense)\n",
    "X_test_pca =  pca.transform(X_test_dense)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train_pca, train_data.Party)\n",
    "pred = clf.predict(X_test_pca)\n",
    "print('PCA Accuracy: {}'.format(accuracy_score(pred, test_data.Party)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b1025f",
   "metadata": {},
   "source": [
    "For 300 features, PCA and SVD have a pretty low accuracy. Now let's split the data and build the vectors - it might take a few minutes to get the vectors for all training and test data. Print the shape of the output vector so we get an idea of the number of features that our model is going to use now. You should see that our feature vector is now of 300 features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = build_sentence_vecs(train_data.Tweet.values)\n",
    "X_test = build_sentence_vecs(test_data.Tweet.values)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d081c",
   "metadata": {},
   "source": [
    "Let's run the same model and see how much accuracy we can get out of our 300 features:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
