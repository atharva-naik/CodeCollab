{
    "Loading Data::s": {
        "STEPS": ["Setup", "Import Necessary Libraries", "Access Data in the Dataset", "Iterate over Data", "Visualize the Data"],
        "Setup::s::has step(s)" : {
            "Setup for PyTorch::s::has instance": {
                "STEPS": ["Install torch", "Install torchaudio", "Install torchvision"],
                "PyTorch::M::uses": null
            }
        },
        "Import Necessary Libraries::s::has step(s)": {
            "Import Libraries for Pytorch::s::has instance": {
                "STEPS": ["Import torch", "Import torchaudio", "Import torchvision"],
                "PyTorch::M::uses": null
            }
        },
        "Access Data in the Dataset::s::has step(s)": {
            "STEPS": ["Download Dataset", "Split Train Test"], 
            "Access Data in the Dataset in PyTorch::s::has instance": {
                "STEPS": ["Download Dataset", "Split Train Test", "Create PyTorch Dataset Object"],
                "PyTorch::M::uses": null
            }
        },
        "Iterate over Data::s::has step(s)": {
            "Iterate over Data in PyTorch::s::has instance": {
                "STEPS": ["Create PyTorch DataLoader Object", "Iterate over DataLoader"],
                "PyTorch::M::uses": null
            }
        },
        "Visualize the Data::s::has step(s)": null
    },
    "Define Neural Network::s": {
        "STEPS": ["Setup", "Import Necessary Libraries", "Define and Initialize Neural Network", "Define Forward Pass", "Test Model by Passing Data"],
        "Setup::s::has step(s)": {
            "Setup for PyTorch::s::has instance": {
                "STEPS": ["Install torch"],
                "PyTorch::M::uses": null
            }
        },
        "Import Necessary Libraries::s::has step(s)": {
            "Import Libraries for Pytorch::s::has instance": {
                "STEPS": ["Import torch", "Import torch.nn", "Import torch.nn.functional"],
                "PyTorch::M::uses": null
            }
        },
        "Define and Initialize Neural Network::s::has step(s)": {
            "Define Neural Network in PyTorch::s::has instance": {
                "STEPS": ["Subclass nn.Module", "Define Layers in __init__ function"],
                "PyTorch::M::uses": null
            },
            "Define Convolutional Neural Network::s::has instance": null
        },
        "Define Forward Pass::s::has step(s)": {
            "Define Forward Function in PyTorch::s::has instance": null
        },
        "Test Model by Passing Data::s::has step(s)": {
            "STEPS": ["Create Random Input", "Call Forward Pass on Input", "Test Output"]
        }
    },
    "Saving Model State::s": {
        "STEPS": ["Setup", "Import Necessary Libraries", "Define and Initialize Neural Network", "Initialize Optimizer", "Access Model and Optimizer State"],
        "Setup::s::has step(s)": {
            "Setup for PyTorch::s::has instance": {
                "STEPS": ["Install torch"],
                "PyTorch::M::uses": null
            }
        },
        "Import Necessary Libraries::s::has step(s)": {
            "Import Libraries for Pytorch::s::has instance": {
                "STEPS": ["Import torch", "Import torch.nn", "Import torch.optim"],
                "PyTorch::M::uses": null
            }
        },
        "Access Model and Optimizer State::s::has step(s)": {
            "Access Model and Optimizer State in Pytorch::s::has instance": {
                "STEPS": ["Call state_dict Function", "Save state_dict with torch.save"],
                "State Dict::C::uses": {
                    "DESC": "In PyTorch, the learnable parameters (i.e. weights and biases) of a torch.nn.Module model are contained in the model's parameters (accessed with model.parameters()). A state_dict is simply a Python dictionary object that maps each layer to its parameter tensor."
                },
                "PyTorch::M::uses": null
            }
        }
    },
    "Developing Custom PyTorch DataLoaders::s": {
        "STEPS": ["Setup", "Import Necessary Libraries", "The Dataset", "Data Transformation", "The DataLoader"],
        "Setup::s::has step(s)": {
            "STEPS": ["Install torch"],
            "PyTorch::M::uses": null
        },
        "Import Necessary Libraries::s::has step(s)": {
            "STEPS": ["Import torch", "Import torch.utils.data"],
            "PyTorch::M::uses": null
        },
        "The Dataset::s::has step(s)": {
            "STEPS": ["Download Dataset", "Analyze Dataset Format", "Helper Function to Show Data Items", "Create Dataset Class", "Iterate over Data"],
            "Create Dataset Class::s::has step(s)": {
                "STEPS": ["Override __init__ function", "Override __len__ function", "Override __getitem__ function"],
                "Override __len__ function::s::has step(s)": {
                    "DESC": "Define function to return length of the dataset"
                },
                "Override __getitem__ function::s::has step(s)": {
                    "DESC": "Define function to take index as input and return data item corresponding to the index"
                }
            },
            "Iterate over Data::s::has step(s)": {
                "DESC": "Instantiate Dataset class and iterate over data samples."
            }
        },
        "Data Transformation::s::has step(s)": {
            "DESC": "custom",
            "STEPS": ["Create Callable Classes", "Compose Transforms", "Apply to Sample", "Iterate over Data"]
        }
    },
    "Robustness::C": {
        "Fast Gradient Sign Method FGSM::M::has instance": null,
        "Projected Gradient Descent PGD::M::has instance": null,
        "Attack Comparator::M::has instance": null,
        "Min Param Perturbation::M::has instance": null
    },
    "Influential Example Interpretability Algorithms::M": {
        "Data Influence::M::has instance": null,
        "Similarity Influence::M::has instance": null,
        "Trac in CP Base::M::has instance": null,
        "Trac in CP::M::has instance": null,
        "Trac in CP Fast::M::has instance": null,
        "Trac in CP Fast Random Projection::M::has instance": null
    },
    "Model Interpretability::C": {
        "Network Pruning and Feature Selection::C": {
            "Binary Concrete Stochastic Gates::M::uses": null,
            "Gaussian Stochastic Gates::M::uses": null
        },
        "Model Interpretability Algorithms::M::uses": {
            "Concept Based Interpretability Algorithms::M::subclass of": {
                "Concept Interpreter::C::has part(s)": {
                    "Testing with Concept Activation Vectors TCAV::M::instance of": null
                },
                "Concept::C::has part(s)": null,
                "Classifier::C::has part(s)": null 
            },
            "Attribution Algorithms::M::superclass of": {
                "Integrated Gradients::M::has instance": {
                    "DESC": "Integrated Gradients is an axiomatic model interpretability algorithm that assigns an importance score to each input feature by approximating the integral of gradients of the model's output with respect to the inputs along the path (straight line) from given baselines / references to inputs. Baselines can be provided as input arguments to attribute method. To approximate the integral we can choose to use either a variant of Riemann sum or Gauss-Legendre quadrature rule. More details regarding the integrated gradients method can be found in the original paper: https://arxiv.org/abs/1703.01365"
                },
                "Saliency::M::has instance": {
                    "DESC": "A baseline approach for computing input attribution. It returns the gradients with respect to inputs. More details about the approach can be found in the following paper: https://arxiv.org/abs/1312.6034"
                },
                "DeepLift::M::has instance": {
                    "DESC": "Implements DeepLIFT algorithm based on the following paper: Learning Important Features Through Propagating Activation Differences, Avanti Shrikumar, et. al. https://arxiv.org/abs/1704.02685 and the gradient formulation proposed in: Towards better understanding of gradient-based attribution methods for deep neural networks, Marco Ancona, et.al. https://openreview.net/pdf?id=Sy21R9JAW"
                },
                "DeepLiftShap::M::has instance": {
                    "DESC": "Extends DeepLift algorithm and approximates SHAP values using Deeplift. For each input sample it computes DeepLift attribution with respect to each baseline and averages resulting attributions. More details about the algorithm can be found here: https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf"
                },
                "GradientShap::M::has instance": {
                    "DESC": "Implements gradient SHAP based on the implementation from SHAP's primary author. For reference, please view the original implementation and the paper: A Unified Approach to Interpreting Model Predictions. GradientShap approximates SHAP values by computing the expectations of gradients by randomly sampling from the distribution of baselines/references. It adds white noise to each input sample n_samples times, selects a random baseline from baselines' distribution and a random point along the path between the baseline and the input, and computes the gradient of outputs with respect to those selected random points. The final SHAP values represent the expected values of gradients * (inputs - baselines). GradientShap makes an assumption that the input features are independent and that the explanation model is linear, meaning that the explanations are modeled through the additive composition of feature effects. Under those assumptions, SHAP value can be approximated as the expectation of gradients that are computed for randomly generated n_samples input samples after adding gaussian noise n_samples times to each input for different baselines/references. In some sense it can be viewed as an approximation of integrated gradients by computing the expectations of gradients for different baselines."
                },
                "Input X Gradient::M::has instance": {
                    "DESC": "A baseline approach for computing the attribution. It multiplies input with the gradient with respect to input. https://arxiv.org/abs/1605.01713"
                },
                "Guided Backprop::M::has instance": {
                    "DESC": "Computes attribution using guided backpropagation. Guided backpropagation computes the gradient of the target output with respect to the input, but gradients of ReLU functions are overridden so that only non-negative gradients are backpropagated. More details regarding the guided backpropagation algorithm can be found in the original paper here: https://arxiv.org/abs/1412.6806"
                },
                "Guided GradCAM::M::has instance": {
                    "DESC": "Computes element-wise product of guided backpropagation attributions with upsampled (non-negative) GradCAM attributions. GradCAM attributions are computed with respect to the layer provided in the constructor, and attributions are upsampled to match the input size. GradCAM is designed for convolutional neural networks, and is usually applied to the last convolutional layer. Note that if multiple input tensors are provided, attributions for each input tensor are computed by upsampling the GradCAM attributions to match that input's dimensions. If interpolation is not possible for the input tensor dimensions and interpolation mode, then an empty tensor is returned in the attributions for the corresponding position of that input tensor. This can occur if the input tensor does not have the same number of dimensions as the chosen layer's output or is not either 3D, 4D or 5D. Note that attributions are only meaningful for input tensors which are spatially alligned with the chosen layer, e.g. an input image tensor for a convolutional layer. More details regarding GuidedGradCAM can be found in the original GradCAM paper here: https://arxiv.org/abs/1610.02391"
                },
                "Deconvolution::M::has instance": {
                    "DESC": "Computes attribution using deconvolution. Deconvolution computes the gradient of the target output with respect to the input, but gradients of ReLU functions are overridden so that the gradient of the ReLU input is simply computed taking ReLU of the output gradient, essentially only propagating non-negative gradients (without dependence on the sign of the ReLU input). More details regarding the deconvolution algorithm can be found in these papers: https://arxiv.org/abs/1311.2901 https://link.springer.com/chapter/10.1007/978-3-319-46466-4_8"
                },
                "Feature Ablation::M::has instance": {
                    "DESC": "A perturbation based approach to computing attribution, involving replacing each input feature with a given baseline / reference, and computing the difference in output. By default, each scalar value within each input tensor is taken as a feature and replaced independently. Passing a feature mask, allows grouping features to be ablated together. This can be used in cases such as images, where an entire segment or region can be ablated, measuring the importance of the segment (feature group). Each input scalar in the group will be given the same attribution value equal to the change in target as a result of ablating the entire feature group."
                },
                "Occlusion::M::has instance": {
                    "DESC": "A perturbation based approach to compute attribution, involving replacing each contiguous rectangular region with a given baseline / reference, and computing the difference in output. For features located in multiple regions (hyperrectangles), the corresponding output differences are averaged to compute the attribution for that feature. The first patch is applied with the corner aligned with all indices 0, and strides are applied until the entire dimension range is covered. Note that this may cause the final patch applied in a direction to be cut-off and thus smaller than the target occlusion shape. More details regarding the occlusion (or grey-box / sliding window) method can be found in the original paper and in the DeepExplain implementation. https://arxiv.org/abs/1311.2901 https://github.com/marcoancona/DeepExplain/blob/master/deepexplain/tensorflow/methods.py#L401"
                },
                "Feature Permutation::M::has instance": {
                    "DESC": "A perturbation based approach to compute attribution, which takes each input feature, permutes the feature values within a batch, and computes the difference between original and shuffled outputs for the given batch. This difference signifies the feature importance for the permuted feature.\n\nExample pseudocode for the algorithm is as follows:\n\nperm_feature_importance(batch):\n    importance = dict()\n    baseline_error = error_metric(model(batch), batch_labels)\n    for each feature:\n        permute this feature across the batch\n        error = error_metric(model(permuted_batch), batch_labels)\n        importance[feature] = baseline_error - error\n        \"un-permute\" the feature across the batch\n\n    return importance"
                },
                "Shapley Value Sampling::M::has instance": {
                    "DESC": "A perturbation based approach to compute attribution, based on the concept of Shapley Values from cooperative game theory. This method involves taking a random permutation of the input features and adding them one-by-one to the given baseline. The output difference after adding each feature corresponds to its attribution, and these difference are averaged when repeating this process n_samples times, each time choosing a new random permutation of the input features. By default, each scalar value within the input tensors are taken as a feature and added independently. Passing a feature mask, allows grouping features to be added together. This can be used in cases such as images, where an entire segment or region can be grouped together, measuring the importance of the segment (feature group). Each input scalar in the group will be given the same attribution value equal to the change in output as a result of adding back the entire feature group. More details regarding Shapley Value sampling can be found in these papers: https://www.sciencedirect.com/science/article/pii/S0305054808000804 https://pdfs.semanticscholar.org/7715/bb1070691455d1fcfc6346ff458dbca77b2c.pdf"
                },
                "Local Interpretable Model-Agnostic Explanations LIME::M::has instance": {
                    "DESC": "Lime is an interpretability method that trains an interpretable surrogate model by sampling points around a specified input example and using model evaluations at these points to train a simpler interpretable 'surrogate' model, such as a linear model. LimeBase provides a generic framework to train a surrogate interpretable model. This differs from most other attribution methods, since the method returns a representation of the interpretable model (e.g. coefficients of the linear model). For a similar interface to other perturbation-based attribution methods, please use the Lime child class, which defines specific transformations for the interpretable model. LimeBase allows sampling points in either the interpretable space or the original input space to train the surrogate model. The interpretable space is a feature vector used to train the surrogate interpretable model; this feature space is often of smaller dimensionality than the original feature space in order for the surrogate model to be more interpretable. If sampling in the interpretable space, a transformation function must be provided to define how a vector sampled in the interpretable space can be transformed into an example in the original input space. If sampling in the original input space, a transformation function must be provided to define how the input can be transformed into its interpretable vector representation. More details regarding LIME can be found in the original paper: https://arxiv.org/abs/1602.04938"
                },
                "KernelShap::M::has instance": {
                    "DESC": "Kernel SHAP is a method that uses the LIME framework to compute Shapley Values. Setting the loss function, weighting kernel and regularization terms appropriately in the LIME framework allows theoretically obtaining Shapley Values more efficiently than directly computing Shapley Values. More information regarding this method and proof of equivalence can be found in the original paper here: https://arxiv.org/abs/1705.07874"
                },
                "Layer-wise Relevance Propagation LRP::M::has instance": {
                    "DESC": "Layer-wise relevance propagation is based on a backward propagation mechanism applied sequentially to all layers of the model. Here, the model output score represents the initial relevance which is decomposed into values for each neuron of the underlying layers. The decomposition is defined by rules that are chosen for each layer, involving its weights and activations. Details on the model can be found in the original paper [https://doi.org/10.1371/journal.pone.0130140]. The implementation is inspired by the tutorial of the same group [https://doi.org/10.1016/j.dsp.2017.10.011] and the publication by Ancona et al. [https://openreview.net/forum?id=Sy21R9JAW]."
                }
            },
            "NoiseTunnel::M::has instance": {
                "DESC": "Adds gaussian noise to each input in the batch nt_samples times and applies the given attribution algorithm to each of the samples."
            },
            "Layer Attribution::M::superclass of": {
                "Layer Conductance::M::has instance": "Computes conductance with respect to the given layer. The returned output is in the shape of the layer's output, showing the total conductance of each hidden layer neuron. The details of the approach can be found here: https://arxiv.org/abs/1805.12233 https://arxiv.org/abs/1807.09946",
                "Layer Activation::M::has instance": "Computes activation of selected layer for given input.",
                "Internal Influence::M::has instance": "Computes internal influence by approximating the integral of gradients for a particular layer along the path from a baseline input to the given input. If no baseline is provided, the default baseline is the zero tensor. More details on this approach can be found here: https://arxiv.org/abs/1802.03788",
                "Layer Gradient X Activation::M::has instance": "Computes element-wise product of gradient and activation for selected layer on given inputs.",
                "GradCAM::M::has instance": "Computes GradCAM attribution for chosen layer. GradCAM is designed for convolutional neural networks, and is usually applied to the last convolutional layer. GradCAM computes the gradients of the target output with respect to the given layer, averages for each output channel (dimension 2 of output), and multiplies the average gradient for each channel by the layer activations. The results are summed over all channels. GradCAM attributions are generally upsampled and can be viewed as a mask to the input, since a convolutional layer output generally matches the input image spatially. More details regarding the GradCAM method can be found in the original paper here: https://arxiv.org/abs/1610.02391",
                "Layer DeepLift::M::has instance": "Implements DeepLIFT algorithm for the layer based on the following paper: Learning Important Features Through Propagating Activation Differences, Avanti Shrikumar, et. al. https://arxiv.org/abs/1704.02685 and the gradient formulation proposed in: Towards better understanding of gradient-based attribution methods for deep neural networks, Marco Ancona, et.al. https://openreview.net/pdf?id=Sy21R9JAW. DeepLIFT's (Rescale Rule) attribution quality is comparable with Integrated Gradients, it runs significantly faster than Integrated Gradients and is preferred for large datasets.",
                "Layer DeepLiftShap::M::has instance": "Extends LayerDeepLift and DeepLiftShap algorithms and approximates SHAP values for given input layer. For each input sample - baseline pair it computes DeepLift attributions with respect to inputs or outputs of given layer averages resulting attributions across baselines. Whether to compute the attributions with respect to the inputs or outputs of the layer is defined by the input flag attribute_to_layer_input. More details about the algorithm can be found here: https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf",
                "Layer GradientShap::M::has instance": "Implements gradient SHAP for layer based on the implementation from SHAP's primary author. For reference, please, view: https://github.com/slundberg/shap#deep-learning-example-with-gradientexplainer-tensorflowkeraspytorch-models, A Unified Approach to Interpreting Model Predictions https://papers.nips.cc/paper7062-a-unified-approach-to-interpreting-model-predictions. GradientShap approximates SHAP values by computing the expectations of gradients by randomly sampling from the distribution of baselines/references. It adds white noise to each input sample n_samples times, selects a random baseline from baselines' distribution and a random point along the path between the baseline and the input, and computes the gradient of outputs with respect to selected random points in chosen layer. The final SHAP values represent the expected values of gradients * (layer_attr_inputs - layer_attr_baselines). GradientShap makes an assumption that the input features are independent and that the explanation model is linear, meaning that the explanations are modeled through the additive composition of feature effects. Under those assumptions, SHAP value can be approximated as the expectation of gradients that are computed for randomly generated n_samples input samples after adding gaussian noise n_samples times to each input for different baselines/references. In some sense it can be viewed as an approximation of integrated gradients by computing the expectations of gradients for different baselines.",
                "Layer Integrated Gradients::M::has instance": "Layer Integrated Gradients is a variant of Integrated Gradients that assigns an importance score to layer inputs or outputs, depending on whether we attribute to the former or to the latter one. Integrated Gradients is an axiomatic model interpretability algorithm that attributes / assigns an importance score to each input feature by approximating the integral of gradients of the model's output with respect to the inputs along the path (straight line) from given baselines / references to inputs. Baselines can be provided as input arguments to attribute method. To approximate the integral we can choose to use either a variant of Riemann sum or Gauss-Legendre quadrature rule. More details regarding the integrated gradients method can be found in the original paper: https://arxiv.org/abs/1703.01365",
                "Layer Feature Ablation::M::has instance": "A perturbation based approach to computing layer attribution, involving replacing values in the input / output of a layer with a given baseline / reference, and computing the difference in output. By default, each neuron (scalar input / output value) within the layer is replaced independently. Passing a layer mask allows grouping neurons to be ablated together. Each neuron in the group will be given the same attribution value equal to the change in target as a result of ablating the entire neuron group.",
                "Layer LRP::M::has instance": "Layer-wise relevance propagation is based on a backward propagation mechanism applied sequentially to all layers of the model. Here, the model output score represents the initial relevance which is decomposed into values for each neuron of the underlying layers. The decomposition is defined by rules that are chosen for each layer, involving its weights and activations. Details on the model can be found in the original paper [https://doi.org/10.1371/journal.pone.0130140]. The implementation is inspired by the tutorial of the same group [https://doi.org/10.1016/j.dsp.2017.10.011] and the publication by Ancona et al. [https://openreview.net/forum?id=Sy21R9JAW]." 
            },
            "Neuron Attribution::M::superclass of": {
                "Neuron Gradient::M::has instance": null,
                "Neuron Integrated Gradients::M::has instance": null,
                "Neuron Conductance::M::has instance": null,
                "Neuron DeepLift::M::has instance": null,
                "Neuron DeepLiftShap::M::has instance": null,
                "Neuron GradientShap::M::has instance": null,
                "Neuron Guided Backprop::M::has instance": null,
                "Neuron Deconvolution::M::has instance": null,
                "Neuron Feature Ablation::M::has instance": null
            },
            "Infidelity::E": null,
            "Sensitivity::E": null
        },
        "Model Interpretability with Captum::C::has instance": {
            "STEPS": ["Setup", "Prepare Model", "Define Input and Baseline Tensors", "Select Algorithm", "Computing Attribution", "Visualizing the Results"],
            "Captum::M::uses": null,
            "PyTorch::M::uses": null,
            "Model Interpretability for PyTorch::C::is the same as": null,
            "Attribute Predictions of Image Classifier to Image Features::T::has use": null,
            "Visualize Feature Attribution Results::V::has use": null,
            "Setup::s::has step(s)": {
                "Setup for PyTorch::s::has instance": {
                    "STEPS": ["Install torch", "Install Captum"],
                    "Install Captum::s::has step(s)": {
                        "DESC": "via conda (recommended):\nconda install captum -c pytorch\n\nvia pip:\npip install captum"
                    },
                    "PyTorch::M::uses": null
                }
            }
        }
    },
    "Dynamic Quantization::C": null
}