{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddc584be",
   "metadata": {},
   "source": [
    "# Testing and Evaluating Models\n",
    "\n",
    "Model evaluation supervised in machine learning revolves around holding back part of the data and seeing how the model deals with it. A model that performs well on new data is a model that can be expected to perform well in production, when all data is new and unlabelled.\n",
    "\n",
    "Good model evaluation is not just a single test on a single model. A well-tested model is tested many times with different configurations: this is known as hyper-parameter tuning. And a model that is rigorously tested is one that is pitted against other models: linear regression vs. trees vs. SVMs.\n",
    "\n",
    "It's important to test ML models thoroughly. Machine learning models aren't complete black boxes, but they're certainly more opaque than classic models. Machine learning models don't come with statistical assumptions that let you calculate the standard errors of coefficients, or let you assume that you have the [best estimator possible](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem).\n",
    "\n",
    "Ultimately, testing machine learning models is fun. This guide covers\n",
    "\n",
    "* Test-train splits\n",
    "* k-fold cross-validation (helpful for smaller datasets)\n",
    "* Validation sets vs. test sets\n",
    "* Using meaningful test sets\n",
    "* Extra: standard metrics in scikit-learn\n",
    "\n",
    "The question we're always trying to answer here is *how well will this model perform in the wild*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# get scikit-learn's datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# get some model selection tools\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# load various models from scikit-learn's library\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# also get some metrics to try\n",
    "from sklearn.metrics import *\n",
    "\n",
    "# matplotlib for graphs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "# itertools\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1e8312",
   "metadata": {},
   "source": [
    "## Measuring training score\n",
    "\n",
    "I'll use the boston dataset to show you scikit-learn's regression metrics. We'll be using the dataset for the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30a4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices = datasets.load_boston()\n",
    "\n",
    "# Make things a bit cleaner\n",
    "X = house_prices.data\n",
    "y = house_prices.target\n",
    "\n",
    "print(\"Features: {0}\".format(\", \".join(house_prices.feature_names)))\n",
    "print(\"Feature data shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], X.shape)])))\n",
    "print(\"Target data shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], y.shape)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8654d653",
   "metadata": {},
   "source": [
    "As before, I've copy-pasted all of scikit-learn's [regression metrics](http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) so that we can see them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d391c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_metrics = {\n",
    "                        \"explained_variance_score\" : \"Explained variance regression score function\", \n",
    "                        \"mean_absolute_error\" : \"Mean absolute error regression loss\", \n",
    "                        \"mean_squared_error\" : \"Mean squared error regression loss\", \n",
    "                        \"mean_squared_log_error\" : \"Mean squared logarithmic error regression loss\", \n",
    "                        \"median_absolute_error\" : \"Median absolute error regression loss\", \n",
    "                        \"r2_score\" : \"R^2 (coefficient of determination) regression score function.\"\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a4f650",
   "metadata": {},
   "source": [
    "There are fewer metrics for regression models. R^2 is the most common since it's a single value and is more or less comparable between models. It returns values between 0 and 1, and it has an easy interpretability like the accuracy score for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad39a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "# fit instance\n",
    "fit = model.fit(X=X, y=y)\n",
    "\n",
    "for key, value in regression_metrics.items():\n",
    "    print(\"*** {0} ***\".format(key))\n",
    "    print(value)\n",
    "    print(getattr(sklearn.metrics, key)(y, model.predict(X=X)))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8bc0e",
   "metadata": {},
   "source": [
    "## Train-test Splits\n",
    "\n",
    "In the examples above we evaluated the model's performance by only considering its performance with the training set. Since the model already learns from this data, it's expected to familiar with it. This kind of testing does not give you a reliable idea of how your model will perform with new \"real-world\" data.\n",
    "\n",
    "By using a test set kept separate from the model's training, we can get a glimpse of how the model reacts to unexpected data. This idea is the basis for most model evaluation in machine learning.\n",
    "\n",
    "I'll start by showing you the technique of randomized test-train splits, a technique that works in most situations. In situations where the test set is small, take test scores with a grain of salt: the scores will be less representative.\n",
    "\n",
    "Let's use the Boston dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices = datasets.load_boston()\n",
    "\n",
    "# Make things a bit cleaner\n",
    "X = house_prices.data\n",
    "y = house_prices.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b5857",
   "metadata": {},
   "source": [
    "Scikit-learn's `train_test_split` function will split a dataset for you. Using the `shuffle` option is a good idea if the dataset is sorted in some way and you want a randomized test set (this is the case here).\n",
    "\n",
    "Set the `random_state` option if you want your code to shuffle the exact same way each time it's run.\n",
    "\n",
    "If your dataset has duplicates, they should be removed since otherwise some data will be in both the training set and the test set, making the split imperfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=1234)\n",
    "\n",
    "print(\"Training data X shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], X_train.shape)])))\n",
    "print(\"Training data y shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], y_train.shape)])))\n",
    "print(\"Test data X shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], X_test.shape)])))\n",
    "print(\"Test data y shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], y_test.shape)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccdb875",
   "metadata": {},
   "source": [
    "Thanks to the magic of scikit-learn, I can evaluate the train-test split on many different models quickly.\n",
    "\n",
    "This is an important part of machine learning model evaluation, and you'll see me apply this technique repeatedly: **see how your type of model compares against other types of models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb5eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {\"OLS\" : LinearRegression(), \n",
    "              \"KNN\" : KNeighborsRegressor(), \n",
    "              \"DecisionTrees\" : DecisionTreeRegressor(), \n",
    "              \"AdaBoost\" : AdaBoostRegressor(random_state=1234), \n",
    "              \"Random Forests\" : RandomForestRegressor(random_state=1234), \n",
    "              \"GradientBoost\" : GradientBoostingRegressor(random_state=1234)\n",
    "             }\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# You can use the zip() function to iterate over multiple series\n",
    "for name, regressor in regressors.items():\n",
    "    model = regressor\n",
    "    # fit instance\n",
    "    fit = model.fit(X=X_train, y=y_train)\n",
    "    # R^2\n",
    "    train_score = r2_score(y_true = y_train, y_pred = model.predict(X=X_train))\n",
    "    test_score = r2_score(y_true = y_test, y_pred = model.predict(X=X_test))\n",
    "    \n",
    "    # print results\n",
    "    print(f\"{name:<20} training R^2 {train_score:5.2f}, test R^2 {test_score:5.2f}\")\n",
    "    \n",
    "    ax.scatter(train_score, test_score, s=50, label=name)\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "ax.axis((0.5, 1.05, 0.5, 1.05))\n",
    "ax.grid(True)\n",
    "ax.add_line(lines.Line2D([0.5, 1], [0.5, 1], c=\"grey\", ls=\"--\"))\n",
    "plt.xlabel(\"Train score\")\n",
    "plt.ylabel(\"Test score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f5d09",
   "metadata": {},
   "source": [
    "A higher test score is better. We can see below that the ensemble methods ([AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor), [Random Forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), [GradientBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)) generally beat the other models. While ensemble models are usually pretty good, there may be some rare situation where OLS performs the best: comparing models is easy and it helps a lot.\n",
    "\n",
    "The relationship between training score and validation score are **important** for determining overfit and underfit, and you should always check this.\n",
    "\n",
    "* A training score much higher than the validation score means overfitting\n",
    "* A training score and a validation score similar in value indicates good generalization\n",
    "    * However a model with both low training and low validation scores likely has trouble modelling the data\n",
    "* A validation score higher than the training score is evidence of either underfitting or a bug in the model\n",
    "\n",
    "Note that if you're performing multi-class classification over a lot of classes, your accuracy is bound to be lower (it's harder to get the right 1 out of 1000).\n",
    "\n",
    "It can be worthwhile to visualize your results. Below are graphs showing the errors/residuals along the `AGE` variable. You may sometimes suspect something strange is happening with your models, and a library like `matplotlib` makes the oddities stand out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60571533",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {\"OLS\" : LinearRegression(), \n",
    "              \"KNN\" : KNeighborsRegressor(), \n",
    "              \"DecisionTrees\" : DecisionTreeRegressor(), \n",
    "              \"AdaBoost\" : AdaBoostRegressor(random_state=1234), \n",
    "              \"Random Forests\" : RandomForestRegressor(random_state=1234), \n",
    "              \"GradientBoost\" : GradientBoostingRegressor(random_state=1234)\n",
    "             }\n",
    "\n",
    "error_dict = {}\n",
    "score_dict = {}\n",
    "\n",
    "# You can use the zip() function to iterate over multiple series\n",
    "for name, regressor in regressors.items():\n",
    "    model = regressor\n",
    "    # fit instance\n",
    "    fit = model.fit(X=X_train, y=y_train)\n",
    "    # score\n",
    "    score_dict[name] = r2_score(y_true = y_test, y_pred = model.predict(X=X_test))\n",
    "    # errors\n",
    "    error_dict[name] = model.predict(X=X_test) - y_test\n",
    "\n",
    "print(\"*** Error along feature values ***\")\n",
    "print(\"---\")\n",
    "for i in [6]:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    i_name = house_prices[\"feature_names\"][i]\n",
    "    i_data = np.expand_dims(X_test[:, i], axis=1)\n",
    "    cbind = np.hstack([i_data] + [error_dict[key].reshape(-1, 1) for key in error_dict.keys()])\n",
    "    cbind = cbind[cbind[:,0].argsort()]\n",
    "    err_mean = np.mean(cbind[:,1:], axis=0)\n",
    "    err_std = np.std(cbind[:,1:], axis=0)\n",
    "    quintile_data = np.array_split(cbind, 5)\n",
    "    quintile_min = [min(i[:,0]) for i in quintile_data]\n",
    "    quintile_max = [max(i[:,0]) for i in quintile_data]\n",
    "    print(\"Quintile ranges for {0}: {1}\".format(\n",
    "        i_name, \"; \".join([\"Q{0} {1} to {2}\".format(a+1, b, c) for a, b, c in zip(range(5), quintile_min, quintile_max)]))\n",
    "         )\n",
    "    print(\"---\")\n",
    "    q_err_means = [np.mean(i[:,1:], axis=0) for i in quintile_data]\n",
    "    q_err_stds = [np.std(i[:,1:], axis=0) for i in quintile_data]\n",
    "    for k, key in zip(range(len(error_dict.keys())), error_dict.keys()):\n",
    "        print(key)\n",
    "        print(\"QU mean/std {:}\".format(\n",
    "            \" \".join([\"[{:5.2f}/{:5.2f}]\".format(m[k], s[k]) for m, s in zip(q_err_means, q_err_stds)]))\n",
    "        )\n",
    "        print(f\"        all [{err_mean[k]:5.2f}/{err_std[k]:5.2f}]\")\n",
    "        print()\n",
    "        plt.subplot(2, 3, k+1)\n",
    "        plt.errorbar(list(map(lambda s: \"Q\"+str(s), range(1, 5+1))), \n",
    "                     list(map(lambda m: m[k], q_err_means)), \n",
    "                     list(map(lambda m: m[k], q_err_stds)), \n",
    "                     label=key, \n",
    "                     fmt='--o', \n",
    "                     capsize=5)\n",
    "        plt.axhspan(err_mean[k]-err_std[k], err_mean[k]+err_std[k], alpha=0.1, color='red')\n",
    "        plt.xlabel(i_name)\n",
    "        plt.ylabel(\"Error\")\n",
    "        plt.axis((0.5, 4.5, -10, 10))\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8082a",
   "metadata": {},
   "source": [
    "## k-fold cross-validation\n",
    "\n",
    "Train-test splits are nice but they omit something important: the stability of model performance over different datasets. A high score may be nice, but it's worthless if it merely comes from a lucky draw. We don't want to let the models win the lottery, in other words.\n",
    "\n",
    "k-fold cross-validation works by peforming the train-test split `k` times. For each fold, the performance of the model is measured, and afterwards the mean and the variance of these measures are looked at.\n",
    "\n",
    "k-fold cross-validation helps with smaller datasets because you'll have a mesure of score variance. The higher the variance of a score, the more suspicious you ought to be of it.\n",
    "\n",
    "Below you can better see how the models compare. Notice also the different score standard deviations.\n",
    "\n",
    "1. The OLS model has fluctuating scores and seems to inderfit.\n",
    "2. KNN has the worst performance.\n",
    "3. DecisionTrees overfit.\n",
    "\n",
    "The ensemble methods all work well, but GradientBoost is the best overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = {\"OLS\" : LinearRegression(), \n",
    "              \"KNN\" : KNeighborsRegressor(), \n",
    "              \"DecisionTrees\" : DecisionTreeRegressor(), \n",
    "              \"AdaBoost\" : AdaBoostRegressor(random_state=1234), \n",
    "              \"Random Forests\" : RandomForestRegressor(random_state=1234), \n",
    "              \"GradientBoost\" : GradientBoostingRegressor(random_state=1234)\n",
    "             }\n",
    "\n",
    "kf = KFold(n_splits = 5, random_state=1234, shuffle=True)\n",
    "\n",
    "names, train_means, train_stds, test_means, test_stds = [], [], [], [], []\n",
    "\n",
    "for name, regressor in regressors.items():\n",
    "    train_r2s = []\n",
    "    test_r2s = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model = regressor\n",
    "        # fit instance\n",
    "        fit = model.fit(X=X_train, y=y_train)\n",
    "        # R^2\n",
    "        train_r2s.append(r2_score(y_true = y_train, y_pred = model.predict(X=X_train)))\n",
    "        test_r2s.append(r2_score(y_true = y_test, y_pred = model.predict(X=X_test)))\n",
    "    mean_train_r2s = np.mean(train_r2s, axis=0)\n",
    "    stdev_train_r2s = np.std(train_r2s, axis=0)\n",
    "    mean_test_r2s = np.mean(test_r2s, axis=0)\n",
    "    stdev_test_r2s = np.std(test_r2s, axis=0)\n",
    "    print(\"{:<20} training R^2s {:} (mean {:5.2f} stdev {:5.2f})\".format(name, \", \".join([\"{:5.2f}\".format(i) for i in train_r2s]), mean_train_r2s, stdev_train_r2s))\n",
    "    print(\"{:<20}     test R^2s {:} (mean {:5.2f} stdev {:5.2f})\".format(name, \", \".join([\"{:5.2f}\".format(i) for i in test_r2s]), mean_test_r2s, stdev_test_r2s))\n",
    "    print(\"{:<15} train^2 - test R^2 {:} (mean {:5.2f})\".format(name, \", \".join([\"{:5.2f}\".format(i-j) for i, j in zip(train_r2s, test_r2s)]), mean_train_r2s-mean_test_r2s))\n",
    "    print()\n",
    "    # Data for graph\n",
    "    names.append(name)\n",
    "    train_means.append(mean_train_r2s)\n",
    "    train_stds.append(stdev_train_r2s)\n",
    "    test_means.append(mean_test_r2s)\n",
    "    test_stds.append(stdev_test_r2s)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# pyplot wants index numbers\n",
    "ind = np.arange(len(names))\n",
    "\n",
    "trains_bars = ax.bar(ind, train_means, 0.4, yerr=train_stds, capsize=5)\n",
    "test_bars = ax.bar(ind+0.4, test_means, 0.4, yerr=test_stds, capsize=5)\n",
    "\n",
    "plt.ylabel('Test score')\n",
    "plt.title('Means and standard deviations')\n",
    "plt.xticks(ind+0.2, names)\n",
    "plt.legend([\"train score\", \"test score\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a2d9b4",
   "metadata": {},
   "source": [
    "## Training sets, validation sets, test sets\n",
    "\n",
    "I am going to give you more precise definitions of the different sets\n",
    "\n",
    "* Training sets are for training\n",
    "* Validation sets are for model tweaking, not training\n",
    "* Test sets are for comparing models, for neither training not tweaking\n",
    "\n",
    "We need to keep validation sets separate from test sets because of things called **hyper-parameters**. Hyper-parameters are model parameters (such as weights, intercepts, etc) that are not directly estimated by the model; however, these hyper-parameters can be optimized through trial and error. We call this trial an error hyper-parameter searching or hyper-parameter tuning.\n",
    "\n",
    "Since we're optimizing on the validation sets using brute force, we need to keep the test sets separate. This is why we use train-validation-test splits. You will see examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test split\n",
    "X_, X_test, y_, y_test = train_test_split(X, y, test_size=0.3, shuffle=True, random_state=1234)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_, y_, shuffle=True, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b4060",
   "metadata": {},
   "source": [
    "We'll perform a grid search of parameters here. A very useful function for this is `itertools.product`, which finds all combinations of items among lists. It's equivalent to a nested for-loop.\n",
    "\n",
    "With the `RandomForestRegressor` we'll try a ton of different parameters to see which work best.\n",
    "\n",
    "What you see below is that the models (`n`, `m`) that have the best training scores are not the same that have the best validation or test scores. Looking at these scores, you can better foresee how your models will perform in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4196e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for n, m, d in itertools.product(\n",
    "    [5, 25, 50, 75, 100], \n",
    "    [\"auto\", \"sqrt\", \"log2\"], \n",
    "    [2, 4, 6, 8, 10]\n",
    "    ):\n",
    "    model = RandomForestRegressor(n_estimators=n, max_features=m, max_depth=d, random_state=1234)\n",
    "    # fit instance\n",
    "    fit = model.fit(X=X_train, y=y_train)\n",
    "    # R^2\n",
    "    train_score = r2_score(y_true = y_train, y_pred = model.predict(X=X_train))\n",
    "    valid_score = r2_score(y_true = y_valid, y_pred = model.predict(X=X_valid))\n",
    "    test_score = r2_score(y_true = y_test, y_pred = model.predict(X=X_test))\n",
    "    \n",
    "    results.append({\"n estimators\":n, \"max features\":m, \"max depth\":d, \n",
    "                    \"train_score\":train_score, \"valid_score\":valid_score, \"test_score\":test_score})\n",
    "    \n",
    "# print results\n",
    "for score_type in [\"train_score\", \"valid_score\", \"test_score\"]:\n",
    "    print(f\"Top 5 according to {score_type}\")\n",
    "    results.sort(key=lambda x: x[score_type], reverse=True)\n",
    "    i=1\n",
    "    for row in results:\n",
    "        print(f'{i:5}.  n: {row[\"n estimators\"]:3.0f} / m: {row[\"max features\"]:4} / d: {row[\"max depth\"]:2} '\n",
    "              f'| training R^2 {row[\"train_score\"]:5.2f} '\n",
    "              f'| valid R^2 {row[\"valid_score\"]:5.2f} '\n",
    "              f'| test R^2 {row[\"test_score\"]:5.2f}')\n",
    "        i+=1\n",
    "        if i > 5: break\n",
    "\n",
    "# make a graph\n",
    "# this is a little confusing at first\n",
    "# pyplot wants a list of lists for boxplots\n",
    "# each list will be a tick on the x-axis\n",
    "# the elements within the lists will be rendered into boxes and whiskers by pyplot\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "for index_number, key in zip(range(3), [\"n estimators\", \"max features\", \"max depth\"]):\n",
    "    plt.subplot(1, 3, index_number+1)\n",
    "    # I use set() to get unique x_ticks (values of n estimators, max features, max depth)\n",
    "    # then I use .sort() to sort them properly (set() randomizes order)\n",
    "    x_ticks = list(set([i[key] for i in results]))\n",
    "    x_ticks.sort()\n",
    "    # I loop through each x_tick and retrieve only the test scores that belong to it\n",
    "    series = []\n",
    "    for i in x_ticks:\n",
    "        series.append([j[\"test_score\"] for j in results if j[key] == i])\n",
    "    plt.boxplot(series)\n",
    "    plt.xlabel(key)\n",
    "    # by default pyplot will display the x labels as integers\n",
    "    # I replace these with the actual names\n",
    "    plt.xticks(np.arange(len(x_ticks))+1, x_ticks)\n",
    "    plt.ylabel(\"Test score\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e55a011",
   "metadata": {},
   "source": [
    "### Specific test sets\n",
    "\n",
    "Randomly selecting your test set can be too easy for your model. Its training and validation sets can have observations that are too closely related to those in the test set, so the model can learn things too easily and give you a false sense of security.\n",
    "\n",
    "Here is a scenario with the Boston data to illustrate. Suppose we have good data about *most* house prices, but we are suspicious about the prices of homes along the river; perhaps these prices along the river are collected less frequently. How could we measure a model's ability to predict these riverside home prices *without ever seeing the data*? If we were in the business of modeling the missing data, how could we get an idea how well we're doing?\n",
    "\n",
    "We can split the riverside home prices into our test set and evaluate our model that way. In our Boston dataset, the `CHAS` is the `Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51101edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAS is variable 3\n",
    "river_indexes = np.flatnonzero(X[:,3] == 1)\n",
    "river_mask = np.ones(X.shape[0], np.bool)\n",
    "river_mask[river_indexes] = 0\n",
    "\n",
    "# Create train/test split\n",
    "X_, X_test, y_, y_test = X[river_mask,], X[river_indexes,], y[river_mask], y[river_indexes]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_, y_, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de91600",
   "metadata": {},
   "source": [
    "This is similar to the previous example, except this time I'm randomly trying 2000 different hyper-parameters. I also note the rank of the model to give an idea how its compares.\n",
    "\n",
    "What you see below gives you an idea of the \"hit\" to modeling accuracy you'll get if you try to predict riverside house by training on landlocked houses.\n",
    "\n",
    "Test scores aren't a magic metric. They're there to give you a general idea of what works and what doesn't. Using graphs like the ones below, you can start studying the effects of the different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(2000):\n",
    "    #l, n, d in itertools.product(\n",
    "    #np.random.uniform(low=0.01, high=1.0, size=20), \n",
    "    #np.random.randint(5, high=200, size=20), \n",
    "    #[1, 2, 3, 4, 5]):\n",
    "    lo = np.random.choice([\"ls\", \"lad\", \"huber\", \"quantile\"])\n",
    "    lr = np.random.ranf()\n",
    "    n = np.random.randint(1, 100+1)\n",
    "    d = np.random.choice(np.arange(20)+1)\n",
    "    model = GradientBoostingRegressor(loss=lo, learning_rate=lr, n_estimators=n, max_depth=d, random_state=1234)\n",
    "    # fit instance\n",
    "    fit = model.fit(X=X_train, y=y_train)\n",
    "    # R^2\n",
    "    train_score = r2_score(y_true = y_train, y_pred = model.predict(X=X_train))\n",
    "    valid_score = r2_score(y_true = y_valid, y_pred = model.predict(X=X_valid))\n",
    "    test_score = r2_score(y_true = y_test, y_pred = model.predict(X=X_test))\n",
    "    \n",
    "    results.append({\"loss\":lo, \"learning rate\":lr, \"n estimators\":n, \"max depth\":d, \n",
    "                    \"train_score\":train_score, \"valid_score\":valid_score, \"test_score\":test_score})\n",
    "    \n",
    "# rank results\n",
    "for score_type in [\"train_score\", \"valid_score\", \"test_score\"]:\n",
    "    results.sort(key=lambda x: x[score_type], reverse=True)\n",
    "    for i in range(len(results)):\n",
    "        results[i][score_type+\"_rank\"] = i+1\n",
    "    print(\"Top {:}: {:5.2f}\".format(score_type, results[0][score_type]))\n",
    "\n",
    "# print results\n",
    "for score_type in [\"train_score\", \"valid_score\", \"test_score\"]:\n",
    "    print(\"Top 10 results by {0}\".format(score_type))\n",
    "    results.sort(key=lambda x: x[score_type], reverse=True)\n",
    "    i=1\n",
    "    for row in results:\n",
    "        print(f'{i:3}. lo:{row[\"loss\"][:3]:4} lr:{row[\"learning rate\"]:4.2f} '\n",
    "              f'n:{row[\"n estimators\"]:3.0f} d:{row[\"max depth\"]:2} '\n",
    "              f'| train R^2 {row[\"train_score\"]:5.2f} #{row[\"train_score_rank\"]:<4} '\n",
    "              f'| val R^2 {row[\"valid_score\"]:5.2f} #{row[\"valid_score_rank\"]:<4} '\n",
    "              f'| test R^2 {row[\"test_score\"]:5.2f} #{row[\"test_score_rank\"]:<4}'\n",
    "             )\n",
    "        i+=1\n",
    "        if i > 10: break\n",
    "\n",
    "# make a graph\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "for index_number, key in zip(range(4), [\"learning rate\", \"n estimators\", \"max depth\"]):\n",
    "    plt.subplot(1, 3, index_number+1)\n",
    "    for loss in [\"ls\", \"lad\", \"huber\", \"quantile\"]:\n",
    "        plt.scatter([i[key] for i in results if i[\"loss\"] == loss], \n",
    "                    [i[\"test_score\"] for i in results if i[\"loss\"] == loss], s=25, alpha=0.25)\n",
    "    plt.xlabel(key)\n",
    "    plt.ylabel(\"Test score\")\n",
    "    plt.legend([\"ls\", \"lad\", \"huber\", \"quantile\"])\n",
    "    plt.grid(True)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced3925",
   "metadata": {},
   "source": [
    "## Appendix: standard classification metrics in scikit-learn\n",
    "\n",
    "If you're looking for precision/recall, confusion matrices, and ROC, scikit-learn has all of these and more.\n",
    "\n",
    "### Classification\n",
    "\n",
    "First, let's see the classification metrics. Since this is just for fun, I'll create the same kind of fake data as in chapter 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a binary classification problem\n",
    "X, y = datasets.make_moons(n_samples=1000, noise=0.3)\n",
    "\n",
    "print(\"Feature data shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], X.shape)])))\n",
    "print(\"Target data shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], y.shape)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8582b4",
   "metadata": {},
   "source": [
    "Herea re all of scikit-learns's [classfication metrics](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics). I've just copy-pasted there here into a dictionary. The reason I did this is that I'll be able to iterate over this dictionary to try all of the metrics at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b04ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_metrics = {\"precision_recall_curve\" : \"Compute precision-recall pairs for different probability thresholds\", \n",
    "                            \"roc_curve\" : \"Compute Receiver operating characteristic (ROC)\", \n",
    "                            \"cohen_kappa_score\" : \"Cohenâ€™s kappa: a statistic that measures inter-annotator agreement.\", \n",
    "                            \"confusion_matrix\" : \"Compute confusion matrix to evaluate the accuracy of a classification\", \n",
    "                            \"hinge_loss\" : \"Average hinge loss (non-regularized)\", \n",
    "                            \"matthews_corrcoef\" : \"Compute the Matthews correlation coefficient (MCC)\", \n",
    "                            \"accuracy_score\" : \"Accuracy classification score.\", \n",
    "                            \"classification_report\" : \"Build a text report showing the main classification metrics\", \n",
    "                            \"f1_score\" : \"Compute the F1 score, also known as balanced F-score or F-measure\", \n",
    "                            \"hamming_loss\" : \"Compute the average Hamming loss.\", \n",
    "                            \"jaccard_similarity_score\" : \"Jaccard similarity coefficient score\", \n",
    "                            \"log_loss\" : \"Log loss, aka logistic loss or cross-entropy loss.\", \n",
    "                            \"precision_recall_fscore_support\" : \"Compute precision, recall, F-measure and support for each class\", \n",
    "                            \"precision_score\" : \"Compute the precision\", \n",
    "                            \"recall_score\" : \"Compute the recall\", \n",
    "                            \"zero_one_loss\" : \"Zero-one classification loss.\", \n",
    "                            \"average_precision_score\" : \"Compute average precision (AP) from prediction scores\", \n",
    "                            \"roc_auc_score\" : \"Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\"\n",
    "                         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f18522",
   "metadata": {},
   "source": [
    "With a bit of awkward python code, I can call each of the metrics from the `sklearn.metrics` library and see their results.\n",
    "\n",
    "I think the most useful ones below are the [classification_report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) (precision+recall+f1), the [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) and the [accuracy score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score). The accuracy score is the most frequently used of all because it's a single value.\n",
    "\n",
    "If you are working on a classification task with more than two labels, it's a really good idea to look at the confusion matrix. It will tell you if your model favors certain classes over others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a38cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "# fit instance\n",
    "fit = model.fit(X=X, y=y)\n",
    "\n",
    "for key, value in classification_metrics.items():\n",
    "    print(\"*** {0} ***\".format(key))\n",
    "    print(value)\n",
    "    print(getattr(sklearn.metrics, key)(y, model.predict(X=X)))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afb591",
   "metadata": {},
   "source": [
    "### Confusion matrix multi-class classification\n",
    "\n",
    "It's worth seeing the usefulness of the confusion matrix with an example. The confusion matrix will inform you how well your model is doing on each class, so you can see if it's giving unequal attention to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the iris data\n",
    "X, y = datasets.load_iris().data, datasets.load_iris().target\n",
    "\n",
    "print(\"Feature data shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], X.shape)])))\n",
    "print(\"Target data shape: {0}\".format(\"; \".join([\" \".join(map(str, i)) for i in zip([\"rows\", \"columns\"], y.shape)])))\n",
    "\n",
    "model = LogisticRegression()\n",
    "# fit instance\n",
    "fit = model.fit(X=X, y=y)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y, model.predict(X=X)))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(y, model.predict(X=X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df91a6",
   "metadata": {},
   "source": [
    "To read the confusion matrix, you need to know how the columns and rows correspond to the labels."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
