{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b5681",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "For this tutorial, we will use a IMDb dataset with plots and associated genres. We define our task as classifying the plots as those of action or romance movies based only on the plot text. \n",
    "\n",
    "The data is in `data/imdb/budgetandactors.txt`, which contains the plot and genre information about movies from the IMDb database. We pre-process the data below to generate the following:\n",
    "* Featurize the plot text using bag-of-words representation\n",
    "* Split the dataset into train, val, and test\n",
    "\n",
    "**TODO: Featurize in loader and load in the words each feature represents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader import DataLoader\n",
    "dl = DataLoader()\n",
    "train_primitive_matrix, val_primitive_matrix, test_primitive_matrix, \\\n",
    "train_ground, val_ground, test_ground, mode, frameNums = dl.load_data(mode = 'auto', numFramesToLoad = 1000)\n",
    "# mode stores the method of loading data here\n",
    "print(train_primitive_matrix.shape)\n",
    "print(val_primitive_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9e6a4",
   "metadata": {},
   "source": [
    "# Reef Steps\n",
    "Reef generates heuristics in an iterative manner, with each iteration consisting of the following steps:\n",
    "1. Synthesize Heuristics\n",
    "2. Prune Heuristics\n",
    "3. Verify Heuristics\n",
    "\n",
    "In this tutorial, we go through the three stages of Reef individually and then repeat the process iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ef4ae",
   "metadata": {},
   "source": [
    "In the cell below, we run a single iteration by calling the `run_synthesizer` function. We pass in the primitive matrices for the `train` and `val` sets, along with ground truth labels for `val`. While we also pass in ground truth labels for `train`, this is solely for evaluation purposes. \n",
    "\n",
    "`max_cardinality` is the maximum number of primitives a heuristic takes as input, `keep` is how many heuristics the pruner should select (3 for the first iteration, 1 after that) and `model` is the type of heuristic to generate, in this case, `decision_tree`. \n",
    "\n",
    "_This cell does not output anything, only saves values in HeuristicGenerator._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from program_synthesis.heuristic_generator import HeuristicGenerator\n",
    "print(train_ground)\n",
    "print(val_ground)\n",
    "hg = HeuristicGenerator(train_primitive_matrix, val_primitive_matrix, val_ground, train_ground, b=0.5)\n",
    "hg.run_synthesizer(max_cardinality=1, idx=None, keep=3, model='dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b153576",
   "metadata": {},
   "source": [
    "## 1. Synthesize Heuristics\n",
    "We start by generating all possible heuristics based on the labeled, validation set that take in a single feature (i.e. word for this example) as input. \n",
    "\n",
    "For this example, we use decision trees with maximum depth 1 (`dt`) as our heuristic form. This translates to checking whether a certain word exists or does not exist in the text to assign a label. We first generate all possible heuristics that take a single feature in as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from program_synthesis.synthesizer import Synthesizer\n",
    "syn = Synthesizer(val_primitive_matrix, val_ground, b=0.5)\n",
    "\n",
    "heuristics, feature_inputs = syn.generate_heuristics('nn', 1)\n",
    "print(\"Total Heuristics Generated: \", np.shape(heuristics)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eb90b2",
   "metadata": {},
   "source": [
    "For each generated heuristic, we find an associated $\\beta$ value.  This corresponds to defining a region of **low confidence** labels, which the heuristic will abstain for, while labeling the rest of the datapoints as $1$ or $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3842b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_betas = syn.find_optimal_beta(heuristics[0], val_primitive_matrix, feature_inputs[0], val_ground)\n",
    "plt.hist(optimal_betas, range=(0,0.5));\n",
    "plt.xlabel('Beta Values');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc512ff",
   "metadata": {},
   "source": [
    "## 2. Prune Heuristics\n",
    "In the first iteration, we simply pick the 3 heuristics that perform the best on the labeled validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd758db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_idx = hg.prune_heuristics(heuristics, feature_inputs, keep=3)\n",
    "print('Features chosen heuristics are based on: ', top_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b602a0ea",
   "metadata": {},
   "source": [
    "In subsequent iterations (step 4), we weight the Jaccard score (overlap of how many datapoints in the train set receive labels and how many are labeled by existing heuristics) and F1 score equally. We demonstrate this with a toy vector of previously labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106f198",
   "metadata": {},
   "source": [
    "## 3. Verify Heuristics\n",
    "In this step, we use the labels the heuristics assign to the **unlabeled train set** to estimate heuristic accuracies and assign probabilistic training labels to the same set accordingly (see [snorkel.stanford.edu](http://snorkel.stanford.edu) for more details). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5816da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from program_synthesis.verifier import Verifier\n",
    "verifier = Verifier(hg.L_train, hg.L_val, val_ground, has_snorkel=False)\n",
    "\n",
    "verifier.train_gen_model()\n",
    "verifier.assign_marginals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351ad66",
   "metadata": {},
   "source": [
    "We visualize what these labels look like. Note that with a single iteration, none of the datapoints receive a probabilistic label greater than 0.5, but this is fixed after running the process iteratively (Step 4). __These labels are then used to train an end model, such as an LSTM, and not used as final predictions.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85865f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(verifier.train_marginals); plt.title('Training Set Probabilistic Labels');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bfe724",
   "metadata": {},
   "source": [
    "Since we do not have access to ground truth labels for the train set, we use the distribution of labels for the labeled validation set to decide what feedback to pass to the synthesizer. We pass datapoints with low confidence (labels near 0.5, i.e. equal probability of being +1 or -1) to the synthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e729bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(verifier.val_marginals); plt.title('Validation Set Probabilistic Labels');\n",
    "feedback_idx = verifier.find_vague_points(gamma=0.1,b=0.5)\n",
    "print('Percentage of Low Confidence Points: ', np.shape(feedback_idx)[0]/float(np.shape(val_ground)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7afb231",
   "metadata": {},
   "source": [
    "## 4. Repeat Iterative Process of Generating Heuristics\n",
    "We repeat this process of synthesizing, pruning, and verifying heuristics iteratively. In this example, we generate 25 total heuristics. \n",
    "\n",
    "_Note that this process wil take a few minutes to run since heuristic generation is not currently optimized in any manner_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a05fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy = []\n",
    "training_accuracy = []\n",
    "validation_coverage = []\n",
    "training_coverage = []\n",
    "\n",
    "training_marginals = []\n",
    "idx = None\n",
    "\n",
    "hg = HeuristicGenerator(train_primitive_matrix, val_primitive_matrix, \n",
    "                            val_ground, train_ground, \n",
    "                            b=0.5)\n",
    "plt.figure(figsize=(12,6));\n",
    "for i in range(3,26):\n",
    "    if (i-2)%5 == 0:\n",
    "        print(\"Running iteration: \", str(i-2))\n",
    "        \n",
    "    #Repeat synthesize-prune-verify at each iterations\n",
    "    if i == 3:\n",
    "        hg.run_synthesizer(max_cardinality=1, idx=idx, keep=3, model='dt')\n",
    "    else:\n",
    "        hg.run_synthesizer(max_cardinality=1, idx=idx, keep=1, model='dt')\n",
    "    hg.run_verifier()\n",
    "    \n",
    "    #Save evaluation metrics\n",
    "    va,ta, vc, tc = hg.evaluate()\n",
    "    validation_accuracy.append(va)\n",
    "    training_accuracy.append(ta)\n",
    "    training_marginals.append(hg.vf.train_marginals)\n",
    "    validation_coverage.append(vc)\n",
    "    training_coverage.append(tc)\n",
    "    \n",
    "    #Plot Training Set Label Distribution\n",
    "    if i <= 8:\n",
    "        plt.subplot(2,3,i-2)\n",
    "        plt.hist(training_marginals[-1], bins=10, range=(0.0,1.0)); \n",
    "        plt.title('Iteration ' + str(i-2));\n",
    "        plt.xlim([0.0,1.0])\n",
    "        # plt.ylim([0,825])\n",
    "    \n",
    "    #Find low confidence datapoints in the labeled set\n",
    "    hg.find_feedback()\n",
    "    idx = hg.feedback_idx\n",
    "    \n",
    "    #Stop the iterative process when no low confidence labels\n",
    "    if idx == []:\n",
    "        break\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f1d17",
   "metadata": {},
   "source": [
    "In the plots above, we show the distribution of probabilistic labels Reef assigns to the training set in the first few iterations.\n",
    "\n",
    "Next, we look at the accuracy and coverage of labels assigned to the training set in the _last_ iteration. The coverage is the percentage of training set datapoints that receive at least one label from the generated heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06520a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(training_marginals[-1], bins=10, range=(0.0,1.0)); \n",
    "plt.title('Final Distribution');\n",
    "\n",
    "print(\"Program Synthesis Train Accuracy: \", training_accuracy[-1])\n",
    "print(\"Program Synthesis Train Coverage: \", training_coverage[-1])\n",
    "print(\"Program Synthesis Validation Accuracy: \", validation_accuracy[-1])\n",
    "\n",
    "print(training_accuracy)\n",
    "print(validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed18de6",
   "metadata": {},
   "source": [
    "### Save Training Set Labels \n",
    "We save the training set labels Reef generates that we use in the next notebook to train a simple LSTM model."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
