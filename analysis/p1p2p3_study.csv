id,p1_code,p2_code,p3_code,op_name,p1_paths,p2_paths,p3_paths
405988,"test_multiple_dates_url = 'https://en.wikipedia.org/wiki/1950_Air_France_multiple_Douglas_DC-4_accidents'
summary_html = try_request(test_multiple_dates_url)","def getUrl(crypto_name):
    # Build URL
    url = r'https://www.coingecko.com/en/coins/' + crypto_name + r'/developer'
    # Open page
    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
    html = urllib.request.urlopen(req).read()
    # Extract important stuff with BeautifulSoup
    soup = BeautifulSoup(html, 'lxml')
    try:
        base_url = soup.find('a', href=True, text='Code Repository')['href']
        return base_url
    except TypeError:
        return None","# Here we're sorting the df so we can find out which countires are podiuming together 
df= df.sort_values(by=['Host_City', 'Year', 'Summer', 'Winter', 'Gender', 'Event'], ascending=False)",,Solutions 4->Question a: Setting up the dataframe->Iterating over all `<li>` elements and extract information,Explainer Notebook-Copy1->3. The analysis->3.2 Analyzing followers vs. marketcap->3.2.4 Step 3: New variables,150-Network Analysis-Competitiveness->Creating the csvs for GEPHI
291234,"#get distinct list of types
city_types = main_df.type.unique()

#create dictionary to hold all the data
data_dict = {}
#create dfs for each city type
for city_type in city_types:
    data_dict[city_type] = {}
    data_dict[city_type]['average_fare'] = main_df[main_df['type'] == city_type].groupby(['city']).mean()['fare']
    data_dict[city_type]['ride_count'] = main_df[main_df['type'] == city_type].groupby(['city']).count()['ride_id']
    data_dict[city_type]['driver_count'] = main_df[main_df['type'] == city_type].groupby(['city']).count()['driver_count']","attack_year_type = []
attack_types = []
year_type_counter = defaultdict(lambda: defaultdict(int))
for _, event in main_df.iterrows():
    attack_year_type.append((event['iyear'], event['attacktype1_txt']))
    year_type_counter[int(event['iyear'])][event['attacktype1_txt']] += 1
    if event['attacktype1_txt'] not in attack_types:
        attack_types.append(event['attacktype1_txt'])","image2 = Image('emission.png')
image2",,Pyber->Pyber Ride Sharing,Notebook->Exploring Global Terrorism Data->Preliminary Data Visualization->Map,Hidden Markov Model->Simple HMM Demo->Emission matrix: rows and columns means states and symbols respectively
423889,"# Generate sample data
rng = np.random.RandomState(0) # Random seed, for reproducibility 
X = 30 * rng.rand(200, 1) # Shape (200,1). 30 * [0, 1) = [0, 30)
y = np.sin(X).ravel() # Compute the sin for [0, 30)
y[::2] += rng.normal(scale = 1.0, size = X.shape[0] // 2) # adds noise to y

# [0, 40]. Shape (10000, 1)
X_plot = np.linspace(0, 40, 10000)[:, None] # A larger range to evaluate on 
true_y = np.sin(X_plot) # and the 'true' target function (true sin)

pl = plt.figure(figsize=(10, 5))
pl = plt.scatter(X, y, c='k', s = 5, label=""Data : [0, 30)"")
pl = plt.plot(X_plot, true_y, label = ""sin(x) : [0, 40]"");
pl =  plt.legend()","rng = np.random.RandomState(1)
X = np.sort(5 * rng.rand(80, 1), axis=0)
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(16))
plt.scatter(X, y);","from sklearn.linear_model import LinearRegression
model = LinearRegression()
results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print(results.mean())",,Assignment2->Support Vector Bananas (4 points (2+2))->2 . Pick the RBF kernel and optimize the two most important hyperparameters (the $C$ parameter and the kernel width $\gamma$).,08 Decision Trees->Regression,02-Regression-Algorithm-Spot-Checking->Linear MLA: Linear Regression
229097,"# Grabbing subject data with no scores for partner 
##(partner has scored subject)
df_atr_null = subset_df_clean[(subset_df_clean.attr.isnull()) & (subset_df_clean.sinc.isnull()) 
                            & (subset_df_clean.fun.isnull()) & (subset_df_clean.intel.isnull()) 
                             &  (subset_df_clean.amb.isnull()) & (subset_df_clean.shar.isnull())]
                              
# View data                              
df_atr_null[['iid', 'pid', 'like', 'prob', 'like_o', 'prob_o']]","## Look at match_sum ~ female attrbiute ratings for self('how subject perceives themselves) - all surveys 

model = smf.ols(formula = 'match_sum ~ attr_oPercveMe_1 + intel_oPercveMe_1 + fun_oPercveMe_1 + amb_oPercveMe_1 + sinc_oPercveMe_1', data = subsetdf_OPME1).fit()

model.summary()","import numpy as np
import pandas as pd
from sqlalchemy import create_engine
import os, sys
import warnings

warnings.filterwarnings('ignore')",,Speed Dating Dataset Final Project Remmer Draft3-Copy1->Compress Features Within Dataset to Get Averages & Sums->Q: Does one’s perception of themselves predict their dating outcomes?,Speed Dating Dataset Final Project Remmer Draft3-Copy1->SPEED DATING EXPERIMENT->match_sum ~ attr_oPercveMe_1 + intel_oPercveMe_1 + fun_oPercveMe_1 + amb_oPercveMe_1 + sinc_oPercveMe_1,20170215-Tc-Jde Etl Source Design->JDE ETL Source Design
150301,"registered.groupby(['Vehicle Fuel Code','Vehicle Fuel Label']).size()","#Get responses into workable format:
#for every column:
#    sum yes (1) responses 

#define codes
occur_codes = [""Q2211Murder"", ""Q2212StrRob"", ""Q2213HomeRob"", ""Q2214BussRob"", ""Q2215Hijack"", ""Q2216Assault"", ""Q2217SexAssult"", 
            ""Q2218ChildAbuse"", ""Q2219Political"", ""Q22110Vigilante"", ""Q22111BagSnatch"", 
            ""Q22112Bicycle"", ""Q22113Car"", ""Q22114Burglary"", ""Q22115Crop"", ""Q22116Livestock"", ""Q22117WhiteCollar"", ""Q22118damage"", 
            ""Q22119IDtheft"", ""Q22120HumanTrafficking"", ""Q22121DrugTrafficking"", ""Q22122HateCrime"", ""Q22123Other""]
crime_labels = [""Murder"", ""Street Robbery"", ""Home Roberry"", ""Business Robbery"",""Vehicle Hijacking"", ""Assault"", ""Sexual Assault"", 
          ""Child Abuse"", ""Political Violence"", ""Vigilantism"", ""Bag Snatching"", ""Bicycle Theft"", ""Vehicle Theft"", ""House Breaking"", 
          ""Crop Theft"", ""Livestock Theft"", ""White Collar"", ""Damage"", ""Id theft"", ""Human Trafficking"", ""Drug Trafficking"",
          ""Hate Crime"", ""Other Crime""]

def freqCount(region, topic_code, response_code):
    return region.groupby(topic_code).size().reset_index(name = 'frequency')['frequency'][response_code]

def multiFreqCount(region, occur_codes):
    #create list of counts
    crime_count = []
    for topic in occur_codes:
        crime_count.append(freqCount(region, topic, 0))
    
    #create list of topics
    label = []
    for topic in occur_codes:
        label.append(topic)
     
    #gather lists to data frame using numpy
    countdata = pd.DataFrame(
    {'CrimeCode': label,
    'Frequency': crime_count}
    )
       
    return countdata


gautengdata = multiFreqCount(gauteng, occur_codes)
gautengdata.info()","accuracy = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='accuracy')
print(""Accuracy: %0.5f (+/- %0.5f)"" % (accuracy.mean(), accuracy.std() * 2))
precision = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='precision')
print(""Precision: %0.5f (+/- %0.5f)"" % (precision.mean(), precision.std() * 2))
recall = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='recall')
print(""Recall: %0.5f (+/- %0.5f)"" % (recall.mean(), recall.std() * 2))
f = cross_val_score(clf_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='f1')
print(""F-measure: %0.5f (+/- %0.5f)"" % (f.mean(), f.std() * 2))",,Car Data->Vehicle registration in Belgium (2014)->Fuel characteristics,Crime Perception->Perceptions of Crime in Gauteng->What do people perceive as the most frequently occuring crimes in their area?,"Decision Tree Ids->Cross Validation: Accuracy, Precision, Recall, F-measure"
270972,"for g, data in dataset_sliced.groupby([""database""]):
    popularity_fb_metrics(data, getlabel(g), figsize=(15, 3), all_data=False)","print dataset.shape
dataset.Popular.value_counts()",gender_df,,0->Saving all the documents in output/documents.txt,Ipython Notebook->Predicting which New York Times blog articles will be the most popular->Data Exploration,Final Project 3 - New Coders Survey->Calculate Odds Ratio->????->CREATE DUMMY VARIABLES FOR GENDER
197889,"W_conv2 = weight_variable([5,5,32,64])
b_conv2 = bias_variable([64])","W_conv2 = weight_variable([5,5,32,64])
b_conv2 = bias_varible([64])","batchsize = 1024
datashape = (64, 257)
libridev='/local_data/teams/magnolia/librispeech/processed_dev-clean.h5'
# libridev='/local_data/teams/magnolia/processed_train-clean-100.h5'",,"Tensorflow2->Second convolutional layer
Mnist->1 Layer network->Second convolutional Layer",First Convolutional Deep Learning Code-Checkpoint->Training->Train Model->Iteratively Train model,Iterator-Examples->LibriSpeech Dev File
111123,"#Standard Deviation
np.std((Y_test-predicted)**2)","y_hat_L1 = mod_L1.predict(x_scale_test)

plot_reg(x_scale_test[:,0], y_hat_L1, y_test)

print(np.std(y_hat_L1 - y_test))","%%tikz -s 100,100

\draw [rotate around={-45:(0,0)}] (-.5,-1) rectangle (0.5,1);
\filldraw (0,0) circle (0.125);

\draw [->] (0,0) -- (0,1.5);
\draw [->] (0,0) -- (1.5,0);
\draw [->] (0,0) -- (1.5,1.5);
\draw (1.2, -0.2) node {$x$};
\draw (-0.2, 1.2) node {$y$};
\draw (1, 1.2) node {$v$};",,1->Linear Regression->Ordinary Least Squares Method,Intro To Regularization->Introduction to Regularization for Deep Neural Nets->3.0 l1 regularization->3.2 Neural network with l1 regularization,Time Optimal Smartmouse Controls->Trajectory Planning With a Simple Dynamics Model
130213,"from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=6)
knn.fit(iris['data'], iris['target'])","from KNearestNeighbor import KNearestNeighbor
knn = KNearestNeighbor(k=1)
knn.fit(X_train, y_train)","## Creates a new figure 
plt.figure(figsize=(10, 8))
## Draw a distribution plot (histogram) from fraudulent transactions data
sns.distplot(df_fraudulent['Amount'], kde=True, hist=True, norm_hist=True)
## Check that most transactions are clustered around $0 and $500.",,15->Numerical EDA,Knn->3 Train the data,Credit Card Fraud Detection->**Random Forest Classifier**->With balanced dataset
354390,"plt.plot(range(1,50), df_percent)
plt.xlabel('Document count of words')
plt.ylabel('Percentage of words')
plt.title('Cumulative Distribution of Document Frequencies')
plt.show()","plt.plot(range(1,50), df_percent)
plt.xlabel('Document count of words')
plt.ylabel('Percentage of words')
plt.title('Cumulative Distribution of Document Frequencies')
plt.show()","data = pd.read_csv(""telecom_churn_data.csv"", header = None,names = [""State"", ""Account_Length"", ""Area_Code"", ""Phone"",""International_Plan"",""VMail_Plan"", ""VMail_Message"",""Day_Mins"",""Day_Calls"",""Day_Charge"",""Eve_Mins"",""Eve_Calls"",""Eve_Charge"",""Night_Mins"",""Night_Calls"",""Night_Charge"",""International_Mins"",""International_Calls"",""International_Charge"",""CustServ_calls"",""Churn""])
data.head()",,Mini Project Naive Bayes,Mini Project Naive Bayes,Logistic Regression->Logistic Regression Model to Predict the Churn Value->Reading the Data to CSV file and assigning names to the header
321549,"just_depression = df[df['trackable_name'] == 'Depression']
depression_by_user = just_depression.groupby('user_id')
print len(depression_by_user)","just_depression = df[df['trackable_name'] == 'Depression']
depression_by_user = just_depression.groupby('user_id')
print len(depression_by_user)","import numpy as np
import pandas as pd
from load_data import loadData
from util import IoU, Dice, masked, remove_small_regions
from keras.models import load_model, model_from_json
from keras.preprocessing.image import ImageDataGenerator
from skimage import morphology, io, color, exposure, img_as_float, transform
from matplotlib import pyplot as plt
import json
import os
import glob",,"Exploration->Since Depression is well represented in the data, how many individual users are reporting depression?","Exploration->Since Depression is well represented in the data, how many individual users are reporting depression?",Segmentation->Import
27464,"print stats.f_oneway(sample_control,sample_T1,sample_T2,sample_T3)","t1 = ( 1, 2, 3, 4, 5)
t2 = ( 6, 4, 7, 8, 9)
t3 = t1 + t2
print( t3 )","from medusa.flux_analysis import flux_balance
biomass_fluxes = flux_balance.optimize_ensemble(ensemble,return_flux=""bio1"")",,"Notebook
Anova-Checkpoint->One-way Analysis Of Variance (ANOVA)",Python Ninja Bootcamp 9-Tuple->Tuple->Mutability,Machine Learning->Applying machine learning to guide ensemble curation
47491,"students = pd.concat([student_mat, student_por]).groupby([""school"",
                                                          ""sex"",
                                                          ""age"",
                                                          ""address"",
                                                          ""famsize"",
                                                          ""Pstatus"",
                                                          ""Medu"",
                                                          ""Fedu"",
                                                          ""Mjob"",
                                                          ""Fjob"",
                                                          ""reason"",
                                                          ""nursery"",
                                                          ""internet""]).mean().reset_index()

students['alc'] = students['alc'].apply(np.rint)
students.shape","# Saving our cleaned dataset and results of our Naive OLS fit
student_both.to_pickle('data/student_both_v3.pkl')
student_por.to_pickle('data/student_por_v3.pkl')
student_mat.to_pickle('data/student_mat_v3.pkl')

results1_disc_both.save('results/Naive_OLS1_disc_both.pickle')
results1_disc_por.save('results/Naive_OLS1_disc_por.pickle')
results1_disc_mat.save('results/Naive_OLS1_disc_mat.pickle')
results2_disc_both.save('results/Naive_OLS2_disc_both.pickle')
results2_disc_por.save('results/Naive_OLS2_disc_por.pickle')
results2_disc_mat.save('results/Naive_OLS2_disc_mat.pickle')

results1_cont_both.save('results/Naive_OLS1_cont_both.pickle')
results1_cont_por.save('results/Naive_OLS1_cont_por.pickle')
results1_cont_mat.save('results/Naive_OLS1_cont_mat.pickle')
results2_cont_both.save('results/Naive_OLS2_cont_both.pickle')
results2_cont_por.save('results/Naive_OLS2_cont_por.pickle')
results2_cont_mat.save('results/Naive_OLS2_cont_mat.pickle')","X_dtm_all_count = (X_train_dtm_all_count, X_test_dtm_all_count)
X_dtm_count = [X_dtm_all_count]",,Student-Analysis->Technical appendix->Summary data analysys and preprocessing,Model Fitting 1->Model Fitting I->Regression Results - Continuous->Regression Output Table Reproducibility Note:,"Feature Enginering Bow->So the set is 16; combined, step 1 FS, w2v, svd at 1422"
87806,"def Cost(theta, X, y):
    h_X = X.dot(theta)
    m, _ = X.shape
    return (h_X - y).T.dot(h_X - y) / (2*m)","def cost_function(theta, X, y):
    m = y.size
    h = expit(X.dot(theta)) # sigmoid function
    J = -1 * (1 / m) * (np.log(h).T.dot(y) + np.log(1 - h).T.dot(1 - y))         
    if np.isnan(J): # if h = 0
        return(np.inf)
    return(J)",df_engagements.tail(10),,Linear Regression->I.Linear Regression->1.Linear regression with one variable->2.1.Feature Normalization->1.2.Gradient Descent->1.2.2.Implementation->Compute cost $J(\theta)$,Logistic Regression,Relax Challenge->Solution to a Relax Challenge->Load data from both files
376701,"y = np.exp(-0.3*x)*np.sin(2*x)
title = ""Exponentially decayed sine function""
plotx(x,y,title)","y = np.exp(x)
title = ""Exponential function""
plotx(x,y,title)","sorted(zip(RMSE_scores, max_depth_range))[0]",,"Chapter 2 - Functions, Equations, Plots, And Limit->HEADING 2: Python libraries for numerical computing and visualization->Visualization using Matplotlib library->Proving a mathematical identity using NumPy operations","Chapter 2 - Functions, Equations, Plots, And Limit->HEADING 2: Python libraries for numerical computing and visualization->Visualization using Matplotlib library->Proving a mathematical identity using NumPy operations",To The Net->2. Classification Trees
292584,"df1 = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],
                    'data': range(6)})
display(df1)
pd.get_dummies(df1['key'])","l = len(df_train.columns.values)
rang = range(1, l)

# create DataFrame
simple_medians = pd.DataFrame()
simple_medians['Page'] = df_train.loc[:,'Page'].values

#add new columns to the DataFrame
for i in rang:
    get_n_days_median(simple_medians, df_train, i)

# set 'Page' column as an index
simple_medians.set_index('Page')
simple_medians.head()","# Define a function to compute color histogram features  
# Pass the color_space flag as 3-letter all caps string
# like 'HSV' or 'LUV' etc.
# KEEP IN MIND IF YOU DECIDE TO USE THIS FUNCTION LATER
# IN YOUR PROJECT THAT IF YOU READ THE IMAGE WITH 
# cv2.imread() INSTEAD YOU START WITH BGR COLOR!
# Define a function to compute color histogram features  
# Pass the color_space flag as 3-letter all caps string
# like 'HSV' or 'LUV' etc.
def bin_spatial(img, color_space='RGB', size=(64, 64), debug=False):
    # Convert image to new color space (if specified)
    cmap=None
    if color_space != 'RGB':
        if color_space == 'HSV':
            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
        elif color_space == 'LUV':
            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)
        elif color_space == 'HLS':
            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)
        elif color_space == 'YUV':
            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)
        elif color_space == 'YCrCb':
            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)
    else: 
        feature_image = np.copy(img)
    
    # Use cv2.resize().ravel() to create the feature vector
    features = cv2.resize(feature_image, size).ravel()
    if debug:
        plt.plot(features)
        plt.show()
    # Return the feature vector
    return features",,Pandas->More on selection and assignment->Computing dummy variables,Smape-Checkpoint->create simple_medians DataFrame,Car Nd-Vehicle-Detection->Vehical Detection project code->Function to return feature bin of image for the given color space
204446,"def create_song_vector(song):
    """"""Returns list of moods in percent, an array of 8 items (values 0-1)""""""
    list_of_words = str(song).split("" "")
    dim_array_28b = []
    list_mood = []
    list_mood_percent = []
    mood_list_v2 = [
        'happy', 'sad', 'angry', 'confused', 'surprised', 'calm', 'unknown',
        'disgusted']
    for i in range(len(list_of_words)):
        try:
            dim_array_28b.append(model3[list_of_words[i]])
        except:
            pass
    song_vector = (np.average(dim_array_28b, axis=0))
    for i in range(len(mood_list_v2)):
        mood_list_v2.append(mood_list_v2[i])
        list_mood_percent.append(
            scipy.spatial.distance.cosine(song_vector,
                                          model3[mood_list_v2[i]]))
    return list_mood_percent



def song_100D_vector(song):
    """"""Returns 100D Song Vector""""""
    list_of_words = str(song).split("" "")
    dim_array_28b = []
    list_mood = []
    list_mood_percent = []
    mood_list_v2 = [
        'happy', 'sad', 'angry', 'confused', 'surprised', 'calm', 'unknown',
        'disgusted']
    for i in range(len(list_of_words)):
        try:
            dim_array_28b.append(model3[list_of_words[i]])
        except:
            pass
    song_vector = (np.average(dim_array_28b, axis=0))
    return song_vector","#Function to convert string to one hot encoding
def to_onehot(data):
    ascii_list = []
    elements_per_song = []
    for songs in data:
        elements_per_song.append(len(songs))
        for chars in songs:
            ascii_list.append(ord(chars))
    onehot_vals = np.eye(128)[ascii_list]
    return onehot_vals, np.array(ascii_list), np.array(elements_per_song)",lines = sc.textFile('datasets/slurmd/slurmd.log.c6601'),,"Code->Phase 5, Assign Song Distances to each Emotion using Word2Vec Model->Import Scraped Songs (From a Pickle) -->Create song vectors for each song for each mood",Daniel Hw4->Functions,Unit 4 Word Count->WordCount->Load data
305100,"# Visualising distribution of data
from pandas.plotting import scatter_matrix

#the scatter matrix is plotting each of the columns specified against each other column.
#You would have observed that the diagonal graph is defined as a histogram, which means that in the 
#section of the plot matrix where the variable is against itself, a histogram is plotted.

#Scatter plots show how much one variable is affected by another. 
#The relationship between two variables is called their correlation
#negative vs positive correlation

#HTGD - Home team goal difference
#ATGD - away team goal difference
#HTP - Home team points
#ATP - Away team points
#DiffFormPts Diff in points
#DiffLP - Differnece in last years prediction

scatter_matrix(data[['HTGD','ATGD','HTP','ATP','DiffFormPts','DiffLP']], figsize=(10,10))
# display(data)","from pandas.tools.plotting import scatter_matrix
_ = scatter_matrix(data, alpha=0.3, figsize=(14,14), diagonal=""kde"")","#create identities for each being:
    #give them a number
identities = np.arange(2*N)
identities = np.reshape(identities,(200,1))
    #give them a base rate of stealing the cookies
base_rates_blue = np.ones((N, 1))*mu_blue
base_rates_green = np.ones((N, 1))*mu_green
base_rates = np.concatenate((base_rates_blue, base_rates_green), axis=0)

#create X_blue
thisisatest= np.random.normal(mu_blue*3, 1, (N,3))
blue_vector2 = np.random.exponential(mu_blue + 1, (N,3))
blue_vector3 = np.random.binomial(100, mu_blue**2, (N,3))
blue_vector4 = np.random.standard_normal((N,1))
blue_features = (blue_vector1, blue_vector2, blue_vector3, blue_vector4)

X_blue = np.concatenate(blue_features, axis=1)

#create X_green
green_vector1 = np.random.normal(mu_green*3, 1, (N,3))
green_vector2 = np.random.exponential(mu_green+1, (N,3))
green_vector3 = np.random.binomial(10, mu_green**2, (N,3))
green_vector4 = np.random.standard_normal((N,1))
green_features = (green_vector1, green_vector2, green_vector3, green_vector4)

X_green = np.concatenate(green_features, axis=1)

#append identities, base rates, and mix everything together
X = np.concatenate((X_blue, X_green), axis=0)
X = np.concatenate((X, identities, base_rates), axis=1)
np.random.shuffle(X)


identities = X[:,10]
base_rates = X[:,11]
X = X[:,0:10]",,Prediction->Data Exploration,Customer Segments->Creating Customer Segments->Clustering->How Many Clusters?,Preliminary Investigations->This is the actual simulation->Simulation
41390,"races = [row[7] for row in data]

race_counts = {}
for race in races:
    if race in race_counts:
        race_counts[race] += 1
    else:
        race_counts[race] = 1

race_counts","#finding the count of gun deaths involving a police officer against each race
race_police_count = {}
for row in police_shooting:
    race = row[7]
    if race not in race_police_count:
        race_police_count[race] = 1
    else:
        race_police_count[race] += 1
print race_police_count",# COMPAS recidivism confusion matrix,,"Basics->Exploring gun deaths by Race
Basics->US GUN DEATHS GUIDED PROJECT->Exploring Gun Deaths By Race And Sex",Gun Deaths Analysis,Week-5-1-Machine-Bias-Class-Empty->Week 5-1: Breaking down Machine Bias->2. Predictive calibration and accuracy
453741,"M = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 0]
])
LA.det(M)","trace_N = np.trace(N)
det_N = la.det(N)
I = np.eye(2)
N@N - trace_N *N +det_N *I","def compileFile(filelist, compiledName):
    
    import pandas as pd 
        
    compiled = pd.DataFrame()
    for f in filelist: 
        df = pd.read_csv(f)
        # Remove any coordinates that do not fall within Singapore's latitude range. 
        #df = df.loc[df['Latitude'] >1.2 ,:]
        #df = df.loc[df['Latitude'] <1.5 ,:]

        compiled = compiled.append(df)
        # Remove duplicates
        compiled.drop_duplicates(inplace = True)
        
        print(compiled.shape)
    compiled.to_csv(compiledName, index = False)
    return True",,Basic->determinant,Notes-2017-03-08->Math 210->1. Linear algebra with Scipy->Example: Characteristic polynomials and Cayley-Hamilton Theorem,Combine Files->Combine Results File->Function to Combine Files
365417,"myArr = np.random.randint(3, size=60).reshape((20, 3))
myList = list([tuple(L) for L in myArr])
print(""a list of lists:"")
pprint(myList)","A = np.random.randint(0, 10, size=(3,4)).astype(np.float)
print(""A"")
print(A)
print('A * I')
print(A.dot(I))","def distance_feature_engineering(sample):
    ONE_MINUTE = 60
    ONE_HOUR = 60
    boolean_map = {
        True: 1,
        False: 0
    }
    sample = sample[sample[""Trip_distance""] != 0]
    sample[""duration""] = (sample[""Lpep_dropoff_datetime""] - sample[""lpep_pickup_datetime""])
    sample[""duration""] = np.array([elem.seconds/(ONE_MINUTE*ONE_HOUR) for elem in sample[""duration""]])
    sample[""ave_speed""] = sample[""Trip_distance""] / sample[""duration""]  
    
    sample[""under_one""] = sample[""Trip_distance""] <= 1
    sample[""three_and_half_or_more""] = sample[""Trip_distance""] >= 3.5
    sample[""between_one_and_three""] = (sample[""Trip_distance""] > 1) & (sample[""Trip_distance""] < 3.5)
    sample[""during_heavy_traffic""] = (sample[""hour""] >= 7) & (sample[""hour""] <= 19)
    
    sample[""under_one""] = sample[""under_one""].map(boolean_map)
    sample[""three_and_half_or_more""] = sample[""three_and_half_or_more""].map(boolean_map)
    sample[""between_one_and_three""] = sample[""between_one_and_three""].map(boolean_map)
    sample[""during_heavy_traffic""] = sample[""during_heavy_traffic""].map(boolean_map)
    return sample

sample = distance_feature_engineering(sample)
sample = sample.replace([np.inf, -np.inf], np.nan)
sample = sample.dropna()",,Sorting->using np.sort(...) function->Builtin function sorted(...),"Week 03 Matrix Math Answers->BREAK FOR LOTS OF CHALKBOARD TALK
Week 03 Matrix Math->BREAK FOR LOTS OF CHALKBOARD TALK","Question 4-Checkpoint->Dealing with class imbalance
Part 5 - Non-Linear Regression->Some distance based features"
328328,"for n_neighbors in [1, 5, 10, 20, 30]:
    knn = KNeighborsClassifier(n_neighbors)
    knn.fit(X_train, y_train)
    print(n_neighbors, knn.score(X_test, y_test))","# Nearest Neighbor object creation
knn = KNearestNeighbor()

# Fit the model
knn.fit(X_train, y_train)","ex_5
time_ex_5",,"Part3
Scikit-Learn-Validation->Validation and Model Selection->Validation Sets
Cross-Validation
Workshop Cross Validation->Validation and Model Selection->Validating Models",Classification With Mnist Nishanth Gandhidoss->Question 1,Deep Learning Experiments Mnist->2). Experiments with Three Hidden Layers->2b). Run-time and Accuracy Scores for Each Sub-experiment
429088,pattern.match('<HTML>  '[:6]),"pattern.match('<HTML>  ', 0, 6)","import matplotlib.image as mpimg
import numpy as np
import cv2
import pandas
import random
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras.models import Sequential, Model
from keras.layers import Dense, Activation, Flatten, Dropout, Lambda, ELU, concatenate, GlobalAveragePooling2D, Input
from keras.activations import relu, softmax
from keras.layers.convolutional import Convolution2D
from keras.layers.pooling import MaxPooling2D
from keras.optimizers import Adam
from keras.regularizers import l2
import math",,Chapter2->`findall` method->`UNICODE`,Chapter2->`findall` method->`UNICODE`,Self-Drivng-Car-Beta-Clean->End-to-End Learning to Steer for Self-Driving Cars with Small and Efficient Networks
210424,"sns.violinplot(x='Pclass', y='Age', hue='Survived', data=df, split=True)","sn.jointplot(y=df.incomeperperson, x=df.relectricperperson, kind='reg');","x = 5
print('x = 5\t\tmemory address:', hex(id(x)))
x = 6
print('x = 6\t\tmemory address:', hex(id(x)))",,Titanic Data Analysis->Plotting the graph for Pclass & Age vs Survival,Week 4->1. Week 4 assignment->1.2 Bivariate graphs->1.2.1 Income per person vs Electric per person,05 Functions->Functions->locals / globals
446041,"print(""Original k.shape :"", k.shape, "" <-- 1D"")
k = k.reshape((len(k), 1, 1))
print(""New k.shape      :"", k.shape, "" <-- 3D"")","my_dict={""aaaa"":10, ""b"":20, ""bz"":20, ""xz"":55, ""sss"":55}
for k in my_dict:
    if len(k)==2 and ((k.endswith(""z"") or k.startswith(""s""))):
        print(k)","K = 2
kf = KFold(n_splits=K, shuffle=True, random_state=15)
data = df_features
target = df.Close
train_errors = []
validation_errors = []

for train_index, val_index in kf.split(data, target):

    # split data
    X_train, X_val = data.iloc[train_index], data.iloc[val_index]
    y_train, y_val = target[train_index], target[val_index]

    # instantiate model
    model = LinearRegression(fit_intercept=True)

    # calculate errors
    train_error, val_error = calc_metrics(X_train, y_train, X_val, y_val, model)

    # append to appropriate list
    train_errors.append(train_error)
    validation_errors.append(val_error)

# generate report
print('K: {} | mean(train_error): {:7} | mean(val_error): {}'.
      format(K,
             round(np.mean(train_errors) ** .5,4),
             round(np.mean(validation_errors) ** .5,4)))
print(model.coef_)
print(model.intercept_)

# Calculate y-hat and RMSE for this model
y_hat = model.predict(df_features)
print((np.square(np.subtract(df.Close, y_hat).mean())**.5))
print(y_hat)",,Arrays For Engineers->Broadcasting,Unit Six->Conditionals & Loops->Use of Parantheses,Project 2 Lin Reg->K-fold CV results (OLS)
241029,"from helpers import process_titanic_line
print(process_titanic_line(lines[0]))","PostScript = ""/x 1 def""
PS2 = ""x 3 eq {x 1 add} if""

PS = [PostScript, PS2]

for x in PS:
    print(x)
    processLine(x)
    
    
printOP()
printD()",weathervsc = weathervsc.fillna(method = 'ffill'),,"03->Combining Numerical and Categorical Features
03->Combining Numerical and Categorical Features",Sps-Final->Comment: interpret,05 Missing Data->1) fillna
174007,"config_logging('INFO')
possible = IterChannel(KeyedArtifact(i, DerivedArtifact(expensive_deriver, i, name='expensive')) for i in range(10))
extant = IterChannel(KeyedArtifact(i, ExampleExtantArtifact(i)) for i in keys)
all_ = merge_keyed_channels(possible, extant)
print_chans(all_.tee())
config_logging('WARN')","# Run as though we failed after 85 minutes and are picking up again
possible = IterChannel(KeyedArtifact(i, DerivedArtifact(expensive_deriver, i, name='expensive')).transform(data_writing_transform, i) for i in range(10))
extant = IterChannel(KeyedArtifact(i, ExampleExtantArtifact(i)) for i in sorted(storage.keys()))
all_ = merge_keyed_channels(possible, extant)
print_chans(all_.tee(), mode='ensure', func=lambda a: a)","param = {'alpha':[0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 1, 10, 50]}
model = Lasso()
clf = GridSearchCV(model, param, scoring='neg_mean_squared_error', refit='True', n_jobs=1, cv=5)
clf.fit(X_train, y_train)
print (""The best parameter is "" + str(clf.best_params_)) 
print (""Root mean squared error is "" + str(np.sqrt(abs(clf.best_score_))))",,04->Possible/Extant/All pattern,04->Possible/Extant/All pattern,House Prices Regression->This kernel contains the solution codes for House Prices: Advanced Regression Techniques competion of www.kaggle.com website
331685,"histograms = []
for img in thresholds:
    histogram = np.sum(img[img.shape[0]//2:,:], axis=0)
    histograms.append(histogram)
    
idx = range(len(thresholds))
fig, axes = plt.subplots(4, 2, figsize=(24,9),)
for ii, ax in zip(idx, axes.flatten()):
    ax.plot(histograms[ii])
    ax.xaxis.set_visible(False)
    ax.yaxis.set_visible(False)
plt.subplots_adjust(wspace=0.1, hspace=0.1)

# Extract peak values for left and right lane
midpoint = np.int(histogram.shape[0]/2)
leftx_base = np.argmax(histogram[:midpoint])
rightx_base = np.argmax(histogram[midpoint:]) + midpoint","idx = np.random.randint(0, trainset['X'].shape[3], size=36)
fig, axes = plt.subplots(6, 6, sharex=True, sharey=True, figsize=(5,5),)
for ii, ax in zip(idx, axes.flatten()):
    ax.imshow(trainset['X'][:,:,:,ii], aspect='equal')
    ax.xaxis.set_visible(False)
    ax.yaxis.set_visible(False)
plt.subplots_adjust(wspace=0, hspace=0)","TrainDMX = DesignMatrix(Train_Lighting.Timestamp)
TestDMX = DesignMatrix(Test_Lighting.Timestamp)",,Model->Calibrate camera->Finding lanes from threshold features,"Dcgan->Deep Convolutional GANs->Getting the data
Dcgan Exercises->Getting the data
Dcgan Exercises
Implementing->Loading dataset",Code->Course 12-752: Term Project->Project Title: Comparison of the accuracy of linear regression model based on the granularity of data: Scaife Hall->Section 5: Predicting for Lighting consumption for Scaife building using train and test datasets
371341,df['Hour']=df['timeStamp'].apply(lambda x: x.hour),"df['Hour'] = df['timeStamp'].apply(lambda ts: ts.hour)
df['Month'] = df['timeStamp'].apply(lambda ts:ts.month)
df['Day of Week'] = df['timeStamp'].apply(lambda ts: ts.dayofweek)
df[['timeStamp','Hour','Month','Day of Week']].head(2)","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import quandl
import re

from datetime import datetime

%matplotlib inline
%config InlineBackend.figure_format = 'retina'

quandl.ApiConfig.api_key = ""9tQnPMV9zakeCm4K6S5X""",,911 Calls Data Capstone Project,911 Calls Data Project->911 Calls Project->Analysis,"1 Eda->Exploratory Data Analysis (EDA)
Apple Eda->Apple, Inc. (AAPL) - Exploratory Data Analysis (EDA)"
126706,"# Create a data frame to join MVPs to their stats for that year they won. 
df_mvp = df1.merge(df_all, on=[""Player"", ""Year""], how=""inner"")
df_mvp = df_mvp[['YTID','Year', 'Player', 'Age_y', 'Tm', 'PER',""FTr"",""TS%"",""TRB%"",""AST%"",""STL%"",""BLK%"",""TOV%"",'PTS']]
df_mvp = df_mvp.rename(index=str, columns={""Age_y"": ""Age"", ""Tm"": ""Team""})
df_mvp","merg.to_csv('data/merged_data.csv', encoding='utf-8')
#merg = pd.read_csv('data/merged_data.csv', encoding='latin1')","ruler_types = {""Ivan"": ""Czar"", ""Julius Caesar"": ""Emperor"", ""Enver Hoxha"": ""Dictator""}
this_ruler = ""Ivan""
ruler_types.get(this_ruler,  ""Some other type of ruler"")
other_ruler = ""Elizabeth""
ruler_types.get(other_ruler, ""Some other type of ruler"")",,Final Project->Data Collection->MVP Data,Assignment-02->Exercise 23,"Python-As-Fp-3-En
Python-As-Fp-3-Sq"
360754,"# Training helpers

def get_trainable(model_params):
    return (p for p in model_params if p.requires_grad)


def get_frozen(model_params):
    return (p for p in model_params if not p.requires_grad)


def all_trainable(model_params):
    return all(p.requires_grad for p in model_params)


def all_frozen(model_params):
    return all(not p.requires_grad for p in model_params)


def freeze_all(model_params):
    for param in model_params:
        param.requires_grad = False


# list(get_trainable(model.parameters()))
# list(get_frozen(model.parameters()))
# all_trainable(model.parameters())
# all_frozen(model.parameters())","spBVN = set(p_BVN)
p_All = set(list(nameAndSp.ParameterName))
dead_param = spBVN - p_All

dead_param","len(segment), segment.framerate, spectrum.freq_res",,"01 Transfer Learning->Visualization with Tensorboard and TensorboardX->Init, helpers, utils, ...",Master Content - Combined->FAMILY TYPES->No. of Family Types,"Chap05->ThinkDSP
Chap05->ThinkDSP"
387933,"from sklearn.cluster import AgglomerativeClustering

ac = AgglomerativeClustering(n_clusters=5).fit(X)
labels_ac = ac.labels_

print(""Silhouette Coefficient: %0.3f""
      % metrics.silhouette_score(X, labels_ac, metric='sqeuclidean'))","import scipy.spatial.distance as dist

def distance_functions(generated_seq):
    generated_sequence = np.asarray(generated_seq)
    original_sequence = np.asarray(y_test)

    print 'Euclidean distance: ', dist.euclidean(original_sequence, generated_sequence)
    print 'Squared Euclidean distance: ', dist.sqeuclidean(original_sequence, generated_sequence)
    print 'Chebyshev distance: ', dist.chebyshev(original_sequence, generated_sequence)
    print 'Cosine distance: ', dist.cosine(original_sequence, generated_sequence)
    return generated_sequence

def train_and_evaluate(model, model_name):
    print 'Done building'
    print 'Training...'
    model.fit(X_train, y_train, batch_size=500, nb_epoch=500, validation_split=0.15,verbose=0)
    print 'Generating sequence...'
    generated_sequence = model.predict(X_test)
    return distance_functions(generated_sequence)","# Cummulative percolation (2nd pulse)
cum_leach_135mmh_SF = water2_ktest[0][:, 2]
cum_leach_135mmh_SA = water2_ktest[0][:, 4]
cum_leach_135mmh_LF = water2_ktest[0][:, 6]
cum_leach_135mmh_LA = water2_ktest[0][:, 8]

cum_leach_55mmhA_SF = water2_ktest[0][:, 10]
cum_leach_55mmhA_SA = water2_ktest[0][:, 12]
cum_leach_55mmhA_LF = water2_ktest[0][:, 14]
cum_leach_55mmhA_LA = water2_ktest[0][:, 16]

cum_leach_55mmhB_SF = water2_ktest[0][:, 18]
cum_leach_55mmhB_SA = water2_ktest[0][:, 20]
cum_leach_55mmhB_LF = water2_ktest[0][:, 22]
cum_leach_55mmhB_LA = water2_ktest[0][:, 24]

cum_leach_30mmh_SF = water2_ktest[0][:, 26]
cum_leach_30mmh_SA = water2_ktest[0][:, 28]
cum_leach_30mmh_LF = water2_ktest[0][:, 30]
cum_leach_30mmh_LA = water2_ktest[0][:, 32]",,Mini Project Clustering->Other Clustering Algorithms,Rnn->Creating sequence of close price from the stock data->Distance metrics:,S-Metolachlor Vine Soil 4 Events->Hydrology - 2nd pulse->Plotting transport->Computation transport - 1st pulse->Test factor X (Sterile)
40172,"# Visualizing count of logins per quarter-hour
sns.barplot(x='15_minute', y='count', data=minutes)
plt.ylabel('Average count')
plt.xlabel('Minute of Hour')
plt.title('Average count of logins vs. Minute of hour for 3.5 Months')","# Creates the chart
monthly_profit_bar = sn.barplot(x = monthly_profit2.index, y = monthly_profit2.profit, data = monthly_profit2)

# Sets the size of the chart
monthly_profit_bar.figure.set_size_inches(15,8)

# Sets titlte
monthly_profit_bar.axes.set_title('Profit by monthly releases',color=""#660033"", fontsize = 18, alpha = 0.6, fontname=""Cooper Std"")

# Sets x and y axes
monthly_profit_bar.set_xlabel(""Months"", color=""#99004C"", fontsize = 18, style='italic')
monthly_profit_bar.set_ylabel(""Profits"", color=""#CC0066"", fontsize = 18, style='italic')

monthly_profit_bar.tick_params(labelsize = 15, labelcolor=""black"")

month_list = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']

monthly_profit_bar.set_xticklabels(month_list, rotation = 30, size = 15)

plt.show()","Image(filename='images/05_01.png', width=400)",,Ultimate Challenge->Part 1 ‑ Exploratory Data Analysis->Test Neural Network Model,Final Tm Db Movie Data Analysis-><center>TMDb Movie Data Analysis </center>->Q10: What is the best month to release a movie in terms of profit?,"Dimensionality Reduction 2->Unsupervised dimensionality reduction via principal component analysis->The main steps behind principal component analysis
Dimensionality Reduction 2->Chapter 5 - Compressing Data via Dimensionality Reduction"
142801,"print('Number of Loans that are Fully Paid (0) or Charged Off (1):')
df.loan_status.value_counts()","#loanDF.registerTempTable(""loanDFTable"")

loanDF.groupBy('loan_status').count().sort(""count"", ascending=False).show()
#print(loanDF.loan_status.value_counts())

#print((loanDF[loanDF.loan_status.isin([""Default""])]).verification_status.value_counts())

#loanDF.loan_status.describe()","from IPython.display import Image
Image(filename='../Chapter 4 Figures/Boosting.png', width=500)",,"Eda->Capstone Project 1: Exploratory Data Analysis->Current Loan Status
Data Story->Capstone Project 1: Data Story->Current Loan Status",Cisc-5950 Project Notebook->Loan Counts group by loan_status,Boosting->Boosting
373271,"%matplotlib inline
import pandas as pd
import numpy as np
import glob
import os
from IPython.core.display import display
sns.set_context('notebook')","%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import mpld3
import numpy as np
from datetime import timedelta
import os

sn.set_context('notebook')","#This will import the pyplot module of the matplotlab package. 
import matplotlib.pyplot as pyplot
#Observe how we are calling the plot method within the pyplot module. 
pyplot.plot([1,2,3,4])
pyplot.ylabel('Sample numbers')
pyplot.show()",,Homework 1->Table of Contents->Task 1. Compiling Ebola Data,Explore Sensor Data 2,"Intro-Python-Overview->Python and Using Packages
01-Intro-Python-Overview->Python and Using Packages"
455288,"print(f""{len(dataset_features)} regions of interest detected."")","news_featur = news_data.drop(['url', 'timedelta', 'shares', 'log_shares'], axis=1)","# Create preproc output graph
preproc.write_graph(graph2use='colored', format='png', simple_form=True)

# Visualize the graph
from IPython.display import Image
Image(filename=opj(preproc.base_dir, 'preproc', 'graph.dot.png'))",,Features->Examples of feature extraction->Reading features of several images->Train data,News Popularity- Classification Model Evaluation->2. Read data->4. Data reduction->6.2 Use the top 20 features to train Randome Forest Classification model</h4>,"Example Preprocessing->Visualize the workflow
Preproc Analysis->Resampling to MNI152 Template->Specify input & output stream"
18276,plt.style.available,"# plot parameters

plt.rcParams[""figure.figsize""] = (10, 5)
plt.rcParams[""font.size""] = 14","from sklearn.linear_model import LinearRegression

lr_sc = LinearRegression()
lr_sc = lr_sc.fit(X_std, y_std)",,"09 Python Visualization 1->Customizing figures
Matplotlib->Based on [Effective Matplotlib](http://pbpython.com/effective-matplotlib.html) by Chris Moffitt
Matplotlib->Getting Started With Matplotlib->Upping Your Style
Matplotlib->Matplotlib->Setup/Configuration
Matplotlib->Other Styling Options
5->Introduction to Plotting using Matplotlib->Bar Plots
Lecture 1->List Assignment
Intro To Matplotlib-><center>Introduction to matplotlib</center>->Example 3
S04 Graphics Notes-Checkpoint->Basic Graphics->Using `matplotlib`->Styles
Matplotlib - Getting Started
Data Transformation And Visualization In Python->Matplotlib->Another Merge example - what are the most common destination airports?->Alternate plot styles
Ch4->Ch4.0 Simple Line Plots
Customizing Plots->Customizing Plots->Using styles
Homework 8-Checkpoint->this is a bar chart of programmers' age
Matplotlib Tutorial->Effectively Using Matplotlib
Crime-Against-Women-Data-Analysis->Data Analysis of Crime Against Women in India->Webscrapping to fetch hex color codes from a website->Available Graph formatting styles
Icos Big Data Camp Data Analysis Presentation-Checkpoint->Module 6 - Introduction to Python for Data Analysis->1. pandas->2.6. FOR FUN: Now let's make it look like xkcd.com->In fact, we can use many styles!
Data Vis Basics With Python->Data Visualization Basics with Python->Using matplotlib in the Jupyter Notebook
02C Exploratory Analysis->Understanding the Distribution of Data->Setting the Style
Matplotlib-Configuration->Customize plots with style sheets
Intro Data Visualization With Python->Intro to data visualisation with python->Legends, annotations, and styles->To see the available styles
Plotting With Matplotlib->Plotting with python/matplotlib->Customizing your plots
Plotting With Matplotlib->Plotting in Python->Basic plotting with matplotlib->Alternate plot styles
Data Manipulation->Visualization
Prediction Of The Used Car Toyota Corolla->Creating New Variables->The other continuous values are not as impressive as previous ones
Data Manipulation Filled->Grouping data
Pandas Plotting->Plotting with Pandas->Matplotlib Styles
Matplotlib Demo-Checkpoint->Effectively Using Matplotlib
02 - Python And Pandas-Summarizing Voter List Data V1
Extras->DAML 03 - `matplotlib` Extras->Styles
Matplotlib 2016
Bootcamp Advgraphics Seaborn Update->Better Defaults->Jointplot
Matplotlib4Papers->Matplotlib for presenting results->Learning curves
Matplotlib-Pandas-Viz 2018-01-30->Demo: Building a Histogram->In addition to setting visual attributes individually, matplotlib also has pre-defined styles we can use. We can see the names of all the styles by using the code below:
Introductory-Material->Introductory Material->4. A Brief Introduction to `matplotlib`
Matplotlib Styling
Bootcamp Graphics->Styles (optional)
03B Graphics->Styles
03-Matplotlib-Tut->Matplotlib tutorial->Customize plots with style sheets
10 Advanced Visualization Seaborn-><span style=""color:blue"">Matplotlib Style sheets</span>
M02I-Styles->Modern Data Science->matplotlib styling
Learning Notebook - Slu4 - Data Visualization->Bar/Column chart
14 Break Per Country->Import stuff->Look at available styles for plots
Icos Big Data Camp Data Analysis Presentation
Scrape Soccer Data->SportRadar API->Weight vs height
Ecg Classification - Revision->Neural Network: Train and test
Bootcamp Advgraphics Seaborn
Crack Xgb Iterative Tune-Checkpoint->Step 5: reduce learning rate",Project->Machine learning project: wine reviews->Goal,Linear Regression->Linear Regression->Sklearn implementation
237218,"singers['Li Jian'] = '心升明月' # new entry!
singers['Li Jian']","# returns the artist lyrics
def load_lyrics(singer):
    dfs = df[df.singer == singer]
    lyrics = dfs['lyrics']
    return lyrics","%matplotlib inline
import numpy as np
import scipy.integrate as integrate
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
sns.set_context('poster')
import MDSplus as mds",,"Lab05->Mutable Lists, Dictionaries and Trees->Dictionaries->Question 9: Replace Leaf",Final Project Part C->Train Text Generetor->Data Preprocessing,Example Mach Probe Analysis->Load Libraries
255372,"def blacklitterman(delta, weq, sigma, tau, P, Q, Omega):

  pi = weq.dot(sigma * delta)
  ts = tau * sigma
  # Compute posterior estimate of the mean
  # This is a simplified version of formula (8) on page 4.
  middle = linalg.inv(np.dot(np.dot(P,ts),P.T) + Omega)
  er = np.expand_dims(pi,axis=0).T + np.dot(np.dot(np.dot(ts,P.T),middle),(Q - np.expand_dims(np.dot(P,pi.T),axis=1)))
  # Compute posterior estimate of the uncertainty in the mean
  # This is a simplified and combined version of formulas (9) and (15)
  posteriorSigma = sigma + ts - ts.dot(P.T).dot(middle).dot(P).dot(ts)
  # Compute posterior weights based on uncertainty in mean
  w = er.T.dot(linalg.inv(delta * posteriorSigma)).T
  # Compute lambda value
  # We solve for lambda from formula (17) page 7, rather than formula (18)
  # just because it is less to type, and we've already computed w*.
  lmbda = np.dot(linalg.pinv(P).T,(w.T * (1 + tau) - weq).T)
  return [er, w, lmbda]","beta_delta = C.T.dot(np.linalg.pinv(C.T.dot(C)).dot(C.T).T).dot(betas)
print(""Shape B-delta: %s"" % (beta_delta.shape,))","import re
def word_in_text(word, text):
    if(text != None):
        word = word.lower()
        text = text.lower()
        match = re.search(word, text)
        if match:
            return True
        return False
    else:
        return False",,Untitled->Defining the black_litterman function,Cv Manova->MGLM,Antaki-Twitter-Tutorial->Bloom filter in python->Result of matched to unmatched tweets->Finding Tweets with some KeyWords
5134,"# Each sample is of size 256 (dimension 1)

# You can think of it as a flattened square of black and white pixels of size 16x16

# In the training datasets there are 93000 samples
# We are going to break them into batches
train = dlt.load_hdf5('/data/uji/train.hdf')
# print(train.x.shape)  # (93000, 256)
# print(train.y.shape)  # (93000,)

# print(train.x[0, :])           # [ 0.00 0.00 ... ] - a single example (256-vector of floats [0 1])#
# print(train.y[0])              # 21 - label representation
# print(train.vocab[train.y[0]]) # L  - actual label

# In the validation dataset there are only 620 samples
# A single batch is fine if you've got the memory (no backprop on it anyway)
valid = dlt.load_hdf5('/data/uji/valid.hdf')
# print(valid.x.shape)  # (620, 256)
# print(valid.y.shape)  # (620,)",morpho.loadHDF5(timestep=200),"# plot a correlation matrix to visualize the relationship between most occured words and other predictor variables

corr_matrix = reviews_dat.corr()
sns.heatmap(corr_matrix, 
        xticklabels=corr_matrix.columns,
        yticklabels=corr_matrix.columns)",,Tutorial->CNTK - Hand Writing Recognition Tutorial->Data Loading,Morphometrics->2. Interpolation of TIN data,Count Vectorizer Process Parsed Reviews->EDA and Summary Statistics->Relationship between number of book reviews and the word counts
408487,"import pefile

pe = pefile.PE(r'C:\64bit_exe')
data = pe.trim()

spec = construct.Struct(
    construct.Seek(0x6555),
    construct.Const(b'\x48\x8D\x15'),  # lea rdx, ...
    'data_ptr' / construct.DWORD,
    'inst_end' / construct.Tell,
    'data' / construct.PEPointer64(
        lambda ctx: ctx.data_ptr, lambda ctx: ctx.inst_end, construct.Bytes(10))
)

spec.parse(data, pe=pe)","spec = construct.Struct(
	construct.Seek(0x6BD),
	'data_ptr' / construct.DWORD,
	'data' / construct.PEPointer(lambda ctx: ctx.data_ptr, construct.CString())
)

spec.parse(data, pe=pe)","# Create an empty dataframe for predictions
predictions2 = pd.DataFrame(index=questionlist, columns=predictionlist)",,Construct->PE Physical Address 64-bit,Construct->Using Construct->Ranges (lists),Annie Pi Exercise Cbh->Recommendation Engines Lab 2->4. Hybrid - Switching
190139,"### Train your model here.
### Calculate and report the accuracy on the training and validation set.
### Once a final model architecture is selected, 
### the accuracy on the test set should be calculated and reported as well.
### Feel free to use as many code cells as needed.
from sklearn.utils import shuffle

with tf.Session(graph=graph) as sess:
    sess.run(tf.global_variables_initializer())
    
    print(""Training..."")
    print()
    for i in range(EPOCHS):
        X_train, y_train = shuffle(X_train, y_train)
        for offset in range(0, n_train, BATCH_SIZE):
            end = offset + BATCH_SIZE
            batch_x, batch_y = X_train[offset:end], y_train[offset:end]
            sess.run(training_operation, feed_dict={X: batch_x, y: batch_y,
                                                    is_training: True})
            
        validation_accuracy = evaluate(X_valid, y_valid)
        print(""EPOCH {} ..."".format(i+1))
        print(""Validation Accuracy = {:.3f}"".format(validation_accuracy))
        print()
        
    saver.save(sess, './color_ah02_c5_contr_d8_xavier_bn_elu')
    print(""Model saved"")","with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    num_examples = n_train
    
    print(""Training..."")
    print()
    for i in range(EPOCHS):
        X_train, y_train = shuffle(X_train, y_train)
        for offset in range(0, num_examples, BATCH_SIZE):
            end = offset + BATCH_SIZE
            batch_x, batch_y = X_train[offset:end], y_train[offset:end]
            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})
            
        validation_accuracy = evaluate(X_valid, y_valid)
        print(""EPOCH {} ..."".format(i+1))
        print(""Validation Accuracy = {:.3f}"".format(validation_accuracy))
        print()
        
    saver.save(sess, './lenet')
    print(""Model saved"")","### Replace each question mark with the appropriate value. 
### Use python, pandas or numpy methods rather than hard coding the results
import numpy as np

# TODO: Number of training examples
n_train = np.shape(X_train)[0]

# TODO: Number of validation examples
n_validation = np.shape(X_valid)[0]

# TODO: Number of testing examples.
n_test = np.shape(X_test)[0]

# TODO: What's the shape of an traffic sign image?
image_shape = np.shape(X_train)[1:4]

# TODO: How many unique classes/labels there are in the dataset.
n_classes = len(np.unique(y_train))

print(""Number of training examples ="", n_train)
print(""Number of testing examples ="", n_test)
print(""Number of validation examples ="", n_validation)
print(""Image data shape ="", image_shape)
print(""Number of classes ="", n_classes)",,"Traffic Sign Classifier->Train, Validate and Test the Model","Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Train, Validate and Test the Model","Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Provide a Basic Summary of the Data Set Using Python, Numpy and/or Pandas"
166685,"x = 4

if x < 2: 
    x += 1
elif x == 3:
    x = x
else:
    x-=1
    
print(x)","x = 7
if x > 5:
    x = x + 1
    print(x)","# Get the shortest review
raw_review = min(train_data['review'].values, key=len)

print(""%s\n"" % raw_review)

# Test tokenize function
print(""%r\n"" % tokenize(clean(raw_review)))

# Test tokenize_n_stem function
print(""%r\n"" % tokenize_n_stem(clean(raw_review)))

# Test tokenize_n_lemmatize function
print(""%r\n"" % tokenize_n_lemmatize(clean(raw_review)))",,"3->Conditional statements IF
3->Conditional statements IF",Python Bootcamp- Math->Math Operations->Comparison with Boolean Operators,Logistic Regression->Text Sentiment Classification
458664,"sql = """"""
select * from df1 join 
(
    select name, count(*) as number from df2 group by name
) as my_count
on 
    df1.name = my_count.name
""""""
sqlContext.sql(sql).show()","sqlContext.sql(""SHOW TABLES"").show()","img = plt.imread(f'{PATH}valid/frenchtoast/{files[0]}')
plt.imshow(img);",,Sql->Embeded SQL,"Sparksql Basic->**SparkSQL Lab: **->Part 2: Loading data programmatically->** (2b) Read from Hive **
Sparksql Basic->**SparkSQL Lab: **->Part 4: Caching for performance->Show time: Query top 2 row
Sparksql Basic->Part 2: Loading data programmatically->** (2b) Read from Hive **",Breakfast->Predictions->Most incorrect waffles
420911,"# Create a function to draw bootstrap replicates of the mean
def perm_reps_mean(data1, data2, size=1):
    '''This function draws the bootstrap replicates of the mean for a given data set'''
    # Set the random seed
    np.random.seed(25)
    
    # Initialize an empty array
    reps = np.empty(size)

    # Concatenate arrays
    concat = np.concatenate((data1, data2))
    
    # Create for loop to create bootstrap replicates of the mean
    for i in range(size):
        permed = np.random.permutation(concat)
        new_data1 = permed[:len(data1)]
        new_data2 = permed[len(data1):]
        reps[i] = np.mean(new_data1) - np.mean(new_data2)
    
    return reps","# Create permutation sample
def permutation_sample(data1, data2):
    """"""Generate a permutation sample from two data sets.""""""

    # Concatenate the data sets: data
    data = np.concatenate((data1, data2))

    # Permute the concatenated array: permuted_data
    permuted_data = np.random.permutation(data)

    # Split the permuted array into two: perm_sample_1, perm_sample_2
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]

    return perm_sample_1, perm_sample_2

# Create permutation replicates
def draw_perm_reps(data_1, data_2, func, size=1):
    """"""Generate multiple permutation replicates.""""""

    # Initialize array of replicates: perm_replicates
    perm_replicates = np.empty(size)

    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)

        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)

    return perm_replicates

# Find difference of means
def diff_of_means(data_1, data_2):
    """"""Difference in means of two arrays.""""""

    # The difference of means of data_1, data_2: diff
    diff = np.mean(data_1)-np.mean(data_2)

    return diff","with open(""/home/shriansh/Downloads/tree.dot"") as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)",,"Sliderule Dsi Inferential Statistics Exercise 2->Examining Racial Discrimination in the US Job Market->3) Compute margin of error, confidence interval, and p-value. Try using bootstrapping and the frequentist statistical approaches.",Mean Body Temperature->Gender Difference?->Permutation Test->Required Functions,Decision Trees->This notebook is developed to understand the working of decision trees. The decision tree is a classic machine learning algorithm that can be used for classification purposes. Here the same is applied on the red wine quality dataset to classify the wine quality as good or bad->Observing the formed decision tree
107845,"# Estimator state is stored in instance attributes
clf._tree","clf.coef_  # Coefficient of the features in the decision function
# This is for multiclass classification; for the binary case, you should use clf.coef_[0] only.",saved_dataset.head(),,"1
1->Transformers, pipelines and feature unions->Transformer zoo",Scikit-Learn - Machine Learning In Python->Unsupervised,Scan Merra2 Inst1 2D Asm Nx M2I1Nxasm One Year V2->Scan MERRA-2 atmospheric properties during one Year->5)  Save dataset  in file pandas (csv)->Check
492489,"vector = np.zeros(10)
vector[4] = 1
print(vector)","zero_vector = np.zeros((4))
print(""The zero vector"")
print(zero_vector)
print(zero_vector.shape)","# first we introduce the emission specis CO2 and the emission category GHG
scen.add_set('emission', 'CO2')
scen.add_cat('emission', 'GHG', 'CO2')

# we now add CO2 emissions to the coal powerplant
base_emission_factor = {
    'node_loc': country,
    'year_vtg': vintage_years,
    'year_act': act_years,
    'mode': 'standard',
    'unit': 'USD/GWa',
}

emission_factor = make_df(base_emission_factor, technology= 'coal_ppl', emission= 'CO2', value = 100.)
scen.add_par('emission_factor', emission_factor)",,Numpy,Basic Numpy 2->Using built-in methods to create matrices->1. Create a zero matrix,"Westeros Emissions Bounds->Westeros Tutorial Part II - Introducing emissions->Introducing Emissions
Westeros Emissions Bounds->Westeros Tutorial Part II - Introducing emissions->Introducing Emissions"
82122,"# height and weight are available as a regular lists

# Import numpy
import numpy as np

# Create array from height with correct units: np_height_m
np_height_m = np.array(height) * 0.0254

# Create array from weight with correct units: np_weight_kg


# Calculate the BMI: bmi


# Print out bmi","# height is available as a regular list

# Import numpy
import numpy as np

# Create a numpy array from height: np_height
np_height = np.array(height)

# Print out np_height
print(np_height)

# Convert np_height to m: np_height_m
np_height_m = np_height * 0.0254

# Print np_height_m
print(np_height_m)","plt.rcParams['figure.figsize'] = (10,8)

hourly_plot = hourly_count['Number_of_tweets'].plot(kind='bar', title = ""BarPlot for No of Tweets each H0ur"");
hours = list(range(1,25));                                                #GENERATE LIST FROM 1 TO 24
plt.xticks(np.arange(24), hours, rotation = 0,fontsize = 12);                  #USE THE CUSTOM TICKS

hourly_plot.set_xlabel('Hour of the Day', weight='bold', labelpad=15);     #SET X-AXIS LABEL, ADD PADDING TO TOP OF X-AXIS LABEL
hourly_plot.set_ylabel('# Tweets (Total Number of Tweets Each Hour)', weight='bold', labelpad=15); #SET Y-AXIS LABEL, ADD PADDING TO RIGHT OF Y-AXIS LABEL",,4->Hints->Exercises 3 Baseball player's BMI,"Num Py->**Blend it all together**
Dami-Mod2",Augmento Intern->Let's Analyse our Twitter Data with Time Period Perspective
344931,"viewer = Viewer(vars=phi, datamin=0., datamax=1., cmap='hot')","viewer = Viewer(vars = (phi), datamin=0., datamax=1.)
viewer2 = Viewer(vars = (xVelocity), datamin=0.7, datamax=0.9)
viewer3 = Viewer(vars = (yVelocity), datamin=0., datamax=1.)
viewer4 = Viewer(vars = (pressure), datamin=0., datamax=50.)","diff = ts_body

diff = (diff  - diff.shift(12))[12:] #Seasonal derivative

# diff = (diff  - diff.shift(1))[1:] #Simple derivative

diff.plot(figsize=(15,3))",,Diffusion->Accelerating FiPy with `pyamgx`->Example 1: 2-D steady-state diffusion equation,Explanation Optimized Code-Checkpoint->Presentation of the code->Viewers,Baghdasaryan-Afternoon-Lab2
14798,"sxx = mps.S.XX
assert (amax(sxx) - sa) / amax(sxx) < 1e-8","sxx = mps.S.XX
assert (amax(sxx) - sa) / amax(sxx) < 1e-8","import skimage
import matplotlib.patches as mpatches

img_example = frames[id_example]

# Label elements on the picture
white = 255
label_image, number_of_labels = skimage.measure.label(img_example, background=white, return_num=True)
print(""Found %d features""%(number_of_labels))
fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(12, 12))
ax.imshow(img_example)
for region in skimage.measure.regionprops(label_image, intensity_image=img_example):
    # Everywhere, skip small and large areas
    if region.area < 5 or region.area > 800:
        continue
    # Only black areas
    if region.mean_intensity > 1:
        continue
    # On the top, skip small area with a second threshold
    if region.centroid[0] < 260 and region.area < 80:
        continue
    # Draw rectangle which survived to the criterions
    minr, minc, maxr, maxc = region.bbox
    rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,
                              fill=False, edgecolor='red', linewidth=1)

    ax.add_patch(rect)",,Getting Started->Getting Started,Getting Started->Getting Started,Custom-Feature-Detection->Custom feature detection
291717,"data = None
with open(""q_table.save"", ""rb"") as f:
    data = np.load(f)

data.shape","with gzip.open('../Basener_Sanford_Data/bs5_2desired.pickled.gz') as f:
    data = pickle.load(f)
np.max(data['Psolution'][0])","name = ""Michael""
age = 19
schooling = 4
print(""Name: %s Age: %d Schooling Years: %0.1f"" % (name, age, schooling))",,Q-Learning-Taxi-V2-Complete->Loading the Q-table,"1 Run Basener Script-Copy1->Section 5.3: Gaussian
1 Run Basener Script->Generate data by running Basener's script->Section 5.2: No mutations","Print Formatting->Print Formatting->Percent Notation with Strings, Floats, and Integers"
328008,x**2 - 5 * x + 10,"def dummy_cluster(x):
    if x=='Yes':
        return 1
    else:
        return 0","# are you an adult?
age = 25


# Note that python uses 1 tab for spacing.
if age <= 18:
    print(""You are a minor."")
else:
    print(""You are an adult!"")",,Completed-01-Getting Started With Python->Lists->Storing values into variables,K Means Clustering Project->Evaluation,1->Introduction to Programming in Python->Variables
185987,"import pickle

training_file = 'train_test_data_set/train.p'
testing_file = 'train_test_data_set/test.p'

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)
    
X_train, y_train = train['features'], train['labels']
X_test, y_test = test['features'], test['labels']

print(""Updated Image Shape: {}"".format(X_train[0].shape))

label_list = (y_train.tolist())","training_file = ""dataset/train.p""
validation_file = ""dataset/valid.p""
testing_file = ""dataset/test.p""

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(validation_file, mode='rb') as f:
    valid = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)
    
X_train, y_train = train['features'], train['labels']
X_valid, y_valid = valid['features'], valid['labels']
X_test, y_test = test['features'], test['labels']

n_train = X_train.shape[0]
n_validation = X_valid.shape[0]
n_test = X_test.shape[0]
image_shape = (X_train.shape[1:])
n_classes = len(set(y_train))

print(""Number of training examples ="", n_train)
print(""Number of validation examples ="", n_validation)
print(""Number of testing examples ="", n_test)
print(""Image data shape ="", image_shape)
print(""Number of classes ="", n_classes)","import seaborn as sns
%matplotlib inline
import matplotlib.pyplot as plt
g = sns.FacetGrid(tips_data,row=""smoker"",col=""sex"")
g.map(plt.hist, ""total_bill"")",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree,Traffic-Signs-Classifier-Tl->Self-Driving Car Engineer Nanodegree->1. Load the data and visualize all labels,L3 Python Practice->Module 3 Python Practice->References for plotting in Python
418704,"ffplot(region, ['ellipse'], 3, 6, grid=False, label=True)","plot_region(x_bounds=(-50, -40), y_bounds=(-20, -10))",case2.total_middle,,Demo-Checkpoint->Probing Dendrogram Values->Another Approach: Detection,Thrones2Vec->Explore the trained model.,Demo->Tutorial: Intermediate->Calling Case attributes->Advanced techniques: multiple selections.
22403,"# Mapping titles
title_mapping = {""Mr"": 1, ""Miss"": 2, ""Mrs"": 3, ""Master"": 4, ""Millitary"": 5, ""Honor"": 6}
df_data[""TitleCat""] = df_data['Title'].map(title_mapping)
df_data[""TitleCat""] = df_data[""TitleCat""].astype(int)
df_train[""TitleCat""] = df_data[""TitleCat""][:891]
df_test[""TitleCat""] = df_data[""TitleCat""][891:]
print('Title Category created')","cos_titl_quer = []
for i in (range(len(title_vec))):
    cos_titl_quer.append(cosine_distance(title_vec[i], search_query_vec[i]))","kernel = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], np.float32)
dx = show_differences(kernel)",,Titanic->Re-evaluate the on new features->Title Cetegory,Model V4->Cosine distance between title_vec and search_query_vec,Understanding-Deep-Convolutional-Neural-Networks-With-A-Practical-Use-Case-In-Tensorflow-And-Keras->4 - 1 Builiding a convnet from scratch->Data preparation
118049,"for base in ['A','C','G','T']:    
    c_s = np.sum(list(ALLFIX_base[base].values()))
    for x in ALLFIX_base[base]:
        print(base, '->', x, int(ALLFIX_base[base][x])/c_s)
        MUT_PROB[base][x]['ALL_FIXED'] = int(ALLFIX_base[base][x])/c_s","MUT_PROB = {'A': {'C': {}, 'G': {}, 'T': {}},
            'C': {'A': {}, 'G': {}, 'T': {}},
            'G': {'A': {}, 'C': {}, 'T': {}},
            'T': {'A': {}, 'C': {}, 'G': {}}
           }

for base in ['A','C','G','T']:    
    c_s = np.sum(list(FFD_base[base].values()))
    for x in FFD_base[base]:
        print(base, '->', x, int(FFD_base[base][x])/c_s)
        MUT_PROB[base][x]['FFD_FIXED'] = int(FFD_base[base][x])/c_s","#variables with value
budget = 50.00
price = 37.50

#if/else statement that works with given variables and computes it
if price <= budget:
    print(""Price:"" , price, ""is within budget:"", budget)
    print(""Buy the product"")
    remaining = budget - price
    budget = remaining
    print(""New Budget amount:"", budget)
else:
    print(""Price:"", price, ""bigger than budget:"", budget)
    print(""DO NOT buy the product!"")",,1->Proportion of nonsynonymous to synonymous mutations (pNS) in serial isolates from patients->GC content,1->Proportion of nonsynonymous to synonymous mutations (pNS) in serial isolates from patients->GC content,"1-12 In-Class Work->Budget    (<=, >=, ==, !=)"
248132,"# and a simple scheduler with an initialization and 2 concurrent tasks
s = Scheduler()
j1 = Job(aprint(""j1""), label=""init"", critical=False, scheduler=s)
j2 = Job(aprint(""j2""), label=""critical job"", critical=True, scheduler=s, required=j1)
j3 = Job(aprint(""j3""), label=""forever job"", critical=False, forever=True, scheduler=s, required=j1)
s.graph()","# let us now add 8 jobs that take 0.5 second each
s = Scheduler(jobs_window=4)

for i in range(1, 9):
    s.add(Job(aprint(""{} {}-th job"".format(watch.elapsed(), i), 0.5)))","df_all_filtered = df_guinea_all[df_guinea_all['Description'].str.contains(""New deaths registered"", na=False)]
print(df_all_filtered.shape)
df_all_filtered.head()",,Readme->README->Other useful features on the `Scheduler` class->Visualization - in a notebook : `Scheduler.graph()`,Readme->README->Other useful features on the `Scheduler` class->Visualization - the long way : `Scheduler.export_as_dotfile()`,New-Checkpoint->Guinea
300112,"import os
import json

DOWNLOAD_DIR = '/imatge/amontes/work/datasets/ActivityNet/v1.3/videos'

videos = os.listdir(DOWNLOAD_DIR)

videos_ids = []
for video in videos:
    videos_ids.append(video.split('.mp4')[0])","#show video
from IPython.display import HTML
import os

video_names = list(filter(lambda s:s.endswith("".mp4""),os.listdir(""./kungfu_videos/"")))

HTML(""""""
<video width=""640"" height=""480"" controls>
  <source src=""{}"" type=""video/mp4"">
</video>
"""""".format(""./kungfu_videos/""+video_names[-1])) #this may or may not be _last_ video. Try other indices","# %load solutions/interact-basic-list/sample-linked-widgets.py
# This example links two selection widgets

options = ['yes', 'no', 'maybe']

drop = widgets.Dropdown(options=options)
radio = widgets.RadioButtons(options=options)

widgets.jslink((drop, 'index'),  
               (radio, 'index'))

display(drop, radio)",,01 Checking Downloaded Videos->Checking Downloaded Videos,"Practice Pytorch->Let's play!
Practice Tensorflow->Let's play!",05->Exercises->interact/interactive and/or other widgets->Choose two widgets and link their values
146708,"interval_type = 'lower'
if interval_type == 'lower':
    ppf = confidence
    t = ss.t.ppf(ppf, len(data))
    top = s / np.sqrt(len(data)) * t
    print(center, center + top)","%%writefile utilities.py

import scipy.stats as ss
import numpy as np

def conf_interval(data, interval_type='double', confidence=0.95):
    '''This function takes in the data and computes a confidence interval
    
    Examples
    --------

        data = [4,3,2,5]
        center, width = conf_interval(data)
        print('The mean is {} +/- {}'.format(center, width))
    
    Parameters
    ----------
        data : list
            The list of data points
        interval_type : str
            What kind of confidence interval. Can be double, upper, lower.
        confidence : float
            The confidence of the interval
    Returns
    -------
    center, width
        Center is the mean of the data. Width is the width of the confidence interval. 
        If a lower or upper is specified, width is the upper or lower value.
    '''

    center = np.mean(data)
    s = np.std(data, ddof=1)
    if interval_type == 'lower':
        ppf = confidence
    elif interval_type == 'upper':
        ppf = 1 - confidence
    else:
        ppf = 1 - (1 - confidence) / 2
    t = ss.t.ppf(ppf, len(data))
    width = s / np.sqrt(len(data)) * t
    
    if interval_type == 'lower' or interval_type == 'upper':
        width = center + width
    return center, width",eigen.largest_eigen(ex5.transpose()),,Lecture 2,Lecture 2,Example With Weird Results->Trying to make a slightly less inconsistent example->Example 5. (all consistent but one entry)
309075,"G.has_edge(2,4)","edges = edg.detect(image, is_lab=False)
show_two_images(image, edges, size=(12, 10))","print(x)
print(t)",,I->Basic Graph Properties->Node Neighbors,4 Edge Detection->Edge processing for lane detection->Test implementation,Python Data Visualization (1)->Matplotlib
241946,cosineSim.shape,sim._results_dict['SCS'],ratio_intl_students_location.sort_values(ascending=False).head(10).plot.bar(),,03->1. Data loading and reading->1.1 Adding own personal ratings to the dataset,Using Jcmpython - The Mie2D-Project->Preparations->Extensions,Homework 2->Solution->Exploratory data analysis:->Discussion
262069,"Left = Line()
Right = Line()
challenge_output = 'challenge_result.mp4'
clip1 = VideoFileClip(""challenge_video.mp4"")
challenge_clip = clip1.fl_image(process_vid) 
challenge_clip.write_videofile(challenge_output, audio=False)","Left = Line()
Right = Line()
video_output = 'result_MZ.mp4'
clip1 = VideoFileClip(""project_video.mp4"").subclip(0,50)
white_clip = clip1.fl_image(process_vid) 
white_clip.write_videofile(video_output, audio=False)","#rename all the columns:
orders.columns = ['order_id','user_id','eval_set','order_number','order_dow','order_hour_of_day',
                  'days_since_prior_order','average_order_hour_of_day']",,Example->Challange Video,Advanced Lane Finding Mz-W Multiview Video,Instacart Project->User-Products Features
245364,"from v1_db import session, Text, Citation","from sqlalchemy import text

sql = text('select count(*) from users')
result = session.execute(sql)
result.first()[0] * 10","# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
h = .02  # step size in the mesh

# Create variables x_min, x_max, y_min, y_max for the bounds of your two observable variables
# (it's a bit confusing because y is your target data)
# YOUR CODE HERE

xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])",,Author Gender Prediction->Extracting author names,Sqlalchemy->SQLAlchemy - using an ORM->try raw SQL,3
371382,"df['Date'] = df['timeStamp'].apply(lambda t: t.date())
df['Date']","dataset['Date'] = dataset.timeStamp.apply(lambda x:x.date())
dataset.Date.value_counts()","a_list = [1, 2, 3]
2 in a_list",,911 Calls Data Capstone Project,911 Calls Eda->911 Calls Data Analysis->Data Analysis,"Slides->What is a `container`?
02-Control Flow
02-Control Flow
02-Control Flow->ANTEA (I)PYTHON INTRODUCTION <br> Getting started with python->Control flow->try/except: catching exceptions"
341058,"gas1.TP = 1200, 101325","gas1.TP = 1200, 101325","t1 = 'a', #<--- Note the comma, this is a tuple
type(t1)",,Python Tutorial->Cantera Tutorial: Python->Setting the State,Python Tutorial->Cantera Tutorial: Python->Setting the State,Chapter13-Tuples->Chapter 12 - Tuples
478032,"# create figure for plots
fig = plt.figure(figsize=(12, 8))
# run experiments and add the resulting MSE to the plot function
for bias in bias_init_search_array:
    print('for non-shared 28x28 bias %f' %bias)
    train_mse_for_iterations, _ = run_convnet(
                                            learning_rate, 
                                            num_epochs, 
                                            train_set_x,  
                                            num_filters, 
                                            batch_size, momentum_type='sgd', 
                                            bias_type='non-shared', bias_init=bias)
    
    print('Training MSE after training is done: %f' %train_mse_for_iterations[-1])
    plt.plot(train_mse_for_iterations, label = 'bias(28x28): '+str(bias))
plt.xlabel('iterations')
plt.ylabel('Training MSE')
plt.title('Training MSE over Iterations With Different NonShared Biases')
plt.legend()    
plt.show()","train_valid_accuracy = []
test_accuracy = []

x_train2 = np.concatenate((x_train,x_valid),axis=0)
y_train2 = np.concatenate((y_train, y_valid),axis=0)
# Train model on Training + validation data
with tf.Session() as sess:
    #initialize vairables and queue
    sess.run(init_op)

    for epoch in range(NUM_EPOCHS):
        # shuffle training data
        
        for batch in range(BATCHING_ITER):
            x_batch, y_batch = shuffle(x_train2, y_train2, n_samples=BATCH_SIZE, random_state=batch)
            _, c = sess.run([optimizer, cost], feed_dict={x_: x_batch, y_: y_batch})
        
        # Evaluate accuracy at each epoch
        train_valid_accuracy.append(accuracy.eval({x_: x_train2, y_: y_train2}))
        test_accuracy.append(accuracy.eval({x_: x_test, y_: y_test}))
    
    V = sess.run(weights[""out""])
    sess.close()
    
# extended data set + preprocessing
t=list(range(0,NUM_EPOCHS))
plt.plot(t, train_valid_accuracy, 'r--',t,test_accuracy, 'b--')
plt.xlabel(""epoch"")
plt.ylabel(""accuracy"")
plt.legend([""Training set"",""Test set""])
plt.show()",data.career_level_user.value_counts().sort_index().to_dict(),,Assignment3->Optimization of Fully Convolutional Neural Networks->1. Parameter Initialization,Classification Of Ted Talk Category->Part 1: Download GloVe embeddings->Training with train data extended,Recruiters Analysis->Let's study each one of the columns separately->Career level
350019,"idxs = np.random.randint(0, len(trn), 10)","one_temp_idx = np.random.randint(0,high=n_trn_ones)    ##random template selection
zero_temp_idx = np.random.randint(0,high=n_trn_zeros)
one_temp = trn_ones[one_temp_idx]
zero_temp = trn_zeros[zero_temp_idx]

##view your templates
plt.subplot(1,2,1)
plt.imshow(one_temp,cmap='gray')
plt.title('template for one')
plt.subplot(1,2,2)
plt.imshow(zero_temp,cmap='gray')
plt.title('template for zero')","# add relevant demographics data to walking activity feature data frame
Demographics_df = demographics_df.drop(['recordId','createdOn','appVersion','phoneInfo','are-caretaker', 'deep-brain-stimulation',
                                        'education','employment','health-history','healthcare-provider',
                                        'home-usage','last-smoked','maritalStatus','medical-usage',
                                        'medical-usage-yesterday','packs-per-day',
                                        'past-participation','phone-usage','race','smartphone',
                                        'smoked','surgery','video-usage','years-smoking',
                                        'diagnosis-year','medication-start-year','onset-year'],axis=1)
Combined_df = pd.merge(Accelerometer_df, Demographics_df, on='healthCode')",,5->Speeding up feature generation->Numba polynomial features->Row-Major vs Column-Major Storage,Diy Classifier->A DIY classifier->The feature space,"Get Rawdata->Cross-reference Accelerometer data with Demographics data
Get Rawdata->Pickle the DataFrames for Step 3!"
330627,"df_cap.sort('phat', ascending=False)[:5]","utilization_df = pd.merge(
    plant_gen_df,
    plant_cap_df,
    on=[""year"", ""plant_id""],
    how='outer',
    suffixes=['_gen','_cap']
).sort_values('year')","import string

tree_model_string = model.trees[0].toDebugString
for index, column_name in enumerate(feature_column_names):
    tree_model_string = tree_model_string.replace(
        'feature {} '.format(index),
        '{} '.format(column_name)
    )

print(tree_model_string)",,Model->Geodemographic Segmentation Model->Parameters Interpretation->Test model with test data,Plants->Transforming plant utilization data->Utilization,04-Random-Forests->Model inspection
447219,"myAr = makeDesign(df,4,firstCon=1,secondCon=0)","from nistats.design_matrix import make_design_matrix

TR = 2
measurement_times = np.arange(0., event_times.max() + 10, TR)
design_regular = make_design_matrix(measurement_times, paradigm=regular_paradigm, period_cut=64)
design_jittered = make_design_matrix(measurement_times, paradigm=jittered_paradigm, period_cut=64)","conn = pg8000.connect(database=""menupages"")",,Manual Analysis->simple analysis of timecourses using z-transformation and averaging->Face vs. Hands Analysis,Gp->Estimating the HRF in a GLM,10 Python To Sql-Checkpoint->Putting data into SQL->Inserting both restaurants AND their cuisines->An aside:
350856,df_ted['tags'].values[:5],df_ted['related_talks'].values[0],"#Select All Passenger Ships in drop down selection
ship_selector=Select(browser.find_element_by_id(""ContentPlaceHolder1_ship_dlist""))
ship_selector.select_by_visible_text(""All Passenger Ships"")
ship_selector=Select(browser.find_element_by_id(""ContentPlaceHolder1_ship_dlist""))
print(ship_selector.all_selected_options[0].text)",,"Ted Talks Proj Indiv Artem Kuznetsov->Part 8. Cross-validation, hyperparameter tuning->tags",Ted Talks Proj Indiv Artem Kuznetsov->Part 2. Exploratory data analysis->Duplicates check,"Get New Schedule->Select All Passenger Ships option
Get New Schedule-Checkpoint->Select All Passenger Ships option"
101628,"def do_cross_validation(X, y,clf, n_folds=5):
    cv = KFold(len(y), n_folds)
    accuracies = []
    for train_ind, test_ind in cv: 
        clf.fit(X[train_ind], y[train_ind])
        predictions = clf.predict(X[test_ind])
        accuracies.append(accuracy_score(y[test_ind], predictions))
    avg = np.mean(accuracies)
    return avg","def do_cross_validation(X, y, n_folds=5, verbose=False):
    """"""
    Perform n-fold cross validation, calling get_clf() to train n
    different classifiers. Use sklearn's KFold class: http://goo.gl/wmyFhi
    Be sure not to shuffle the data, otherwise your output will differ.
    Params:
        X.........a csr_matrix of feature vectors
        y.........the true labels of each document
        n_folds...the number of folds of cross-validation to do
        verbose...If true, report the testing accuracy for each fold.
    Return:
        the average testing accuracy across all folds.
    """"""
    ###TODO
    ###
    cv = KFold(len(y), n_folds)
    accuracies = []
    index=0
    for train_idx, test_idx in cv:
        clf = get_clf()
        clf.fit(X[train_idx], y[train_idx])
        predicted = clf.predict(X[test_idx])
        acc = accuracy_score(y[test_idx], predicted)
        if verbose:
            print 'fold %d accuracy=%.4f' %(index,acc)
            index+=1
        accuracies.append(acc)
    avg = np.mean(accuracies)
    return avg

print('average cross validation accuracy=%.4f' %
      do_cross_validation(X, y, verbose=True))","import math
doReg(df.map(lambda x: LabeledPoint(math.log(x[5]),x[1:4])))",,"Project->Twitter Analysis on Marijuana
Project->Twitter Analysis on Marijuana",A3,W261-Spring-2016-End Of Term Exam-Krishnawami->ET:16
344287,"%pylab inline
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt","import numpy as np
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt","# Specify types of images to download
download_info = {
    # Imagenet_ID: type_of_image 
    'n03325403': 'chair',
    'n03015149': 'couch',
    'n04379243': 'table',
    'n03636649': 'lamp',
    'n03225988': 'bed'
}",,Data Analyst Nd Project2->3. Analyzing of the data->We can construct a graph showing the number of the passengers fall into each category.,"Physics 12 Python Tutorial 3->Physics 12 Python Tutorial 3
Jupyter-Basics->Jupyter Basics->Plotting->Matplotlib",Mobile Net V2 Fine Tune->Fine-tune MobileNetV2->Load data
247270,"graph_x = list(sms_fifteen_largest.index)
graph_y = list(sms_fifteen_largest.get_values())
# Add spacing between bars
spacing = np.arange(len(graph_x))
plt.figure(figsize=(16,5))
# Create the bar graph showing word frequency
plt.bar(spacing,graph_y,width=0.5,align='center',alpha=1.0)
plt.xticks(spacing,graph_x)
# Set axes and title
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.title('Frequency of Top 15 Words Used in Spam SMS Messages')
plt.show()","categories_names = list(categories['Category']) # X-ticks labels
y = list(categories['Count']) #Y-values
x = np.arange(len(categories_names)) #X-values

plt.figure(figsize=(16,12))
plt.bar(x,y,alpha=.5,color='green',align='center')
plt.xticks(x,categories_names,rotation='vertical')
plt.title(""Histogram of Categories of Crime"")
plt.xlabel('Categories')
plt.ylabel(""Frequency"")
plt.margins(0.01,0)
plt.ylim(0,450000)
plt.show()","rules.run(Chem.MolFromSmiles(""c1cc(=O)[nH]c2nccn12""))",,Spamifier->SPAMIFIER - A Spam Text Classifier->Introduction->Tutorial Content->SMS Data,Week 3 - Let The Data Science Begin->Part 1: Getting the dataset,03 Rules->**`rules`**: run structure-normalisation transformations->Usage
21070,df_train.sample(5),"X_train_normalized, X_valid_normalized, X_test_normalized = normalize_features_sampl_by_f(X_train_pairs, X_valid_pairs, X_test_pairs)","import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets
from sklearn.neighbors import KNeighborsClassifier
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from KNN_wine import KNN_plot_wine",,"Titanic->Import Data
House Price Regression->Data Exploration
Testing And Training Data->Read in and examine training data",Svm Rbf Predictions From Sequence Features-Checkpoint,Knn->Toy exercise
345155,datetime_analys = pd.to_datetime(monty_data[(monty_data['MasterClass']==500) | (monty_data['MasterClass']==600) | (monty_data['MasterClass']==800)]['Dispatch Date / Time'])#Takes too long to process,"pd.options.display.max_rows = 6
data","%time

# Earth's radius, in meters
EARTH_RADIUS = 6378137.

def lon_to_mercx(lon):
    x = EARTH_RADIUS * np.radians(lon)
    return x

def lat_to_mercy(lon, lat):
    x = lon_to_mercx(lon)
    y = 180. / np.pi * np.log(np.tan(np.pi / 4. + lat * (np.pi / 180.) / 2.)) * x / lon
    return y

def wkt_to_x(pt):
    return lon_to_mercx(wkt.loads(pt).x)

def wkt_to_y(pt):
    point = wkt.loads(pt)
    return lat_to_mercy(point.x, point.y)

points['y'] = np.vectorize(wkt_to_y)(points['location'].values)
points['x'] = np.vectorize(wkt_to_x)(points['location'].values)
points.tail()",,Finding Crime Patterns In Montgomery County->Analysing the distribution of violent crimes.,Nyc Restaurant->NYC Restaurant,Post Gis Demo->Intake-Postgres Plugin: PostGIS Demo->Geometry (point) data
221784,"from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators = 1000, random_state = 18, max_depth = 5) # We chose 1000 trees in the forest, 18 is the seed for the forest and we chose the maximumn depth of the trees to be 5.
ouput_random_forest = model.fit(X_train, y_train)
random_forest_predictions = model.predict(X_test)","X_train_1 = X_train[:10000]
X_test_1 = X_test[:10000]
Y_train_1 = Y_train[:10000]
Y_test_1 = Y_test[:10000]
#Import Library
from sklearn.ensemble import RandomForestRegressor 
# Create Random Forest object
model= RandomForestRegressor(n_estimators=10)
# Train the model using the training sets and check score
model.fit(X_train_1, Y_train_1)
#Predict Output
prediction3= model.predict(X_test_1)",similar_to_movie1 = movieMatrix.corrwith(movie1_user_ratings),,Machine Learning->Random Forest Regression,Predicting Number Of Arrivals In Krishi Marata Vahini Dataset->Model Building->The dictionary is then mapped to the test data and the proper values are filled in the Group column.,Movie Recommenders->Item-Item Collaborative Filtering - Recommending Similar Movies->First Approach - Pearson
20046,"from sklearn.metrics import confusion_matrix
rf.fit(X_train, Y_train)
confusion_matrix(Y_test, rf.predict(X_test))","from sklearn.metrics import confusion_matrix
rf.fit(X_train, Y_train)
confusion_matrix(Y_test, rf.predict(X_test))","y_values = df.iloc[:, -1]
print(y_values.shape)",,Titanic->Titanic dataset exploration->Model selection->Random forrest,Titanic->Titanic dataset exploration->Model selection->Random forrest,"Lasso Feature ""Shrinking""->Sparsify features Using Lasso->Read in Data"
394069,"import pandas as pd
import seaborn
import numpy as np
from pandas.tools.plotting import scatter_matrix
import statsmodels.api as sm
%pylab inline","import pandas as pd
import seaborn
import numpy as np
from pandas.tools.plotting import scatter_matrix
import statsmodels.api as sm
%pylab inline","res = ana.exp_res('2016-6-09/12')
res.seq_latency()
res.avg_latency()",,Final->Credit Card Data,Final->Credit Card Data,Exp-Res->2016-7-30->CDF for Captioning
438191,"input_shape = (IMG_SIZE, IMG_SIZE, IMG_SLICES)

# First Layer
num_filter_1 = 16
filter_length_col_1 = 5
filter_length_row_1 = 5
pooling_1 = (4, 4)

#Second Layer
num_filter_2 = 32
filter_length_col_2 = 5
filter_length_row_2 = 5
pooling_2 = (4, 4)","filter_1_size_w_h = 5
num_of_filter_1 = 32

max_pool_size = 2

filter_2_size_w_h = 3
num_of_filter_2 = 16

dropout_rate = 0.2

dense_1_unit = 130
dense_2_unit = 50

optimizer = 'rmsprop'","print(""Bonjour World!"")
print(""How are you?"")",,Classification->Build Convolutional Neural Network->Prediction using Test Data Split,Ocr-Cnn->Building model->Separate the label column from the pixel columns,Introductory Exercises->Introduction to print->First Exercise
408625,"# Isolate in function, to keep variables isolated
def kovrigin_fig2_fig5():
    # From fig 2 + 5
    # Set sfrq
    g = {'1H':42.576, '15N':4.3156, '13C':10.705}
    w0_1H_1 = 800. # MHz
    B0_1 = w0_1H_1 / g['1H']
    w0_1H_2 = 500. # MHz
    B0_2 = w0_1H_2 / g['1H']
    print(""B0_1: %.1f T, B0_2: %.1f T""%(B0_1, B0_2))
    print(""w0_1H_1: %.1f MHz, w0_1H_2: %.1f MHz""%(w0_1H_1, w0_1H_2))
    # Calculate for isotope 15N
    isotope = '15N'
    w0_isotope_1 = g['15N']*B0_1
    w0_isotope_2 = g['15N']*B0_2
    print(""w0_%s_1: %.1f MHz, w0_%s_2: %.1f MHz""%(isotope, w0_isotope_1, isotope, w0_isotope_2))
    # Set compare value
    R20_c = 15. # rad/s
    kex_c = 1000. # rad/s
    pA_c = 0.95
    print(""\nR20_c: %.1f rad/s, kex_x: %.1f rad/s, pA_c: %.2f""%(R20_c, kex_c, pA_c))
    dw_rad_1H_1 = 1500 # rad/s
    dw_rad_1H_2 = w0_1H_2 / w0_1H_1 * dw_rad_1H_1 # Just scale accoring to sfrq
    print(""\ndw_rad_1H_1: %.1f rad/s, dw_rad_1H_2: %.1f rad/s""%(dw_rad_1H_1, dw_rad_1H_2))
    # Calculate dw in ppm for 1H
    dw_1H_1 = dw_rad_1H_1 / (w0_1H_1*2*np.pi) # ppm
    dw_1H_2 = dw_rad_1H_2 / (w0_1H_2*2*np.pi) # ppm
    print(""dw_1H_1: %.2f ppm, dw_1H_2: %.2f ppm""%(dw_1H_1, dw_1H_2))
    # Calculate dw in ppm for 15N
    dw_isotope_1 = dw_rad_1H_1 / (w0_isotope_1*2*np.pi) # ppm
    dw_isotope_2 = dw_rad_1H_2 / (w0_isotope_2*2*np.pi) # ppm
    print(""dw_%s_1: %.2f ppm, dw_%s_2: %.2f ppm""%(isotope, dw_isotope_1, isotope, dw_isotope_2))
    
# Calc
kovrigin_fig2_fig5()","# Isolate in function, to keep variables isolated
def baldwin(relax_time=0.02, 
            num=35, isotope = '1H', cpmg_e = 100.,
            w0_1H = 200., dw = 0.5, R20 = 10.):
    # Determine if to use logspace
    logspace = True
    # Range of values
    kex_min = 10
    kex_max = 5000
    pB_min = 0.1
    pB_max = 40
    if logspace:
        kex_range = np.logspace(np.log10(kex_min), np.log10(kex_max), num=num)
        pB_range = np.logspace(np.log10(pB_min), np.log10(pB_max), num=num)
    else:
        kex_range = np.linspace(kex_min, kex_max, num=num)
        pB_range = np.linspace(pB_min, pB_max, num=num)

    # Make meshgrid
    X,Y = np.meshgrid(kex_range, pB_range, indexing='xy')
    # Calculate Z
    Z_CR72 = np.zeros(X.shape)
    Z_B14 = np.zeros(X.shape)

    # Get shape of matrix
    # Loop over x. The columns
    for i in range(len(kex_range)):
        # Loop over y. The rows
        for j in range(len(pB_range)):
            # Get value, by row/column indexing
            kex=X[j][i]
            # Get population of excited state
            pB=Y[j][i]
            # Calculate for pA
            pA=1.0 - pB/100.
            x_cpmg_frqs, y_CR72, y_B14, w0_isotope_s1, w0_isotope_s2 = model_calc(
                                            model='CR72_B14', cpmg_e=cpmg_e, isotope=isotope, relax_time=relax_time, 
                                            w0_1H_s1=w0_1H, w0_1H_s2=w0_1H, R20_s1=R20, R20_s2=R20, 
                                            dw_s1=dw, dw_s2=dw, pA_s1=pA, pA_s2=pA, 
                                            kex_s1=kex, kex_s2=kex)
            # Fill value
            Z_CR72[j][i] = y_CR72[-1]
            Z_B14[j][i] = y_B14[-1]
            # Test
            #if y_B14[-1] > 65.0:
            #    print(""kex=%.1f, pB=%.1f, pA=%.2f, R2eff=%.1f, i=%i, j=%i""% (kex, pB, pA, y_B14[-1], i, j))

    # Figure
    fig, axes = plt.subplots(2, 2, figsize=(5, 4))
    #fig, axes = plt.subplots(2, 2, figsize=(10, 8))
    (ax1, ax2),(ax3, ax4) = axes

    # Plot for B14
    im1 = ax1.pcolormesh(kex_range, pB_range, Z_B14, cmap='autumn_r', vmin=0, vmax=70)
    # Plot for CR72
    im2 = ax2.pcolormesh(kex_range, pB_range, Z_CR72, cmap='autumn_r', vmin=0, vmax=70)
    # Plot for CR72 - B14
    Z3 = Z_CR72-Z_B14
    # Fill with nan to make white
    Z3[Z3 < 0.1] = np.nan

    im3 = ax3.pcolormesh(kex_range, pB_range, Z3, cmap='autumn_r', norm=colors.LogNorm(vmin=0.1, vmax=10) )
    # Plot for (CR72 - B14)/B14 * 100
    Z4 = (Z_CR72 - Z_B14)/Z_B14 * 100
    im4 = ax4.pcolormesh(kex_range, pB_range, Z4, cmap='autumn_r', vmin=0, vmax=14)

    # Set label
    ax1.set_title(""B14"")
    ax2.set_title(""CR72"")
    ax3.set_title(""CR72-B14"")
    ax4.set_title(""(CR72 - B14)/B14 * 100"")
    ax1.set_xlabel(""k$_{ex}$ rad/s"")
    ax2.set_xlabel(""k$_{ex}$ rad/s"")
    ax3.set_xlabel(""k$_{ex}$ rad/s"")
    ax4.set_xlabel(""k$_{ex}$ rad/s"")
    ax1.set_ylabel(""P$_B$(%)"")
    ax3.set_ylabel(""P$_B$(%)"")

    # If log scale
    if logspace:
        # Loop over axes
        for ax in axes.ravel():
            # x
            ax.set_xscale('log') 
            ax.xaxis.set_major_formatter(ScalarFormatter())
            # y
            ax.set_yscale('log') 
            ax.yaxis.set_major_formatter(ScalarFormatter())

    # Adjust layout
    plt.tight_layout()
    #plt.tight_layout(pad=2.0)
    
    # Add colorbar
    # First row
    plt.colorbar(im1, ax=[ax1])
    plt.colorbar(im2, ax=[ax2], label=""R$_{2}$,eff rad/s"")
    #plt.colorbar(im1, ax=[ax1, ax2], label=""R$_{2}$,eff rad/s"")
    # Second row
    #plt.colorbar(im3, ax=[ax3], label=""R$_{2}$,eff rad/s"")
    plt.colorbar(im3, ax=[ax3])
    plt.colorbar(im4, ax=[ax4], label=""Relative difference (%)"")",dict_to_save == dict_loaded,,"Cpmg Nmr Relax Interactive->Jump to interactive plotting widgets->Rotten banana widget <a name=""banana""></a>","Cpmg Nmr Relax Interactive->Jump to interactive plotting widgets->Model differences <a name=""modeldiff""></a>",Cheatsheet->Code->Cluedo 01->correlation dataframe to correlation pairs
475569,"hover = HoverTool(tooltips=[
    (""Month"", ""@month_label""),
    (""Price"", ""@price""),
])
tools = [
    hover,
    CrosshairTool(),
    PanTool(),
    ResetTool(),
    ResizeTool(),
    SaveTool(),
    UndoTool(),
    WheelZoomTool(),
]
housing_figure = figure(
    plot_width=FIGURE_WIDTH,
    plot_height=FIGURE_HEIGHT,
    x_range=unemployment_figure.x_range,
    x_axis_type=""datetime"",
    tools=tools,
    title=""House Price Index"",
)
line = housing_figure.line(""month_data"", ""price"",
                           source=housing_source,
                           line_color=HOUSING_COLOR)","hover = HoverTool(tooltips=[
    (""Month"", ""@month_label""),
    (""Price"", ""@price""),
])
tools = [
    hover,
    CrosshairTool(),
    PanTool(),
    ResetTool(),
    ResizeTool(),
    SaveTool(),
    UndoTool(),
    WheelZoomTool(),
]
housing_figure = figure(
    plot_width=FIGURE_WIDTH,
    plot_height=FIGURE_HEIGHT,
    x_range=unemployment_figure.x_range,
    x_axis_type=""datetime"",
    tools=tools,
    title=""House Price Index"",
)
line = housing_figure.line(""month_data"", ""price"",
                           source=housing_source,
                           line_color=HOUSING_COLOR)","v_w_more_views_less_comments = df[(df['comment_count'] > 600000)& (df['views'] > 90000000)]
v_w_more_views_less_comments",,Portland-Unemployment-With-Holoviews->Plotting->Some Constants,Portland-Unemployment-With-Holoviews->Plotting->Some Constants,You Tube Us Analysis->Relationships Between Views and Likes/Dislikes/Comments on YouTube->Views vs. Comments->Videos with over 0.6M comments and over 90M views
218792,"healthy_food_df.groupby([healthy_food_df.datetime.dt.year,
                         healthy_food_df.datetime.dt.month]).agg('count')['asin'].plot(fontsize=18).set_title('Amount of reviews about healthy food products over time', fontsize=22)
plt.xlabel('Time', fontsize=20)
plt.ylabel('Amount of reviews', fontsize=20)
plt.savefig(PLOT_DIR + 'reviews_healthy_food.pdf', bbox_inches='tight')","healthy_sports_df.groupby([healthy_sports_df.datetime.dt.year,
                           healthy_sports_df.datetime.dt.month]).agg('count')['asin'].plot(fontsize=18).set_title('Number of reviews about healthy sport products over time', fontsize=22)
plt.xlabel('Time', fontsize=20)
plt.ylabel('Number of reviews', fontsize=20)
plt.savefig(PLOT_DIR + 'reviews_healthy_sports.pdf', bbox_inches='tight')","final_data = data.copy()
final_data['distance'] = np.nan #fill with NaNs to start
final_data",,"Report->Was the healthy lifestyle trend predictable?->Enrich, filter and transform the data->Reviews over time","Report->Was the healthy lifestyle trend predictable?->Enrich, filter and transform the data->Reviews over time",Distances->Distance from high school to college campuses->Add the Distance Data to our Dataframe
469016,"# Inverses the fast fourier transform
yinv = np.fft.ifft(Y)
newsignal = yinv.real.astype(np.int16)
newsignal = newsignal.copy(order='C')","# transformation from FD to TD
FD2TD = np.fft.ifft(FDFD,axis=0)
FD2TD = np.real(FD2TD)
tmp1 = None

# extract FD2TD data up to TmaxTD
tmp1 = FD2TD[1:nmaxFD,:]
FD2DTD = None

# clean memory
tmp = None","def print_entities(matrix, name, label=['Uber/0', 'Lyft/1', 'USA/2', 'NewYork/3', 'France/4', 'Paris/5'],lrange=[0,1,2,3,4,5], domain_mi=0, domain_ma=1):
    colormap = toyplot.color.brewer.map(""Paired"",domain_min=domain_mi, domain_max=domain_ma)    
    tlocator = toyplot.locator.Explicit(lrange)
    llocator = toyplot.locator.Explicit(lrange, label)
    canvas, table = toyplot.matrix((matrix, colormap), tlocator=tlocator, llocator=llocator, label=name, width=350);
    table.body.gaps.columns[...] = 2
    table.body.gaps.rows[...] = 2",,Fourier->Sound waves as equations->Inverse Fast Fourier Transform,Germaine Te Analytical Td Conv Fd,Own Nlp->3. Tensorlog for KB reasoning
101136,"mod_grad = mod.fit(X_train, grad_train)
mod_ret = mod.fit(X_train, ret_train)
grad_pred = mod_grad.predict(X_test)
ret_pred = mod_ret.predict(X_test)","mod_grad = mod.fit(X_train, grad_train)
mod_ret = mod.fit(X_train, ret_train)
grad_pred = mod_grad.predict(X_test)
ret_pred = mod_ret.predict(X_test)","datasets = {}
for dataset_name in ('en_framenet', 'en_masc_crowdsourced', 'en_ritter_sst', 'eu_semcor', 'all_rounds'):
    dataset = pd.read_csv(""../data/feats/{}.feats"".format(dataset_name), sep=""\t"")
    dataset['dataset_name'] = dataset_name
    datasets[dataset_name] = dataset

D_all = pd.concat(datasets.values())",,Project->Table of Contents->Predicting:,Project->Table of Contents->Predicting:,Data Exploration
419493,"img = cv2.imread('res.jpg')
cv2.imwrite('res_orig.jpg',img)

grayscaled = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
retval, threshold = cv2.threshold(grayscaled, 80, 255, cv2.THRESH_BINARY)
cv2.imwrite('res_threshold.jpg',threshold)
# cv2.imshow('original',img)
# cv2.imshow('Binary threshold',threshold)
# cv2.waitKey(0)
# cv2.destroyAllWindows()","img = cv2.imread('../images/water_coins.jpg')
gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# Binarize gray-scaled image
# cv2.threshold(grayimg, thresh, maxval, method)
# Args:
# - grayimg: input image (single-channel 8-bit or 32-bit floating point)
# - thresh: threshold value
# - maxval: maximum value to use with the cv2.THRESH_BINARY and cv2.THRESH_BINARY_INV thresholding method
# - method: thresholding types
#   - cv2.THRESH_BINARY
#   - cv2.THRESH_BINARY_INV
#   - cv2.THRESH_TRUNC
#   - cv2.THRESH_TOZERO
#   - cv2.THRESH_TOZERO_INV
#   - cv2.THRESH_OTSU
# Returns:
# - ret: used threshold value.
# - thresh: thresholded image
ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)

plt.figure(figsize=(10, 6))
plt.subplot(1,2,1); plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.subplot(1,2,2); plt.imshow(thresh, cmap='gray')","# Import Histogram, output_file, and show from bokeh.charts
from bokeh.charts import Histogram, output_file, show

# Make the Histogram: p
p = Histogram(df, 'female_literacy', title='Female Literacy', bins=40)

# Set axis labels
p.xaxis.axis_label = 'Female Literacy (% population)'
p.yaxis.axis_label = 'Number of Countries'

# Specify the name of the output_file and show the result
output_file('histogram.html')
show(p)",,Dirtiness Detector->Extract germs from image based from threshold,2 Segmentation->[OpenCV-Python Tutorial] Segmentation->1. Finding an approximate estimate of the coins,"13->3. High-level Charts->Histograms - Video->Controlling the number of bins
3 High-Level Charts->Interactive Data Visualization with Bokeh(3) - High-level Charts->Histograms->Controlling the number of bins"
488046,"x = np.linspace(0, 10)    # generate 50 points along the x
y = np.cos(x)             # calculate y=cos(x)

plt.plot(x, y)            # plot the graph y = x","co = df1.Interest
#print (co)
fig = plt.figure(figsize=(10, 8))
co.plot.hist(stacked=True, bins=70)
plt.xlabel('Interest (BP)', fontsize=12)
plt.show()","features_by_kbest =get_kbest_features(df)[:6]
common_features = [i for i in features_by_kbest if i  in  features_by_rfe]",,Jupyter->Welcome to Jupyter Notebook->The only thing you must do for now is play around and see what works!! See what you can do,Qmii Data Science Chris Preuss Part 1->Bank Loan Calculations->Frequency of Loans,Submission-Revision->Recursive Feature Elimination
272580,"def rmse(x, y): return math.sqrt(((x-y) ** 2).mean())

m = RandomForestRegressor(n_jobs=-1)
%time m.fit(X_train, y_train)

print(f'RMSE train: {rmse(m.predict(X_train), y_train)}')
print(f'RMSE val: {rmse(m.predict(X_valid), y_valid)}')
print(f'R^2 val: {m.score(X_valid, y_valid)}')","def rmse(x,y): return math.sqrt(((x-y)**2).mean())

def print_score(m):
    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid), m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)","N = dist.Normal(mu, sigma)
N",,Lesson2->Lesson 2->00:00:45 - Review->00:17:32 - Measuring overfitting->00:23:56 - Audience questions,Ml-Lecture3->Machine Learning Lecture 3->Validation is one of the important part->LastLec: Ran the Random Forest Regressor,"03-Common Distributions->Common Probability Distributions->TensorFlow
03-Common Distributions->Common Probability Distributions->2 Continuous probability distributions->2.4 Beta distribution"
158388,"### find the mean values for the subset of movies that belong to the Drama genre
drama_movies = movie_genre[movie_genre['g1'] == 'Drama']

### group the mean values by 'lustrums'
drama_means = drama_movies.groupby('lustrums').mean()",df_drama_men.head(2),"fig = plt.figure(figsize=(8,4))
ax = fig.add_subplot(111)
ax.errorbar(ts, x_dat, yerr=dx, fmt="".k"")
ax.set_xlabel('time (sec)')
ax.set_ylabel('position (cm)');",,Investigate A Dataset->EXPLORATORY DATA ANALYSIS->Q1b: Which genres receive the highest popularity ratings?,Aflugel04-Notebook2->LDA->LDA - TF-IDF,Fitting Curve->Example: fitting a curve to data->simulate data
222738,"#make sure user ids in each dataframe match up
df2['user_id'].index.name == countries_df['user_id'].index.name","df2 = df.loc[:,['fixed.acidity',""volatile.acidity"",""chlorides"",'total.sulfur.dioxide',""pH"",'sulphates','alcohol','quality']]
df2","ggplot(aes(x = 'rating_value', fill = 'isDog'), data = dog_df) +\
    geom_histogram(binwidth = 0.2, alpha = 0.5, color = 'black') +\
    xlim(0,2) +\
    scale_fill_manual(values = ['#664343','#FC913A']) +\
    ggtitle('Rating score in tweets')",,Analyze Ab Test Results->Analyze A/B Test Results,Analysis Of Wine Quality Data->Linear Regression->Drop unsignificant features->Model 2 Feature selection for K-NN,Wrangle Act->Assessing data->Dog dataframe
123876,"words = set([""the"", ""of"", ""to"", ""a""])",wordSet,"renamed_patients = temp_df[patients_to_use]
# for col in list(renamed_patients):
#     if col[0:12] in list(rt):
#         print(col+'-R')
renamed_patients.rename(lambda col: col+'-t1' if col[0:12] in list(t1) else col, axis='columns', inplace=True)
renamed_patients.rename(lambda col: col+'-t2' if col[0:12] in list(t2) else col, axis='columns', inplace=True)
renamed_patients.rename(lambda col: col+'-t3' if col[0:12] in list(t3) else col, axis='columns', inplace=True)
renamed_patients.rename(lambda col: col+'-t4' if col[0:12] in list(t4) else col, axis='columns', inplace=True)
renamed_patients",,Cscw,Tf-Idf First Walkthrough-Checkpoint->TF-IDF: First Walkthrough->Corpus Tokenizing,Select Samples-Part2->2018-04-16 Filtering data
440980,"from statsmodels.formula.api import logit
model0=' ~ majmin_tract'
model1=' ~ majmin_tract + C(income_cat, Treatment(""income_high""))'
model2=' ~ majmin_tract + C(income_cat, Treatment(""income_high"")) + C(unemp_cat, Treatment(""unemp_belownatl""))'
model3=' ~ majmin_tract + C(income_cat, Treatment(""income_high"")) + C(unemp_cat, Treatment(""unemp_belownatl"")) + C(vacancy_cat, Treatment(""vacancy_low""))'
model4=' ~ majmin_tract + C(income_cat, Treatment(""income_high"")) +                                              C(vacancy_cat, Treatment(""vacancy_low""))'
model5=' ~ majmin_tract +                                           C(unemp_cat, Treatment(""unemp_belownatl"")) + C(vacancy_cat, Treatment(""vacancy_low""))'","# dictionary to plot bar graph
classifiers = {'Logistic Regression':model1_acc,
              'K Nearest Neghbour':model2_acc,
              'Gaussian Naive Bayes':model3_acc,
              'Decision Tree':model4_acc,
              'Random Forest':model5_acc}","import cPickle

fileName = '../data/svhn_64.p'
print(""Loading {}..."".format(fileName))

with open(fileName) as f:
    svhn = cPickle.load(f)",,"Dr Dc Crime->Crime and Race in Washington, D.C.->Cluster Analysis",Titanic Case Study Part2->Comparing Models (Plotting accuracy),03 Writing Your Own Dataset-Answer Key->Writing a custom dataset
147352,"cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))","# loss
cross_entropy = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))","# Set of parameters to test in Grid Search
parameters = [{'C':[0.1,1,10,100,500,800], 'solver':['lbfgs'], 'fit_intercept':[True]},
              {'C':[0.1,1,10,100,500,800], 'solver':['newton-cg'], 'fit_intercept':[True]},
              {'C':[0.1,1,10,100,500,800,1000], 'solver':['liblinear'], 'penalty':['l1', 'l2'], 'fit_intercept':[True]}]
# Instantiating and fitting model through grid search
grid_logR = GridSearchCV(logR, param_grid=parameters)
grid_logR.fit(X_rus, y_rus)

# Printing the best score from the model
print('Best Score:', grid_logR.best_score_)

# Saving and printing the best parameters from the model
best_params = grid_logR.best_params_
print('Best Parameters:', best_params)",,"Tf Presentation->Introduction to Tensorflow->Simple MNIST recognition (TensorFlow's ""Hello world"")->Training
Mnist->1 Layer network->Calculate error at the output using softmax cross entropy
Dobler 072617 Part2->Handwritten digit recognition with CNNs
Sklearn&Tensorflow->PCA with sklearn - Intro tensorflow->Part 1: sklearn->2) Linear Regression->Generalities->Create x1 and x2 variables
Getting Started With Tensorflow - Linear Regression->Model definition->Loss function definition",3->Simple MNIST Handwritten digits detection->Loss Function,"Capstone 3 Income Data-Copy1->Gender Pay Gap: Predicting income based on demographic data->Uploading, Cleaning, and Describing Data->Logistic Regression"
415451,"from __future__ import print_function
import keras
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D
from keras.layers.normalization import BatchNormalization
from keras.regularizers import l2

# Loads the CIFAR dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Display our data shape/dimensions
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# Now we one hot encode outputs
num_classes = 10
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)","from __future__ import print_function
import keras
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D


# The data, shuffled and split between train and test sets:
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

# Convert class vectors to binary class matrices.
num_classes = 10

y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)","class Line():
    
    def __init__(self):
        
        self.detected = False
        
        self.recent_xfits = []
        self.best_xfit = None
        
        self.current_fit_coeffs = [np.array([False])]
        self.best_fit_coeffs = None
                
        self.radius_of_curvature = None
        self.line_base_position = None
        
        self.allx = None
        self.ally = None
        
def verify_lane_width(left, right):
    print(left-right)
    if np.absolute(left - right) > 1100 | np.absolute(left - right) < 900:
        return True
    else:
        return False
    
def annotate(img, left_curverad, right_curverad, camera):
    font = cv2.FONT_HERSHEY_COMPLEX_SMALL
    cv2.putText(img, 'Left Radius of Curvature: {0:6f} meters'.format(left_curverad), (10,30), font, 1, (255,255,255), 2)
    cv2.putText(img, 'Right Radius of Curvature: {0:6f} meters'.format(right_curverad),(10,60), font, 1, (255,255,255), 2)
    cv2.putText(img, 'Camera Offset: {0:6f} meters'.format(camera), (10,90), font, 1, (255,255,255), 2)",,13->Let's construct AlexNet in Keras!,Zero-Shot Learning - Part 1 - New Class Features->Zero-Shot Learning - Part 1 - New Class Features->Load CIFAR-100 Set,P4 Advance Lane Finding->Advanced Lane Finding Project->2. Distortion Correction->Locate lane pixels and fit to find the lane boundary
275855,"X_missing_perc = X.isnull().mean()[X.isnull().any()]
X_missing_perc.sort_values(ascending=False).plot.bar(figsize=(6,3))
plt.title('Missing values')
plt.ylabel('Share')
plt.show()","# better version with perc as y axis
glassdoor_employee_perc = glassdoor.groupby('employees').agg({""company"": ""count""})

glassdoor_employee_perc['perc'] = (glassdoor_employee_perc['company']*100)/76
glassdoor_employee_perc['perc'].sort_values(ascending = False).plot.bar(color = 'red')
ax = plt.gca()
ax.set_facecolor('white')
plt.ylabel('Attacks (%)')
dummy = plt.xticks(rotation = 45)
plt.legend('')","import pandas as pd
import math",,Kernel->Missing Values->True missing values,Cyberattacks Complete-Checkpoint->Data Cleaning: glassdoor->attacks by employees (fix - perc),"Sliderule Dsi Inferential Statistics Exercise 1->What is the true normal human body temperature?
Naive Bayes Notes"
113160,"figure5c_tidy = tidify_and_log(figure5c_expression)
print(figure5c_tidy.shape)
figure5c_tidy.head()",print(log_mean_price.head(5)),"# show random example of every transformation

random_num = random.randint(0, len(X_train))
random_image = X_train[random_num].squeeze()

plt.subplot(2,2,1)
plt.imshow(random_image, cmap='gray');
plt.title(""image"")
plt.subplot(2,2,2)
plt.imshow(random_scale(random_image), cmap='gray');
plt.title(""scale"")
plt.subplot(2,2,3)
plt.imshow(random_rotate(random_image), cmap='gray');
plt.title(""rotate"")
plt.subplot(2,2,4)
plt.imshow(random_translate(random_image), cmap='gray');
plt.title(""translate"")",,1->Recreate Figure 5->Figure 5a->Exercise 10,"Data Exploration->Data Exploration and Visualization->Basic Feature Exploration->User Id->Top 5 cities by log_mean_price:
Matan-><span style=""color:blue"">train.csv</span>->Categories->Top 5 cities by log_mean_price:",Traffic Sign Classifier->Top 5 Probabilities for Road work->Generate new data and equalize distributions
11868,"volume = transactions.resample('M').type.count()
volume","# iloc (strictly integer position based)
df = all_transact.copy()
df.iloc[0]                           # First row
df.iloc[0,0]                         # Second item of first row
df.iloc[0,1] = 7.45
all_transact.shape

# Gives error
'''
for i in range(all_transact.shape[0]):
    row = df.iloc[i]
    row.Amount *= 10.0
'''

# ix supports mixed integer and label calls
df = all_transact.copy()
df.loc[0]
df.loc[0,'Amount']                    # or >> df.ix[0,1]

for i in range(df.shape[0]):
    df.loc[i,'Amount'] += 20.0
print(df)","# look at the df
display(df_adult)
#df_adult.show(5)",,Analysis->KPI 2:  Monthly volume of transactions,Chapter04->Pandas: Data Manipulation->Rows Selection,"Spark Df, Sql, Ml Exercise->Exercise Overview->Spark SQL"
340057,"#V is still volumeo of the cube

n = 10**6 #number of pieces
x_p = np.random.random(n)*2 -1
y_p = np.random.random(n)*2 - 1
z_p = np.random.random(n)* 2 -1

radius = 1.0


def f(xi,yi,zi):
    return (xi**2*(yi-1)**2*(zi-2)**2 * np.exp(xi*yi*zi) )

def dist(xi,yi,zi):
    x_origin = 0
    y_origin = 0
    z_origin = 0
    return ((xi - x_origin)**2 + (yi - y_origin)**2 + (zi - z_origin)**2)**0.5
def summF(x,y,z):
    summ = 0
    for xi,yi,zi in zip(x,y,z):
        distance = dist(xi,yi,zi)
        if distance <= radius:
            summ+= f(xi,yi,zi)
    return summ* 8./n
summF(x_p,y_p,z_p)","#%%timeit
# regridding data with 0's removed data
x, y, z = df.longitude.values, df.latitude.values, df.ice_conc.values
zi = interpolate.griddata((x, y),z, (xi, yi), method='linear')","grouped_exp3 = frame2_clean.groupby(['sample', 'position'])
grouped_exp3.mean()['temperature']",,Exercise 08 Monte Carlo-Checkpoint->Geometric Brownian Motion.,Quick Look Ssmi->options for thinning data out,Index->Introduction to pandas->DataFrame
265806,"df = df.drop('Id', axis = 1)

df = pd.concat([
        df.drop([
                'Hood', 
                'BldgType', 
                'Style', 
                'RoofStyle', 
                'MonthSold', 
                'month_sold2']
                , axis = 1), 
    pd.get_dummies(df[['Hood', 'BldgType', 'Style', 'RoofStyle', 'MonthSold']],\
    drop_first=True)], axis = 1)","# purpose.
df = pd.concat([df, pd.get_dummies(df.purpose, drop_first=True, prefix='purpose')], axis=1)
df.drop('purpose', axis=1, inplace=True)",snowball.stem('embarassed'),,Coffey Project-3-Housing-Final->LINEAR REGRESSION,[6] Modelling->[6] Modelling->Random Forest->Gridsearch,"Intro To Text Analysis->Introduction to Text Analysis->Topic Modeling
Intro To Text Analysis->Introduction to Text Analysis->Stemming and Lemmatizing
3-1 Preprocessing Text Jon->[LEGALST-190] Preprocessing Text - Lab 3-1->Tokenization  <a id='section 1'></a>->How to tokenize"
122216,"model = LinearRegression(fit_intercept=False)
model.fit(X_train, y_train)
print(""> model.intercept_"", model.intercept_)
print(""> model.coef_"", model.coef_)","#Fitting a simple linear regression model to the data
regress = LinearRegression()
regress.fit(X,Y)
print('The coefficients for fitted regression model are: a = ', regress.intercept_,' and b = ',regress.coef_[0])
print ('\nThe simple linear regression line fitted is: ',regress.intercept_,'+',regress.coef_[0],'x')","def run_sigopt_local_rsi_roc(samples=50, experiment_id=None,
                    client_token=None, **algo_descr):

    import sigopt.interface

    if client_token is None:
        raise ValueError('No sigopt credentials passed, find them at https://sigopt.com/user/profile')

    conn = sigopt.interface.Connection(client_token=client_token)

    if experiment_id != None:
        experiment = conn.experiments(experiment_id).fetch()
    else:
        experiment = conn.experiments().create(
            name='Quantopian_POC_talib_5_hedge_width',
            parameters=[
                {
                    'name': 'rsi_timeperiod',
                    'type': 'int',
                    'bounds': { 'min': 5, 'max': 126 }
                },
                {
                    'name': 'rsi_lower',
                    'type': 'int',
                    'bounds': { 'min': 0, 'max': 90 }
                },
                {
                    'name': 'rsi_width',
                    'type': 'int',
                    'bounds': { 'min': 10, 'max': 30 }
                },
                {
                    'name': 'roc_timeperiod',
                    'type': 'int',
                    'bounds': { 'min': 2, 'max': 63 }
                },
                {
                    'name': 'roc_lower',
                    'type': 'double',
                    'bounds': { 'min': 0.00, 'max': 0.30 }
                },
                {
                    'name': 'roc_width',
                    'type': 'double',
                    'bounds': { 'min': 0.05, 'max': 2.00 }
                },
                {
                    'name': 'rebalance_freq',
                    'type': 'int',
                    'bounds': { 'min': 3, 'max': 21 }
                },
            ],
        )

    for trial in range(samples):
        print(""running trial: {0}"".format(trial))
        suggestion = conn.experiments(experiment.id).suggestions().create()

        # Run the Zipline Algo with the SigOpt suggestion
        params_sugg = suggestion.assignments.to_json()
        print params_sugg
        
        signals = OrderedDict([
            ( 'rsi', {'func': talib.RSI, 'func_kwargs': {'timeperiod': params_sugg['rsi_timeperiod'] }, 
                           'lower': params_sugg['rsi_lower'],  'width': params_sugg['rsi_width']} ),
            ( 'roc', {'func': talib.ROCP, 'func_kwargs': {'timeperiod': params_sugg['roc_timeperiod'] }, 
                           'lower': params_sugg['roc_lower'], 'width': params_sugg['roc_width']} )  
            ])
        
        params_sugg['signals_input'] = signals
        algo_run = TradingAlgorithm(initialize=initialize_local_sigopt_rsi_roc, 
                                    handle_data=handle_data_local_sigopt_rsi_roc,
                                    **params_sugg
                                    )
        
        perf = algo_run.run(data)
        daily_rets = perf.portfolio_value.pct_change().dropna()
        
        if daily_rets.std() > 0:
            sharpe_ratio_calc = daily_rets.mean() / daily_rets.std() * np.sqrt(252)
        else:
            sharpe_ratio_calc = -999
        obj = sharpe_ratio_calc
        print obj      

        if obj < -900:
            print ""*** Flagging as failed""
            conn.experiments(experiment.id).observations().create(
              assignments=suggestion.assignments,
              failed=True,
            )
        else:
            conn.experiments(experiment.id).observations().create(
              assignments=suggestion.assignments,
              value=obj,
            )

    exp_hist = conn.experiments(experiment.id).observations().fetch()
    exp_data = exp_hist.to_json()['data']
    exp_data.reverse() # start at the beginning

    # load up dataframe
    from collections import defaultdict
    sigopt_df_dict = defaultdict(list)
    for data_point in exp_data:
        for k, v in data_point['assignments'].iteritems():
            sigopt_df_dict[k].append(v)

        try:
            sigopt_df_dict['failed'].append(data_point['failed'])
        except:
            sigopt_df_dict['failed'].append(False)

        try:
            sigopt_df_dict['objective'].append(data_point['value'])
        except:
            sigopt_df_dict['objective'].append(-999)

    sigopt_df = pd.DataFrame(sigopt_df_dict)

    return sigopt_df",,01->Linear Regression with One Variable->Linear Regression->Optimize->Batch Gradient Descent,Regression Analysis,Blog Post Zipline Sig Opt->Using SigOpt for Optimization
426530,"quality_dummies = pd.get_dummies(wine['quality'], prefix='quality')
wine = wine.join(quality_dummies)

non_quality_cols = [col for col in wine.columns if 'quality' not in col]
X = wine[non_quality_cols].copy()
y = wine['quality_0'].copy()","y = wine['quality_0'].copy()
cols = [col for col in wine if 'quality' not in col]
X = wine[cols].copy()


logreg = LogisticRegression(penalty='l1')
logreg.fit(X, y)","xbar=np.mean(iris_sub.column(1))
sd=np.std(iris_sub.column(1))

xbar+np.array([-1,1])*stats.t.ppf(.975,num_obs-1)*sd/sqrt(num_obs)",,Feature Selection And Grid Search->Automatic Feature Selection with [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest),Feature Selection And Grid Search-Checkpoint->Correlations->Check for Understanding (Until end of break),Math 377 Lesson 25 Solutions->Lesson 25: Central Limit Theorem->Example
3879,"months = conn.get_avail_months('2013')
print(months)","from pygld import HeatPump

heatpump = HeatPump()
heatpump.print_avail_heatpump_models()
print('\nFluid Types:', heatpump.get_avail_fluid_types())","df_hier = df.set_index(['Lead Studio','Genre']).sort_index(level=['Lead Studio','Genre'])
df_hier.head()",,Tutorial->Tutorial->Query methods->Get available months in a year,Example Heatpump->Example for the `HeatPump` class->Instantiate `HeatPump` and print the list of available heatpump models and heat carrier fluids.,"Data-Aggregation-Group-Operations-One-Mar27->Data Aggregation and Group Operations I->GroupBy Mechanics->OK, let's resume..."
277922,dticket=dticket.map(lambda x:len(x) if x != 'LINE' else 0),"stats2['numOutliers'] = stats2['outliers'].map(lambda x: len(x.split("" "")) if x != 0 else 0)
stats2.head()","# Find out what weather columns contain NaNs.
train_df[weather_columns].isna().any()",,Preprocess->Extract Ticket by finding ticket length,Errors Plotting->False positive and false negatives->False positives->Count number of outliers for each gene,Rossmann Sales->Rossmann Sales Prediction->Feature Engineering->Categorical Embeddings
63207,"def cv_and_fit(frame2008, frame2012, featureslist, n_folds=5):
    bp, bs = cv_optimize(frame2008, featureslist, n_folds=n_folds)
    predict, clf = fit_logistic(frame2008, frame2012, featureslist, reg=bp['C'])
    return predict, clf","def cv_and_fit(frame2008, frame2012, featureslist, n_folds=5):
    bp, bs = cv_optimize(frame2008, featureslist, n_folds=n_folds)
    predict, clf = fit_logistic(frame2008, frame2012, featureslist, reg=bp['C'])
    return predict, clf","data.pivot_table(index=""Embarked"")",,"Hw2
Hw2->The Logistic Regression","Hw2
Hw2->The Logistic Regression",Titanic Exploration->Titanic Exploration->Analysis->Embarked
429719,"dataframe['TotalDistance2'] = dataframe['TotalDistance'].apply(lambda x: x*x)
dataframe['EuclideanDistance2'] = dataframe['EuclideanDistance'].apply(lambda x: x*x)
dataframe['MixedDistance'] = dataframe['TotalDistance']*dataframe['EuclideanDistance']","# def addProfile(data,email,time,lines):

def webData_to_DF(system_datafram, dict_of_webData):
    
    for ea in dict_of_webData.keys():
        addProfile(system_datafram, ea, dict_of_webData[ea][0], dict_of_webData[ea][1:])","# normalized
dataset[[mapping[""Y1""], mapping[""Y2""]]].hist(bins=10, figsize = (10,5), normed=1)",,Taxi-Prediction->Taxi Trip Time Prediction->Prediction->Linear Regression,Project Yassine20171209-Checkpoint->Function that takes a list of affected lines and returns the reason of the delays or works->Below function takes data dict from web or text file and adds it to existing system dataframe,Energy-Residential-Buildings->About->Histograms
141236,"dfs_map = dfs[['id', 'year', 'latitude', 'longitude',
               'attacktype', 'target', 'summary', 'fatalities', 'injuries']]

dfs_map = dfs_map.rename(columns={'latitude': 'lat', 'longitude': 'lon', 'attachtype': 'attack'})","# Let's clean the column names...just to clear the ease the access
import re
_unit = '(erg.s-1.cm-2)'
renam_cols = {}
for c in df.columns:
    nc = c
    if _unit in c:
        _i = re.search(_unit,c).start() -1
        nc = c[:_i]
    renam_cols[c] = nc

df.rename(columns=renam_cols, inplace=True)
df.head()","number_unit = 225
unit = 'temp_{}'.format(number_unit)
repetitions = protocol[unit]

plt.figure(figsize=(10,5))
for kidx,krep in enumerate(repetitions):
    data_plot = repetitions[krep][...]/sample
    plt.plot(data_plot,kidx*np.ones_like(data_plot),marker='|',linestyle='')
plt.xlim([-1,time_stim+1])
plt.title(unit)
plt.xlabel('Time [s]')
plt.ylabel('Trials')
plt.show()",,Eda->Global Terrorism since 1970->data process->Data used for map animaion,Xmatch Sds Vs La Massa->Swift DeepSky->Cleaning: column names,2018-07-18 Crp Eda Structured Hdf5->Examples->Plot raster for a single unit
192476,"plt.figure(figsize=(15,15))
for ii,pic in enumerate(pics0):
    objects = [signnames[lab] for lab in mytop5.indices[ii]]
    y_pos = np.arange(len(objects))+0.5
    performance = mytop5.values[ii]
    ax = plt.subplot(numimages,2,2*ii+1)
    plt.imshow(np.squeeze(pic))
    plt.xlabel(signnames[ylabels0[ii]])
    ax.get_xaxis().set_ticks([])
    ax.get_yaxis().set_ticks([])
    ax = plt.subplot(numimages,2,2*ii+2)
    ax.barh(y_pos, performance, align='center', alpha=0.8)
    ax.invert_yaxis()
    plt.axis([0.0,1.0,5.0,0.0])
    plt.yticks(y_pos, objects)
    plt.xlabel('Probability')
#     plt.title('Classification Probabilities')
 
plt.show()","plt.figure()
ax = plt.subplot(111)
attendance.plot.barh(ax=ax)
ax.invert_yaxis()
ax.set_title('Attendance for the workshop')
ax.set_xlabel('Participants')
plt.tight_layout()
plt.savefig('figures/attendance.jpg')
plt.show()","filepath = os.path.join('..', '..', 'tests', 'data', 'single_ended')

ds = read_silixa_files(
    directory=filepath,
    timezone_netcdf='UTC',
    file_ext='*.xml')

ds100 = ds.sel(x=slice(-30, 101))  # only calibrate parts of the fiber, in meters
sections = {
            'probe1Temperature':    [slice(20, 25.5)],  # warm bath
            'probe2Temperature':    [slice(5.5, 15.5)],  # cold bath
            }
ds100.sections = sections",,Traffic Sign Classifier->---->Plot the classification results,Demographics-Analysis->Analyse the demographics of the workshop->Attendance,05Calibrate Single Ols->5. Calibration of double ended measurement with OLS
492652,"r2 = r[:4,:4]
r2",R == R2,"# Check ratio of positive ROIs in a set of images.
if random_rois:
    limit = 10
    temp_g = modellib.data_generator(
        dataset, crop_config, shuffle=True, random_rois=10000, 
        batch_size=1, detection_targets=True)
    total = 0
    for i in range(limit):
        _, [ids, _, _] = next(temp_g)
        positive_rois = np.sum(ids[0] > 0)
        total += positive_rois
        print(""{:5} {:5.2f}"".format(positive_rois, positive_rois/ids.shape[1]))
    print(""Average percent: {:.2f}"".format(total/(limit*ids.shape[1])))",,Numpy,02->Units,Final Blog->Big Data Systems and Intelligence Analytics - Srujani Elango->ROIs
100821,"#filling na with average
elem_df = elem_df.fillna(elem_df.mean(),inplace= True)
high_df = high_df.fillna(high_df.mean(),inplace = True)","df3=df.copy()
df3['average_yearly_high']= df.average_monthly_high.mean()","#Using Model acc to above values, finding n_estimator number
from sklearn.model_selection import GridSearchCV

scaler = StandardScaler().fit(X_train)
rescaled_X_train = scaler.transform(X_train)
param_grid = dict(n_estimators=np.array([50,100,200,300,400]))
model = XGBRegressor(random_state=21)
kfold = KFold(n_splits=10, random_state=21)
grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=kfold)
grid_result = grid.fit(rescaled_X_train, Y_train)

means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print(""%f (%f) with: %r"" % (mean, stdev, param))

print(""Best: %f using %s"" % (grid_result.best_score_, grid_result.best_params_))
#----------------------------------------------------------------------------------------------------------",,Project->575 Project,Learning Exercise0-Checkpoint->Quiz 0->Create new columns for `average_yearly_high` and `average_yearly_low`,"Sales Forecasting - Predictive Model->Sales Forecasting for Daily Sales on Time-series Data->Checking what model would be more efficient?, and Evaluating Model"
270166,"gformColumns = pd.Index([
        
            # Timestamp
        #'Timestamp', #01
            QTimestamp,

# Basic engagement questions
        #'Are you interested in learning more about...Biology', #02
            QCuriosityBiology,
        #'Are you interested in learning more about...Synthetic biology', #03
            QCuriositySyntheticBiology,
        #'Are you interested in learning more about...Video games', #04
            QCuriosityVideoGames,
        #'Are you interested in learning more about...Engineering', #05
            QCuriosityEngineering,

# Experience with Hero.Coli
        #'Have you ever played Hero.Coli?', #06
            QPlayed,

# Basic demographics questions
        #'How old are you?', #07
            QAge,
        #'What is your gender?', #08
            QGender,
        #'Are you interested in video games?', #09
            QInterestVideoGames,
        #'Are you interested in biology?', #10
            QInterestBiology,
        #'How long have you studied biology?', #11
            QStudiedBiology,
        #'Do you play video games?', #12
            QPlayVideoGames,

# Basic biology questions
        #'Have you ever heard about synthetic biology or BioBricks, outside of Hero.Coli?', #13
            QHeardSynBioOrBioBricks,

# Basic engagement questions
        #'Do you volunteer to contribute to our study by answering 9 more questions? (5 min)', #14
            QVolunteer,
        #'Did you enjoy playing the game?', #15
            QEnjoyed,

# General mechanics of the game
        #'In order to modify the abilities of the bacterium, you have to...', #16
            QGenotypePhenotype,
        #'What are BioBricks and devices?', #17
            QBioBricksDevicesComposition,
        #'Find the antibiotic:', #18
            QAmpicillin,

# BioBrick names and functions
        #'Plasmid is...', #19
            QBBNamePlasmid,
        #'Represents the end of a device... TER', #20
            QBBFunctionTER,
        #'Promoter is...', #21
            QBBNamePromoter,
        #'Represents the ability given... CDS', #22
            QBBFunctionGameCDS,
        #'Terminator is...', #23
            QBBNameTerminator,
        #'Codes a protein... CDS', #24
            QBBFunctionBiologyCDS,
        #'RBS is...', #25
            QBBNameRBS,
        #'Can represent GFP... CDS', #26
            QBBExampleCDS,
        #'Coding Sequence is...', #27
            QBBNameCDS,
        #'Controls when the device is active... PR', #28
            QBBFunctionPR,
        #'Controls the level of expression, and thus how much the ability will be affected... RBS', #29
            QBBFunctionRBS,
        #'Makes it possible to equip an additional device. Plasmid', #30
            QBBFunctionPlasmid,
        #'Operator is... XXX', #31
            QBBNameOperator,

# Device symbols
        #'What does this device do? RBS:PCONS:FLHDC:TER XXX', #32
            QDeviceRbsPconsFlhdcTer,
        #'What does this device do? PCONS:RBS:FLHDC:TER', #33
            QDevicePconsRbsFlhdcTer,
        #'What does this device do? PBAD:RBS:GFP:TER', #34
            QDevicePbadRbsGfpTer,
        #'What does this device do? PBAD:GFP:RBS:TER XXX', #35
            QDevicePbadGfpRbsTer,
        #'What does this device do? GFP:RBS:PCONS:TER XXX', #36
            QDeviceGfpRbsPconsTer,
        #'What does this device do? PCONS:GFP:RBS:TER XXX', #37
            QDevicePconsGfpRbsTer,
        #'What does this device do? AMPR:RBS:PCONS:TER XXX', #38
            QDeviceAmprRbsPconsTer,
        #'What does this device do? RBS:PCONS:AMPR:TER XXX', #39
            QDeviceRbsPconsAmprTer,

# Beyond the game
        #'When does green fluorescence happen?', #40
            QGreenFluorescence,
        #'What happens when you unequip the movement device?', #41
            QUnequipDevice,
        #'Last question. Next page only contains remarks.Guess: you have crafted a functional device containing an arabinose-induced promoter and an arabinose Coding Sequence (CDS). What will happen?', #42
            QDevicePbadRbsAraTer,

# Remarks
        #'You can write down remarks here.', #43
            QRemarks,

# Technical data
        #'userId', #44
            QUserId,
        #'Language', #45
            QLanguage,
        #'Temporality' #46
            QTemporality,
        ])","# Rename columns of the Google Forms table with tags independantly of form language
columnTagsDictionary = {
    QTimestamp : 'timestamp',
    QCuriosityBiology : 'QCuriosityBiology',
    QCuriositySyntheticBiology : 'QCuriositySyntheticBiology',
    QCuriosityVideoGames : 'QCuriosityVideoGames',
    QCuriosityEngineering : 'QCuriosityEngineering',
    QPlayed : 'previousPlay',
    QAge : 'age',
    QGender : 'gender',
    QInterestVideoGames : 'gameInterest',
    QInterestBiology : 'biologyInterest',
    QStudiedBiology : 'biologyStudy',
    QPlayVideoGames : 'gameFrequency',
    QHeardSynBioOrBioBricks : 'synthBioKnowledge',
    QVolunteer : 'QVolunteer',
    QEnjoyed : 'QEnjoyed',
    QGenotypePhenotype : 'QGenotypePhenotype',
    QBioBricksDevicesComposition : 'QBioBricksDevicesComposition',
    QAmpicillin : 'QAmpicillin',
    QBBNamePlasmid : 'QBBNamePlasmid',
    QBBFunctionTER : 'QBBFunctionTER',
    QBBNamePromoter : 'QBBNamePromoter',
    QBBFunctionGameCDS : 'QBBFunctionGameCDS',
    QBBNameTerminator : 'QBBNameTerminator',
    QBBFunctionBiologyCDS : 'QBBFunctionBiologyCDS',
    QBBNameRBS : 'QBBNameRBS',
    QBBExampleCDS : 'QBBExampleCDS',
    QBBNameCDS : 'QBBNameCDS',
    QBBFunctionPR : 'QBBFunctionPR',
    QBBFunctionRBS : 'QBBFunctionRBS',
    QBBFunctionPlasmid : 'QBBFunctionPlasmid',
    QBBNameOperator : 'QBBNameOperator',
    QDeviceRbsPconsFlhdcTer : 'QDeviceRbsPconsFlhdcTer',
    QDevicePconsRbsFlhdcTer : 'QDevicePconsRbsFlhdcTer',
    QDevicePbadRbsGfpTer : 'QDevicePbadRbsGfpTer',
    QDevicePbadGfpRbsTer : 'QDevicePbadGfpRbsTer',
    QDeviceGfpRbsPconsTer : 'QDeviceGfpRbsPconsTer',
    QDevicePconsGfpRbsTer : 'QDevicePconsGfpRbsTer',
    QDeviceAmprRbsPconsTer : 'QDeviceAmprRbsPconsTer',
    QDeviceRbsPconsAmprTer : 'QDeviceRbsPconsAmprTer',
    QGreenFluorescence : 'QGreenFluorescence',
    QUnequipDevice : 'QUnequipDevice',
    QDevicePbadRbsAraTer : 'QDevicePbadRbsAraTer',
    QRemarks : 'comments',
    QUserId : 'anonymousID',
    QLanguage : 'lang',
    QTemporality : 'temporality',
}
columnQuestions = gfdf.columns.values.tolist()
googleData = gfdf.rename(columns=columnTagsDictionary)
#googleData.head()","params, steps = train(x,y,[1,2.5],max_iter=2000)",,0,Data Formating->Prepare data->Question renaming,Notes-2017-02-03->UBC Scientific Software Seminar->2. A bad example of gradient descent
231586,"# your turn: scatter plot between *PTRATIO* and *PRICE*
plt.scatter(bos.DIS, bos.PRICE)
plt.xlabel(""Weighted Distances to Five Boston Employment Centres (DIS)"")
plt.ylabel(""Housing Price"")
plt.title(""Relationship between DIS and Price"")","# fittedvalues are the predicted values
plt.scatter(m3.fittedvalues, m3.resid)
plt.xlabel(""Fitted Values"")
plt.ylabel(""Residuals"")","sns.boxplot(x=""Species"", y=""PetalWidthCm"", data=iris_df)
plt.show()",,Mini Project Linear Regression,Mini Project Linear Regression-Solved->Akaike Information Criterion (AIC),Munging Pd->Iris dataset Munging and Analysis->Importing and Describing the Iris dataset->Visualization of the Iris dataset
301100,"std_threshold = 4
threshold_val = pageview_df['views'].mean() + pageview_df['views'].std() * std_threshold
peak_days = pageview_df[pageview_df['views'] > threshold_val]

peak_days.head(10)","def peak_trend_regression(df):
    """"""
    """"""
    #convert datetimes to days for regression
    peak_date = df.index
    peak_date = pd.to_datetime(df.index.get_level_values(1))
    df['peak_d'] = (peak_date - peak_date.min()) / np.timedelta64(1,'D')
    #df['peak_d'] = (df['peak_dt'] - df['peak_dt'].min())  / np.timedelta64(1,'D')
    
    #normalize the peak discharge values
    df['peak_va'] = (df['peak_va'] - df['peak_va'].mean())/df['peak_va'].std()
    
    slope, intercept, r_value, p_value, std_error = stats.linregress(df['peak_d'], df['peak_va'])
    
    #df_out = pd.DataFrame({'slope':slope,'intercept':intercept,'p_value':p_value},index=df['site_no'].iloc[0])
    
    #return df_out
    return pd.Series({'slope':slope,'intercept':intercept,'p_value': p_value,'std_error':std_error})","df[df['Reason'] == 'EMS'].groupby('Date')['twp'].count().plot()
plt.title('EMS')",,Lab 4 - Pageviews->Lab 4 - Pageviews->Get the pageviews for the hyperlink network,Nwis Demo 1->National trends in peak annual streamflow->Preparing the regression,911 Calls Data Analysis
197598,"#np.save('train_2', x_train2)
#np.save('test_2', x_test2)
#np.save('valid_2', x_valid2)

x_train2 = np.load('train_2.npy')
x_test2  = np.load('test_2.npy')
x_valid2 = np.load('valid_2.npy')","# train the model
sizes=[type_max,100,num_class]
momentum=0.9
num_epochs = 20
lr0 = 0.09
batch_size = 64

model2 = MLPnet(sizes)
model2.apply(model2.init_weights_glorot)
ll_train2,ll_valid2,ll_test2,acc_train2,acc_valid2,acc_test2=\
    model2.train_model(torch.optim.SGD, lr0, momentum, 
        num_epochs, batch_size, tfidf_train_data,
        tfidf_valid_data,tfidf_test_data,display=5)

# Epoch:20, [ACC] TRAIN 0.625554569654 / VALID 0.575676875277 / TEST 0.526715522985 lr = 0.01
# Epoch:20, [ACC] TRAIN 0.934671694765 / VALID 0.832667554372 / TEST 0.763890739507 lr = 0.05
# Epoch:20, [ACC] TRAIN 0.977817213842 / VALID 0.864181091877 / TEST 0.778814123917 lr = 0.07
# Epoch:20, [ACC] TRAIN 0.987023070098 / VALID 0.876608965823 / TEST 0.786942038641 lr = 0.08
# Epoch:20, [ACC] TRAIN 0.993234250222 / VALID 0.880159786951 / TEST 0.786009327115 lr = 0.09
# Epoch:20, [ACC] TRAIN 0.994787045253 / VALID 0.877940523746 / TEST 0.790273151233 lr 0.1","tracker1.get_screen_loc(4, 3)",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include,Assignment1 Code->Problem 1->IF-IDF,02-Deeper Into Python->Built-in functions->min
21065,"## Add new label 'Young or Older' that check whether passengers are under 18 or above
def Young_Older(data):
    data['Young or Older'] = 'Young' if data['Age'] < 18 else 'Old'
    return data

survived_by_age = survived_by_age.apply(Young_Older, axis = 1)
#survived_by_age.head()

## Make a new data frame sorted by Age
age_grouped = survived_by_age.groupby('Young or Older')
age_total = age_grouped.count()['PassengerId'] # total number of passengers in terms of their age
age_survived = age_grouped.sum()['Survived'] # number of survived passengers in terms of their age
age_df = pd.concat([age_survived, age_total], axis = 1)
# age_df

## Draw figures
plt.figure(figsize=(6, 6))
plt.xticks(fontsize = 15)
plt.yticks(fontsize = 15)

top_plot = sns.barplot(x = ['Old', 'Young'], y = age_df['PassengerId'], color = ""red"", alpha = 0.2)
bottom_plot = sns.barplot(x = ['Old', 'Young'], y = age_df['Survived'], color = 'purple', alpha = 0.5)

plt.ylabel('Number of Survivals', size = '14')
plt.xlabel('Passengers Grouped by Age', size = '14')
plt.title('Comparison of Survivals by Age', size = '14')

topbar = plt.Rectangle((0,0),1,1,fc=""red"", alpha = 0.2)
bottombar = plt.Rectangle((0,0),1,1,fc='purple', alpha = 0.5)
legend = plt.legend([topbar, bottombar], ['Total by Age', 'Sruvived by Age'], loc=0, ncol = 1, prop={'size':15})","temp = appli_train[""NAME_HOUSING_TYPE""].value_counts()
temp_y1 = []
for val in temp.index:
    temp_y1.append(np.sum(appli_train[""TARGET""][appli_train[""NAME_HOUSING_TYPE""]==val] == 1))
plt.figure(figsize=(19,10))
sns_plot = sns.barplot(((temp_y1/temp.sum())*100),temp.index,palette=""Greens_d"")
sns_plot.set_title(""% of loans unpaid by NAME_HOUSING_TYPE"")
sns_plot.set_xlabel(""Percentage"")
sns_plot.set_ylabel(""NAME_HOUSING_TYPE"")
fig = sns_plot.get_figure()
fig.savefig('./Output_Images/% of loans unpaid by NAME_HOUSING_TYPE.png')","show([""Residence Hall/Dormitory""])",,Titanic->Titanic Passengers Analysis->Introduction,Home Credit Risk Final->5. Modelling->5.1 Modelling - Feature engg on original data,Eui Year Shi->Plot how EUI varies with year for main Property Types->Residence Hall/Dormitory
63429,"data_1.columns = ['Name_1'] + list(data_1.columns)[1:]
data_2.columns = ['Name_2'] + list(data_2.columns)[1:]

data_left = pd.merge(data_1,match_pair,how='outer')

data_right = pd.merge(data_2,match_pair,how='outer')

data_combined = pd.merge(data_left,data_right,how='outer',on=['Name_1','Name_2'],suffixes=['_1','_2'])
data_combined = data_combined[data_combined['Country_1'] == data_combined['Country_2']]
data_combined = data_combined[['Name_1','Rank_1','Rank_2','Country_1','Region_1','Number of students_1','Number of students_2','Number of faculty members_1','Number of faculty members_2','Students-Staff ratio','Number of international members','Percentage of international students','Score_1','Score_2','Score_3','Score_4','Score_5','Score_6','Score_7','Score_8','Score_9','Score_10','Score_11']]
data_combined['Name_1'] = data_combined['Name_1'].apply(lambda x: x.replace('%','University').replace('#','Institute'))
data_combined","plt.plot([x_1_left, y_1_left], [x_2_left, y_2_left], 'b-', lw=4)
plt.plot([x_1_right, y_1_right], [x_2_right, y_2_right], 'r-', lw=4)
plt.imshow(edges, cmap='Greys_r')
plt.show()","def embarked_binary(emb):
    if emb == 'S':
        return 0
    elif emb == 'C':
        return 1
    elif emb == 'Q':
        return 2
    else:
        return 0",,Hw2->Question 3,P1->Self-Driving Car Engineer Nanodegree->1. Detect lane on test image->Tricky point,Kaggle Titanic->Data clean and feature engineer
235545,"pygaps.db_upload_material(pygaps.DATABASE, novel_material)","%%time
with abcd as db:
    db.upload(file.as_posix())","import tensorflow as tf # to define the neural network
import numpy as np # manipulate tensors and arrays of training samples
from threading import Thread, Lock # to make each of the drones asynchronous
import gym # the OpenAI API to run the Atari Breakout emulator
import cv2 # image manipulation functions for preprocessing
import time, random",,Database->Database examples->Material,Abcd Uploading->Basic usage of ABCD database->Cleanup,Atari-Breakout-A3C->ATARI-BREAKOUT WITH A3C IN TENSORFLOW->3. Asynchronous Actor-Critic Advantage (A3C)->Implementation
402814,"import pandas as panda

## changed as pylab is deprecated
%matplotlib inline","#SQL preliminaries.
from pandasql import PandaSQL
pdsql = PandaSQL()","data_age_dist = data_M.sum(axis=1)
data_age_dist /= data_age_dist.sum()

y_age_dist = mcmc['y_hat_M'][mle_place,:,:].sum(axis=1)
y_age_dist /= y_age_dist.sum()

x = np.arange(len(data_age_dist))
fig, ax = plt.subplots(figsize=(16,9))
# https://matplotlib.org/2.0.0/examples/api/barchart_demo.html
width = 0.4
ax.bar(x,data_age_dist, width, label=""Data"")
ax.bar(x+width, y_age_dist, width, label=""Model"")

ax.legend()",,Titanic Eda Lab->Titanic EDA Lab - Week 3,"Tsrc3 Dmhr Assignment 2018->Assignment C->Question 5: Using SQL, produce a table that provides the number of GP practices per city, ordered in descending order.",Backup Hetro Self Mcmc 0403 Single Chain->PAPER->D Bars
32787,"import math

def grayscale(img):
    """"""Applies the Grayscale transform
    This will return an image with only one color channel
    but NOTE: to see the returned image as grayscale
    (assuming your grayscaled image is called 'gray')
    you should call plt.imshow(gray, cmap='gray')""""""
    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    # Or use BGR2GRAY if you read an image with cv2.imread()
    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
def canny(img, low_threshold, high_threshold):
    """"""Applies the Canny transform""""""
    return cv2.Canny(img, low_threshold, high_threshold)

def gaussian_blur(img, kernel_size):
    """"""Applies a Gaussian Noise kernel""""""
    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)

def region_of_interest(img, vertices):
    """"""
    Applies an image mask.
    
    Only keeps the region of the image defined by the polygon
    formed from `vertices`. The rest of the image is set to black.
    """"""
    #defining a blank mask to start with
    mask = np.zeros_like(img)   
    
    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image
    if len(img.shape) > 2:
        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image
        ignore_mask_color = (255,) * channel_count
    else:
        ignore_mask_color = 255
        
    #filling pixels inside the polygon defined by ""vertices"" with the fill color    
    cv2.fillPoly(mask, vertices, ignore_mask_color)
    
    #returning the image only where mask pixels are nonzero
    masked_image = cv2.bitwise_and(img, mask)
    return masked_image


def draw_lines(img, lines, color=[255, 0, 0], thickness=2):
    """"""
    NOTE: this is the function you might want to use as a starting point once you want to 
    average/extrapolate the line segments you detect to map out the full
    extent of the lane (going from the result shown in raw-lines-example.mp4
    to that shown in P1_example.mp4).  
    
    Think about things like separating line segments by their 
    slope ((y2-y1)/(x2-x1)) to decide which segments are part of the left
    line vs. the right line.  Then, you can average the position of each of 
    the lines and extrapolate to the top and bottom of the lane.
    
    This function draws `lines` with `color` and `thickness`.    
    Lines are drawn on the image inplace (mutates the image).
    If you want to make the lines semi-transparent, think about combining
    this function with the weighted_img() function below
    """"""
    for line in lines:
        for x1,y1,x2,y2 in line:
            cv2.line(img, (x1, y1), (x2, y2), color, thickness)

def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap, drawf=draw_lines):
    """"""
    `img` should be the output of a Canny transform.
        
    Returns an image with hough lines drawn.
    """"""
    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)
    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)
    drawf(line_img, lines)
    return line_img

# Python 3 has support for cool math symbols.

def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):
    """"""
    `img` is the output of the hough_lines(), An image with lines drawn on it.
    Should be a blank image (all black) with lines drawn on it.
    
    `initial_img` should be the image before any processing.
    
    The result image is computed as follows:
    
    initial_img * α + img * β + λ
    NOTE: initial_img and img must be the same shape!
    """"""
    return cv2.addWeighted(initial_img, α, img, β, λ)","import math

def grayscale(img):
    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    # Or use BGR2GRAY if you read an image with cv2.imread()
    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

def gaussian_blur(img, kernel_size):
    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)


def canny(img, low_threshold, high_threshold):
    return cv2.Canny(img, low_threshold, high_threshold)


def region_of_interest(img, vertices):
   
    #defining a blank mask to start with
    mask = np.zeros_like(img)  
    
    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image
    if len(img.shape) > 2:
        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image
        ignore_mask_color = (255,) * channel_count
        
 
    else:
        ignore_mask_color = 255
    #filling pixels inside the polygon defined by ""vertices"" with the fill color    
    
    cv2.fillPoly(mask, [vertices], ignore_mask_color)
    
    #returning the image only where mask pixels are nonzero
    masked_image = cv2.bitwise_and(img, mask) 
    return masked_image

def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):
    """"""
    `img` should be the output of a Canny transform.
        
    Returns an image with hough lines drawn.
    """"""
    theta = np.pi * (theta /180)
    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)
    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)
    draw_lines(line_img, lines,thickness=3)
    return line_img

# Python 3 has support for cool math symbols.
def draw_lines(img, lines, color=[255, 0, 0], thickness=2):
    for line in lines:
        for x1,y1,x2,y2 in line:
            cv2.line(img, (x1, y1), (x2, y2), color, thickness = 10)
def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):
   
    return cv2.addWeighted(initial_img, α, img, β, λ)","# We show that there exists a hyperplane that allows to perfectly divides
# points in the two classes.
# In 2-d this hyperplane corresponds to a line that is represented in green.


x1 = np.linspace(0, 20, 2000)
y1 = x1/2.0

plt.figure()

plt.plot(xA, yA, 'ro')
plt.plot(xB, yB, 'bs')

blue_patch = mpatches.Patch(color='blue', label='Class I')
red_patch = mpatches.Patch(color='red', label='Class II')
plt.legend(handles=[blue_patch,red_patch])

plt.xlabel(r'$x$', fontsize=fsize)
plt.ylabel(r'$y$', fontsize=fsize)


# Show hyperplane dividing the classes
plt.plot(x1, y1,'g', lw=4)
plt.show()",,Notebook->Self-Driving Car Engineer Nanodegree->Read an input Image,Lane Detction->Self-Driving Car Engineer Nanodegree->Helper Functions to build Pipeline,Lab1 Classifiers-Practical->Topic 1. Introduction to Machine learning
473777,print(A.dot(B)),print(B.dot(A)),corpus.features['wordcounts'].top(5)    # The top 5 words in the FeatureSet.,,"Linear Algebra->Linear Algebra in NumPy->Multiplication->Matrix-Matrix Multiplication
Matrices->1. Matrices and the basic matrix operations->Matrix multiplication
Cheating On Your Math Homework With Sym Py->Warning sym-pify uses eval.  Don't use with unsanitized inputs!->Calculus","Linear Algebra->Linear Algebra in NumPy->Multiplication->Matrix-Matrix Multiplication
Matrices->1. Matrices and the basic matrix operations->Matrix multiplication",6->Introduction to Tethne: Words and Topic Modeling->Working with featuresets->Applying a stoplist
390406,"import matplotlib.pyplot as plt
%matplotlib inline
from IPython.core.display import display, HTML
display(HTML(""<style>.container { width:100% !important; }</style>""))","from IPython.display import Image, HTML
import matplotlib.pyplot as plt
%matplotlib inline","X, y = brazil_sk_df[['Deaths', 'ADR', 'KAST%', 'Rating']], brazil_sk_df['Kills']",,Compare,Python Course Lecture 5 Matrices-Checkpoint,Cs->Top 10 most First names in CS:GO.
196499,"def normalize(img):    
    
    max_val = np.max(img)
    min_val = np.min(img)
    
    return (img - min_val)*2.0/(max_val - min_val) - 1.0","def normalize(image_data, max_val = 255, min_val = 0):
    """"""
    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]
    :param image_data: The image data to be normalized
    :return: Normalized image data
    """"""
    a = 0.1
    b = 0.9
    normalised_img = a + ( ( (image_data - min_val)*(b - a) )/( max_val - min_val))
    return normalised_img

def zero_center(normalised_img):
  
    centred_img = normalised_img - 0.5
    return centred_img

def norm_zero(img):
    norm_img = (img-128)/128.0
    return norm_img

# normalise to 0-1 range
print(""Test max: {}, min: {}"".format(np.max(X_test),np.min(X_test)))
print(""Train max: {}, min: {}"".format(np.max(X_train),np.min(X_train)))
print(""Valid max: {}, min: {}"".format(np.max(X_valid),np.min(X_valid)))

print(""Normalising Dataset.."")
X_test = norm_zero(X_test)
X_train = norm_zero(X_train)
X_valid = norm_zero(X_valid)
print(""Test max: {}, min: {}"".format(np.max(X_test),np.min(X_test)))
print(""Train max: {}, min: {}"".format(np.max(X_train),np.min(X_train)))
print(""Valid max: {}, min: {}"".format(np.max(X_valid),np.min(X_valid)))","# Standardize the dataset
pipelines = []
pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LinearRegression())])))
pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))
pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))
pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))
pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))
pipelines.append(('ScaledSVR', Pipeline([('Scaler', StandardScaler()),('SVR', SVR())])))
results = []
names = []
for name, model in pipelines:
	kfold = KFold(n_splits=num_folds, random_state=seed)
	cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = ""%s: %f (%f)"" % (name, cv_results.mean(), cv_results.std())
	print(msg)",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Dataset Summary & Exploration->Pre-process the Data Set,Cifar10 Classifier->Normalise and Zero Center,Py-Regression-Boston-Housing-Baseline->Simple Regression Model for Boston Housing Price->Section 4 - Evaluate Algorithms->4.b) Evaluate Algorithms: Data Standardization
481361,"# Drop records with missing values
df_original = df.copy()
df_clean = df.dropna()
print(""Original"", df.shape)
print(""Cleaned"", df_clean.shape)",origins = pd.Series(clean_df.Origin.unique()),"morning_dict={}
for i in range(len(morning.index)):
    mytuple = (morning['C/A'][i], morning.UNIT[i], morning.SCP[i], morning.STATION[i])
    if mytuple not in morning_dict.keys():
        morning_dict[mytuple]={morning.TIME[i]:morning.EXITS[i]}
    else:
        if morning.TIME[i] not in morning_dict[mytuple]:
            morning_dict[mytuple][morning.TIME[i]] = morning.EXITS[i]
        else:
            morning_dict[mytuple][morning.TIME[i]].append(mornig.EXITS[i])
print(morning_dict[('A002', 'R051', '02-00-00','59 ST')])",,3 3 Data Munging With Pandas->Data munging with Pandas and Scikit-learn->Null and missing values,Data Story - Draft->Migrant Crisis - Data Story,"Unit 8 - Mta Problem Set->1.Save the Saturday, June 17th, 2017 data to a csv file. You can find it here: http://web.mta.info/developers/turnstile.html and read it in using read_csv. View the head of the dataframe."
272297,"ffi.cdef('''
typedef int int32_t;
typedef struct {
    int32_t error_code;
    char* description;
} FfiResult;
typedef struct {
    int32_t error_code;
    FfiResult* result;
} RandomStruct;
typedef struct {
    RandomStruct* randomData;
    FfiResult* result;
} RandomOtherStruct;
typedef struct {
    void* core_tx;
    void* _core_joiner;
} Authenticator;
''')","from cffi import FFI
ffi = FFI()
ffi.cdef(""""""
    int printf(const char *format, ...);   // copy-pasted from the man page
"""""")
C = ffi.dlopen(None)                     # loads the entire C namespace
arg = ffi.new(""char[]"", ""world"")         # equivalent to C code: char arg[] = ""world"";
C.printf(""hi there, %s.\n"", arg)         # call printf",A @ A,,Safe Auth Concept->this is the Authenticator-Class containing all important methods that an authenticator needs to work,"Lecture-7-Interoperability->Lecture 7: Interoperability with Legacy and Modern Environments (45 Minutes)->``ctypes``, ``cffi``, and Cython — Interoperating with C Language Projects","05 Numerical Data In Numpy->Numerical data in `numpy`->Working with matrices
Jupyter-Python-Scipy-Complete->Exercise: Collatz conjecture->NumPy array operations
Day-2->BCDATA Data Science Workshop
Jupyter-Python-Scipy->Python->Datatypes and built-in functions->Numbers: integers and floats
Notes-2018-03-07->MATH 210 Introduction to Mathematical Computing->Linear algebra with SciPy->Matrix multiplication"
426630,weather['Sunset'].dtype,weather.iloc[0],"# *might* need to double check correctly formatted input
def plot_loss (train_loss_data, dev_loss_data):
    
    plt.plot(train_loss_data[:,1], label=""train"")
    plt.plot(dev_loss_data[:,1], linewidth=3, label=""valid"")
    plt.grid()
    plt.legend()
    plt.xlabel(""epoch"")
    plt.ylabel(""loss"")
    plt.yscale(""log"")
    plt.show()",,Cleaning Weather Data->Cleaning the weather  data,4 Ames Housing Case Study->Predicting House Prices - A case study with SciKit Learn and Pandas->Plotting the Regression Line,Ak1 Anamika Scott Arunima Kathleen Facial Keypoints Draft 0->Facial Keypoints Detection->Helper functions
328985,"# generate data range from start - end
rng = pd.date_range(start='6/1/2017',end='6/30/2017',freq='B')","rng = pd.date_range('2018 Apr 7 6:30', periods=10, freq='M')
rng","first_ride = Ride()
print(first_ride)
print(Ride)",,Time Series->date_range,1->Generate series of time,Index->Defining a Class
460836,"vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)","from nltk.stem.snowball import SnowballStemmer
import re

stemmer = SnowballStemmer(""spanish"")

def tokenize(text):
    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
    filtered_tokens = []
    for token in tokens:
        if re.search('[a-zA-Z]', token):
            filtered_tokens.append(token)
    return filtered_tokens

def stem(tokens):
    return [stemmer.stem(t) for t in tokens]

totalvocab_stemmed = []
totalvocab_tokenized = []
for i in text:
    allwords_tokenized = tokenize(i)
    totalvocab_tokenized.extend(allwords_tokenized)
    
    allwords_stemmed = stem(allwords_tokenized)
    totalvocab_stemmed.extend(allwords_stemmed)
    

vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)
print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')
print(vocab_frame.head())","list_crimes = list()
for crime in df_model4.crime_category:
    if crime in violent_crimes:
        list_crimes.append(0)
    elif crime in property_crimes:
        list_crimes.append(1)
    else:
        list_crimes.append(2)
df_model4.crime_category = list_crimes",,"Cluster Analysis
Mission Clustering->Part I Web Scraping
New Analysis-Normal Original->New Analysis (2017-02-16)->Data extraction and cleaning",Clustering->Topic Modeling of App Reviews,Model4->Contents:->Model 4: Predict crimes as Violent/Property/Part 2 crimes
275131,"plt.figure(figsize=(20, 10))
plt.plot( sub_df[sub_df[""min_gain""] == 0.001][""iter""], sub_df[sub_df[""min_gain""] == 0.001][""cost""], label=""min_gain = 0.001"" ) 
plt.plot( sub_df[sub_df[""min_gain""] == 0.01][""iter""], sub_df[sub_df[""min_gain""] == 0.01][""cost""], label=""min_gain = 0.01"" )
plt.plot( sub_df[sub_df[""min_gain""] == 0.1][""iter""], sub_df[sub_df[""min_gain""] == 0.1][""cost""], label=""min_gain = 0.1"" )
plt.title(""impact of the min_gain on the cost function"")
plt.legend()
plt.show()","plt.figure(figsize=(20, 10))
plt.plot( sub_df[sub_df[""learning_rate""] == 250][""iter""], sub_df[sub_df[""learning_rate""] == 250][""cost""], label=""learning_rate = 250"" ) 
plt.plot( sub_df[sub_df[""learning_rate""] == 500][""iter""], sub_df[sub_df[""learning_rate""] == 500][""cost""], label=""learning_rate = 500"" )
plt.plot( sub_df[sub_df[""learning_rate""] == 750][""iter""], sub_df[sub_df[""learning_rate""] == 750][""cost""], label=""learning_rate = 750"" )
plt.title(""impact of the learning rate on the cost function"")
plt.legend()
plt.show()","df_accidents = pd.read_csv('../Data/Accidents_zones_dbscan.csv')
df_accidents = df_accidents = df_accidents[['longitud', 'latitud', 'zona']]
df_accidents = df_accidents[df_accidents['zona'] != -1]

df_works = pd.read_csv('../Data/Works2007_filtered.csv')
df_works = df_works[['longitud', 'latitud']]",,Results->Evolution->min_gain,Results->Evolution->learning_rate,Works Prediction
299558,"a = np.array([1,2,3,np.nan,5,6,7,np.nan])
a[~np.isnan(a)]","# Remove time slices with only NaN pixels:
valid_indx = np.where( ( (~np.isnan(B2_xarray)).sum('x').sum('y')!=0 ).values )[0] 
B2_xarray = B2_xarray[valid_indx]
tmp = np.where( ( (~np.isnan(B3_xarray)).sum('x').sum('y')!=0 ).values )[0]
if not (valid_indx==tmp).all(): print( ""B3: Valid dates discrepancy found!"" )   # should be identical between bands!...
B3_xarray = B3_xarray[tmp]
tmp = np.where( ( (~np.isnan(B4_xarray)).sum('x').sum('y')!=0 ).values )[0]
if not (valid_indx==tmp).all(): print( ""B4: Valid dates discrepancy found!"" )
B4_xarray = B4_xarray[tmp]
tmp = np.where( ( (~np.isnan(B5_xarray)).sum('x').sum('y')!=0 ).values )[0]
if not (valid_indx==tmp).all(): print( ""B5: Valid dates discrepancy found!"" )
B5_xarray = B5_xarray[tmp]
tmp = np.where( ( (~np.isnan(B7_xarray)).sum('x').sum('y')!=0 ).values )[0]
if not (valid_indx==tmp).all(): print( ""B7: Valid dates discrepancy found!"" )
B7_xarray = B7_xarray[tmp]
tmp = np.where( ( (~np.isnan(WQ_xarray)).sum('x').sum('y')!=0 ).values )[0]
if not (valid_indx==tmp).all(): print( ""WQ: Valid dates discrepancy found!"" )
WQ_xarray = WQ_xarray[tmp]
tmp = np.where( ( (~np.isnan(WQ_pqm_xarray)).sum('x').sum('y')!=0 ).values )[0]
WQ_pqm_xarray = WQ_pqm_xarray[tmp]   # more dates could need removing due to cloud masking
print( ""Nr. available dates:"", B2_xarray.shape[0])","x = np.array([0, 1, 1, 3, 2, 1, 7])
print(""ans=\n"", np.bincount(x))",,Numpy Basics->Numpy Basics->Boolean Array,Aews Python Notebook 08C->AEWS Python Notebook 08c: AEWS miscellanea->Select ROI from shape file,Statistics Solutions->Statistics->Histograms
462551,"x = pd.Series([1,2,3,4,5])

print(x)

# apply
def f(x):
    if x % 2 == 0:
        return x ** 2
    else:
        return x ** 3

x = x.apply(f)

print(x)

# Converting types
x = x.astype(np.float64)

print(x)","#Reading the test data
appli_test = pd.read_csv('./Data/application_test.csv')
appli_test.head()","def iszero(m): return m==0 # handling logging-on-0 or /0 issue.

def ppmi(n2v):
    row_sums, col_sums, total_sums = n2v.sum(axis=1), n2v.sum(axis=0), n2v.sum()
    pn, pv, ppmi_m = row_sums/total_sums, col_sums/total_sums, n2v/total_sums
    pn[iszero(pn)] = 1e-10 # NB: this is an expedient, may need to reconsider.
    pv[iszero(pv)] = 1e-10
    ppmi_m /= pn[:,np.newaxis] # * 1/pwi by row.
    ppmi_m /= pv # * 1/pwj by col.
    ppmi_m[iszero(ppmi_m)] = 1e-10 
    ppmi_m = np.log(ppmi_m) # compute pmi.
    ppmi_m = np.maximum(ppmi_m, 0) # compute ppmi.
    return ppmi_m

def cosine(n2v):
    n2v_norm = n2v / np.apply_along_axis(lambda r: np.sqrt(np.dot(r,r)), 1, n2v)[:,np.newaxis]
    return np.dot(n2v_norm, n2v_norm.T)",,Python Pandas->Python Pandas,Home Credit Risk Final->1. Introduction->1.2 Import data packages,Selectional Preference Erk07->Erk (2007)->B. Similarity Measure by Paradigmatic Co-Subj/Obj
275295,"df = pd.read_csv(os.path.join(RMC.OUTPUT_DIR, 'TR011_MV03R00_OV01R00_DP02R00_train_results.csv'), header=0)
df['SCENARIO'] = df['SCENARIO'].astype('int32')
df.set_index('SCENARIO', inplace=True)","df = pd.read_csv(os.path.join(RMC.OUTPUT_DIR, 'TR011_MV03R00_OV01R00_DP02R00_train_results.csv'), header=0)
df['SCENARIO'] = df['SCENARIO'].astype('int32')
df.set_index('SCENARIO', inplace=True)","import pandas as pd
import numpy as np
import os

parent_path = os.path.dirname(os.path.dirname(os.getcwd()))

replace_double_slash = parent_path.replace('\\', '/')

data_path = replace_double_slash + '/data/shot_logs_clean_1.csv'

nba_data_clean_1 = pd.read_csv(data_path)",,Results->ANN Training of Prophet Predictions: Validation Results,Results->ANN Training of Prophet Predictions: Validation Results,Data Cleaning Part Ii->Data Cleaning Starring NBA Shot Log Data: Part II->Data Cleaning (Continued)
379148,"plot_describe_numerical('2ndFlrSF', dataset, 'SalePrice')","from pole_zero_plot import pole_zero_plot

X2 = X.subs(W0, sym.pi/4)
pole_zero_plot(sym.roots(sym.denom(X2), z), sym.roots(sym.numer(X2), z))","df1 = library_dd_pandas.loc[:'Home Library Definition']
df2 = library_dd_pandas.loc['Circulation Active Year':]
library_dd_new = df1.append(circulation_am).append(df2)
library_dd_new",,02->Features overview->2ndFlrSF: Second floor square feet,Properties->The $z$-Transform->Properties->Symmetry for Real-Valued Signals,Exploring Library Usage In San Francisco->Drop the Patron Type Code->Cut old data dictionary to pieces and splice in new row
187428,"# Train the model.
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    num_examples = len(x_train)
    
    print(""Training..."")
    print()
    for i in range(EPOCHS):
        x_train, y_train = shuffle(x_train, y_train)
        for offset in range(0, num_examples, BATCH_SIZE):
            end = offset + BATCH_SIZE
            batch_x, batch_y = x_train[offset:end], y_train[offset:end]
            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})
            
        validation_accuracy = evaluate(x_validation, y_validation)
        print(""EPOCH {} ..."".format(i+1))
        print(""Validation Accuracy = {:.3f}"".format(validation_accuracy))
        print()
        
    saver.save(sess, './lenet')
    print(""Model saved"")","### Train your model here.
### Calculate and report the accuracy on the training and validation set.
EPOCHS = 100
BATCH_SIZE = 128

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    num_examples = len(X_train)
    
    print(""Training..."")
    print()
    for i in range(EPOCHS):
        X_train, y_train = shuffle(X_train, y_train)
        for offset in range(0, num_examples, BATCH_SIZE):
            end = offset + BATCH_SIZE
            batch_x, batch_y = X_train[offset:end], y_train[offset:end]
            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})
            
        validation_accuracy = evaluate(X_valid, y_valid)
        print(""EPOCH {} ..."".format(i+1))
        print(""Validation Accuracy = {:.3f}"".format(validation_accuracy))
        print()
        
    saver.save(sess, './lenet')
    print(""Model saved"")","from nltk.corpus import stopwords

tokens = get_tokens()
filtered = [w for w in tokens if not w in stopwords.words('english')]
count = Counter(filtered)",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Question 3,"Traffic Sign Classifier->Train, Validate and Test the Model",Lab2->Task 2.1. Get Started with NLTK->2.1.4. Stop words
71455,"### TODO: Write your algorithm.
### Feel free to use as many code cells as needed.
def predict_breed_full(img_path):
    if dog_detector(img_path):
        print(""Hello, dog!"")
    elif face_detector(img_path):
        print(""Hello, human!"")
    else:
        print(""Hello... object?"")
    print(""You look like a..."")
    print(InceptionV3_predict_breed(img_path))","### TODO: Write your algorithm.
### Feel free to use as many code cells as needed.
def breed_predictor(img_path):
    if face_detector(img_path):
        breed = InceptionV3_predict_breed(img_path)
        print(""Detected a Human that resembles a "" + str(breed) + "" dog."")
    elif dog_detector(img_path):
        breed = InceptionV3_predict_breed(img_path)
        print(""Detected a "" + str(breed) + "" dog."")
    else:
        print(""Error: could not detect a dog or a human in this image."")",X_train['Time'].max() / 24,,Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Predict Dog Breed with the Model,Dog App->(IMPLEMENTATION) Predict Dog Breed with the Model,Credit Card Fraud Detection->2. Getting Started->4.4 V1-V28
120646,"import pandas as pd

# http://pandas.pydata.org/index.html","import numpy as np
import pandas as pd
from pandasql import PandaSQL","X = features.values

y = data_dummies['income_ >50K'].values

print (X)

print (y)",,01,Create An Ads-Prod1->Import some additional libraries,Ml With Scikit-Learn - Cancer Dataset->Logistic Regression->Ensembles: combines multiple ML models into a more powerful model
310800,"sorted(zip(terms_selected, model_LR.coef_),  key=lambda (_,x):x, reverse=True)","def calcChurn(x):
    return x[x==True].count()*100.0/x.count()
pctgChurnByState = df.groupby('state')['churn'].apply(
    lambda x: calcChurn(x)).sort_values(ascending=False)[:10]","array([[0,1], [1,0]], float64)",,Sentiment->Working with sentiment->Selected vars,Telecom-Churn-Checkpoint->Churn in Telecom->Calculating churn by state,"Numpy, Matplotlib And Scipy - Introduction->Scipy->[Statistical Functions](https://docs.scipy.org/doc/scipy/reference/stats.html#) in Scipy->[Two-samples Kolmogorov-Smirnov test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html#scipy.stats.ks_2samp)"
329891,"no_of_users = 1000
reader = Reader(rating_scale=(-10, 10))
data = Dataset.load_from_df(ratings[ratings.userID < no_of_users], reader)","reader = Reader(rating_scale=(1, 5))
rating_data = Dataset.load_from_df(ratings[['user_id', 'movie_id', 'rating']], reader)","loaded_data = qc.load_data(loc_provider.formatter.format(loc_provider.fmt,counter='008'))

plot = qc.QtPlot()

#plot.add(x=loaded_data.VNA_fixed_freq_set, 
#         y=loaded_data.VNA_fixed_pow_set, 
#         z=loaded_data.VNA_magnitude)
plot.add(loaded_data.VNA_magnitude)
#plot.add(loaded_data.VNA_phase, subplot=2)
#plot.save()",,Collaborative Filtering-Checkpoint->Joke Recommender System->Joke Dataset->Defining the parser to read data into surprise dateframe,Movie Recommendation System->Movie Recommender->Transforming data to surprise's format,Vna Notebook->Load in data
133984,"RedRef,RedSlopes,RedIntensity,RedSTD,NewExposureTimes = GetSlopes(RedIntensity,RedSTD,ExposureTimes)
GreenRef,GreenSlopes,GreenIntensity,GreenSTD,NewExposureTimes = GetSlopes(GreenIntensity,GreenSTD,ExposureTimes)
BlueRef,BlueSlopes,BlueIntensity,BlueSTD,NewExposureTimes = GetSlopes(BlueIntensity,BlueSTD,ExposureTimes)","# Note: we use the np.copy() function rather than just saying red_channel = image
# because in Python, such a statement would set those two arrays equal to each other
# forever, meaning any changes made to one would also be made to the other!
red_channel = np.copy(image)
# Note: here instead of extracting individual channels from the image
# I'll keep all 3 color channels in each case but set the ones I'm not interested 
# in to zero.  
red_channel[:,:,[1, 2]] = 0 # Zero out the green and blue channels
green_channel = np.copy(image)
green_channel[:,:,[0, 2]] = 0 # Zero out the red and blue channels
blue_channel = np.copy(image)
blue_channel[:,:,[0, 1]] = 0 # Zero out the red and green channels

#Extra personal channels:
yellow_channel = np.copy(image)
yellow_channel[:,:,[2]] = 0

new_channel = np.copy(image)
new_channel[:,:,[0]] = 0","def assign(M,I,H):
    M_temp = M
    np.ravel(M_temp)[I] = H
    return np.reshape(M_temp,(n,n))

iblock = lambda H : assign(np.zeros([n,n]), I, H)",,Data Analysis->Curve Fitting,Process Image->Now we filter the image's colours,Denoisingwav 4 Block
32490,"MDS = manifold.MDS(n_components=2, dissimilarity=""precomputed"", random_state=6)","tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)","df={'n':[],
    'sample':[],
    'eigenvalues':[]
   }
nsample=40
for n in [5,10,20,40,100]:
  for sample in range(nsample):
    M=np.random.randn(n,n)
    M=M+M.T
    w=np.linalg.eigvalsh(M)
    df['n']+=[n]*n # this makes a list n long 
    df['sample']+=[sample]*n
    df['eigenvalues']+=list(w)",,Notebook->doc2vec and t-SNE/MDS to cluster business classifications->Try t-SNE & MDS,"W11 Sankaranarayanan Arun Ram->W11 Lab Assignment->Visualizing the `Digits` dataset
Sathler Carlos Lab10",Using Numpy->Data generation
33963,"# Import the wordcloud library
# -- YOUR CODE HERE --
from wordcloud import WordCloud
# Join the different processed titles together.
long_string = "" "".join(papers['title_processed'])

# Create a WordCloud object
wordcloud = WordCloud()

# Generate a word cloud
# -- YOUR CODE HERE --
wordcloud.generate(long_string)
# Visualize the word cloud
wordcloud.to_image()","sensing_cloud = wordcloud.WordCloud().generate(
    "" "".join(full_training_data_words.loc[full_training_data_words.sn == ""S""][""word""])
)

plt.imshow(sensing_cloud, interpolation='bilinear')
plt.axis(""off"")","x = [1,2,3,4,5,6,7]
y_out = [Counter(g_df['go_out_x'].mean().values)[i+1] for i in range(7)]
labels=['sevTimes/week', 'twice/week', 'once/week', 'twice/month', 'once/month', 'sevTimes/year', 'almost never']
plt.bar(x, y_out)
x2 = [1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5]
plt.xticks(x2, labels)
plt.title('Going out Frequency')
plt.legend();",,Notebook->5.  A word cloud to visualize the preprocessed text data,Myers Briggs Classification - Full->Myers Briggs Personality Type Classification - Full->All of the categorizations,Exploratory Stats->Pearson Correlation->Create a table to compare the above 4 plots
6671,"class TSP:
    def __init__(self, mst):
        """"""
        Initialize an TSP
        
        Args:
            mst: minimum spanning tree
        """"""
        self.nodes = mst.nodes
        self.edges_ud = {}
        self.weights = {}
        for n in mst.nodes.index:
            self.edges_ud[n] = []
        for i in mst.tree.index:
            self.edges_ud[mst.tree.loc[i, 'from']].append(mst.tree.loc[i, 'to'])
            self.edges_ud[mst.tree.loc[i, 'to']].append(mst.tree.loc[i, 'from'])
            
    def get_path(self, start):
        """"""
        get the proorder of mst
        
        Args:
            start: starting point

        Returns:
            tree: tree edges
        """"""
        stack = []
        path = []
        stack.append(start)
        while len(stack) != 0:
            city = stack.pop()
            path.append(city)
            next_citys = self.edges_ud[city]
            for c in next_citys:
                if c not in path:
                    stack.append(c)
        data = {'from':[], 'to':[]}
        for i in range(len(path)-1):
            data['from'].append(path[i])
            data['to'].append(path[i+1])
        return pd.DataFrame(data=data, columns=['from', 'to'])
    
    def opt_path(self, start):
        """"""
        get the optimized path
        
        Args:
            start: starting point

        Returns:
            tree: tree edges
        """"""
        edges_d = {}
        stack = []
        stack.append(start)
        while len(stack) != 0:
            node = stack.pop()
            edges_d[node] = []
            connect_nodes = self.edges_ud[node]
            for c in connect_nodes:
                if c not in edges_d:
                    edges_d[node].append(c)
                    stack.append(c)
        self.get_weight(edges_d, start)
        stack = []
        path = []
        stack.append(start)
        while len(stack) != 0:
            city = stack.pop()
            path.append(city)
            if city in edges_d:
                next_citys = edges_d[city]
                next_citys = sorted(next_citys, key=lambda c: -1 * self.get_distance(city, c))
                for c in next_citys:
                    stack.append(c)
        data = {'from':[], 'to':[]}
        for i in range(len(path)-1):
            data['from'].append(path[i])
            data['to'].append(path[i+1])
        return pd.DataFrame(data=data, columns=['from', 'to'])
        
        
    def get_weight(self, edges_d, node):
        """"""
        get the everage coordinate of a node and set all sub-tree of the node
        
        Args:
            edges_d: directed edges
            node: node
        """"""
        if not node in edges_d:
            self.wieghts[node] = {'number': 1, 'x': self.nodes.loc[node, 'x'], 'y': self.nodes.loc[node, 'y']}
            return
        connect_nodes = edges_d[node]
        number = 1;
        x = self.nodes.loc[node, 'x']
        y = self.nodes.loc[node, 'y']
        for cn in connect_nodes:
            self.get_weight(edges_d, cn)
            weight = self.weights[cn]
            number = number + weight['number']
            x = x + weight['x'] * weight['number']
            y = y + weight['y'] * weight['number']
        x = x / number
        y = y / number
        self.weights[node] = {'number': number, 'x': x, 'y': y}
    
    def get_distance(self, na, nb):
        """"""
        get the distance for two nodes
        
        Args:
            na: node a
            nb: node b

        Returns: distance
        """"""
        dx = self.nodes.loc[na, 'x'] - self.weights[nb]['x']
        dy = self.nodes.loc[na, 'y'] - self.weights[nb]['y']
        return math.sqrt(dx * dx + dy * dy)","class Bootstrapping():
    def __init__(self,F=100,cf=2):
        self.F = F
        self.cf = cf
        
    def getStrips(self,y,T):
        rn = {}
        for i in np.arange(len(y)):
            if T[i]<0.5:
                rn[T[i]]=y[i]
        if len(rn.keys())==0:
            print(""No strips retrieved!"")
        return rn
   
    def fadjust(self,y,T):
        x = np.isnan(y)
        T_ = []
        y_ = []
        for i in np.arange(len(y)):
            if x[i]==False:
                T_.append(T[i])
                y_.append(y[i])
        return np.array(y_),np.array(T_)
   
    def interpol(self,y,T_):
        self.y=y        
        self.T = np.arange(0.5,30.5,0.5)
        N = self.cf*self.T
        self.N = [int(x) for x in N]
        self.yn = np.interp(self.T,T_,y)
        self.rn = self.getStrips(y,T_) 
    
        
        if len(self.y)>4:      
            self.seed = [self.y[-1],self.y[0]-self.y[-1],self.y[1],self.y[2]]
        else:
            self.seed = [self.y[-1],self.y[0]-self.y[-1],self.y[1],self.y[2]]
    
    def get_price(self,y,T):
        C = y*self.F
        N = T*self.cf
        pv = 0
        for n in np.arange(N):
            pv += C/(1+y)**n
        pv += self.F/(1+y)**N
        return pv
    
    def solve(self,y,T_):      
        self.interpol(y,T_)
        for i in self.N:
            C = self.yn[i-2]*self.F/2
            rate_sum = self.get_price(self.yn[i-2],i/self.cf) 
            for j in np.arange(0.5,i/self.cf,0.5):
                rate_sum -= C*((1+self.rn[j]/self.cf)**(-j))
            self.rn[i/self.cf] = (((self.F+C)/rate_sum)**(1/i)-1)*2 
            
    def plot(self):
        d = pd.DataFrame.from_dict(self.rn,orient='index')
        d.plot(kind='scatter')","import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from random import randint
import scipy.stats as ss
#from scipy.stats import norm, kurtosis, skew",,Tutorial->Minimum Spanning Tree and Travelling Salesman Problem->Travelling Salesman Problem->Implementation of TSP,Outdated -  Bootstrap-Nelson Siegel Vasicek - Tutorial->RSM2310 - Fixed income securities->Bootstrapping class:,3
477699,AllFresh,"#list comprehension to get all the nutrients for all the foods
all_nuts = [ndb_report(x) for x in fresh_new.ndbno]","# est.params[0] is the intercept term derived from our model above, -10.6513
# est.params[1] is the slope term derived from our model above, 0.0055
prob_default = (np.exp(est.params[0]+est.params[1]*1000)) / (1 + (np.exp(est.params[0]+est.params[1]*1000)))
prob_default",,Assignment3->Assignment 3,Assignment4->Assignment 4,Chapter 4 Classification->Chapter 4 - Classification->4.3 Simple Logistic Regression->Statsmodel
49279,"%%time
%%capture
%%pypy
N = int(10e6)
s = 0
for j in xrange(1000):
    for i in xrange(N):
        s+=i","%%time
def count(N):
    s = 0
    ex = int(math.log(math.sin(math.pi / 2)))
    for i in xrange(N):
        for j in xrange(N * i):
            # now a simple addition
            s += ex + 1
    return s
print count(250)","#ignore
#import csv

#csvfile = ""/Users/tuleg/Documents/Columbia University/Semester 2/Data Science/dailyimpressions""

#with open(csvfile, ""w"") as output:
 #   writer = csv.writer(output, lineterminator='\n')
  #  for val in Impressions:
   #     writer.writerow([val])",,Comparison->Numpy->Pypy,Pffcourse 06->Improved Python,Bitcoin Prediction Final Project-Checkpoint
339427,"prob_model = create_UniLSTMwithAttention(X_vocab_len,X_max_len,y_vocab_len,y_max_len,HIDDEN_DIM,return_probabilities=True)","def create_UniLSTMwithAttention(X_vocab_len, X_max_len, y_vocab_len, y_max_len, hidden_size, num_layers, return_probabilities = False):
    # TO-DO
    # create and return the model for unidirectional LSTM encoder decoder
    model = Sequential()

    # Creating encoder network
    model.add(Embedding(X_vocab_len, HIDDEN_DIM, input_length=X_max_len, mask_zero=True))
    model.add(LSTM(HIDDEN_DIM,return_sequences=True))
    #model.add(RepeatVector(y_max_len))
    model.add(AttentionDecoder(HIDDEN_DIM, y_vocab_len))
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
    # create and return the model for unidirectional LSTM encoder decoder with attention
    return model",print(j['sources'][0]['chemicals'][0]['externalLink']),,Hw4-Checkpoint->Unidirectional LSTM Encoder Decoder->Bad Examples,Project->Analysis,Use-Snur-Data->Looking at 'sources': can we get SNUR info?->Try to get SNUR information for a known ID
412608,"# Let's display some of the data to ensure data is good
plt.plot(test_labels)
plt.show()","pred_test = model.predict(x_test)
plt.figure()
pca_label_plot(x_test, y_test, pred_test, pca=pca)
plt.show()
plt.close()",Image(filename='4a_ensemble1.png'),,1 Notmnist,Multiclass Logistic Regression->Multiclass Logistic Regression->Plotting the predictions,Problem Set Zhen->Problem Set->Problem 4
127452,"fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(6, 1, figsize=(14,20))
l1 = tmp[tmp['gt'] == 'stand']['acc'].plot(ax=ax1, color = sns.color_palette()[0], label='stand')
l2 = tmp[tmp['gt'] == 'sit']['acc'].plot(ax=ax2, color = sns.color_palette()[1], label='sit')
l3 = tmp[tmp['gt'] == 'walk']['acc'].plot(ax=ax3, color = sns.color_palette()[2], label='walk')
l4 = tmp[tmp['gt'] == 'stairsup']['acc'].plot(ax=ax4, color = sns.color_palette()[3], label='stairsup')
l5 = tmp[tmp['gt'] == 'stairsdown']['acc'].plot(ax=ax5, color = sns.color_palette()[4], label='stairsdown')
l6 = tmp[tmp['gt'] == 'bike']['acc'].plot(ax=ax6, color = sns.color_palette()[5], label='bike')

for ax in (ax1, ax2, ax3, ax4, ax5, ax6):
    ax.axes.xaxis.set_ticklabels([])
    ax.set_xlabel('')
    ax.set_ylim([4, 20])
    ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
    
ax6.set_xlabel('Time (5 minute window)')
ax1.set_title('Acceleration for the 6 Activities')","##Plotting the results for the 3rd sub-experiment:
#f1 = plt.figure(figsize=(10,3))
#ax1 = f1.add_subplot(121)
#ax2 = f1.add_subplot(122)

#f2 = plt.figure(figsize=(10,3))
#ax3 = f2.add_subplot(121)
#ax4 = f2.add_subplot(122)


##Subplots for acc2
ax1.plot(time_array3, acc_array3)
ax1.set_title('Acc3 vs. Time')
ax1.set_xlabel(""Time [s]"")
ax1.set_ylabel('Acceleration [m/s^2]')
ax1.set_ylim([-6,6])
ax2.plot(time_array3, dacc_array3)
ax2.set_title(""Detrended Acc3 vs. Time"")
ax2.set_xlabel(""Time [s]"")
ax2.set_ylabel('Acceleration [m/s^2]')
ax2.set_ylim([-6,6])

##Subplots for acc3
ax3.plot(ro_time3, ro_array3)
ax3.set_title('Removed Acc3 Outliers vs. Time')
ax3.set_xlabel(""Time [s]"")
ax3.set_ylabel('Acceleration [m/s^2]')
ax3.set_ylim([-6,6])
ax4.plot(time_array3, interp_array3)
ax4.set_title(""Cubic Spline Interpolated Acc3 vs. Time"")
ax4.set_xlabel(""Time [s]"")
ax4.set_ylabel('Acceleration [m/s^2]')
ax4.set_ylim([-6,6])

plt.show()","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import Imputer
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from mpl_toolkits.mplot3d import Axes3D
import scipy
import math
import random",,Final Project->Table of Contents->Initial Data Analysis->Exploratory Analysis,"Psd E3 Buoy Callibrator1->Plot the data, to see visually, focus on t=[300,800]:",Gpu Research->1. Imports of essential modules/ Global settings->1.1 Modules
178338,"# Not even in the Twitter API the urls of these tweets could be found
image_predictions[image_predictions['tweet_id'].isin(list(map(str, my_non_url_non_replies_ids)))]","## If a hashtag appeared in training set, discard this record

Validation_set_no_train = Validation_set.where(Validation_set.tweet_id.isin(Invalid_Val_ids_list) == False)","%%cython

cpdef double array_mean_cython_loop(double [:] array):
    cdef double total = 0
    cdef int    length = len(array)
    cdef int    i
    for i in range(length):
        total += array[i]
    return total / len(array)",,Wrangle Act->Assess,Spark-Etl-Ml-Pipeline-Merged->Validation Labeling,Compare->Making Python run at the speed of light (C).->Introducing Numpy
147533,"import pandas as pd
import numpy as np
import random
import statsmodels.api as sm
import matplotlib.pyplot as plt
#We are setting the seed to assure you get the same answers on quizzes as we set up
random.seed(42)","import pandas as pd
import numpy as np
import random
import matplotlib.pyplot as plt
import statsmodels.api as sm
%matplotlib inline
random.seed(42)","print(""TOP 5 negative Correlation with Salary:"")
print(corr.salary.sort_values()[0:5],""\n"")

print(""TOP 5 positive Correlation with Salary:"")
print(corr.salary.sort_values()[-6:])",,Analyze Ab Test Results Notebook->Analyze A/B Test Results,Ab Testing->Table of Contents,Indeed-Prediction->TOP 5 CORRELAITONS
102610,"#group by dates
date_df = df_perTalk.sort_values(by=['Date'])
count_date_df = date_df['Date'].value_counts()
print(count_date_df)
print(count_date_df.plot(linewidth=7, figsize=(18,6)))","# Date is chosen as count column, any column could have been chosen
df.groupby('City').Date.count().sort_values(ascending=False)","data.REVENUE = data.REVENUE.astype(""float"")
data[""paid""] = (data.REVENUE > 0).astype(int)
data[""condition""] = data.VARIANT_NAME.apply(lambda x: 0.0 if x == 'control' else 1.0).astype(""float64"")
data[""constant""] = 1.0

def kde_scipy(x, x_grid, bandwidth=0.2):
    kde = gaussian_kde(x, bw_method=bandwidth / x.std(ddof=1))
    return kde.evaluate(x_grid)",,Project->An interesting observation is that talks that the least number of views had a very high Net Sentiment i.e. Net Sentiment >90 where as talks with the Highest views had a net sentiment lesser than <83.,"School Shootings Exploration->School Shootings Dataset, Exploration->Rank cities by number of school shootings",Badoo Ab Test->Primary analysis
196833,"def augment_brightness(imgArray):
    retArr = {}
    i = 0
    for img in imgArray:
        retImg = cv2.cvtColor(img,cv2.COLOR_RGB2HSV)
        retImg = np.array( retImg, dtype = np.float64)
        random_bright = .5+np.random.uniform()
        retImg[:,:,2] =  retImg[:,:,2]*random_bright
        retImg[:,:,2][ retImg[:,:,2]>255]  = 255
        retImg = np.array( retImg, dtype = np.uint8)
        retImg = cv2.cvtColor( retImg,cv2.COLOR_HSV2RGB)
        retArr[i] = retImg
        i=i+1
    return imgArray","def augment_brightness_camera_images(image):
    image1 = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)
    random_bright = .25+np.random.uniform()
    #print(random_bright)
    image1[:,:,2] = image1[:,:,2]*random_bright
    image1 = cv2.cvtColor(image1,cv2.COLOR_HSV2RGB)
    return image1","from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB 
X_train, X_test, Y_train, Y_test = train_test_split(traindata, trainlabels, test_size=.40, random_state=56)",,"Traffic Sign Classifier->Pre-process the Data Set (normalization, grayscale, etc.)->Define Optimization Functions","Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree
Main Eda Drive Ps4 Final->Python code for generating the trainer->Augmentation 2: Brightness augmentation",Algorithms On Sentence Corpus Data->Dataset: Sentence Corpus->Splitting Training & Testing Data
238960,"class Animal:
    """"""This is an Animal""""""
    def __init__(self, can_fly = False):
        self.can_fly = can_fly
    def fly(self):
        if self.can_fly:
            print(""I CAN fly!"")
        else:
            print(""I can not fly!"")
class Bird:
    """"""This is a Bird""""""
    def fly(self):
        print(""I'm flying high!"")
bird = Bird()
bird.fly()    # I'm flying high!","class Bird():
    '''
    Creating a class called Bird - note classes first letter is in caps.
    '''
    
    #what if we want to initialize with some attributes?
    def __init__(self, species = 'Nothing Yet'):
        self.species = species
        
    #create a method for this class
    #note the syntax where self is in the function - ie 
    #the function ACTS on itself. 
    def fly(self):
        print('The {} is out to fly.'.format(self.species))","fig, ax1=plt.subplots(figsize=(13,6))
ax = df['Memory_Bandwidth'].fillna(0).astype(float).groupby(df['Release_Year']).mean().plot(kind='line', zorder=9999); 
df['Memory_Speed'].str[:-5].fillna(0).astype(float).groupby(df['Release_Year']).mean().plot(ax=ax, kind='line',secondary_y=True)
ax.set_ylabel('Memory Speed MHz', fontsize=10);

plt.title('Mean memory bandwidth and speed by release year')
plt.xlabel('Release year')
plt.ylabel('Memory bandwidth GB/sec')
plt.show()",,Py3-In-One-Pic->Override,Lab8 Space Weather Bootstraps->Lab Eight - CLaSP 405 - 002 - Space Weather Part 2->*Part 4 - In the below section we try a different grouping of data*,"Script675->GPU EDA->2.0 Release dates<a class=""anchor"" id=""release-dates""></a>->5.3 Memory speed and bandwidth  <a class=""anchor"" id=""memory-speed""></a>"
47757,from scipy.interpolate import interp1d,"from scipy.integrate import quad
from scipy.interpolate import interp1d","mape_tree2 = mean_absolute_percentage_error(Y_flow_test2, predict2)
mape_tree2",,"Day3->TAURUS 2018->Interpolating Data
7->Implementing TF-IDF->Plotting Loss and Accuracy
Scipy->Scipy->1D interpolation
Interpolation->Interpolation->1d data
Interpolation->Interpolation->2d structured
Ana Eq Wdt Calib Spectrum->Use spectra of CTIO Data in June 2017 to do atmospheric studies
Fourier Series->main functions->Another example
Scan Aerosols2->Scan aerosols
05 Plotting Analysing->Plotting and Analysing Plotted Data->Matplotlib->Multiple Plots
First Mtrt Comparison->First Modtran LibRadTran Models comparison
Gyro Exercise Py->Processing exercise data with Python->We will use scipy.interpolate to do a spline (cubic interpolation)
Initial Mass Function->Crude Initial Mass Function
Ixs-Alignment-Clean->Alignment and interpolation of inelastic scattering data->Set up the interpolator
Unit04 08->Interpolate
Scan Aerosols3",Pycse-3-4-Math-Polynomials->Kitchin — Python for Science and Engineering->Math,Tree-Regression-With-Rainfall->Tree regression on three features->MAPE for tree regression for three features
183181,"# extract columns of dog information from origin_tweet dataframe
dog_df = origin_tweet[['tweet_id', 'rating_numerator', 'rating_denominator', 'rating_value', 'name', 'doggo', 'floofer', 'pupper', 'puppo']]
# add image prediction results to dog_df
dog_df = dog_df.merge(image_prediction[['tweet_id', 'p1', 'p1_conf', 'p1_dog', 'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog']], how = 'left', on = 'tweet_id')
dog_df.head()","lm = smf.ols('Counts_of_Rodent_Complaints~Number_of_business', data=dfMerg).fit()
lm.summary()","# Get some parameters for the Stereographic Projection
lon_0 = lons.mean()
lat_0 = lats.mean()

m = Basemap(width=5000000,height=3500000,
            resolution='l',projection='stere',\
            lat_ts=40,lat_0=lat_0,lon_0=lon_0)",,Wrangle Act->Assessing data->Dog dataframe,Problem Set1->Prioritizing Service Delivery of Rodent Issue in NYC->4. Analysis->Counts_of_Rodent_Complain = $w_1$ * Number_of_business + $w_2$,2013-10-12-Plot-Netcdf-Data->How can I plot netcdf data using python?
140044,age_gender_bkts_data.head(3),"dataAg = pd.ExcelFile('ModifiedData/Ag.xlsx')
print(dataAg.sheet_names)
dfAg = dataAg.parse('sheet1')
dfAg.head(5)

df = dfAg.drop(['Reference', 'Run #', 'D(Ag)', '1/D'], axis=1)
df.dropna()
# df.options.display.max_rows = 999
df.info()
df.head(5)","import xml.etree.ElementTree as ET
import re
from pymongo import MongoClient",,"Eda->History Kaggle: Airbnb New User Bookings->Data Exploration->Exploratory Visualization->Supplementary dataset: Age, Gender Statistics (1 file)",06 Validation Ag Cu->Ag vs Cu,Osm Mongo
64584,"##################
wordlist = []
for review in rowdf['Review']:
        review = review.lower()
        # Create a list with all the terms
        terms_stop = [term for term in preprocess(review) if term not in stop]
        wordlist.append(terms_stop)
        # Update the counter
        count_all2.update(terms_stop)
        terms_bigram = bigrams(terms_stop)
        count_all3.update(terms_bigram)
    # Print the first 5 most frequent words
print(count_all2.most_common(10))
print(count_all3.most_common(10))
print wordlist[:2]","# Amazon - Top parameters
from nltk import bigrams
from collections import Counter
#Top Keywords
count_bigram=Counter()
count_all = Counter()
for text in db1['text']:
    terms_all = [term for term in preprocess(text) if term not in stop and not term.startswith(('@','#','http'))]
    terms_bigram = bigrams(terms_all)
    count_all.update(terms_all)
    count_bigram.update(terms_bigram)
m1=(count_all.most_common(15))
bi=(count_bigram.most_common(15))

#Top Hashtags   
count_hash = Counter()
for text in db1['text']:
    terms_hash = [term for term in preprocess(text) if term.startswith('#')]
    terms_hash=[c.lower() for c in terms_hash]
    count_hash.update(terms_hash)
m2=(count_hash.most_common(15))

#Top @-Mentions  
count_at = Counter()
for text in db1['text']:
    terms_at = [term for term in preprocess(text) if term.startswith('@')]
    count_at.update(terms_at)
m3=(count_at.most_common(20))
    
# eBay - Top parameters
#Top Keywords
stop=stop+ ['Read','ebay','Ebay','eBay','10','Please','8','16','7','Now','Only', 'New', 'GB','4','NEW','5','6','\u2026','HOLY','25','Gift','pm','2','1','Entire','follows','d83d','6.00','A','give','dogs','dog','amp','The','I','away','person','giving','one','random','tweet','rt','50','EST','3','RTs','RT','\u2026','Amazon','Clinton', 'via']
count_all = Counter()
for text in eb1['text']:
    terms_all = [term for term in preprocess(text) if term not in stop and not term.startswith(('@','#','http'))]
    count_all.update(terms_all)
m4=(count_all.most_common(15))

#Top Hashtags   
count_hash = Counter()
for text in eb1['text']:
    terms_hash = [term for term in preprocess(text) if term.startswith('#')]
    terms_hash=[c.lower() for c in terms_hash]
    count_hash.update(terms_hash)
m5=(count_hash.most_common(15))

#Top @-Mentions  
count_at = Counter()
for text in eb1['text']:
    terms_at = [term for term in preprocess(text) if term.startswith('@')]
    count_at.update(terms_at)
m6=(count_at.most_common(20))","X = happy_index_simple[metrics]
y = happy_index_simple[""score""]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111)",,"Hw2->Question E->Get a list of words and each word's count. By get the top counts words list, we could choose out what is the most popular positive words and negative words in this context. Then we could get a list of positive words and negative words.","I Python Notebook Data Miners Twitter Sentiment Analysis Jigar Mehta Vishesh Dosaj->amazon vs eBay - Top Keywords, Hashtags and @-Mentions",Learn
397520,"from sklearn.datasets.samples_generator import make_moons

X, y = make_moons(n_samples=200, noise=0.10, random_state=0)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=discrete_cmap)

clf = SVC(kernel='linear')
clf.fit(X, y)
plots.draw_svm_decision_function(clf, colors='black')","from sklearn.datasets.samples_generator import make_moons

X, y = make_moons(n_samples=200, noise=0.10, random_state=0)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=discrete_cmap)

clf = SVC(kernel='linear')
clf.fit(X, y)
plots.draw_svm_decision_function(clf, colors='black')","a = np.array((4., 2.))",,Smd Ml->Classifier Validation->Validation on independent test sets,Smd Ml->Classifier Validation->Validation on independent test sets,Lesson-3-Shape-Manipulation->Splitting one array into serveral smaller ones->Stacking together different arrays
437307,"# You have seen this before!
# If you are so inclined, you may want to tweak the test_size and see how the model performs
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
model = GaussianNB()
model.fit(X_train, y_train)
model.score(X_test, y_test)","# Scikit has helpers for testing and evaluating models in a proper train/validate paradigm.
from sklearn.model_selection import train_test_split

# This function returns four sets:
# Training features
#       # Testing Features
#       #        # Training Expected Result 
#       #        #        # Testing expected result
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)

# Get ""blank model""
model = GaussianNB()
model.fit(X_train, y_train) # Train it 
model.score(X_test, y_test) # Validate its training with some withheld training data.",empty_taxons_not_world = pd.read_csv('../../data/empty_tags_not_world.csv'),,Classification->Module 1: Classification->Make the train/validation split and then train the model,Train And Validate->Module 1: Training and Validation->Hold out 25% for validation only,Eda-Count-Data->Content data->Empty taxons
368961,"paid_engagement_in_first_week = []
for engagement_record in paid_engagement:
    account_key = engagement_record['account_key']
    join_date = paid_students[account_key]
    engagement_record_date = engagement_record['utc_date']

    if within_one_week(join_date, engagement_record_date):
         paid_engagement_in_first_week.append(engagement_record)

len(paid_engagement_in_first_week)","# Find number of engagement records from a student's first week
paid_engagement_in_first_week = []
for engagement_record in paid_engagement: 
    account_key = engagement_record['account_key']
    join_date = paid_students[account_key]
    engagement_record_date = engagement_record['utc_date']
    # Check if two dates are within one week of each other 
    if within_one_week(join_date, engagement_record_date):
        paid_engagement_in_first_week.append(engagement_record)

print ""Number of engagement records from a student's first week:"", len(paid_engagement_in_first_week)","# We display n random images and their associated label to check consistency has been preserved
n = 10 # number of images to display

def checkReshuffling(dataset,labels):
    dataset_size = dataset.shape[0]
    idxs = random.sample(range(dataset_size),n)

    fig = plt.figure(figsize = (n*image_size, image_size))
    for c, idx in enumerate(idxs):
        plt.subplot(n,1,1+c)
        plt.imshow(dataset[idx,:,:], cmap='binary')
        plt.title(letters[labels[idx]])

checkReshuffling(train_dataset,train_labels)",,Lesson 1->Explore Phase,Student Engagement->Data Analysis of Udacity Student Engagements->3) Investigating the Data->Investigation: Does daily engagement affect the project completion rate?,1 Notmnist
420149,"# Proportion of blacks that were called
b_prop=b_call/b_size
b_prop","# Proportion of blacks that were called
b_prop=b_call/b_size
b_prop","exp_data = exp_with_gene_info.loc[:,'7441_129S1_HFD':]
exp_data.shape",,Sliderule Dsi Inferential Statistics Exercise 2->Examining Racial Discrimination in the US Job Market->Exercises,Sliderule Dsi Inferential Statistics Exercise 2->Examining Racial Discrimination in the US Job Market->Exercises,Pandas And Plotting->Another boxplot shoing the effect of diet
47408,"filter_options = {
    'Attack':   100,
    'Type':     'Bug',

}

filter_pokedex(parsed_poke_info,filter_options, '<=')","filter_options = {
    'Attack':   100,
    'Type':     'Bug',

}

filter_pokedex(parsed_poke_info,filter_options, '<=')","#dfTrain['Age'].describe()

# Create bins
bins = np.linspace(0.42, 80, 9)
print(""bins: {}"".format(bins))

# Binning training set
which_bin = np.digitize(dfTrain['Age'], bins=bins)
print(""\nData points:\n"", dfTrain['Age'][:5])
print(""\nBin membership for data points:\n"", which_bin[:5])

# Binning test set
which_bin2 = np.digitize(dfTest['Age'], bins=bins)",,Project-1-V2-Starter-Code->Test Case #5:,Project-1-V2-Starter-Code->Test Case #5:,Eda & Prediction - Titanic (Score-0->Titanic EDA and Survival Prediction->Missing Values & Feature Engineering
286253,"# ITI = installment to income
data['iti'] = data['installment'] / data['annual_inc']","data = {'a': [4.1, 10, 6.3],
        'b': ['foo', 'bar', 'baz'],
        'c': 42}
data","a_list = ['conner', 'conner', 'conner','evan','max','evan']

def remove_dup(a_list):
    a_list = sorted(a_list)
    out = [a_list[0]]
    n = len(a_list)
    for i in range(1,n-1):
        if a_list[i] != a_list[i + 1]:
            out.append(a_list[i + 1])
    return out

remove_dup(a_list)",,2 1 Data Preparation Feature Engineering->Feature Selection->Annual Income Ratios,"Insights In Python->Insights in Python->Good coding practices <a name=""good-code""></a>->Debugging","Tuesday Lesson->Functions, Scoping, Data Collections 1 & List Comprehensions->Tasks Today:->Remove duplicate in a list"
251753,"best_features = []
for i in indices[:5]:
    best_features.append(features[i])","best_features = []
for i in indices[:5]:
    best_features.append(features[i])","np.savetxt(fname='/Users/Sebastian/Desktop/images.csv', 
           X=X, delimiter=',', fmt='%d')
np.savetxt(fname='/Users/Sebastian/Desktop/labels.csv', 
           X=y, delimiter=',', fmt='%d')",,"Indian-Liver-Patient-Classification->SVM with RBF kernel accuracy and time elapsed caculation->Feature importances with forests of trees
Connect-4 Classification","Indian-Liver-Patient-Classification->SVM with RBF kernel accuracy and time elapsed caculation->Feature importances with forests of trees
Connect-4 Classification",Loadlocal Mnist->Load the MNIST Dataset from Local Files->Examples->Example 1 - Loading MNIST into NumPy Arrays->Store as CSV Files
264083,files = glob.iglob('models/gumbel/*.p'),files = glob.iglob('models/gauss/*.p'),"# Train test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y , test_size = 0.3)",,Visualization->Gumbel,Visualization->Gaussian,Main Python Deployment Pipeline-Checkpoint->MAIN PYTHON DEPLOYMENT PIPELINE->TEAM # Submission->DATA PREPARATION
89092,"%matplotlib inline
import random
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from matplotlib import animation, rc
from sklearn.linear_model import LinearRegression
from IPython.display import HTML","import pandas as panda
import matplotlib.pyplot as plt
from sklearn import linear_model
import numpy as np
import mpld3

%matplotlib inline","# calculate an average of some numbers 

number1 = float(input('number 1 : '))
number2 = float(input('number 2 : '))
number3 = float(input('number 3 : '))
number4 = float(input('number 4 : '))
number5 = float(input('number 5 : ')) # we need to call input function 5 times!

average = (number1+number2+number3+number4+number5)/5

print('Average = ',average)",,Linear Regression->Partial derivative equations,Advanced Scatter->Annotating scatter plots + Web Visualization,Repetition Structure
274996,"data=readAllVASPFiles(folders,potential=pot,AtomicNum=[int(system[4]), int(system[5])],numStructToRead=2500)","import glob

path = DATA_FOLDER + '/microbiome'
allFiles = glob.glob(path + '/MID*.xls')
spreadsheets = [pd.read_excel(file, sheetname='Sheet 1',  header=None, names=['Name', 'Counts']) for file in allFiles]","import pandas as pd # data management
import numpy as np # computations
import matplotlib.pyplot as plt # plotting
from scipy.stats import kurtosis #assess kurtosis, function
from scipy.stats import skew #assess skewness, function
from statsmodels.tsa.stattools import adfuller #for augmented dickey-fuller test

df = pd.read_csv('BB_AirT_12_17.csv', names=['Y','M','D','T(C)'], skiprows=1) #load CSV into variable df

print(df) #see matrix",,Results->Tutorial for generating Al-Ti binary alloy dataset using LAMMPS.->Importing necessary packages,Homework1->Homework 1->Answer,Load Data Bb Air T Analysis->GEOS 518 Notebook 1: Reading in Time Series Data->1) Load libraries and import data
350567,"features = ['average_speed', 'distance', 'average_watts','total_elevation_gain', 'time_min']
target = ['kmeans']","gb_featur_imp = pd.concat([pd.Series(X15.columns,name=""Features""),\
                           pd.Series(gb.feature_importances_,name=""Feature Importance"")],axis=1)
gb_featur_imp.sort_values('Feature Importance',ascending=False,inplace = True)
gb_featur_imp","def astar_search(start, goal, heuristic, return_cost=False, return_nexp=False):
    '''A* search from `start` to `goal`
    start = initial state
    goal = goal state
    heuristic = function for estimated cost to goal (function name)
    return_cost = logical (True/False) for whether or not to return the total path cost
    return_nexp = logical (True/False) for whether or not to return the number of nodes explored
    '''
    # check that the initial state is even solvable
    frontier = Frontier_PQ(start, 0)
    previous = {start : None}
    explored = {}
    n_exp = 0
    while frontier:
        s = frontier.pop()
        n_exp += 1
        if s[1] == goal:
            if return_cost: return (path(previous, s[1]), s[0], n_exp) if return_nexp else (path(previous, s[1]), s[0])
            return path(previous, s[1])
        
        explored[s[1]] = len(path(previous, s[1]))-1   # -1 to offset the initial state (so lowest path cost is 0)
        adjacent = adjacent_states(s[1])
        for s2 in adjacent:
            newcost = explored[s[1]] + 1 + heuristic(s2, goal)
            if (s2 not in explored) and (s2 not in frontier.states):
                frontier.add(s2, newcost)
                previous[s2] = s[1]
            elif (s2 in frontier.states) and (frontier.states[s2] > newcost):
                frontier.replace(s2, newcost)
                previous[s2] = s[1]",,5->Target variables,Capstone-Predicting Expected Profit Str-Checkpoint->AirBnB->Gradient Boosting,"Csci3202 Practicum Samuel Leon->CSCI 3202, Spring 2018: Practicum->Part D"
244089,"def test_hog_features(fnames,name=""Car"",nameHOG=""Car HOG""):
    # Define HOG parameters
    orient = 9
    pix_per_cell = 8
    cell_per_block = 2    
    
    mx=8
    fig, axes = plt.subplots(2, mx, figsize=(40, 12))
    fig.tight_layout()

    for im in range(0,mx):
        img = mpimg.imread(fnames[im])
        axes[0,im].set_title(name, fontsize=30)
        axes[0,im].imshow(img)
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        features, hog_image = get_hog_features(gray, orient, 
                        pix_per_cell, cell_per_block, 
                        vis=True, feature_vec=False)
        axes[1,im].set_title(nameHOG, fontsize=30)
        axes[1,im].imshow(hog_image, cmap='gray')
#-----------------------------------------------------------------------------
test_hog_features(car_fnames,name=""Car"",nameHOG=""Car HOG"")
test_hog_features(notcar_fnames,name=""notCar"",nameHOG=""notCar HOG"")","car_ind = np.random.randint(0, len(cars))
notcar_ind = np.random.randint(0, len(notcars))
car_image = mpimg.imread(cars[car_ind])
notcar_image = mpimg.imread(notcars[notcar_ind])
car_feature_image = cv2.cvtColor(car_image, cv2.COLOR_RGB2YCrCb)
notcar_feature_image = cv2.cvtColor(notcar_image, cv2.COLOR_RGB2YCrCb)

car_features_ch0, car_hog_image_ch0 = get_hog_features(car_feature_image[:,:,0],orient, pix_per_cell, cell_per_block, vis=True, feature_vec=False)
car_features_ch1, car_hog_image_ch1 = get_hog_features(car_feature_image[:,:,1],orient, pix_per_cell, cell_per_block, vis=True, feature_vec=False)
car_features_ch2, car_hog_image_ch2 = get_hog_features(car_feature_image[:,:,2],orient, pix_per_cell, cell_per_block, vis=True, feature_vec=False)

notcar_features_ch0, notcar_hog_image_ch0 = get_hog_features(notcar_feature_image[:,:,0],orient, pix_per_cell, cell_per_block, vis=True, feature_vec=False)
notcar_features_ch1, notcar_hog_image_ch1 = get_hog_features(notcar_feature_image[:,:,1],orient, pix_per_cell, cell_per_block, vis=True, feature_vec=False)
notcar_features_ch2, notcar_hog_image_ch2 = get_hog_features(notcar_feature_image[:,:,2],orient, pix_per_cell, cell_per_block, vis=True, feature_vec=False)

# Plot an example of raw and scaled features
fig = plt.figure()
plt.subplot(221)
plt.imshow(car_feature_image[:,:,0],cmap='gray')
plt.title('Car CH-1')
plt.subplot(222)
plt.imshow(car_hog_image_ch0,cmap='gray')
plt.title('Car CH-1 Hog')
plt.subplot(223)
plt.imshow(notcar_feature_image[:,:,0], cmap='gray')
plt.title('not-Car CH-1')
plt.subplot(224)
plt.imshow(notcar_hog_image_ch0,cmap='gray')
plt.title('not-Car CH-1 Hog')

fig.tight_layout()

plt.show()","components = []
for feat in example_ensemble.features:
    components.append(feat.base_component)

print(components)

# or, use the one-liner which gives the same result:
components = [feat.base_component for feat in example_ensemble.features]

print(components)",,P5->Self-Driving Car Engineer Nanodegree->2. Histogram of Oriented Gradients (HOG) Testing,Training,Medusa Objects->Strategies for getting information about an ensemble and its members
380174,"json_file= '../raw_data/PDAC_forced_phot_results/catalogForced'+str(i)+'.json'
tab = json.loads(open(json_file).read())

#print(tab['result']['table'].keys())
#print(tab['result']['table']['metadata']['elements'])

meta = tab['result']['table']['metadata']['elements']
data = tab['result']['table']['data']
dataT = np.transpose(data)

t = Table()

# iterate over columns...
for i in range(len(dataT)):
    t.add_column(Column(dataT[i], name=meta[i]['name']), index=0)
    
# Save that as a txt file...
fname = '../raw_data/RRLyr_S82_PDAC/'+str(master_table['Num'][i])+'_'+str(i)+'_g.txt'
t.write(fname, format='ascii', overwrite='True')","# Save the model to JSON
model_json = model.to_json()
with open(""COMP5318_CNN_90_deeper_no_fc.json"", ""w"") as json_file:
    json_file.write(model_json)

# Save the weights to h5
model.save_weights(""COMP5318_CNN_90_deeper_no_fc.h5"")
print(""Saved model to disk"")

# load json and create model
json_file = open('COMP5318_CNN_90_deeper_no_fc.json', 'r')
loaded_model_json = json_file.read()
json_file.close()
model = model_from_json(loaded_model_json)

# load weights into new model
model.load_weights(""COMP5318_CNN_90_deeper_no_fc.h5"")
print(""Loaded model from disk"")

#model.compile(optimizer='rmsprop',loss='mse')","NewText1 = ""This movie is great! I like it because it is very good!""
NewText2 = ""Fantastic movie!""
NewText3 = ""Maybe I do not like this movie.""
NewText4 = ""Meh ...""
NewText5 = ""If I were a drunk teenager then this movie might be good.""
NewText6 = ""Worst movie!""
NewText7 = ""Not a nice movie!""
NewText8 = ""This movie really sucks! Can I get my money back please?""
Texts = [NewText1, NewText2, NewText3, NewText4, NewText5, NewText6, NewText7, NewText8]",,"02->Calculate periodograms, and find best period for PDAC lightcurves",Cnn->Import the Packages,3
251244,"plt.figure(figsize=(5, 2))
plt.hist(cyl, alpha=0.5)
plt.xlabel('Number of cylinders')
plt.ylabel('Number of cars')
plt.show()","plt.hist(df['Milk.Prod'],bins=40,)
plt.xlabel('Milk.Prod')
plt.ylabel('Frequency')

plt.show()",Image(filename='tf_files/testing_images/whatis2.jpg'),,Data->Manipulating and Visualizing Data->Histograms,Aml Lab1 Time Series->3: get rid of unneeded columns,Presentation->Tebak Gambar 6
363637,"from IPython.display import YouTubeVideo
YouTubeVideo('iYRQpcJVQx8')","from IPython.display import YouTubeVideo
YouTubeVideo(""3vHqmPF4VBA"")","x.cut_largest_other_s1(largest_other_s1_max=40, plot=True, apply=True, bins=100, 
                       norm=LogNorm(), range=((0, 2000), (0, 200)), vmin=1, vmax=500)
plt.colorbar()
plt.show()",,Proofof Principle Notebook V1->A proof-of-principle Jupyter Notebook using ATLAS dijet data->Experimental Setup,Overview Notes Pytds 3->PYT-DS SAISOFT,Na Minimal->New Na-22 notebook->Cuts->Single scatter S1
119552,"sim = Simulator(PID_Naive(kp=2.0, kd=1.8), commands=[2*np.pi + np.pi/4])
HTML(sim.animate().to_html5_video())","# display inline -- this takes a bit to compute and load
# the ""interval"" argument determines the length of the encode
HTML(anim.to_html5_video())","# SOLUTION
data_set.example_units.keys()",,1->Chapter 1.4 - PID Control->Motivating Example->Naive PID Control,Animation Examples->A Projectile in flight,Ephys Observatory
165913,"%%time
labeler = LabelAnnotator(lfs=[])

# Only grab candidates that have labels
cids = session.query(Candidate.id).filter(Candidate.id.in_(train_ids))
L_train = labeler.load_matrix(session, cids_query=cids)

cids = session.query(Candidate.id).filter(Candidate.id.in_(dev_ids))
L_dev = labeler.load_matrix(session, cids_query=cids)

cids = session.query(Candidate.id).filter(Candidate.id.in_(train_hand_label_ids))
L_train_hand_label = labeler.load_matrix(session, cids_query=cids)","labeler = LabelAnnotator(f=None)

L_train = labeler.load_matrix(session,split=0)
L_dev = labeler.load_matrix(session,split=1)
L_test = labeler.load_matrix(session,split=2)","#storing scraped original info
scraped_info=df2015_with_info
#data I move on with
df2015_with_info=scraped_info.copy()",,3->Load preprocessed data,3->Load preprocessed data,Part I->Data Acquisition
256860,"df = pd.read_csv('../../datasets/international-airline-passengers.csv')
data = df['Passengers']
normalized_data = (data - np.mean(data)) / np.std(data)
normalized_data.head()
# Split to train and test
pos = int(0.8 * len(normalized_data))
train_data = normalized_data[:pos].as_matrix()
test_data = normalized_data[pos:].as_matrix()
plt.plot(data)
plt.show()","#So far: only potholes in our data, grouped-by the datazone where they were reported.

#Next goal: count the potholes/pavement issues in the same zone

dataF = dataF['category'].count()

#Now when we have that, we would lile to store it in a new dataframe 

dataPo = pd.DataFrame(data=dataF).rename(index=str, columns={""name"": ""Datazone"", ""category"": ""Potholes""})

print (dataPo.head(10))","import nltk
import sklearn
import sklearn_crfsuite
from sklearn_crfsuite import scorers
from sklearn_crfsuite import metrics
import gensim
import re",,9->RNN for Time Series->Load sample Time Series,Week1Lecture3->Your turn->Creating a bar chart,Final
379980,"values = list(range(-5, 6))
x, y = np.meshgrid(values, values)
x = x.flatten()
y = y.flatten()
points = np.array([x, y])

plt.plot(points[0, :], points[1, :], 'o')
plt.title('Original Grid')
plt.show()

transformed = A.dot(points)
plt.plot(transformed[0, :], transformed[1, :], 'o')
plt.title('Transformed by A')
plt.show()

transformed = bad_matrix.dot(points)
plt.plot(transformed[0, :], transformed[1, :], 'o')
plt.title('Transformed by bad_matrix')
plt.show()","# Visualize trained linear moder
data_range = np.linspace(-1, 1, 100)
X_meshgrid, Y_meshgrid = np.meshgrid(data_range, data_range)
Z_meshgrid = np.dot(np.c_[X_meshgrid.flatten(), Y_meshgrid.flatten()], W) + b
plt.contourf(X_meshgrid, Y_meshgrid, np.argmax(Z_meshgrid, axis=1).reshape(X_meshgrid.shape), c=y, s=40, cmap=plt.cm.Spectral)
plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)
plt.show()","#Looking at the first 10 rows in the dataset, to see if our method worked.
titanic_df[0:10]",,02->Linear Algebra->Multiplication->Matrix multiplication,Neural-Network-Case-Study->Karpathy Stanford Visual Recognition Course->Evaluate accuracy on train data,Kaggle Titanic->Titanic Dataset: Exploratory Data Analysis->Who were the passengers on the Titanic?
36901,"print('The number of direct gap materials is: {}'.format(lr_df['Direct'].sum()))
print('The number of indirect gap materials is: {}'.format(len(lr_df['Direct']) - lr_df['Direct'].sum()))","#Here it is assumed the the Ipython cluster and engiens are up and running
import ipyparallel as ipp
ipp_client = ipp.Client()
dview = ipp_client.direct_view()
lbview = ipp_client.load_balanced_view()
print('Using {} engines'.format(len(lbview)))","plt.figure(figsize=(14,6))
plt.subplot(121)
sns.boxplot('Fireplaces','SalePrice',data=data)
plt.subplot(122)
sns.boxplot(x=""HeatingQC"", y=""SalePrice"",data=data,hue='CentralAir')

plt.tight_layout()",,Logistic Regression->Direct-Indirect Classifier->Read in and organize the data,Associative Memeory Simulations->Code for measuring basins of attraction and ouput noise robustness of associative memory networks,Notebook->Box Plot
272444,"from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y, preds)","from sklearn.metrics import confusion_matrix
cm_KNN = confusion_matrix(y_test, y_pred_KNN)",hour_data.head(),,"Lesson2->Lesson 2->01:53:00 - Multilabel classification on satellite imagery->01:26:26 - Audience questions - How many items in the val set?
Jt-Kaggle-Plant-Seedling-Classification-1->Analyzing Results
Plant Seedlings V1->Plant Seedlings Classification->6. Analyzing results again
Dog Breed Classification->Check performance at first iteration of trining the last layer, and prior layers.  Check validation with data augmentation
Mastercard Visa Classifier Fastai Resnet34 Pierre Guillou 16July2018->Analyzing results on validation set (Confusion Matrix)
Plants->Analyzing results->Confusion matrix
Stringer Inspector->Improving our model->Data augmentation
Cnn Image Classification->First quick model
Clean Or Green Pools Training->Visualize Results->Analyzing results->Confusion matrix
Fruit-Images Classification->Fruit Images Classification->Learning Rate Finder (LRF)->Confusion Matrix","Nlp Udyot->Natural Language processing->Fitting K-NN to the Training set,",Bike Rentals->Bike Sharing Analysis and Modeling->B. Building a Predictive Model->4) Merging Datasets
27568,"s1 = pd.Series([2, 3, 5, 7, 11, 13], name='prime')
s2 = pd.Series([1, 4, 6, 8, 9, 10], name='other')

numbers = pd.concat([s1, s2], axis=1) # Concatenate the two series
numbers.columns = ['Useful Numbers', 'Not Useful Numbers'] # Rename the two columns
numbers.index = pd.date_range(""2016-01-01"", periods=len(numbers)) # Index change
print numbers","print(pd.concat([s1,s2]))","#Import library
from bokeh.charts import Bar, output_file, output_notebook, show #use output_notebook to visualize it in notebook",,Notebook->Exercise 5 : DataFrames->b. DataFrames Manipulation,Exploring Your Data With Numpy & Pandas->Numpy Matrices for 2D Data->Modifying Panda Series,Tutorial Draft->Plotly->Methodology
345199,"# Display barplot of real earning in '78
fig, ax = plt.subplots(figsize = (8,10))
ax = sns.boxplot(x=""treat"", y=""re78"", data=df_lalonde)
ax.set_xticklabels([""untreated"", ""treated""]);
ax.set_title('Real earning in 1978 for people \nwho attended the formation and people who didn\'t', fontsize = 18);","fig,ax = plt.subplots()
sn.boxplot(data=dataset[['origin','mpg']], x='origin',y='mpg',ax=ax)
ax.set(title=""MPG's per origin"")
plt.axhline(dataset.mpg.mean(),color='r',linestyle='dashed',linewidth=2)","def pressure(v, t, n=6.022e23):
    k = 1.38e-23
    return n * k * t / v",,Homework4-Checkpoint->Part 2,"Mpg Milestone Report->Comparing mpg distributions of cars by different origin
Eda Autompg->MPG's per origin",Chapt 1->Default Argument Values
124074,"EH_X_train, EH_X_test, EH_y_train, EH_y_test = train_test_split(EH_df_X, EH_df_y, test_size=0.2, random_state=12) 

EH_clf = train_SVM(EH_X_train, EH_y_train)
EH_predicted_train_y = EH_clf.predict(EH_X_train)
EH_predicted_test_y = EH_clf.predict(EH_X_test)","X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X, MICH_Y2, test_size = 0.3, random_state = 22)
clf = svm.SVC()
clf.fit(X_train, y_train)
svm_y_pred = clf.predict(X_test)

print accuracy_score(y_test, svm_y_pred)
confusion_matrix(y_test, svm_y_pred)","'''Getting list of columns with all null columns'''
all_null_columns = data.columns[data.isnull().sum()==len(data)]",,"Final Project->Studying Student Diet, Performance, and Quality of Life->Data Analysis and Results->Hypothesis tests on Correlation Coefficients->Eating Habits SVM Model",Project 6; Michelin Star Estimations->EDA: Logistic Regression->Y Variables->Pulling all the Data for Michelin San Francisco restaurants.,Ajinth Christudas Reddit Analysis->Data Analysis->Null Columns
268127,"review_user_data = review_data.merge(user_data,
                                     left_on='user_id',
                                     right_on='user_id',
                                     how='outer',
                                     suffixes=('_review', '_user'))","business_review_user_data = review_user_data.merge(business_data,
                                                   left_on='business_id',
                                                   right_on='business_id',
                                                   how='outer',
                                                   suffixes=('_reviewuser', '_business'))","features, label = iter(train_dataset).next()
print(""Example features:"", features[0])
print(""Example label:"", label[0])",,Data Exploration->[Yelp Dataset Challenge](http://www.yelp.com/dataset_challenge)-><a id='mining'>Let the mining begin!</a>-><a id='section1'>1. Cleaning the Data</a>->1.3 Reading the csv files using pandas. Then we are merging and cleaning the various datasets.,Data Exploration->[Yelp Dataset Challenge](http://www.yelp.com/dataset_challenge)-><a id='mining'>Let the mining begin!</a>-><a id='section1'>1. Cleaning the Data</a>->1.3 Reading the csv files using pandas. Then we are merging and cleaning the various datasets.,Eager Execution->TensorFlow Eager Execution Tutorial->Parse the Training Dataset
458162,"def answer_one():
    import pandas as pd
    energy = pd.read_excel(""Energy Indicators.xls"", sheetname = ""Energy"", 
                   skiprows=17,skip_footer=38,na_values = ""..."",
                   names = ['Country', 'Energy Supply', 'Energy Supply per Capita', '% Renewable'], 
                   parse_cols = ""C:F"")

    namec = energy['Country'].str.split('\s\(')
    energy['Country'] = namec.str[0]
    energy['Country'] = energy['Country'].str.rstrip('0123456789')
    # converstion of 'Energy Supply' in Gigajoules *1000,000
    energy['Energy Supply'] = energy['Energy Supply']*1000000


    #rename, ""China, Hong Kong Special Administrative Region"": ""Hong Kong""
    idx = energy[energy['Country'] == 'China, Hong Kong Special Administrative Region']['Country'].index[0]
    energy.loc[energy.index[idx], 'Country'] = ""Hong Kong""

    #Rename, ""China, Macao Special Administrative Region4"" : ""Macao""
    idx = energy[energy['Country'] == 'China, Macao Special Administrative Region']['Country'].index[0]
    energy.loc[energy.index[idx], 'Country'] = ""Macao""

    #rename, ""Republic of Korea"": ""South Korea"",
    idx = energy[energy['Country'] == 'Republic of Korea']['Country'].index[0]
    energy.loc[energy.index[idx], 'Country'] = ""South Korea""

    #rename : ""United States of America"": ""United States""
    idx = energy[energy['Country'] == 'United States of America']['Country'].index[0]
    energy.loc[energy.index[idx], 'Country'] = ""United States""

    #rename : ""United Kingdom of Great Britain and Northern Ireland"": ""United Kingdom""
    idx = energy[energy['Country'] == 'United Kingdom of Great Britain and Northern Ireland']['Country'].index[0]
    energy.loc[energy.index[idx], 'Country'] = ""United Kingdom""


    #world bank data
    GDP = pd.read_csv(""world_bank.csv"", skiprows=4)

    #rename Column header from ""Country Name""  to ""Country""
    GDP=GDP.rename(columns = {'Country Name': 'Country'})
    #rename, ""Korea, Rep."": ""South Korea""
    idxg = GDP[GDP['Country'] == 'Korea, Rep.']['Country'].index[0]
    GDP.loc[GDP.index[idxg], 'Country'] = ""South Korea""

    #""Iran, Islamic Rep."": ""Iran""
    idxg = GDP[GDP['Country'] == 'Iran, Islamic Rep.']['Country'].index[0]
    GDP.loc[GDP.index[idxg], 'Country'] = ""Iran""

    #""Hong Kong SAR, China"": ""Hong Kong""
    idxg = GDP[GDP['Country'] == 'Hong Kong SAR, China']['Country'].index[0]
    GDP.loc[GDP.index[idxg], 'Country'] = ""Hong Kong""


    ScimEn = pd.read_excel(""scimagojr-3.xlsx"")
    ScimEn[ScimEn['Rank'] <= 15]
    res = pd.merge(pd.merge(GDP[['Country'] + list(GDP.loc[:,'2006':'2015'])],ScimEn[ScimEn['Rank'] <= 15], on='Country'), energy, on='Country')
    res.set_index('Country', inplace=True)
    cols = ['Rank', 'Documents', 'Citable documents', 'Citations', 'Self-citations', 'Citations per document', 'H index', 'Energy Supply', 'Energy Supply per Capita', '% Renewable', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015']
    res = res[cols]
    res.sort(""Rank"", inplace=True)

    return res","def loadRanking():
    ScimEn = pd.read_excel(""scimagojr-3.xlsx"")
    # Use only top 15 countries. Rank 1 -15
    return ScimEn.set_index(""Country"")","i = 0

for line in lines:
    if i == 0:
        i = i + 1
        continue
    
    #get corresponding streeing angle to Y
    steering_center = float(line[3])       
    
    # read in images from center, left and right cameras
    img_center = load_image(change_path_from_to(line[0].strip()))
    img_left = load_image(change_path_from_to(line[1].strip()))
    img_right = load_image(change_path_from_to(line[2].strip()))    

   
    # create adjusted steering measurements for the side camera images
    correction = 0.20 # this is a parameter to tune
    steering_left = steering_center + correction
    steering_right = steering_center - correction    
    
   
    #lets start augmenting data for these set of images

    #flip the center , left and right images and add to the augmented list 
    img_center_flipped, steering_center_flipped = flip_image_and_steering(img_center, steering_center)
    img_left_flipped, steering_left_flipped = flip_image_and_steering(img_left, steering_left)
    img_right_flipped, steering_right_flipped = flip_image_and_steering(img_right, steering_right)


    #finally add to the final list 
    car_images.extend( [img_center, img_left, img_right, img_center_flipped, img_left_flipped, img_right_flipped, 
                          augment_brightness_camera_images(img_center)])
    steering_angles.extend([steering_center, steering_left, steering_right, 
                                   steering_center_flipped, steering_left_flipped, steering_right_flipped, steering_center])",,Assignment+3->Assignment 3 - More Pandas->Question 1 (20%),Assignment+3->Assignment 3 - More Pandas->Country Ranking,Behavioral Cloning Final->A sample image with transformations/preprocessing applied
298275,seasonality = 10 + np.sin(time) * 10,seasonal = 10 + np.sin(time) * 10,"response = request.urlopen('https://dq-content.s3.amazonaws.com/251/storm_data.csv')
data = pd.read_csv(io.TextIOWrapper(response))

# Get familar with the data
data.head(10)",,"Introduction To Stationarity Instructor-Checkpoint->Section 2: Identifying Stationarity->Summary Statistics & Plots->Seasonality
Introduction To Stationarity Student",Introduction To Time Series Student->Python & Library Versions,Main
70214,"### TODO: Write your algorithm.
### Feel free to use as many code cells as needed.

def dog_or_human(img_path):
    print("""")
    isHuman = face_detector(img_path)
    isDog = dog_detector(img_path)
    if not isHuman and not isDog:
        # show image
        img = cv2.imread(img_path)
        cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        plt.imshow(cv_rgb)
        plt.show()

        print('Picture is neither a dog or a human')
    else:
        if isDog:
            print ('Hi Dog!')
        elif isHuman:
            print('Hi Human!')
        # show image
        img = cv2.imread(img_path)
        cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        plt.imshow(cv_rgb)
        plt.show()

        print('You look like a...')
        print(ResNet50_predict_breed(img_path))","### TODO: Write your algorithm.
### Feel free to use as many code cells as needed.

def breed_predict(img_path):

    if face_detector(img_path) == True:
        print(""hello human!"")
        img = cv2.imread(img_path)
        cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        plt.imshow(cv_rgb)
        plt.show()
        print(ResNet50_predict_breed(img_path))
        
        
    elif dog_detector(img_path) == True:
        print(""This is dog"")
        img = cv2.imread(img_path)
        cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        plt.imshow(cv_rgb)
        plt.show()
        print(ResNet50_predict_breed(img_path))
        
    else:
        print(""error! there is not human or dog in the image"")
        img = cv2.imread(img_path)
        cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        plt.imshow(cv_rgb)
        plt.show()","FILENAME = 'data/credit-data.csv'

large_grid = { 
    'RF': {'n_estimators': [1,10,100], 'max_features': ['sqrt','log2'], 'min_samples_split': [2,5,10], 'criterion':['gini', 'entropy']},
    'BA': {'n_estimators': [1,10,100], 'max_samples': [0.5, 1, 2], 'max_features': [0.5, 1 ,2]},
    'AB': {'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [1,10,100]},
    'LR': {'penalty': ['l1','l2'], 'C': [0.0001,0.01,0.1,1,10]},
    'SVM':{'C' :[0.0001,0.01,0.1,1,10],'kernel':['linear']},
    'LSVM':{'C' :[0.0001,0.01,0.1,1,10], 'loss':['hinge', 'squared_hinge']},
    'DT': {'criterion': ['gini', 'entropy'], 'max_depth': [1,10,50,100], 'max_features': ['sqrt','log2'],'min_samples_split': [2,5,10]},
    'KNN':{'n_neighbors': [1,10,50,100],'weights': ['uniform','distance'],'algorithm': ['ball_tree','kd_tree']}
           }

test_grid = { 
    'RF': {'n_estimators': [1], 'max_depth': [1], 'max_features': ['sqrt'],'min_samples_split': [10]},
    'BA': {},
    'AB': { 'algorithm': ['SAMME'], 'n_estimators': [1]},
    'LR': { 'penalty': ['l1'], 'C': [0.01]},
    'SVM':{'C' :[0.01],'kernel':['linear']},
    'LSVM':{'C' :[0.01]},
    'DT': {'criterion': ['gini'], 'max_depth': [1], 'max_features': ['sqrt'],'min_samples_split': [10]},    
    'KNN':{'n_neighbors': [5],'weights': ['uniform'],'algorithm': ['auto']}
           }",,Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Predict Dog Breed with the Model,Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Predict Dog Breed with the Model,Hw3 Code->Homework 3. Carlos Alvarado->Config Variables
167513,"# Figure 3 repeated
Image('multilayerx1.png')","Xm = -1/imag(Yex)
Xm","B = np.array([[0,0,0,0,0],
              [0,0,0,0,0],
              [0,0,0,0,1],
              [0,0,1,0,1],
              [1,1,0,0,0],

             ])
compareSpect(B,[0,1,2,3])",,3->3. Multi-layer Neural Networks->Appendix->Derivation of equation (2),Ch2-Problem 2-06->Excercises Electric Machinery Fundamentals->Problem 2-6->SOLUTION->(c),"Normal Lap-Aug3-Checkpoint->Regular Increase, Normal stays the same"
419425,fac_group.plot.pie(),"listNYC_ng_grouped = listNYC.groupby(by=['neighbourhood_group'])
ng_names = listNYC.neighbourhood_group.unique()
listNYC_ng_grouped.neighbourhood_group.count().plot.pie(labels=ng_names
                                                         ,colors=['r','b','g','y','c']
                                                         ,autopct='%.2f'
                                                         ,fontsize=14
                                                         ,figsize=(6,6)
                                                         ,title='Pie chart for Property listing by Neighbourhood group' )","query = query_schema + \
""""""
SELECT hadm_id, TO_CHAR(charttime, 'HH:MM') as tm
, spec_type_desc, org_name, ab_name
, interpretation
FROM microbiologyevents
WHERE hadm_id = 145167
ORDER BY charttime;
""""""

df = pd.read_sql_query(query, con)
df.head(n=6)",,Ny Facilities Project->1. Evaluating the Data->Facility Summary Visualizations at a Glance through Grouping,"Air Bn B Data Science Project #1 - Exploratory Analysis->AirBnB ""Listings"" Dataset for New York City->1. Business Understanding->Adding code to include seaborn visualizations",Defining-Suspected-Infection->Blood culture
463617,"histories2 = []
# Model
for number_hidden_layers in (1,2,3):
    model = Sequential()
    model.add(Dense(512, input_shape=(784,), activation='relu'))
    model.add(Dropout(0.2))
    while number_hidden_layers > 1:
        model.add(Dense(512))
        model.add(Activation('relu'))
        model.add(Dropout(0.2))
        number_hidden_layers -= 1
    model.add(Dense(10))
    model.add(Activation('softmax'))
    model.summary()
    model.compile(loss='categorical_crossentropy',
                  optimizer=SGD(),
                  metrics=['accuracy'])
    # Training (You don't need to change this part of the code)
    history = model.fit(X_train, Y_train,
                        batch_size=batch_size, nb_epoch=nb_epoch,
                        verbose=1, validation_data=(X_test, Y_test))
    score = model.evaluate(X_test, Y_test, verbose=0)
    print('Test score:', score[0])
    print('Test accuracy:', score[1])
    # list all data in history
    print(history.history.keys())
    histories2.append(history)","for layer in model.layers[-6:]:
    layer.trainable = True
from keras.optimizers import SGD
model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=SGD(lr=0.0001, momentum=0.9),
              metrics=['accuracy'])
model.fit(x_train_VGG, y_train,
          batch_size=BATCH_SIZE,
          epochs=MAX_EPOCH,
          verbose=1,
          validation_data=(x_test_VGG, y_test))
score = model.evaluate(x_test_VGG, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])","fig, ax = plt.subplots(figsize=(18,8)) 
sns.boxplot(data['Month Name'],data.Delayed/data[""Flights.Total""], ax=ax, showfliers=False, palette=""Set3"") 
ax.set(ylabel=""Percentage of delayed flights"", xlabel="""") 
min_month, max_month = ax.artists[3], ax.artists[6] 
min_month.set_linewidth(5), 
max_month.set_linewidth(5) 
plt.show()",,Assignment 4->Foundations of Data Mining: Assignment 4->Training Deep Models (3 points),Cogs 260 Assignment 2->Introduction:->1.2. VGG16,Tutorial-1-Seaborn->1. Boxplots of  Percentage of Total Flights Delayed by Each Month (Month with Minimum & Maximum delays highlighted)
400244,ms_zoning_train.head(),zone_data.head(),"del some_list[2]
print(some_list)
some_list.append(1.0)
print(some_list)
print(""List has"", len(some_list), ""elements"")",,Kaggle Home Price Prediction->Kaggle Home price prediction->Concatenate features,Energy Plus Sql Analysis,Introduction+To+Python->Python objects
42419,"best_so_far_solutions = pfl.rga.fx_best
print(type(best_so_far_solutions), best_so_far_solutions[:3])","firstx = 0
bestSoFar = (firstx, f(firstx))
bestSoFar","print(c2[:2, :3])  # two rows, three columns",,Basics->Using Pandas->Compute EMPC performance on test set,02 Introduction To Python->Assigning values to variables,Numpy-Checkpoint->Trigonometric functions
67605,"# extract pre-trained face detector
face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')

# load color (BGR) image
img = cv2.imread(human_files[10])
# convert BGR image to grayscale
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# find faces in image
faces = face_cascade.detectMultiScale(gray)

# print number of faces detected in the image
print('Number of faces detected:', len(faces))

# get bounding box for each detected face
for (x,y,w,h) in faces:
    # add bounding box to color image
    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
    
# convert BGR image to RGB for plotting
cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# display the image, along with bounding box
plt.imshow(cv_rgb)
plt.show()","%matplotlib inline                               

# extract pre-trained face detector
face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')
# load color (BGR) image
img = cv2.imread(human_files[3])
# convert BGR image to grayscale
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

# find faces in image
faces = face_cascade.detectMultiScale(gray)

# print number of faces detected in the image
print('Number of faces detected:', len(faces))

# get bounding box for each detected face
for (x,y,w,h) in faces:
    # add bounding box to color image
    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
    
# convert BGR image to RGB for plotting
cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# display the image, along with bounding box
plt.imshow(cv_rgb)
plt.show()","frequency.to(u.micron, equivalencies=u.spectral())",,Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Assess the Human Face Detector,Dog App->Artificial Intelligence Nanodegree->Import Human Dataset,"04-Astropy->Astropy->Quantities and Constants
04-Astropy->Astropy->Cosmological Calculations"
401258,"dfproton.replace(['#'],[0],inplace=True)
#dfproton[2013] = dfproton[2013].astype('int')
#dfproton[2012] = dfproton[2012].astype('int')
for x in range(1991,2014):
    dfproton[x] = dfproton[x].astype('int')
dfproton[2013].sum()","for i in rang(1,0):
    print i","# remove ' Authenticated user' from user string
si_df.update(si_df.author.apply(lambda x: x.replace( ' Authenticated user', '')))",,"Data Description-><font color='steelblue'>SEER-Medicare now provides a file ""to give investigators an estimate of specific medical services.""</font>","Introduction To Debugging->Debugging->First Steps
Introduction To Debugging->First Steps",Si-Stats->Author
338285,"pd.concat([femaledf,maledf],axis = 0)","pd.concat([femaledf,maledf],axis = 0)","create_dataframe(acceptances_HMC(deltas, leaps, Covariance, mu))",,2 Introduction To Dataframes & Plotting->Pandas->Combining datasets : pd.concat,2 Introduction To Dataframes & Plotting->Pandas->Combining datasets : pd.concat,Man Ilan Hariharan Sanjay Final Project->Contents
221647,"q_df = pizza.quantile([.25, .75])
q_df.loc['iqr'] = q_df.loc[0.75] - q_df.loc[0.25]
q_df.loc['whisker_length'] = 1.5 * q_df.loc['iqr']
q_df.loc['max_whisker'] = q_df.loc['whisker_length'] + q_df.loc[0.75]
q_df.loc['min_whisker'] = q_df.loc[0.25] - q_df.loc['whisker_length']
q_df","# By default, describe() outputs summary data for numeric data only
pizza_df.describe()","print ""Image Size: "", imge.shape",,Homework+1->Dataset 2: STATA,2 Grokking->Grokking your data->Generating descriptive statistics for your data,00-Reading-And-Inverting-An-Image-Using-Python->Reading and Inverting an Image Using Python->Testing the Code
97034,data.payor_code.value_counts().plot(kind = 'bar'),"# Replace NaN with 'unknown'
vehicle_types = traffic_data['VEHICLE TYPE CODE 1'].replace(np.NaN, 'OTHER')

# Count key/value pairs
collision_count = ex.countSamples(vehicle_types)

# createBarPlot(valueDict, plotTitle, xtitle, ytitle, saveName, bgBorder, ticksAxes, marginValue):
vehicles = ex.createBarPlot(collision_count, 'Primary vehicle in collison, NY 2012-2017', 'Vehicle type', 'Number of incidents', 'vechicle2', bgBorder, ticksAxes, 200)
vehicles","Indicators_Raw.drop(Indicators_Raw[Indicators_Raw.Year >= 2014].index, inplace = True)",,Diabetes3->gender->payor code,"02806 Final Project-Explainer Notebook 2-Checkpoint-><span style=""color:darkred"">Motor Vehicle Collisions in New York City</span>-><span style=""color:darkred"">Summary statistics and basic plotting</span>-><span style=""color:darkred"">Primary vehicle in collision - per vehicle type</span>","Global Energy Usage Data Exploration->Exploring the Relationship Between Energy Usage and Overall Development->In an ideal situation, we would have data for every indicator in every year.  Unfortunately, we're missing many indicators entirely for 2014 and 2015.  Otherwise, the least known entity is energy usage per capital in 2013, as we only have values from 40 countries.  For now, let's exclude these years from our analysis."
467948,p.transform.backward(p.value),p.transform.backward(p.value),"#Imports
from math import *
import numpy as np
import pandas as pd
import scipy as sp
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.cross_validation import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression

sns.set(font_scale=1.5)
sns.set_style(""ticks"")
sns.set_palette(palette='deep')
sns.set_color_codes(palette='deep')
mpl.rcParams.update({'font.family': 'serif', 'font.serif':'DejaVu Serif'})

%matplotlib notebook",,Models->Constraints and trainable variables,Models->Constraints and trainable variables,House Sales Prediction
250326,"def draw_labeled_bboxes(img, labels):
    # Iterate through all detected cars
    rects = []
    for car_number in range(1, labels[1]+1):
        # Find pixels with each car_number label value
        nonzero = (labels[0] == car_number).nonzero()
        # Identify x and y values of those pixels
        nonzeroy = np.array(nonzero[0])
        nonzerox = np.array(nonzero[1])
        # Define a bounding box based on min/max x and y
        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))
        rects.append(bbox)
        # Draw the box on the image
        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)
    # Return the image and final rectangles
    return img, rects","from scipy.ndimage.measurements import label

def apply_threshold(heatmap, threshold):
    heatmap[heatmap<=threshold] = 0 # Zero out pixels below the threshold
    return heatmap # Return thresholded map

def draw_labeled_bboxes(img, labels):
    for car_number in range(1, labels[1]+1): # Iterate through all deteched cars
        nonzero = (labels[0] == car_number).nonzero()  # Find pixels with each car_number label
        nonzeroy = np.array(nonzero[0]) # Identify x and y values of those pixels
        nonzerox = np.array(nonzero[1])
        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy))) #Define a bounding box based on min/max x and y
        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6) # Draw the box on the image
    
    return img # Return the image","A = 132
B = 145
plt.figure(figsize=(15,5))
plt.plot(x.t[64*A:64*B],x.EnvHil[64*A:64*B],""r"")
plt.plot(x.t[64*A:64*B],x.SignalFiltered[64*A:64*B]+1/2,""y"") 
plt.plot(y.t[64*A:64*B],y.EnvHil[64*A:64*B],""b"") 
plt.plot(y.t[64*A:64*B],y.SignalFiltered[64*A:64*B],""m"") 
plt.title (  x.iD +"" (#""+str(x.N)+""): "" + x.description +""\n""+y.iD +"" (#""+str(y.N)+""): "" +y.description)

plt.tight_layout()
FileName = x.iD+""-""+str(x.N)+""first-lines.jpg""
plt.savefig(FileName)
plt.show()
TagImage(FileName,""kretzaw145ba"",x.iD,""lines"",y.description)",,"Project-Checkpoint->Udacity Self-Driving Car Engineer Nanodegree Program->Draw Bounding Boxes for Lables
Vehicle Detection V2->Vehicle Detection Project->6. Video Implementation",Car Nd-Vehicle-Detection-V2->Step 8. Create Pipeline Function,"20180811A-Processing->Checking data->Let's see, once filtered around 3.5MHz"
198081,"# use IsoMap algorythm to project the digits into 2 dimensions
from sklearn.manifold import Isomap
iso = Isomap(n_components=2)
projection = iso.fit_transform(digits.data)","from sklearn.manifold import Isomap
iso = Isomap(n_components=2)
data_projected = iso.fit_transform(data)
data_projected.shape","#Here is a histogram using Seaborn
plt.figure(figsize=(10,10))
sns.distplot(loans['fico'][loans['credit.policy'] == 1], kde=False)
sns.distplot(loans['fico'][loans['credit.policy'] == 0], kde=False)",,Matplotlib->Example: Handwritten Digits,Cv Image Project->Image classification machine learning algorithm->Applying and comparing alogorithms->3 - unsupervised Isomap,Decision Trees And Random Forest Project->Exploratory Data Analysis
158828,"ax = sns.boxplot(x='success', y='runtime', data=tmdf_final, order=['flop','below average','successful','very successful'])
ax.set(xlabel='Success', ylabel='Runtime (Min)', title='Success vs Runtime');","fig,ax= plt.subplots()
fig.set_size_inches(20,5)
sn.boxplot(x=""roomcnt"", y=""logerror"", data=mergedFiltered,ax=ax,color=""#34495e"")
ax.set(ylabel='Log Error',xlabel=""Room Count"",title=""Room Count Vs Log Error"")","L=200
k=8
l=3
sk=sqrt(2/L)*cos(2*pi/L*k*np.arange(0,L))
sl=sqrt(2/L)*cos(2*pi/L*l*np.arange(0,L))",,Investigate A Dataset->Research Question 2: Does length really matter?,Script1055->Room Count Vs Log Error,"Intro Fourier->Introduction to the Fourier representation->Decomposition of periodic functions -- Fourier series <a name=""Fourier_series""></a>
Intro Fourier->Introduction to the Fourier representation->Complex Fourier series->Computer experiment <a name=""Compute_exp""> </a>"
340292,"df2.ContractType = df2.ContractType.map({'ft':'full_time','pt':'part_time',np.nan:'non_specified'})
df2.ContractTime = df2.ContractTime.map({'perm.':'permanent','contr.':'contract',np.nan:'non_specified'})
df2.Company = df2.Company.str.title()","# checking for unique values
df1.ContractType.unique()",df.groupby('fac_type'),,Task2 Ass2-Copy1->FIT5196 Task 2 in Assessment 2->3.2 Resolve data conflicts->Inconsistence,Task1 Ass2->FIT5196 Assessment 2 Task 1->5. Storing the Data->4d. Column: ContractType,Nicar Python 2->Good afternoon!->4. GROUPING
37415,"import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
np.random.seed(123)","import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
np.random.seed(123)

% matplotlib inline","for job in jobs:
    print(job['title'], job['url'])",,Logistic Regression->Logistic Regression in plain Python,Perceptron->Perceptron algorithm in plain Python,00 First Expt->First experiments
395302,"def onehot(seqs):
    X = np.zeros((len(seqs), 10000), dtype=np.uint8)
    for i, seq in enumerate(seqs):
        for j in seq:
            if j < 10000:
                X[i, j] = 1
    return X


# IMDB Dataset loading
(X_train, y_train), (X_val, y_val), (X_test, y_test) = imdb.load_data(path='imdb.pkl', n_words=10000, valid_portion=0.1)

# Data preprocessing

## One-Hot encoding
X_train = onehot(X_train)
X_val = onehot(X_val)
X_test = onehot(X_test)

## Converting labels to binary vectors
y_train = to_categorical(y_train, nb_classes=2)
y_val = to_categorical(y_val, nb_classes=2)
y_test = to_categorical(y_test, nb_classes=2)

# Network building

tf.reset_default_graph()

net = tflearn.input_data([None, 10000])
net = tflearn.fully_connected(net, 256, activation='relu')
net = tflearn.dropout(net, keep_prob=0.2)
net = tflearn.fully_connected(net, 2, activation='softmax')
net = tflearn.regression(net, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')

# Training (only 2 epochs, it overfits super-fast)

mlp_model = tflearn.DNN(net)
mlp_model.fit(X_train, y_train, n_epoch=2, validation_set=(X_val, y_val), show_metric=True, batch_size=32, run_id='mlp')","import tflearn
import tensorflow as tf


tf.reset_default_graph()

net = tflearn.input_data(shape=[None,len(train_x[0])], name='input')
net = tflearn.fully_connected(net, 200)
net = tflearn.fully_connected(net, 200)
net = tflearn.fully_connected(net, 1, activation='softmax')
net = tflearn.reshape(net,[-1])
net = tflearn.regression(net, name='targets')

model = tflearn.DNN(net)


model.fit({'input': train_x}, {'targets': train_y}, 
          validation_set=({'input': test_x}, {'targets': test_y}), 
           n_epoch=10000, batch_size=32, show_metric=True)","guesses = [1,3,1,1,15,1]",,Assignment 2->Q3. RNN (LSTM) for Document Classification->Q3.2 MLP Baseline,Ig Data Exploration->Finding insights from instagram data:->Engagements->Correlation between Enagements and followers:,Data-Modeling-And-Fitting-C->Data modeling and fitting->Guesses
243475,"train = train.dropna()
test = test.dropna()","train.dropna(subset=['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], inplace=True)
test.dropna(subset=['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], inplace=True)

train.shape","%%bash 
pwd",,"P5->DMG2 Assignment : Problem 5->Google Form Answers
P5->DMG2 Assignment : Problem 5->k-Nearest Neighbors Classification->Plotting training and test accuracy for kNN Classification",Titanic Logistic Regression->Missing Data,Ch-5->Chapter 5->5E1.->Ans
463189,"df1.describe()
#3428 entries for men 1084 entries for women, samples must be same size","df1[df1.twitter_lang != 'en'].head()
df1[df1[""gnip:language:value""] != 'en'].head()","ad_rand_scores = []
n_components = [1,2,3,5,10,20,50,100,500,1000]

def report(model, d):
    print(""Results after PCA of {} components: "".format(d))
    print(""Inertia: {:.3f}"".format(model.inertia_))
    score = adjusted_rand_score(y, model.labels_)
    print(""Adjusted rand score: {:.3f}"".format(score))
    ad_rand_scores.append(score)

for d in n_components:
    pca = PCA(n_components=d, random_state=1337)
    X_reduced = pca.fit_transform(X)
    kmeans = KMeans(n_clusters=5, random_state=1337).fit(X_reduced)
    report(kmeans, d)",,Assignment 4->Now redo the test with a subsample of the data: take only 1 ride every of 200->Pearson's test for correlation - performing with subsample set obtained in previous exercise,Nafiz Islam 7358 Assignsubmission File Islam Nafiz 302329 Week3->Plotting with categorical data->slicing & combining,09 Assignment 4->========== Question 1.7 --- [8 marks] ==========
490792,"import torch.nn as nn

class RNN(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):
        
        super().__init__()
        
        self.embedding = nn.Embedding(input_dim, embedding_dim)
        
        self.rnn = nn.RNN(embedding_dim, hidden_dim)
        
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, text):

        #text = [sent len, batch size]
        
        embedded = self.embedding(text)
        
        #embedded = [sent len, batch size, emb dim]
        
        output, hidden = self.rnn(embedded)
        
        #output = [sent len, batch size, hid dim]
        #hidden = [1, batch size, hid dim]
        
        assert torch.equal(output[-1,:,:], hidden.squeeze(0))
        
        return self.fc(hidden.squeeze(0))","class LSTM(nn.Module):
    def __init__(self, hidden_dim, output_dim, 
                 vocab_size, embedding_dim, dropout=0.2):
        
        super(LSTM, self).__init__()
        
        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_size-1)
        self.hidden_dim = hidden_dim
        # TODO: LSTM
        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, dropout=dropout, bidirectional=True)
        self.fc = nn.Linear(2 * hidden_dim, output_dim)
        
    def init_hidden(self, bsz):
        """"""
        Initialize the hidden state values and C matrix for LSTM
        :return (hidden, c)
        """"""
        hidden = nn.Parameter(torch.zeros(bsz, self.hidden_dim))
        c = nn.Parameter(torch.zeros(bsz, self.hidden_dim))
        return hidden, c
        
    def forward(self, x):
        """"""
        Forward function of the network:
        1. take the embedding vectors of each token index
        2. pass embedding vectors to RNN/LSTM module
        3. take last hidden state from RNN/LSTM module
        4. pass last hidden state to linear layer to get output
        :param x: LongTensor of shape (batch_size, pad_length)
        :return a LongTensor of shape (batch_size, 1)
        """"""
        x = self.emb(x)
        # TODO: take last hidden state of RNN
        out, (last_hidden, _) = self.rnn(x, self.init_hidden(x.size()[0]))
        out = F.sigmoid(self.fc(out[:,-1,:]))
        return out","X = sc.fft(x[:4096])
X_mag = np.absolute(X)        # spectral magnitude
f = np.linspace(0, sr, 4096)  # frequency variable
fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2,figsize=(16,5) )
ax1.plot(x[-1000:]); ax1.set_title('Generated Signal - 440 Hz, 3 shapes, 1000 iterations')
ax2.plot(f[:500],X_mag[:500]); ax2.set_title('Spectrum - 440 Hz, 3 shapes, 1000 iterations');",,1 - Simple Sentiment Analysis->1 - Simple Sentiment Analysis->Build the Model,Lab7-Solutions->Deep Learning in Medicine->Goal:->LSTM,Wavenet Experiments->Series of experiments performed with ibab implementation of wavenet->Local Conditioning->Acoustic Scenes Dataset
398112,"# Define conversions in x and y from pixels space to meters
ym_per_pix = 30/720 # meters per pixel in y dimension
xm_per_pix = 3.6/640 # meters per pixel in x dimension

# Fit new polynomials to x,y in world space
left_fit_cr = np.polyfit(ploty*ym_per_pix, left_fitx*xm_per_pix, 2)
right_fit_cr = np.polyfit(ploty*ym_per_pix, right_fitx*xm_per_pix, 2)
# Calculate the new radii of curvature
y_eval = np.max(ploty)
left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])
right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])
# Now our radius of curvature is in meters
print(left_curverad, 'm', right_curverad, 'm')","# Define conversions in x and y from pixels space to meters
ym_per_pix = 30/720 # meters per pixel in y dimension
xm_per_pix = 3.6/640 # meters per pixel in x dimension

# Fit new polynomials to x,y in world space
left_fit_cr = np.polyfit(ploty*ym_per_pix, left_fitx*xm_per_pix, 2)
right_fit_cr = np.polyfit(ploty*ym_per_pix, right_fitx*xm_per_pix, 2)
# Calculate the new radii of curvature
y_eval = np.max(ploty)
left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])
right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])
# Now our radius of curvature is in meters
print(left_curverad, 'm', right_curverad, 'm')",print(False or False),,Pipeline->Advanced Lane Finding Project->II. Pipeline (single images)->2.6 Calculated the radius of curvature,Pipeline->Advanced Lane Finding Project->II. Pipeline (single images)->2.6 Calculated the radius of curvature,"2
(05) Variable Types - Boolean->Variable Types - Boolean->Logical comparisons"
174338,"# create x, t given two number, we pad 0 to binary-inputs 
# so that inputs & target have same seq-len
def create_sum_test(nb1, nb2):
    fmt_bin = '{:b}'
    
    # convert to binary
    bin1    = fmt_bin.format(nb1)[::-1]
    bin2    = fmt_bin.format(nb2)[::-1]
    sum_bin = fmt_bin.format(nb1 + nb2)[::-1]
    
    # create input & target
    max_len = max(len(bin1), len(bin2)) + 1
    x = np.zeros([max_len, 2], dtype = np.float32)
    t = np.zeros(max_len, dtype = np.int32)
    x[0:len(bin1), 0] = [float(b) for b in bin1]
    x[0:len(bin2), 1] = [float(b) for b in bin2]
    t[0:len(sum_bin)] = [int(b) for b in sum_bin]
    return x, t

# convert a binary sequence to str for visualisation
# note that we need to reversed input before convert to str
def bin_seq_tostr(x):
    return ''.join(['{:d}'.format(int(b)) for b in reversed(x)])

# an example of using create_sum_test and bin_seq_tostr
nb1=47
nb2=123
x,t = create_sum_test(nb1, nb2)

print ('', bin_seq_tostr(x[:,0]))
print ('+')
print ('', bin_seq_tostr(x[:,1]))
print ('_'*(len(t)+2))
print ('', bin_seq_tostr(t))","def adder(binary1, binary2):
    
    """"""
    Adds two binary strings together by converting to a list, adding leading zeros to match lengths to assist in addition,
     reverse the lists and continue to the addition. 
      
    Only four sums are possible when adding binary strings together: 0, 1, 2 or 3. We use this information to implement
      if and elif statements to obtain our result and update the carry values
      
    The first portion of this code is used to set up and verify our problem and our expected results.
     The actual implementation of our binary adder does not utilize this information in any way, and we do not use any
     built-in functions that directly assist in the conversion between binary to decimal or vice versa.
    
    Keywords:
        carry: variable that keeps track of our carry when our binary addition is (1+1) or (1+1+1)
        result: list variable that contains our final sums
        bin_list1: list containing our first binary string
        bin_list2: list containing our second binary string
        bin_len1: variable storing the length of our first binary string
        bin_len2: variable storing the length of our second binary string
    """"""
    
    ### THE BELOW CODE AND PRINT STATEMENTS ARE FOR SETUP AND VERIFICATION PURPOSES ONLY ###
    
    bin1_to_dec = int(binary1, base=2)
    bin2_to_dec = int(binary2, base=2)
    sum_to_dec = bin1_to_dec + bin2_to_dec
    sum_in_bin = bin(sum_to_dec)
    
    print(""-----"")
    print(""We are given the following two binary strings: %s and %s"" % (binary1, binary2))
    print(""In base 10, these numbers equal %s and %s and when added together equal %s"" % 
          (bin1_to_dec, bin2_to_dec, sum_to_dec))
    print(""Using this binary adder, we should expect our result to equal %s\n"" % sum_in_bin[2:])
    
    ### END OF SETUP AND VERIFICATION ###
    
    # carry tracker and result variables
    
    carry = 0
    result = []
    
    # convert our binary strings to lists and create length variables for our lists
    
    bin_list1 = [i for i in binary1]
    bin_len1 = len(bin_list1)    
    
    bin_list2 = [i for i in binary2]
    bin_len2 = len(bin_list2)
    
    # check which string is longer 
    # insert leading zeros to the shorter binary string to assist with addition
    
    if bin_len1 < bin_len2:
        while len(bin_list1) < len(bin_list2):
            bin_list1.insert(0, '0')
    
    elif bin_len2 < bin_len1:
        while len(bin_list2) < len(bin_list1):
            bin_list2.insert(0, '0')
            
    # reverse lists to assist with addition from left to right instead of right to left
    
    bin_list1.reverse()
    bin_list2.reverse()
    
    # iterate through the length of our binary strings
    # implement four if/elif statements, one for each possible outcome when adding our binary numbers
    # only possible outcomes are sums of 0, 1, 2, or 3; append outcomes to result, update carry variables accordingly
    
    for x in range(0, len(bin_list1)):
        if (int(bin_list1[x]) + int(bin_list2[x]) + carry == 0):
            result.append('0')
            
        elif (int(bin_list1[x]) + int(bin_list2[x]) + carry == 1):
            result.append('1')
            carry = 0
            
        elif (int(bin_list1[x]) + int(bin_list2[x]) + carry == 2):
            result.append('0')
            carry = 1
            
        elif (int(bin_list1[x]) + int(bin_list2[x]) + carry == 3):
            result.append('1')
            carry = 1
            
    # undo our earlier reverse function, eliminate leading zeros, and join our 'result'
            
    result.reverse()
    
    while result[0] == '0':
        result = result[1:]
    
    result = ''.join(result)
    
    print(""The result from our binary adder is %s"" % result)
    print(""Verified by built-in function, %s equals %s\n"" % (result, int(result, base=2)))
    
#testcases
adder('111111', '0011011')
adder('110000', '1')
adder('000110', '100110')
adder('0', '1')","cgy = np.sum(y*m)/np.sum(m)
print('The center of mass in y is %f' % cgy)",,04->Generate training/testing dataset,"Mth337 Project 2->BONUS FUNCTIONS: Binary -> Decimal, Decimal -> Binary, and Binary Adder->Binary Addition Function","Finding-The-Center-Of-Mass-From-Point-Masses-With-Python->Finding the Center of Gravity(CG) of a point mass in 1, 2 and 3 Dimensions->Now, lets determine the CG of a 2D space"
13320,data4 = build_data2('2018'),"#data frame with Name and all Menu items
data4 = data2.drop(3, axis=1)
data4.columns = ['name','menu']
data4.head(1)","cnn_augmented_bn.fit_generator(train_batches, steps_per_epoch=steps_per_epoch, \
                                               epochs = 10, validation_data = valid_batches, \
                                               validation_steps= validation_steps)",,Analysis->2. Kmeans (fixed number of clusters)->Apply to 2018 data,Script Aiko Dd Total->Due Diligence,Kaggle Digit Recognizer->Kaggle Handwritten Digit Recognizer using Keras->CNN with Data Augmentation and Batch Normalization
419724,"X_boruta_train, X_boruta_test, y_boruta_train, y_boruta_test = cross_validation.train_test_split(scaled_data[boruta_features], 
                                        scaled_data['shares'],train_size = 0.70,test_size=0.30, random_state=0)","X_train, X_valid = train_test_split(trainingData, train_size=0.8, random_state=10)

#Y_train = np.log1p(X_train.Sales)
Y_train = X_train.Sales    #So that we can use Logistic Regression
X_train = X_train.drop([""Sales""], axis=1)

#Y_valid = np.log1p(X_valid.Sales)
Y_valid = X_valid.Sales
X_valid = X_valid.drop([""Sales""], axis=1)

print(X_train.shape)
print(Y_train.shape)
print(X_valid.shape)
print(Y_valid.shape)","import numpy as np
import pandas as pd
D=pd.read_csv('NCAAM2014-2017.csv')[['Date','Away','Away_pts','Home','Home_pts','Spread']]
D.head(10)",,Regression Models->Online news Popularity Data set:->Features selected using Boruta:,Kaggle Rossmann Challenge,Ncaam-Predictor1->Mining for profitable college basketball spread bets->Importing the game data
176035,"I = samples.mean()
print ""The MC approximated integral: "",I","conven_sampl_2 = alphabet[1:5]
conven_sampl_2","pivot_df.loc[1980] = [0,0,0,0,0,0,0,0,0,0]
pivot_df = pivot_df.sort_index()",,Hw3->Homework 3->1.,Activities W2->Sampling->Convenience Sample,600 - Politics->The United States at the Summer Games
244225,"from IPython.display import HTML
HTML('<iframe width=""1000"" height=""400"" src=""https://chi-feng.github.io/mcmc-demo/app.html#HamiltonianMC,banana"" frameborder=""0"" allowfullscreen></iframe>')","from IPython.core.display import HTML
HTML(""<iframe src=https://www.cms.gov/cciio/resources/data-resources/marketplace-puf.html width=1000 height=350></iframe>"")","m_to_mm=1000.
mm_to_m=1e-3
inch_to_mm=25.4
mm_to_inch=1./inch_to_mm
micr_to_m=1e-6
micr_to_mm=1e-3
m_to_micr=1./micr_to_m
m_to_cm=100.
m_to_nm=1e9
nm_to_m=1./m_to_nm",,Bayesian Gan In Py Torch->Learning Posterior Distributions,"Us Health Insurance Marketplace->Project Title->Instruction->The datasets I used in this report contain data on health and dental plans offered to individuals and small businesses through the US Health Insurance Marketplace. This data was originally prepared and released by the Centers for Medicare & Medicaid Services (CMS), which is part of the U.S. Department of Health and Human Services (HHS). In this report I try to find some useful information to support the CMS work. You can watch a short video to get to know about CMS's work, mission, and vision. Please find the data dictionary for this report in the attached document.->Make a summary to introduce our dataset background by Python. (NLP)",Ana Compare Holo Holo->Compare  Hologram resolution performances->Constants for conversions
169360,"def search_mlr_alpha(a):
    # use 5-fold cross validation
    num_folds = 5
    kf = KFold(n_splits=num_folds)
    kf.get_n_splits(train_df)

    wmae_list = []
    count = 0
    for train_index, test_index in kf.split(train_df):
        count+=1
        X_train, X_test = train_df.iloc[train_index, :-1], train_df.iloc[test_index, :-1]
        y_train, y_test = train_df.iloc[train_index, -1], train_df.iloc[test_index, -1]

        temp_df = X_test[['Date','IsHoliday']]
        X_train.drop(['Date'], axis=1, inplace=True)
        X_test.drop(['Date'], axis=1, inplace=True)

        lasso = Lasso(alpha=a)
        lasso_model = lasso.fit(X_train, y_train)
        y_pred = lasso.predict(X_test)
        
        wmae = wmae_error(temp_df, y_test, y_pred, 5, 1, hol_df.iloc[6:,:])
        wmae_list.append(wmae)
    return np.mean(wmae_list)

m_wmae_list = []
for a in list(range(0,5)):
    m_wmae = search_mlr_alpha(a)
    m_wmae_list.append(m_wmae)
    
plt.plot(list(range(0,5)), m_wmae_list)
plt.xlabel(""The alpha"")
plt.ylabel(""The average WMAE under 5-fold CV"")
plt.title(""The optimal alpha: %d (WMAE: %.4f)"" % (np.argmin(m_wmae_list), np.min(m_wmae_list)))
plt.show()","from sklearn.linear_model import Lasso, Ridge
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import KFold

alphas =  10**np.linspace(6,-2,100)*0.5
kf = KFold(n_splits=5, random_state=20)

lasso_max_alpha = (0, np.inf)
ridge_max_alpha = (0, np.inf)

for alpha in alphas:
    lasso_error = 0
    ridge_error = 0
    
    for train_index, test_index in kf.split(X_train):
        X_kf_train, X_kf_test = X_train[train_index], X_train[test_index]
        y_kf_train, y_kf_test = y_train[train_index], y_train[test_index]
        
        lasso = Lasso(alpha)
        ridge = Ridge(alpha)
        
        lasso_fit = lasso.fit(X_kf_train, y_kf_train)
        ridge_fit = ridge.fit(X_kf_train, y_kf_train)
        
        lasso_error += mean_squared_error(lasso_fit.predict(X_kf_test), y_kf_test)
        ridge_error += mean_squared_error(ridge_fit.predict(X_kf_test), y_kf_test)
    
    lasso_error /= 5
    ridge_error /= 5
        
    if (lasso_error < lasso_max_alpha[1]):
        lasso_max_alpha = (alpha, lasso_error)
    if (ridge_error < ridge_max_alpha[1]):
        ridge_max_alpha = (alpha, ridge_error)

print(""Lasso best alpha is "", lasso_max_alpha[0], "" with MSE of "", lasso_max_alpha[1])
print(""Ridge best alpha is "", ridge_max_alpha[0], "" with MSE of "", ridge_max_alpha[1])","# Load data from data/ folder
data = pd.read_csv(os.path.join(DATA_PATH, DATA_NAME), sep = ',', parse_dates = list_dates)

# pd.isna() --> for NaN
# <value> is pd.NaT --> for NaT

# Fill NaT with 01/01/1900
for date_column in list_dates :
    data[date_column] = data[date_column].apply(lambda x: NAT_DATE if x is pd.NaT else x)
    

# Fill NA with ''
data = data.fillna('')

# Fill CSFId empty with -1
data[CSFID_COLUMN] = data[CSFID_COLUMN].apply(lambda x: x if x != """" else CSFID_EMPTY_NUMBER)

# Remove all rows that have a cosmetic product discontinued
data = data[data[DISCONTINUEDDATE_COLUMN].apply(lambda x: x.year) == NAT_DATE.year]

# Remove all rows that have a chemical removed
data = data[data[CHEMICALDATEREMOVED_COLUMN].apply(lambda x: x.year) == NAT_DATE.year]

# Group by InitialDateReported and sum ChemicalCount
data = pd.DataFrame(data.groupby(INITIALDATEREPORTED_COLUMN)
                    .agg({CHEMICALCOUNT_COLUMN : 'sum'}))",,3->Search the optimal alpha for LASSO,Ee380L Hw2->Question 6 - Outliers and Huber Loss (12 pts),Forecasting->6. Final Models->1.1. Load data->From data/ folder
140707,"for x in ['pickup_datetime', 'dropoff_datetime']:
    df[x.replace('datetime', 'month')] = df[x].dt.month
    df[x.replace('datetime', 'weekday')] = df[x].dt.weekday
    df[x.replace('datetime', 'day')] = df[x].dt.day
    df[x.replace('datetime', 'hour')] = df[x].dt.hour
    df[x.replace('datetime', 'minute')] = df[x].dt.minute","# Disable copy warning - it's a weird PANDAS thing
pd.options.mode.chained_assignment = None

X = X.replace('\\N', np.nan)
X['Years']      = X['Years'].replace('s', np.nan).astype('float')
X['pass']       = X['pass'].astype('float')
X['run']        = X['run'].astype('float')
X['receiving']  = X['receiving'].astype('float')
X['passblock']  = X['passblock'].astype('float')
X['runblock']   = X['runblock'].astype('float')
X['passrush']   = X['passrush'].astype('float')
X['rundefense'] = X['rundefense'].astype('float')
X['coverage']   = X['coverage'].astype('float')
X['overall_rk'] = X['overall_rk'].astype('float')

X['DOB'] = (pd.datetime(2017, 1, 1) - pd.to_datetime(X.DOB)).apply(lambda x: np.nan if pd.isnull(x) else x.days/365) # now means age","import alphalens as al
from quantopian.pipeline.filters import Q1500US

MORNINGSTAR_SECTOR_CODES = {
     -1: 'Misc',
    101: 'Basic Materials',
    102: 'Consumer Cyclical',
    103: 'Financial Services',
    104: 'Real Estate',
    205: 'Consumer Defensive',
    206: 'Healthcare',
    207: 'Utilities',
    308: 'Communication Services',
    309: 'Energy',
    310: 'Industrials',
    311: 'Technology' ,    
}

universe = Q1500US",,Eda->New York Taxi Trip Duration EDA (Kaggle),Models->Baltimore Ravens Free Agent Value Prediction Challenge->Clean for model building,Advanced+Qt+Workshop+Practise->Factor Analysis
282423,"from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()","z = np.linspace(-5,5,100)
plt.plot(z, logist_derivative(z), 'r-')
plt.xlabel('$z$')
plt.ylabel('$\\frac{\\partial \\sigma(z)}{\\partial z}$')
plt.title('Derivative of the logistic function')
plt.grid()
plt.show()","sz = 224
tfms = tfms_from_model(resnet50, sz, aug_tfms=transforms_top_down, max_zoom=1.1)",,"6->Predict *species* using four petal and sepal measurements.->Let's use logistic regression as our prediction model. Import the LogisticRegression class and make an 'estimator' object out of it.
01-Classification
01-Classification->Homework->Intuiting the Logistic Regression Model
Titanic Dataset Feature Engineering And Logistic Regression->Titanic Data Analysis and Logistic Regression
Machine Learning In Python With Scikits-Copy1->Data Preparation->B. How to represent labels (aka classes, or targets)
Titanic Data Analytics Project - 2018->Titanic Data Analytics Project - Kaggle->Modeling - Logistics Regression->Training model
Lamp Assay Image Analysis->Decision Tree Test
Ml Mammogram->Final Project->Logistic Regression",Logistic & Softmax Function In Neural Networks->Derivative of a Logistic Function,Kaggle - Plant Seedlings Classification->Our first model: quick start->Fine-tuning and differential learning rate annealing
144487,"# grab the necessary bands from im10 to pick out the kelp
i = 1
kelpgreen = images[i].read_band(23).astype(float)
red = images[i].read_band(33).astype(float)
nir = images[i].read_band(39).astype(float)
nir2 = images[i].read_band(59).astype(float)
swir1 = images[i].read_band(78).astype(float)
swir2 = images[i].read_band(137).astype(float)
NDVI = ((nir - red)/(nir + red))
NDVI[np.isnan(NDVI)] = -1
ratio = nir2/nir
kelp = (NDVI > .6) & (100 < swir1) & (swir1 < 600) & (kelpgreen > 70) & (ratio < .8)","# Allow division by zero
np.seterr(divide='ignore', invalid='ignore')

# Calculate NDVI. This is the equation at the top of this guide expressed in code
ndvi = (band_nir.astype(float) - band_red.astype(float)) / (band_nir + band_red)","from bokeh.io import show, output_notebook

from bokeh.models import Plot
from bokeh.models import ColumnDataSource
from bokeh.models import Range, Range1d, DataRange1d, FactorRange
from bokeh.models import LinearAxis, CategoricalAxis
from bokeh.models import LabelSet

from bokeh.models.scales import CategoricalScale
from bokeh.models.markers import Circle
from bokeh.models.glyphs import HBar, Segment

from bokeh.palettes import OrRd, Blues
from bokeh.layouts import row

output_notebook()",,Full Project->Kelp Project->Analysis and Pictures->Nearest Neighbor,Ndvi Planetscope Sr->Deriving a vegetation index from PlanetScope SR data->Step 3. Perform the NDVI calculation,Index->Setup
223959,"df_states = pandas.read_csv('correlatesofstatepolicyprojectv1_10.csv',low_memory=False)","Train_panda = pd.read_csv('Forest/train.csv')
Train_panda.ix[:,1:11].hist(figsize=(16,12),bins=50)
plt.show()","from statsmodels.graphics.tsaplots import plot_acf

plot_acf(store1_sales['Weekly_Sales'], lags=100)",,Lab 3 - Regression And Income Inequality,"W207-Carin Mahmud Sakhamuri-Copy3-Checkpoint->Loading, preparing, and exploring the DATA",Lecture15-Practice-Solution->Create an autocorrelation plot and explain your findings
11794,"def printRates(start, end, dataframe, label):
    helpPrint(start, end, dataframe, label, dataframe[label])","def printRates(start, end, dataframe, label):
    helpPrint(start, end, dataframe, label, dataframe[label])","class Tweets:
    
    
    def __init__(self,term="""",corpus_size=100):
        self.tweets={}
        if term !="""":
            self.searchTwitter(term,corpus_size)
                
    def searchTwitter(self,term,corpus_size):
        searchTime=datetime.now()
        while (self.countTweets() < corpus_size):
            new_tweets = api.search(term,lang=""en"",tweet_mode='extended',count=corpus_size)
            for nt_json in new_tweets:
                nt = nt_json._json
                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:
                    self.addTweet(nt,searchTime,term)
            time.sleep(120)
                
    def addTweet(self,tweet,searchTime,term="""",count=0):
        id = tweet['id_str']
        if id not in self.tweets.keys():
            self.tweets[id]={}
            self.tweets[id]['tweet']=tweet
            self.tweets[id]['count']=0
            self.tweets[id]['searchTime']=searchTime
            self.tweets[id]['searchTerm']=term
        self.tweets[id]['count'] = self.tweets[id]['count'] +1

    def combineTweets(self,other):
        for otherid in other.getIds():
            tweet = other.getTweet(otherid)
            searchTerm = other.getSearchTerm(otherid)
            searchTime = other.getSearchTime(otherid)
            self.addTweet(tweet,searchTime,searchTerm)
        
    def getTweet(self,id):
        if id in self.tweets:
            return self.tweets[id]['tweet']
        else:
            return None
    
    def getTweetCount(self,id):
        return self.tweets[id]['count']
    
    def countTweets(self):
        return len(self.tweets)
    
    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order
    def mostFrequent(self):
        ps = []
        for t,entry in self.tweets.items():
            count = entry['count']
            ps.append((t,count))  
        ps.sort(key=lambda x: x[1],reverse=True)
        return ps
    
    # reeturns tweet IDs as a set
    def getIds(self):
        return set(self.tweets.keys())
    
    # save the tweets to a file
    def saveTweets(self,filename):
        json_data =jsonpickle.encode(self.tweets)
        with open(filename,'w') as f:
            json.dump(json_data,f)
    
    # read the tweets from a file 
    def readTweets(self,filename):
        with open(filename,'r') as f:
            json_data = json.load(f)
            incontents = jsonpickle.decode(json_data)   
            self.tweets=incontents
        
    def getSearchTerm(self,id):
        return self.tweets[id]['searchTerm']
    
    def getSearchTime(self,id):
        return self.tweets[id]['searchTime']
    
    def getText(self,id):
        tweet = self.getTweet(id)
        text=tweet['full_text']
        if 'retweeted_status'in tweet:
            original = tweet['retweeted_status']
            text=original['full_text']
        return text
                
    ### NEW ROUTINE - add a code to a tweet
    def addCode(self,id,code):
        tweet=self.getTweet(id)
        if 'codes' not in tweet:
            tweet['codes']=set()
        tweet['codes'].add(code)
        
    ### NEW ROUTINE  - add multiple  codes for a tweet
    def addCodes(self,id,codes):
        for code in codes:
            self.addCode(id,code)
        
    ### NEW ROUTINE get codes for a tweet
    def getCodes(self,id):
        tweet=self.getTweet(id)
        if 'codes' in tweet:
            return tweet['codes']
        else:
            return None",,Analysis->Analysis:->Class Results:->Helper Print Functions:,Analysis->Analysis:->Class Results:->Helper Print Functions:,Social Media - Part 2->2.1 Annotating Tweets
457009,"from mymodule import *
print(sayhi())
numb = input(""Enter a non negative number"")
num_factorial = factorial(numb)
print(num_factorial)","import sys
sys.path.append(your_local_path)
from mymodule import *
print(sayhi())


numb = input(""Enter a non negative number: "")
num_factorial = factorial(int(numb))","locations = df2[['Latitude', 'Longitude']]
locationlist = locations.values.tolist()",,Day1,Python Basics Regex Datetime,Final Project->Graphing
223370,"# TODO: Build your pipeline that will draw lane lines on the test_images
# then save them to the test_images directory.

#import imageio

def plotLinesAndSaveToDisk():
    testImageDir = ""test_images/""
    outputImageDir = ""output_images/""
    list = os.listdir(""test_images/"")
    #print (""countList:"",list.count)
    for imgName in os.listdir(""test_images/""):
        imagePath = testImageDir + imgName
        
         
        try:
            im = cv2.imread(imagePath)
            vis1 = plotLinesNew(im)
            outputPathVis = outputImageDir +""Output""+ imgName 
            cv2.imwrite(outputPathVis, vis1)
            #plt.imshow(vis1)
            
        except ValueError as e:
            print(""Unable to open file"")
            #print('errno:', e.errno)
        
            
plotLinesAndSaveToDisk()","def process_test_data():
    testing_data = []
    for img in tqdm(os.listdir(TEST_DIR)):
        path = os.path.join(TEST_DIR, img)
        img_num = img.split(""."")[0]
        img_resized = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (IMG_SIZE, IMG_SIZE))
        testing_data.append([np.array(img_resized), img_num])
    np.save(""testing_data.npy"", testing_data)
    return testing_data","def get_random_subset(df, n=5000):
    sub = random.sample(xrange(len(df)), min(n, len(df)))
    return df.iloc[sub]

def preprocess(df):
    res = df.copy()
    res = res[res.X != res.X.max()]
    datetimes = res.Dates.apply(get_datetime)
    res['Hour'] = datetimes.apply(lambda dt: dt.hour)
    res['Month'] = datetimes.apply(lambda dt: dt.month)
    res['Hour_Minutes'] = datetimes.apply(lambda dt: dt.hour + dt.minute / 60.0)
    res['Minutes_Since_03'] = datetimes.apply(lambda dt: (dt-datetime(2003, 1, 1)).total_seconds() / 60)
    res['Minutes_Since_New_Year'] = datetimes.apply(lambda dt: (dt-datetime(dt.year, 1, 1)).total_seconds() / 60)
    res['DOW'] = train.DayOfWeek.apply(lambda x: dow.index(x))
    res['Street_Corner'] = res['Address'].apply(lambda x: 1 if '/' in x else 0)
    return res

def get_datetime(s):
    dt = datetime.strptime(s, ""%Y-%m-%d %H:%M:%S"")
    return dt

dow = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

def isNight(hour):
    if hour in [0, 1, 2, 3, 4, 5, 6, 19, 20, 21, 22, 23]:
        return ""Night""
    else:
        return ""Day""",,P4->Build a Lane Finding Pipeline,Dogsvscats-0->Dogs v/s Cats Redux,Model Iteration 2->Model Iteration 1->Functions to be used later
47275,hidden_layer_activations,n_hidden_layer = 256  # layer number of features,"s        = .02 
A        = 0.5
mu       = 200 
t0       = .3
h.source = lambda x,t: A*np.exp(-(x-.5)**2*mu)*np.exp(-(t-t0)**2/(2*s))",,Assignment-2B->Step 3,Multilayer Perceptron Network With Tensor Flow->Multilayer Perceptron Network with TensorFlow->Hidden Layer Parameters,Middle Laser-Checkpoint->Laser pulse hitting the probe in the middle
208997,"def geom_prior(x, step=0):
    p = Variable(torch.Tensor([0.5]))
    i = pyro.sample('i{}'.format(step), dist.bernoulli, p)
    if i.data[0] == 1:
        return x
    else: 
        x = x + model_step_sketch(step)
        return geom_prior(x, step + 1)","def model(prior_mean, observations={""x1"": 0, ""x2"": 0}):
    x = pyro.sample(""z"", dist.Normal(prior_mean, torch.tensor(5**0.5)))
    y1 = pyro.sample(""x1"", dist.Normal(x, torch.tensor(2**0.5)), obs=observations[""x1""])
    y2 = pyro.sample(""x2"", dist.Normal(x, torch.tensor(2**0.5)), obs=observations[""x2""])
    return x",hosp.extract_data('ED').head(),,Air->Attend Infer Repeat->Introduction->Generating an entire image,Csis->Compiled Sequential Importance Sampling->Specify the model:,Example Flosp Workflow->Example workflow for using flosp to import and clean ED data->initialise flosp->Importing ED data
150716,"myTuple = ('Suchit', 'Rakesh', 'Roshni', 'Shireen')

print(myTuple[3])","tupl = (1,2,3,4,5)
tupl","def generate_data(N = 10):
    ''' Generates linear target function f and labeled, linearly separable test data generated by f.'''
    
    # choose two random points p1, p2 and compute a vector p orthogonal to their difference
    p1, p2 = (np.random.rand(2,2) - 0.5) * 2.
    p = np.array([1, -(p2 - p1)[0]/(p2 - p1)[1]])
    p /= np.linalg.norm(p)
    
    f = lambda x: np.sign((x - p1) @ p).reshape(-1,1)
    f.db = lambda x: (p2[1] - p1[1])/(p2[0] - p1[0]) * (x - p1[0]) + p1[1]
    
    # generate data points and apply classifier to label them
    X = (np.random.rand(N, 2) - 0.5) * 2
    Y = f(X)
    
    return X,Y,f",,Python Basics 5->14. Lists->14.8 Membership of List - in and not in,"Intro To Python->Course Outline->Basic Data Structures->TUPLES
Intro To Python->APPEND / POP->TUPLES",Hw1->LFD: Homework 1->Setup
243297,"from sklearn.linear_model import Perceptron

perceptron = Perceptron()
perceptron.fit(x_train, y_train)
y_pred = perceptron.predict(x_val)
acc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)","# Perceptron
perceptron = Perceptron()
perceptron.fit(X_train, Y_train)
Y_pred = perceptron.predict(X_test)
train_acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)
test_acc_perceptron = round(perceptron.score(X_test, Y_test) * 100, 2)
train_acc_perceptron, test_acc_perceptron","# Define the nutation rotation matrix
R_gamma = sympy.Matrix([[1, 0, 0],
                        [0, sympy.cos(gamma), sympy.sin(gamma)],
                        [0, -sympy.sin(gamma), sympy.cos(gamma)]])",,Data Analytics 1->Perceptron without Cross validation,Titanic Kaggle Challenge->Feature Engineering->Comparing categorical features,Mche513 - Example 4P2
328145,"import spacy
import string

shift=1
def encrypt(plaintext):
    alphabet = string.ascii_lowercase
    shifted_alphabet = alphabet[shift:] + alphabet[:shift]
    table = str.maketrans(alphabet, shifted_alphabet)
    return plaintext.lower().translate(table)


def decrypt(plaintext):
    alphabet = string.ascii_lowercase
    shifted_alphabet = alphabet[shift:] + alphabet[:shift]
    table = str.maketrans(shifted_alphabet, alphabet)
    return plaintext.translate(table)","def CeaserCipherEncrypt(plaintext):
    m = AlphabetEncode(PreProcess(plaintext))
    c = [(x + 5)%26 for x in m]
    return c

def CeaserCipherDecrypt(ciphertext):
    c = AlphabetEncode(PreProcess(ciphertext))
    m = [(x - 5)%26 for x in c]
    return m
    
def PreProcess(plaintext):
    return plaintext.lower().replace("" "", """")

def AlphabetEncode(lowercaseplaintext):
    #We convert text that is in lowercase and without spaces into a list of ordinal numbers < 26.
    m = [ord(letter)- ord('a') for letter in lowercaseplaintext]
    return m

def AlphabetDecode(m):
    p = """".join([chr(x+ ord('a')) for x in m])
    return p

PreProcess(""ZxylaPH one"")","things = [""mozzarella"", ""cinderella"", ""salmonella""]",,Workshop-Checkpoint->Workshop NLP lang.ai->Introducing Caesar cypher,Cryptography1->Lecture 1: Basics of Cryptography->The Ceasar Cipher,Practice Exercises - Lists (Solutions)->Practice Exercises - Lists
460431,"table = np.zeros(20, dtype=np.uint8)
table[[3, 12, 13]] = 1
c = correlate2d(a, kernel, mode='same')
b = table[c]
print(b)","class ReactionDiffusion(Cell2D):
    """"""Reaction-Diffusion Cellular Automaton.""""""

    kernel = np.array([[.05, .2, .05],
                       [ .2, -1, .2],
                       [.05, .2, .05]])

    options = dict(mode='same', boundary='wrap')

    def __init__(self, n, m=None, params=(0.5, 0.25, 0.035, 0.057)):
        """"""Initializes the attributes.

        n: number of rows
        m: number of columns
        params: tuple of (Da, Db, f, k)
        """"""
        
        self.params = params
        m = n if m is None else m
        self.array = np.ones((n, m), dtype=float)

        self.array2 = np.zeros((n, m), dtype=float)
        island(self.array2, val=0.1, noise=0.1)
        
    def step(self):
        """"""Executes one time step.""""""
        A = self.array
        B = self.array2
        ra, rb, f, k = self.params
        
        cA = correlate2d(A, self.kernel, **self.options)
        cB = correlate2d(B, self.kernel, **self.options)
        reaction = A * B**2
        self.array += ra * cA - reaction + f * (1-A) 
        self.array2 += rb * cB + reaction - (f+k) * B","import matplotlib.patheffects as PathEffects

plt.figure(figsize=(12, 12))

def plot_projection(x, colors):
    """"""
    plot t-SNE projection of digits and label the cluster centers
    """"""
    palette = np.array(sns.color_palette(""hls"", 10))
    ax = plt.subplot(aspect='equal')
    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])
    #TODO: Consider changing with colorbar...
    for i in range(10):
        x_text, y_text = np.median(x[np.where( colors == i )], axis=0)
        txt = ax.text(x_text, y_text, str(i), fontsize=20)
        txt.set_path_effects([PathEffects.Stroke(linewidth=5, foreground=""w""), PathEffects.Normal()])
    
plot_projection(digits_proj, y[:NUM_SAMPLES])",,"Chap06
Chap06Soln->Game of Life->Game of Life entities->Conway's conjecture",Chap07Mine->Physical modeling->Reaction-Diffusion,Digit-Recognizer->Classify handwritten digits using the famous MNIST data->Visualizing the Digits->t-distributed stochastic neighbor embedding (t-SNE)
209878,"print(titanic_df.groupby(['Sex','Pclass']).Survived.mean())*100","def getAge(row):
    surv = row.Survived
    sex = row.Sex
    pclass = row.Pclass
    
    if surv==0 or surv==1:
        condition = (data['Survived']==surv) & (data['Sex']==sex) & (data['Pclass']==pclass)
        df_mean = data[condition].groupby(['Survived','Sex','Pclass'])['Age'].mean()
    else:
        condition = (data['Sex']==sex) & (data['Pclass']==pclass)
        df_mean = data[condition].groupby(['Sex','Pclass'])['Age'].mean()
    
    print(""srv: {}, sex: {}, class: {} -> mean: {}"".format(surv, sex, pclass, df_mean.mean()))
    return df_mean.mean()
    
data['Age'] = data['Age'].fillna(data.apply(getAge, axis=1))","a = np.array([[1, 2], [3, 4]])
print(np.sum(a)) # no axis specified
print(np.sum(a, axis=0)) #along a specified axis (across columns)
print(np.sum(a, axis=1)) #along a specified axis (across rows)

print(np.mean(a))         # no axis specified
print(np.mean(a, axis=0)) #along a specified axis (across columns)
print(np.mean(a, axis=1)) #along a specified axis (across rows)

print(np.std(a))         # no axis specified
print(np.std(a, axis=0)) #along a specified axis (across columns)
print(np.std(a, axis=1)) #along a specified axis (across rows)",,Titanic Data Analysis->4. Exploratory Analysis->Class Distribution->Q2. Is there any relation between class and survival?,Titanic Survival->Tinanic Survival->Data Engineering->Age Range,"4-><span style=""color:#0b486b"">SIT307 - Data Mining and Machine Learning</span>-><span style=""color:#0b486b"">Numpy: Quick Introduction </span>-><span style=""color:#0b486b""> 4 Reading and Writing Files using Pandas"
52964,"myplot = plt.figure(figsize=[5,5])
myax = myplot.add_axes([0,0,1,1])
myax.plot(range(15))
myplot.show()","myplot = plt.figure(figsize=[5,5])
myax = myplot.add_axes([0,0,1,1])
myax.plot(range(15))
myplot.show()","movieData  = pd.read_csv(""../data/movieData/movies.dat"",sep = ""::"", names = [""MovieID"",""Title"",""Genres""])
userData   = pd.read_csv(""../data/movieData/users.dat"",sep = ""::"", names = [""UserID"",""Gender"",""Age"",""Occupation"",""Zip-code""])
ratingData = pd.read_csv(""../data/movieData/ratings.dat"",sep = ""::"", names = ['UserID','MovieID','Rating','Timestamp'])",,2->Plotting the data Woo!,2->Plotting the data Woo!,03 Pandas And More Eda-Class->I hope you've enjoyed working with datasets in pandas so far!
406410,"# Define a function to compute color histogram features
def color_hist(img, nbins=32, bins_range=(0, 256),hist_vec=True):
    # Compute the histogram of the RGB channels separately
    hist1 = np.histogram(img[:,:,0], bins=nbins, range=bins_range)
    hist2 = np.histogram(img[:,:,1], bins=nbins, range=bins_range)
    hist3 = np.histogram(img[:,:,2], bins=nbins, range=bins_range)
    # Generating bin centers
    bin_edges = hist1[1]
    bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2
    
    # Concatenate the histograms into a single feature vector
    hist_features = np.concatenate((hist1[0], hist2[0], hist3[0]))
    
    if hist_vec == False:
        # Return the individual channel histograms, bin_centers and feature vector    
        return hist1, hist2, hist3, bin_centers, hist_features
    else:
        # only return feature vector
        return hist_features","###########################################
# Color Histogram features
###########################################

# Assuming input channel is cv2 order (BGR)
# Also check cv2 reads in png images in range (0, 256)
def color_hist(img, nbins=32, bins_range=(0, 256)):
    
    # Compute the histogram of the RGB channels separately
    #rhist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)
    #ghist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)
    #bhist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)
    rhist = np.histogram(img[:,:,0], bins=nbins)
    ghist = np.histogram(img[:,:,1], bins=nbins)
    bhist = np.histogram(img[:,:,2], bins=nbins)
    
    # Generating bin centers
    bin_edges = rhist[1]
    bin_centers = (bin_edges[1:] + bin_edges[0:len(bin_edges)-1])/2
    
    # Concatenate the histograms into a single feature vector
    hist_features = np.concatenate((rhist[0], ghist[0], bhist[0]))
    
    # Return the individual histograms, bin_centers and feature vector
    return hist_features","weather = weather_data
with open(""data/la_weather.csv"", ""r"") as f:
    # weather_data = [row.rstrip().split(',') for row in f]
    weather = [row.rstrip().split(',')[1] for row in f][1:]
print(""weather[0]:"", weather[0])

# Loop over the weather variable, and set count equal to the number of items in weather.
count = 0
for item in weather:
    count = count + 1
print(""count:"", count)",,Vehicle Detection->Self-driving Car Project 5: Vehicle Detection and Tracking->2. Define Classifier->1.3 Color Histogram Features,"Vehicle Detection Tracking->Vehicle Detection And Tracking->Get Features from Image (HOG, Color Histogram and Bin Spatial)",Dic Template->Python Programming->Dictionaries->3: Pre-defined variables
256925,"import time
start = time.time()

# check if point(s) fall within known geometry - envelopes
sim_data['contains_env'] = sim_data['point'].map(lambda x: True if envelope.contains(x).any()==True else False)

finish = time.time()
elapsed_env = finish - start
print 'Time elapsed: ', elapsed_env","import time
start = time.time()

# check if point(s) fall within known geometry - actual
sim_data['contains_actual'] = sim_data['point'].map(lambda x: True if gdf.contains(x).any()==True else False)

finish = time.time()
elapsed_geo = finish - start
print 'Time elapsed: ', elapsed_geo","def printer(text):
    print(text)

def count(f):
    def wrapper(*args, **kwargs):
        sentence = args[0]
        sentence = sentence.lower().replace(' ', '')
        counter = defaultdict(int)

        for letter in sentence:
            counter[letter] += 1

        for l, c in counter.items():
            print('{}: {}'.format(l, c))
        return f(*args, **kwargs)
    return wrapper",,9->Spatial Data Lab->Project 5 Component,9->Spatial Data Lab->Project 5 Component,Advanced Data Types And Functional Programming Lab->Lab for Advanced Data Types and Functional Programming:
470291,"# all 100 features
print(vect.get_feature_names())","# set of stop words
print vect.get_stop_words()","data_bank = bc.get_mri_subfield_by_genotype(project_id, 'apoe_genotype', 'Banks Superior Temporal Sulcus Volume', 'e2/e2')
data_cauant = bc.get_mri_subfield_by_genotype(project_id, 'apoe_genotype', 'Caudal Anterior Cingulate Volume', 'e2/e2')
data_caumid = bc.get_mri_subfield_by_genotype(project_id, 'apoe_genotype', 'Caudal Middle Frontal Volume', 'e2/e2')",,Nlp Review-Lab-Solutions->Part 3: Stopword Removal->4.1 Shrink the maximum number of features and re-test the model.,"15 Natural Language Processing->Natural Language Processing (NLP)->Part 3: Stopword Removal
15 Natural Language Processing
Text Analytics Examples",Bhc Mjff Ppmi->Brain Commons - Parkinson's Progression Markers Initiative (MJFF-PPMI)->2. PPMI DATA ANALYSIS AND SUMMARY STATISTICS->Summary statistics for clinical test/exam variables->MRI Volume comparison (Wilcoxon non-parametric test)
133972,"# finally, let's look at date variables
date_vars = ['fecha_dato', 'fecha_alta']
df[date_vars].describe()","def date_to_age_in_months(date):
    if date != date or date == 0: #is NaN
        return 0
    date1 = datetime.strptime(date, '%Y-%m-%d')
    date2 = datetime.strptime('2017-01-01', '%Y-%m-%d') #get age until 01/01/2017
    delta =  relativedelta.relativedelta(date2, date1)
    return delta.years * 12 + delta.months


def normalize_date_variables(df):
    date_vars = ['founded_at', 'first_funding_at', 'last_funding_at']
    for var in date_vars:
        df[var] = df[var].map(date_to_age_in_months)
        
    df = normalize_numeric_features(df, date_vars)
    return df
    
    
startups_USA = normalize_date_variables(startups_USA)",df[df.text.str.contains('^Chapter 1 ')].index.tolist(),,Data Analysis->Date Vars,Dataset Preparation->1. Dataset Preparation->Normalize date variables,Gutenberg Collect Peter Pan
44456,"data = pd.read_csv(""thanksgiving.csv"", encoding = ""Latin-1"")",data=pd.read_csv('creditcard.csv'),"v_to_count  = get_count(df, ['likerttime1[perfCheck1]. Please rate the following propositions. There are no right or wrong answers [Do you receive sufficient information on the results of your work?]', 'likerttime1[perfCheck2]. Please rate the following propositions. There are no right or wrong answers [Do you get the opportunity to check on how well you are doing your work?]', 'likerttime1[perfCheck3]. Please rate the following propositions. There are no right or wrong answers [Do you have access to sufficient data and information to do your work?]', 'likertime2[perfCheck4]. Please rate the following propositions. There are no right or wrong answers [Do you receive sufficient information on the purpose of your work?]', 'likertime2[perfCheck5]. Please rate the following propositions. There are no right or wrong answers [Does your work provide you with direct feedback on how well you are doing?]', 'likertime2[perfCheck6]. Please rate the following propositions. There are no right or wrong answers [Does your supervisor/line manager inform you about how well you are doing your work?]', 'likerttime1[perfCheck7]. Please rate the following propositions. There are no right or wrong answers [Do your colleagues inform you about how well you are doing your work?]'],
                                                ""likert"",
                                                ""./../survey_creation/uk_17/listAnswers/likert_time_5.csv"")",,Basics->Converting income to Numerical Values,"K Means Clustering Project->Get the Data
Linear Algebra Assignment",Notebook->Section: 7->Group of question: train->Do your colleagues inform you about how well you are doing your work?
247874,print(event.hits[0]),"print(""Altitude in degrees:"", event.mc.alt.deg)","def electron_energy_loss_rate(B, E):
    """""" energy loss rate of an electron of kinetic energy E in magnetic field B
    """"""
    U_B = B ** 2 / (2 * cst.mu0)
    gamma = (
        E / (cst.m_e * cst.c ** 2) + 1
    )  # note that this works only because E/(cst.m_e*cst.c**2) is dimensionless
    beta = np.sqrt(1 - 1 / gamma ** 2)
    return 4.0 / 3.0 * cst.sigma_T * cst.c * gamma ** 2 * beta ** 2 * U_B


print(electron_energy_loss_rate(1e-5 * u.G, 1 * u.TeV).to(""erg/s""))",,Readme,"Raw Data Exploration->Exploring Raw Data->Explore the contents of an event
Raw Data Exploration->Exploring Raw Data with _ctapipe_->looking at what is in the event",Astropy Introduction->Units and constants->Use Quantities in functions
296375,"if not os.path.exists(output_folder):
    os.makedirs(output_folder)

for file in os.listdir(input_folder):
    input_image_path = os.path.join(input_folder, file)
    output_image_path = os.path.join(output_folder, file)
    image = mpimg.imread(input_image_path)
    out_image = process_image(image)
    plt.imshow(out_image)
    plt.show()
    plt.imsave(output_image_path, out_image)
    
os.listdir(output_folder)","images = []
for file in os.listdir(""test_images""):
    path = os.path.join(""test_images"", file)
    img = mpimg.imread(path)
    images.append(process_frame(img, save=True))
for i in range(len(images)):
    plt.subplot(len(images)/2, len(images)/2, i+1)
    plt.imshow(images[i])","past_ill = pd.crosstab([df['num_employees']], df.current_state)
print(past_ill)

rate = past_ill.div(past_ill.sum(1).astype(float),axis=0) # normalize the value
rate.plot(kind='barh', stacked=True)",,P1->Test Images,P1->My Helper Functions->Test on Test Images,Lab1->Business Understanding->Age
464263,"param_grid_logR = {'C': np.logspace(0, 3, 10)}
grid_logR3 = GridSearchCV(logR, param_grid_logR, cv=kf3)
grid_logR3.fit(X_train, y_train)

param_grid_svc = {'C': np.logspace(-3, -1, 10)}
grid_svc3 = GridSearchCV(svc, param_grid_svc, cv=kf3)
grid_svc3.fit(X_train, y_train)

param_grid_NC = {'shrink_threshold': np.linspace(0, 300, 10)}
grid_NC3 = GridSearchCV(NC, param_grid_NC, cv=kf3)
grid_NC3.fit(X_train, y_train)","# Modeling
logR_param = {'C': [0.6, 0.65, 0.7, 0.75, 0.8]} # define parameter(s) for gridsearch
logR_model = LogisticRegression(random_state=13)
logR_CV = GridSearchCV(logR_model, logR_param)
logR_CV.fit(x_train, y_train)","files = ['./results/M5XL_ZIP2R_-9_-18b8_2B1__01_001.pickle',
        './results/M5XL_ZIP2_-9_-18b8_2B1__01_001.pickle',
        './results/M5XL_ZIP2R_-9_-18b8_2B1__16_001.pickle', ## fastest total completion
        './results/M5XL_ZIP2_-9_-18b8_2B1__20_001.pickle',  ## fastest total completion
        ]
dd = [pickle.load(open(file, 'rb')) for file in files]
sts = [bench.unpack_stats(d, ms=True) for d in dd]",,"Classification Models->For the third time, we use 3 splits as unchanged, and use the random state 40.",2018 Fifa World Cup Predictions Final->Modeling and Evaluation->Logistic Regression Model (for predicting if the home team will win),Rio-Vs-Bespoke-Report->Load data
412440,"# Fit the data to the first n samples in the dataset 
n = 50000 # Max = 200000
x, y = data_preprocess(train_dataset,train_labels, n)

model.fit(x, y)","gss = processing.preprocess_gss()
court = processing.preprocess_court_data()
data, y_col, x_cols, x_cols_nocourt = processing.process_combined_data(gss, court, 'genderValue')

reload(modeling)
train, test = modeling.split_train_test(data)

court, no_court = modeling.fit_model(train, test, y_col, x_cols, x_cols_nocourt)","fitted_models = {}
for name,pipeline in pipelines.items():
    model = GridSearchCV(pipeline,hyperparameters[name],cv=10)
    model.fit(X_train,y_train)
    fitted_models[name] = model
    print(name,""ok"")",,1 Notmnist,Main->ElasticNet regression on gender attitudes composite,3
288533,"print sqft_model.evaluate(test_data) # Previous model with price and sqft_living
print my_features_model.evaluate(test_data) # New model with features like bathroom, bedrooms and more.","df[(df.accommodates < 14) | (df.price > 500)][['accommodates', 'bedrooms', 'bathrooms', 'accommodates_bedroom_ratio', 'accommodates_bathroom_ratio', 'price']].sort_values(by = 'price', ascending = False).describe()","df = df.dropna()

# Aggregating the dyads by player attribute
data_grouped_by_player = df.groupby(""player"")

# Initializing empty numpy arrays to store the data and the labels
X = np.empty((0,17))
y = np.empty((0,1))


player_set = [data_grouped_by_player.get_group(x) for x in data_grouped_by_player.groups]

for player in player_set:

    description = player.iloc[0]
    arr = np.array([description[""club""], description[""leagueCountry""], description['height'], 
                    description['weight'], description['position'], 
                    player['games'].sum(), player['victories'].sum(), player['ties'].sum(), 
                    player['defeats'].sum(), player['goals'].sum(), player['yellowCards'].sum(),
                    player['yellowReds'].sum(), player['redCards'].sum(), player['meanIAT'].mean(), 
                    player['meanExp'].mean(), np.nanstd(player['meanIAT']), np.nanstd(player['meanExp']) ])

    X = np.vstack((X,arr))
    y = np.vstack((y, (description[""rater1""]+description[""rater2""])/2 ))",,House Prices->Evaluate the simple model,"Exploratory Data Analysis->Follow-up Questions:->Follow-up Question 3: From the scatter plot above, why there are data points that can accommodate 14 people but the listing price do not increase linearly?",Hw4->Homework 4 - Applied Machine Learning->TASK 1 - Classification->1A: Pre-processing
345973,"#T-statistic on sub.
scipy.stats.ttest_1samp(sub, usual)","# Exercise 3
print(""Sample mean temperature:"", mean_s)

# One-sample t-test (two-tail)
mean_tscore, mean_pvalue = stat.ttest_1samp(x1,98.6)
print(""t-score for population mean temperature:"", mean_tscore)
print(""p-value for population mean temperature:"", mean_pvalue)","# To read from the DODs/OPeNDAP NetCDF endpoint, use URL
#   'http://www.ncdc.noaa.gov/thredds/dodsC/cdr/snowcover/nhsce_v01r01_19661004_latest.nc'

snowcover_file = 'nhsce_v01r01_19661004_20160201.nc'
dataset = xr.open_dataset(snowcover_file)
dataset",,Sliderule Dsi Inferential Statistics Exercise 1 Vm->Bootstrap hypothesis testing of means,"Sliderule Dsi Inferential Statistics Exercise 1->2) The sample size is sufficiently large. For CLT to apply, a sample size of 30 is sufficient.",Module-5->[xarray](http://xarray.pydata.org/en/stable/)
273048,"i = 0
while i < len(Ctitle):
    filt_sub = dfm['source'] == Ctitle[i]
    new_df = dfm[filt_sub]
    filt_stu = new_df['value'].values.sum()
    if filt_stu >= 10:
        course_list.append(Ctitle[i])
    i+=1","import math
import numpy as np
total_date_list_alluser = []
total_list_alluser = []
total_list_alluser = list(new_df_newtime_series.keys())
# remember new_df_newtime_series contins the owneruser id
for date_index in range(len(new_df_newtime_series)):
    list_all_ones = []
    list_all_ones = [1] * len(new_df_newtime_series[date_index])
    series_date_range = pd.Series(list_all_ones, index = new_df_newtime_series[date_index])
    s_try = series_date_range.resample('1M').sum()
    date_list = s_try.tolist()
    for n,i in enumerate(date_list):
        if math.isnan(date_list[n]):
            date_list[n] = 0.0
    total_date_list_alluser.append(date_list)
#     the date_list stores the list per month each user","# correlation between reads and views
display(analysis_df)",,Course Grade Sankey-Checkpoint->Filter out the subjects having less then 10 students to optimize our plotting,Homework-1-2,Medium-Stats-Public->Overview->Prepare for tag analysis
435029,"rank_by_genre = pd.pivot_table(billboard_long, index=['genre'],values=['rank'],aggfunc='count')
rank_by_genre.sort_values('rank',inplace=True,ascending=False)
rank_by_genre","#Let's start with the location of the listings
table = pd.pivot_table(listings, values='long_term',aggfunc='count',\
                            index=['neighbourhood_cleansed']\
                      )
key2 = table.rank(ascending=False).sort_values(ascending=False)

sorter = np.lexsort((key2, table))

sorted_table = table.take(sorter).sort_values(ascending=False)

print ""The Top Five neighbourhoods with the most number of listings: ""
print ""\n"" 
print sorted_table.head()","# loop through each word 
for word in unique_words:
    if word == word[::-1]:
        print(word)",,"Project 2->Step 3: Visualize your data.->Using a plotting utility of your choice (Tableau or python modules or both), create visualizations that will provide context to your data. There is no minimum or maximum number of graphs you should generate, but there should be a clear and consistent story being told. Give insights to the distribution, statistics, and relationships of the data.",Airbnb Nyc Analysis->Logistic Regression->About Listings,1->5.Palidromes in entire word list --> approach 2
276140,"print(os.listdir('../input'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/007_nagato_yuki'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/046_alice_margatroid'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/065_sanzenin_nagi'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/080_koizumi_itsuki'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/096_golden_darkness'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/116_pastel_ink'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/140_seto_san'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/144_kotegawa_yui'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/164_shindou_chihiro'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/165_rollo_lamperouge'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/199_kusugawa_sasara'))
print(os.listdir('../input/moeimouto-faces/moeimouto-faces/997_ana_coppola'))",print(os.listdir('./data/')),z24 = df[df['Zip'] == 90024],,Kernel->Exploratory Analysis,10 Numpy Dictionary Pandas->NumPy: Fancy Indexing->Selecting Data from `Series` Objects->Dictionary Like Features->Membership Testing with `in`,Timeseries Regression Simulation->First we simulate the data->Then we look at data from a single zip code
150369,"# change the order timestamp to datetime format
# create duration variables in seconds and minutes

response['Driver Response Timestamp'] = pd.to_datetime(response['Driver Response Timestamp'])
response['Order Create Timestamp'] = pd.to_datetime(response['Order Create Timestamp'])
response['Duration'] = response['Driver Response Timestamp'] - response['Order Create Timestamp']
response['Duration'] = response['Duration'].dt.total_seconds()
response['Duration [min]'] = response['Duration'] / 60.0

# create variables in hours, days and day of week

response['Order hour'] = response['Order Create Timestamp'].dt.hour
response['Order dayOfweek'] = response['Order Create Timestamp'].dt.weekday
response['Order day'] = response['Order Create Timestamp'].dt.day

# create duration since last order

response['Duration last order'] = response['Order Create Timestamp'] - response['Order Create Timestamp'].shift(1)
response['Duration last order'] = response['Duration last order'].dt.total_seconds()
response['Duration last order [min]'] = response['Duration last order'] / 60.0",secondOrderPolynomial(4),"n_max=20
print(""Number of face-down triangles when n=1..""+str(n_max)+"":"")
print([num_facedown(n) for n in range(1,n_max+1)])",,Ab Testing->Data preprocessing,4 Functions->Keyword options,Triangles->Result
432779,sigdep_trial_df['subj_idx'] = sigdep_trial_df['subject'],sigdep_trial_df['subj_idx'] = sigdep_trial_df['subject'],"import numpy as np
import utils_datagen
import utils_display
from matplotlib import pyplot as plt
import tensorflow as tf
print(""Tensorflow version: "" + tf.__version__)",,Prt->Standard Analysis->HDDM,Prt->Standard Analysis->HDDM,"00 Rnn Predictions Playground->An RNN for short-term predictions
00 Rnn Predictions Solution->An RNN for short-term predictions"
140729,"# Convert to date
train['offerdate'] = pd.to_datetime(train['offerdate'], format='%Y/%m/%d')","# define arbitary bins
train['sumExternalDefault_arbbin'] = pd.cut(train.sumExternalDefault,
                                            [0, 1, 100, 250, 500, 100000000],
                                            right=False) 
# define binary bins
train['sumExternalDefault_binarybin'] = pd.cut(train.sumExternalDefault,
                                            [0, 1, 100000000],
                                            right=False)","with urlopen(rest.geturl()) as url:
    users = json.loads(url.read())

print('User IDs retrieved from server')
print('==============================')
print(users)",,Eda->Train history data,1 Banking Model Analysis->Option A - Data Challenge: Build a Banking Model - Analysis (1/3)->4. Baseline Model->cutomerID - index->Notes,07 Res Tful Api->CHAPTER 7->Read and print customer IDs
224538,"sess.run(epsilon, feed_dict={train: False})","sess.run(train_op, feed_dict={lr_ph: 0.0, beta_ph: 1.0})","from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingClassifier

from sklearn.ensemble import AdaBoostRegressor",,Vae,"Mnist->Normalizing flows with MNIST a simple implementation of the Glow paper->Train model, define metrics and trainer
Celeba64X64 22Steps->Normalizing flows:  re-implementation of the Glow paper->Build Flow with Resnet blocks",Classifiers Quick Review->Boosting
172966,"# Initial aperiodic signal fit - do a robust background fit (excluding outliers)
#  This recreates an initial fit that isn't ultimately stored in the FOOOF object)
init_bg_fit = gen_background(fm.freqs, fm._robust_bg_fit(fm.freqs, fm.power_spectrum))

# Plot the initial background fit
_, ax = plt.subplots(figsize=(12, 10))
plot_spectrum(fm.freqs, fm.power_spectrum, plt_log, label='original power spectrum', ax=ax)
plot_spectrum(fm.freqs, init_bg_fit, plt_log, label='initial aperiodic fit', ax=ax)","## Create a noisy synthetic power spectrum
# Aperiodic signal parameters, as [offset, slope]
bg_params = [20, 2]
# Gaussian peak parameters 
gauss_params = [10, 1.0, 2.5, 20, 0.8, 2, 32, 0.6, 1]

# Create a synthetic power spectrum
freqs, spectrum = gen_power_spectrum([1, 50], bg_params, gauss_params, nlv=0.1)","states.sort_values('HS_Grad', ascending=False).head(10)",,03-Fooof Algorithm->FOOOF Algorithm,05-Tuning&Troubleshooting->FOOOF: Tuning & Troubleshooting,06 - Pandas->Lesson 06 - Pandas DataFrames->Sorting by Columns
159428,"df_lt_corr = df_lt_corr.assign(Wait_days = lambda x: '< 25')
df_gte_corr = df_gte_corr.assign(Wait_days = lambda x: '>= 25')
df_unified = pd.concat([df_lt_corr, df_gte_corr])

g = sns.factorplot(kind='bar', data=df_unified, x='Feature', y='Show_up_corr', hue='Wait_days', size=7, aspect=2)
plt.title('How dataset features correlate with Show_up between short waiting and long waiting patients')
g.set_ylabels('Correlation with Show_up');","def plot(Y, x1_df, x2_df, x_axis_name, column_name, plot_kind='bar', ax=None):
    
    Y_df = full_processed_df[Y]
    full_df = pd.concat([x1_df, x2_df, Y_df], axis=1)
    
    color_palette = ""pastel""
    
    g = sns.factorplot(x=x_axis_name, y=Y, col=column_name, data=full_df, kind=plot_kind,
                       capsize=.2, palette=color_palette, size=6, aspect=.75)
        
    sns.set(style=""whitegrid"", palette=color_palette, color_codes=True)
    g.set(ylim=(1, 8))
    g.despine(left=True)","maps = pd.DataFrame(df_kiva_loc['country'].value_counts()).reset_index()
maps.columns=['country', 'counts']
maps = maps.reset_index().drop('index', axis=1)",,Investigate A Dataset->Project: Investigate a Medical Appointment No Shows Dataset->Data Cleaning,Step 5 - Data Analysis->Plots,Script1238
261621,"path_stage1 = ""o1.json""
path_stage2 = ""o2.json""
path_stage3 = ""o3.json""

kernel = pytextrank.rank_kernel(path_stage2)

with open(path_stage3, 'w') as f:
    for s in pytextrank.top_sentences(kernel, path_stage1):
        f.write(pytextrank.pretty_print(s._asdict()))
        f.write(""\n"")
        # to view output in this notebook
        print(pytextrank.pretty_print(s._asdict()))","path_stage1 = ""o1.json""
path_stage2 = ""o2.json""
path_stage3 = ""o3.json""

kernel = pytextrank.rank_kernel(path_stage2)

with open(path_stage3, 'w') as f:
    for s in pytextrank.top_sentences(kernel, path_stage1):
        f.write(pytextrank.pretty_print(s._asdict()))
        f.write(""\n"")
        # to view output in this notebook
        print(pytextrank.pretty_print(s._asdict()))","print 'Instances of missing or zero values for RESPOP'
print len(cvar[(cvar['RESPOP'].isnull()) | (cvar['RESPOP']==0)]['RESPOP']),len(cvar[(cvar['RESPOP']==0)]['RESPOP'])",,Example->Stage 3:,Example->Stage 3:,Sas2Csv->Converting SAS to CSV->Resident Population
129198,"t = spider_params.t0 + spider_params.per/4.0

spider_params.plot_system(t)

fname = 'f4'
plt.savefig('../docs/images/'+fname+'.png',bbox_inches='tight')
plt.savefig('../docs/images/'+fname+'.pdf',bbox_inches='tight')

plt.show()","spider_params.plot_planet(t,temp_map=True)

fname = 'f3'
plt.savefig('../docs/images/'+fname+'.png',bbox_inches='tight')
plt.savefig('../docs/images/'+fname+'.pdf',bbox_inches='tight')

plt.show()","coeffs2 = pd.DataFrame(model2.best_estimator_.coef_, columns = Xt[RFECVcolumns].columns)
coeffs_t2 = coeffs2.transpose()
coeffs_t2.columns = ['Logistic Coeffs L2']
coeffs_t['Logistic Coeffs L2'] = coeffs_t2['Logistic Coeffs L2']
print(coeffs_t)",,Quickstart->A straightforward example,Quickstart->A straightforward example,P5 Ms Final Disaster Analyis-Checkpoint->Logistic Regression and Model Validation->2st Log_Reg Model - L2 - All Features - Accuracy Score: .8069 and Classification Report Summary Showing the precision of the model. Model score is slightly better than the L1 All Features model.
324459,"train_balanced = balance_dataset(train_part)

y_train_balanced = train_balanced['CareerSatisfaction']
X_train_balanced = train_balanced.drop(columns=['CareerSatisfaction'])

evaluate_predictions(random_forest, X_train_balanced, y_train_balanced, X_valid, y_valid)","train_balanced = balance_dataset(train_part)

y_train_balanced = train_balanced['CareerSatisfaction']
X_train_balanced = train_balanced.drop(columns=['CareerSatisfaction'])

evaluate_predictions(random_forest, X_train_balanced, y_train_balanced, X_valid, y_valid)","layout = go.Layout(title = ""Sum Counts vs TOF"")
layout.xaxis['title'] = ""TOF (microS)""
layout.yaxis['title'] = ""Counts""
trace1 = Scatter(x = _tof_handler.tof_array,
                y = sum_counts,
                mode = 'markers',
                marker = Marker( color = 'red',
                               symbol = 'square'))
data = Data([trace1])
fig = Figure(data=data, layout=layout)
iplot(fig)",,Stackoverflow Career Satisfaction Kseniia->9.5 Decreasing amount of target classes,Stackoverflow Career Satisfaction Kseniia->9.5 Decreasing amount of target classes,Calibrate Snap Source Detector Distance->Convert 'Image Index' to 'time'
367585,g.edges('Matt'),"edg = EdgeDetection(detect_lines=False, mask=roi_mask)
image = curved

filtered = edg.filter(image, is_lab=False)
show_two_images(image, filtered, size=(12, 10))","from sklearn.cross_validation import KFold (n=len(x), n_folds=35, shuffle=True)
x_trains=[]
y_trains= []
x_",,Intro To Network X->Extremely flexible->Nodes->List all edges related to a node,4 Edge Detection->Edge processing for lane detection->Test implementation,Titanic-Machine-Learning->Titanic: Machine Learning from Disaster->Non-numeric columns
442118,"df.Miles.cumsum().plot()
plt.xlabel(""Day Number"")
plt.ylabel(""Distance"")","plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')","def plot_precision_vs_recall(precisions, recalls, label=""None""):
    plt.plot(recalls, precisions)
    plt.xlabel(""Recall"")
    plt.ylabel(""Precision"")
    plt.ylim([0, 1])
    plt.xlim([0, 1])
    
plot_precision_vs_recall(precisions, recalls)
plt.show()",,"4-7 Pandas Data Frame Summary Statistics, Filtering, Dropping And Adding Rows And Columns, Grouping Basics","4->Factor Analysis->7. Example->Example B. Randomized PCA for High Dimensional Data
Principal Component Analysis->In Depth: Principal Component Analysis->Example: Eigenfaces
Executive Master Stat & Big Data - Optimisation->Kernel Logistic Classification->Stochastic Gradient Descent (SGD)->Logistic classification with Sklearn
Pca Example->Example 1:->Apply PCA to `features_table`
Linear-Algebra-101->Eigen values and vectors->Principal Component Analysis (PCA)","Mnist Classification->Test Set Creation
Mnist Classification->Training a Binary Classifier"
38846,"def resnet20(num_classes=10):
  """"""Constructs a ResNet-20 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 20, num_classes)
  return model

def resnet32(num_classes=10):
  """"""Constructs a ResNet-32 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 32, num_classes)
  return model

def resnet44(num_classes=10):
  """"""Constructs a ResNet-44 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 44, num_classes)
  return model

def resnet56(num_classes=10):
  """"""Constructs a ResNet-56 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 56, num_classes)
  return model

def resnet110(num_classes=10):
  """"""Constructs a ResNet-110 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 110, num_classes)
  return model","def resnet20(num_classes=10):
  """"""Constructs a ResNet-20 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 20, num_classes)
  return model

def resnet32(num_classes=10):
  """"""Constructs a ResNet-32 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 32, num_classes)
  return model

def resnet44(num_classes=10):
  """"""Constructs a ResNet-44 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 44, num_classes)
  return model

def resnet56(num_classes=10):
  """"""Constructs a ResNet-56 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 56, num_classes)
  return model

def resnet110(num_classes=10):
  """"""Constructs a ResNet-110 model for CIFAR-10 (by default)
  Args:
    num_classes (uint): number of classes
  """"""
  model = CifarResNet(ResNetBasicblock, 110, num_classes)
  return model","# Function to generate random split of the data and fit models (to study the model variance):
# ------------------------------------------------------------------------------------------

def rand_split_fit_r_sq(data_total , ratio):
    
    # select a random permutation of indices
    perm = np.random.permutation(data_2_expanded.shape[0])
    
    # Get the train and test data sets:
    data_train = data_2_expanded.iloc[perm[:int(0.25 * data_2_expanded.shape[0])], :]
    data_test = data_2_expanded.iloc[perm[int(0.25 * data_2_expanded.shape[0])]:, :]
    
    # form test and train variables:
    x_t = data_test.iloc[:,:-1].values
    y_t = data_test.iloc[:,-1].values

    x_tr = data_train.iloc[:,:-1].values
    y_tr = data_train.iloc[:,-1].values
    
    # fit models:
    # Fit OLS on training
    reg = Lin_Reg()
    reg.fit(x_tr, y_tr)

    # #valuate R^2 for both training and test:
    train_r = reg.score(x_tr, y_tr)
    test_r = reg.score(x_t, y_t)
    return train_r, test_r",,Project Code->Project Code->ResNet Implementation by Kaiming He->Predefined Models of Different Depth,Project Code->Project Code->ResNet Implementation by Kaiming He->Predefined Models of Different Depth,Bassily Hany Cse109A Hw4-><font color = 'blue'>Solution:</font>
118443,"def my_logger(orig_func):
    import logging
    logging.basicConfig(filename='{}.log'.format(orig_func.__name__), level=logging.INFO)
    
    def wrapper(*args, **kwargs):
        logging.info('Ran with args: {}, and kwargs: {}'.format(args, kwargs))
        return orig_func(*args, **kwargs)
    
    return wrapper

def my_timer(orig_func):
    import time
    
    def wrapper(*args, **kwargs):
        t1 = time.time() # Sets the starting time
        result = orig_func(*args, **kwargs) # Runs the function and gets the result
        t2 = time.time() - t1 # Gets second timepoint and difference
        print('{} ran in: {} sec'.format(orig_func.__name__, t2))
        return result
    
    return wrapper","import time
import logging


def logger(func):
    """"""
    create logging for the function,
    re-define the format to add specific logging time
    """"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        logging.basicConfig(
            filename = '{}.log'.format( func.__name__ ),
            format = '%(asctime)s -- %(levelname)s:%(name)s: %(message)s',
            datefmt = '%Y/%m/%d-%H:%M:%S',
            level = logging.INFO)
        
        # custom the logging information
        logging.info('Ran with args: {} and kwargs: {}'.format(args, kwargs))
        return func(*args, **kwargs)

    return wrapper


def timer(func):
    """"""time the running time of the passed in function""""""
    @wraps(func)
    def wrapper(*args, **kwargs):
        t1 = time.time()
        result = func(*args, **kwargs)
        t2 = time.time() - t1
        print('{} ran in: {} sec'.format(func.__name__, t2))
        return result
    
    return wrapper","tsne_p = makePlot(tsne_Embedding, labels, imgSource)
tsne_t = show(tsne_p)",,1->Practical Example,Decorators->Decorators->Practical Use Cases,Umap Usage->UMAP->Next we'll examine t-SNE
441916,"print('Training data size: {}, test data size: {}'.format(X_train_raw.shape[0], X_test_raw.shape[0]))","X_train  = np.array(X[:train_size])
y_train = np.array(y[:train_size])

X_test = np.array(X[train_size:])
y_test = np.array(y[train_size:])

print(""X_train size: {}"".format(X_train.shape))
print(""y_train size: {}"".format(y_train.shape))
print(""X_test size: {}"".format(X_test.shape))
print(""y_test size: {}"".format(y_test.shape))","from sklearn.model_selection import train_test_split

features = data.iloc[:,0:4]
labels = data.iloc[:,4]

x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.25, random_state = 50)",,Naive Bayes Sms Spam Classification->Load the data and explore the data->Split the data into training data and test data,Test->Step 2. Data preprocessing->Step 2.3 Splitting data to training and testing parts,Chapter2->Chapter 2: Machine Learning Basics->Supervised Learning
293605,"# returns a new DataFrame
df.reindex(['Ryan', 'Chiara', 'Johnny', 'Takaya', 'Kerry'])","df.reindex()
df.shape","# example: classifier with 100 decision stumps as weak learners
from sklearn.datasets import make_hastie_10_2
from sklearn.ensemble import GradientBoostingClassifier

X, y = make_hastie_10_2(random_state=0)
X_train, X_test = X[:2000], X[2000:]
y_train, y_test = y[:2000], y[2000:]

clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    max_depth=1, random_state=0).fit(X_train, y_train)
clf.score(X_test, y_test)",,Pandas->Pandas->Pandas Data Structures: Series,Nlp Pose->Neural Network - Keras,Ensemble-Methods->GRADIENT TREE BOOSTING
237633,"endpoint_views = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/aggregate/{project}/{access}/{agent}/{granularity}/{start}/{end}'

start = '20150701'
end = '20171110'

params_views = {'project' : 'en.wikipedia.org',
            'access' : 'mobile-web',
            'agent' : 'user',
            'granularity' : 'monthly',
          'start' : start+'00',
            'end' : end+'00'
            }

mobile_web_views = requests.get(endpoint_views.format(**params_views))
outputToJson(base_path,'pageviews','mobile-web',start,end,mobile_web_views.json())

params_views['access'] = 'mobile-app'
mobile_app_views = requests.get(endpoint_views.format(**params_views))
outputToJson(base_path,'pageviews','mobile-app',start,end,mobile_app_views.json())

params_views['access'] = 'desktop'
desktop_views = requests.get(endpoint_views.format(**params_views))
outputToJson(base_path,'pageviews','desktop',start,end,desktop_views.json())","# Collect desktop traffic data from January 2008 through July 2016 using the Pagecounts API
endpoint_pagecounts = 'https://wikimedia.org/api/rest_v1/metrics/legacy/pagecounts/aggregate/{project}/{access}/{granularity}/{start}/{end}'

params_pc_desktop = {
                    'project' : 'en.wikipedia.org',
                    'access' : 'desktop-site',
                    'granularity' : 'monthly',
                    'start' : '2008010100',
                    'end' : '2016080100'#use the first day of the following month to ensure a full month of data is collected
                    }

api_call = requests.get(endpoint_pagecounts.format(**params_pc_desktop))
response_pc_desktop = api_call.json()
with open('pagecounts_desktop-site_200801-201607.json', 'w') as outfile:
    json.dump(response_pc_desktop, outfile)","from sklearn import tree
from sklearn.metrics import mean_squared_error

features = list(steam_df.columns)
features.remove('PriceFinal')

reg = tree.DecisionTreeRegressor(random_state=1)
reg.fit(steam_df[features], steam_df['PriceFinal']);
test_predict = reg.predict(test_df[features])
rmse_test = np.sqrt(mean_squared_error(test_predict, test_df['PriceFinal']))
print(rmse_test)",,Hcds-A1-Data-Curation->Step 1: Data acquisition,Hcds-A1-Data-Curation->A1 Data Curation->Step1: Data Acquisition,Steam Ml->Universidade Federal do Rio Grande do Norte->Prediction Performance
70938,"### TODO: Train the model.
CheckpointerVGG19 = ModelCheckpoint(filepath='saved_models/weights.best.VGG19.hdf5', 
                               verbose=1, save_best_only=True)

VGG19_model.fit(train_VGG19, train_targets, 
          validation_data=(valid_VGG19, valid_targets),
          epochs=20, batch_size=20, callbacks=[CheckpointerVGG19], verbose=1)","### TODO: Train the VGG-19 model.

checkpointer_VGG19 = ModelCheckpoint(filepath='saved_models/weights.best.VGG19.hdf5', 
                               verbose=1, save_best_only=True)

VGG19_model.fit(train_VGG19, train_targets, 
          validation_data=(valid_VGG19, valid_targets),
          validation_split=0.25,
          epochs=20, batch_size=200, callbacks=[checkpointer_VGG19], verbose=1)","sem_df = sem_df.drop_duplicates(subset=['SUBJECT_CUI', 'SUBJECT_NAME', 'SUBJECT_SEMTYPE', 'PREDICATE',
                                        'OBJECT_CUI', 'OBJECT_NAME', 'OBJECT_SEMTYPE'])

print('Rows Remaining: {:,}'.format(sem_df.shape[0]))",,Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Predict Dog Breed with the Model,Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Assess the Dog Detector,01-Building The Hetnet->3. Edges File->Fix semmantic types using the UMLS Metathesaurus->De-Duplicate
32183,"params = {'n_estimators':80, 'max_depth' : -1, 'objective': 'binary', 'num_leaves': 96, 'learning_rate': 0.05, 
          'max_bin': 150, 'subsample_for_bin': 200, 'subsample': 1, 'subsample_freq': 1, 'colsample_bytree': 0.8, 
          'reg_alpha': .1, 'reg_lambda': 1, 'min_split_gain': 0.1, 'min_child_weight': 1, 'min_child_samples': 5, 
          'class_weight': 'balanced', 'num_class' : 1, 'metric' : 'binary_error', 'min_data_in_leaf':2}
pos_weight = 1-np.mean(y)
weight = y.apply(lambda x : pos_weight if x == 1 else 1 - pos_weight)
dataset = lgb.Dataset(data=X_vect, label=y, weight=weight)
cv = lgb.cv(params=params, train_set=dataset, num_boost_round=1000, nfold=10, stratified=True,
                early_stopping_rounds=10, metrics=['binary', 'auc'], verbose_eval=True)","# Build model with 10-fold CV
d_train2 = lgb.Dataset(X_train, label=y_train.ravel())

clf_cv = lgb.cv(params, 
                d_train, 
                500, 
                nfold = 10, 
                early_stopping_rounds = 100,
                stratified = True)

# Generate predictions
lgbm_pred = clf.predict(X_test, num_iteration=clf.best_iteration)","# Remove non-alpha characters
r1=nltk.corpus.reuters.words()
r1unique = set(r1)
r1 = [w for w in r1 if w.isalpha()]",,Notebook->Model->Baselines->Retuning numberof trees with new found parameters and lower learning rate,Predicting Home Prices For Zillow->Predicting Zillow Prices->Step 7: Select key features->Principle Component Analysis,Week4-High Freq Words-Checkpoint->The 200 Highest Frequency Words
308240,"def plot_learning_curve(estimator, title, X, y, n_jobs = n_jobs, train_sizes = np.linspace(.1, 1.0, 5)):
    
    plt.figure()
    plt.title(title)
    
    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, 
                                                            cv=kfold, n_jobs=n_jobs, train_sizes=train_sizes)

    train_scores_mean = np.mean(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    
    plt.plot(train_sizes, train_scores_mean, label=""Training score"")
    plt.plot(train_sizes, test_scores_mean, label=""Cross-validation score"")

    plt.legend(loc=""best"")
                        
    return plt","x = np.linspac(0, 10, 20)","tmp = train_class_df.groupby(['Target', 'PatientAge'])['patientId'].count()
df = pd.DataFrame(data={'Exams': tmp.values}, index=tmp.index).reset_index()
tmp = df.groupby(['Exams','Target', 'PatientAge']).count()
df2 = pd.DataFrame(data=tmp.values, index=tmp.index).reset_index()",,Titanic Challenge->2. Load and check data->SVC->6.2.3 Learning curves,Py Exploratory Comp 7 Sol->Notebook->`invalid syntax`,"Rsna-Pneumonia-Detection-Eda-><a id=""2"">Prepare the data analysis</a>-><a id=""310"">Patient Age</a>"
163150,"evensto20 = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]
oddsto10 = []
# Loop through the list *even* to create a list called *odd*
for i in evensto20:
    if i <= 10:
        oddsto10.append(i-1)
oddsto10","i = 0                    # Setting the starting value for i
mylist1 = []             # Blank list to append items to

while i <= 9:            # While loop syntax
    mylist1.append(i)    # Appends the current value of i to the list
    i = i + 1            # Adding 1 to i for the current iteration

mylist1","accuracy = predict_instances(testFile, outputFile, dialogs_dict, dialogs_features_dict)",,3->Intermediate Concepts in Python->Objectives->List Comprehensions,B08->B08: Loops & Iterating->While Loops,Assignment 4 - Dialog Act->EECS 498 - Assignment 4 - Dialog Act Classification->MAIN FUNCTION :->5) predict dialog acts for test data
362033,"plt.figure(figsize=(15, 6))
plt.hist(race_review_df['racer_id'].value_counts(), bins=88)
plt.title('Count of Racers by Number of Reviews Submitted\n', fontsize=20, weight='bold')
plt.xlabel('# of Reviews', fontsize=18)
plt.ylabel('\n# of Racers', fontsize=18)
plt.tick_params(labelsize=16)
plt.savefig('./plots/racer_review_counts.png', dpi=300, bbox_inches='tight', pad_inches=0, transparent=True)","racer_ratings_summary = race_review_train.groupby('racer_id').mean()
racer_ratings_summary = racer_ratings_summary.reset_index()
racer_combined = pd.merge(left=racer_combined, right=racer_ratings_summary, on='racer_id')
racer_combined.head()",soup = BeautifulSoup(urllib2.urlopen(page_url)),,Capstone Recommender Final->Step 4 - Clustering,Capstone Recommender Final->Step 2 - Extracting reviews->Matching Racers and Races,"Alcohol-Consumption->Alcohol consumption choropleth->Scrap the table
Usa-States-Population->USA states population->Scrape the table"
347474,"import torch 
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import torch.nn.functional as F
import matplotlib.pylab as plt
import numpy as np
torch.manual_seed(0)","# Import the libraries and set random seed
from torch import nn
import torch
import numpy as np
import matplotlib.pyplot as plt
from torch import nn,optim
from torch.utils.data import Dataset, DataLoader

torch.manual_seed(1)","print titanic_df['Parch'].unique()
print 'the cases with Parch=0 is',len(titanic_df[titanic_df['Parch']==0])
print 'the cases with Parch=1 is',len(titanic_df[titanic_df['Parch']==1])
print 'the cases with Parch>=2 is',len(titanic_df[titanic_df['Parch']>=2])
titanic_df['Parchnew'] = 0
condition1 = titanic_df['Parch'] >= 2
condition2=titanic_df['Parch'] ==1
titanic_df.loc[condition1, 'Parchnew'] =2
titanic_df.loc[condition2, 'Parchnew'] =1
print 'the cases of parch=0 is{},the cases of parch=1 is{},the cases of parch>=2 is{}'.format(len(titanic_df[titanic_df['Parchnew']==0]),len(titanic_df[titanic_df['Parchnew']==1]),len(titanic_df[titanic_df['Parchnew']==2]))",,5,2,Ipython Notebook Titanic->Exploration:
53801,"# training sentences and their corresponding word-tags
training_data = [
    (""The cat ate the cheese"".lower().split(), [""DET"", ""NN"", ""V"", ""DET"", ""NN""]),
    (""She read that book"".lower().split(), [""NN"", ""V"", ""DET"", ""NN""]),
    (""The dog loves art"".lower().split(), [""DET"", ""NN"", ""V"", ""NN""]),
    (""The elephant answers the phone"".lower().split(), [""DET"", ""NN"", ""V"", ""DET"", ""NN""])
]

# create a dictionary that maps words to indices
word2idx = {}
for sent, tags in training_data:
    for word in sent:
        if word not in word2idx:
            word2idx[word] = len(word2idx)

# create a dictionary that maps tags to indices
tag2idx = {""DET"": 0, ""NN"": 1, ""V"": 2}","# check out what prepare_sequence does for one of our training sentences:
example_input = prepare_sequence(""The dog answers the phone"".lower().split(), word2idx)
print(example_input)","def find_edge(x,grid_size,n):
    if x<(grid_size/2):
        if x>n:
            return n
        else :
            return int(np.floor(x / 2) * 2)
    else :
        if (grid_size-x)>n:
            return n
        else :
            return int(np.floor((grid_size-x) / 2) * 2)",,"2->LSTM for Part-of-Speech Tagging->Preparing the Data
Lstm Training, Part Of Speech Tagging-2->LSTM for Part-of-Speech Tagging->Preparing the Data","2->LSTM for Part-of-Speech Tagging->Preparing the Data
Lstm Training, Part Of Speech Tagging-2->LSTM for Part-of-Speech Tagging->Preparing the Data",Simulate Zp With Tilt Using Fftw-Checkpoint
83693,"run_print(tf.ones([2, 2], np.float32))","## Run Predictions on the 5 examples

with tf.Session() as sess:
    saver.restore(sess,restore_file)
    new_pred = sess.run(pred, feed_dict={x:new_X_subset,y:new_y_subset,prob:1})
    pred_ind = np.argmax(new_pred,1)
    new_pred_acc = np.mean(np.equal(pred_ind,new_y_subset))
    #print(new_pred.shape)
    #print(pred_ind)
    #print(new_y_subset)
    print(""Accuracy on the new 5 subset of pictures is {0:.3f}%"".format(new_pred_acc*100))","df=pd.DataFrame(X_train_S,columns=(X_train[predictors]).columns)
plt.figure(figsize=(12, 8))

plt.bar(np.arange(lrmodl.coef_.shape[0]) - 0.4, lrmodl.coef_)
plt.xticks(np.arange(lrmodl.coef_.shape[0]), df.columns, rotation='vertical')
plt.xlim([-1, lrmodl.coef_.shape[0]])
plt.title(""Sklearn model coefficients"");",,4->TensorFlow Data Types->TF vs NP Data Types->Can pass numpy types to TensorFlow ops,Traffic Sign Classifier->Question 6,Prediction On Big Mart Dataset->Graph for Important Features
75243,"# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
objp = np.zeros((6*9,3), np.float32)
objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)

# Arrays to store object points and image points from all the images.
objpoints = [] # 3d points in real world space
imgpoints = [] # 2d points in image plane.

# Make a list of calibration images
images = glob.glob('camera_cal/calibration*.jpg')

# Step through the list and search for chessboard corners
for fname in images:
    img = cv2.imread(fname)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

    # Find the chessboard corners
    ret, corners = cv2.findChessboardCorners(gray, (9,6),None)

    # If found, add object points, image points
    if ret == True:
        objpoints.append(objp)
        imgpoints.append(corners)
        
        # Draw and display the corners
        img = cv2.drawChessboardCorners(img, (9,6), corners, ret)
        #cv2.imshow('img',img)
        #cv2.waitKey(500)

#cv2.destroyAllWindows()","# camera calibration

# termination criteria
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
objp = np.zeros((9*6,3), np.float32)
objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)

imgpoints = []
objpoints = []

images = glob.glob('./camera_cal/calibration*.jpg')
for fname in images:
    img = cv2.imread(fname)
    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

    # Find the chess board corners
    ret, corners = cv2.findChessboardCorners(gray, (9,6),None)

    # If found, add object points, image points (after refining them)
    if ret == True:
        objpoints.append(objp)

        corners2 = cv2.cornerSubPix(gray,corners,(11,11),(-1,-1),criteria)
        imgpoints.append(corners2)

        # Draw and display the corners
        img = cv2.drawChessboardCorners(img, (7,6), corners2,ret)","print( ""training_text shape :"", training_text.shape )
print( ""training_variants shape :"", training_variants.shape )",,Main->Advanced Lane Finding Project->Camera Calibration,P4->Camera calibration,1->Read data into dataframe->Exploring dataset - DataFrame shape
307686,two_dim_arr.sum(axis=0),"two_dim_slices(sample.trigger_deltas, 0.1)","def getPercentageFastDistanceGroup(init,end):
    d = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')\
        .load('../BDdata/'+str(init)+'.csv')
    for i in range(init+1,end+1):
        d = d.union(sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')\
            .load('../BDdata/'+str(i)+'.csv'))

    d1 = d.where(d[""Cancelled""]==0)\
        .where(d[""DepDelay""] != ""NA"")\
        .where(d[""ArrDelay""] != ""NA"")\
        .where(d[""Distance""] != ""NA"")\
        .select(d[""DepDelay""],d[""ArrDelay""],d[""Distance""])
    
    distanceGroup = udf(lambda x : int(x)//200+1,IntegerType())
    d2 = d1.withColumn(""DistanceGroup"",distanceGroup(d1[""Distance""]))\
        .drop(d1[""Distance""])
    
    d3 = d2.where(""DepDelay > 0"")\
        .filter(d2[""DepDelay""]>=2*d2[""ArrDelay""])\
        .groupBy(d2.DistanceGroup).agg({""*"":""count""})

    d4 = d2.groupBy(d2.DistanceGroup).agg({""*"":""count""})
    
    d5 = d4.join(d3,on=[d3.DistanceGroup==d4.DistanceGroup])\
        .withColumn(""percentage"", d3[""count(1)""]/d4[""count(1)""])\
        .drop(""count(1)"")\
        .select(d4.DistanceGroup,""percentage"").sort(""DistanceGroup"")
    return d5

res4 = getPercentageFastDistanceGroup(1994,2008)
res4.coalesce(1).write.csv(""./results/query3.csv"",header=True)",,Lecture->Numerical Computing with NumPy->Further Readings->np.random.choice,Sepp Testbed->Sample from the process,"Spark-Sql-Queries->Queries:->3. The percentage of flights belonging to a given ""distance group"" that were able to halve their departure delays by the time they arrived at their destinations."
192306,"filename = ""updated_test.p""
file = open(filename, 'rb')
X_test = pickle.load(file)

filename = ""AugmentedData_X.p""
file = open(filename, 'rb')
X_train = pickle.load(file)
filename = ""AugmentedData_Y.p""
file=open(filename,'rb')
y_train= pickle.load(file)

filename = ""updated_valid.p""
file = open(filename, 'rb')
X_valid = pickle.load(file)","filename = ""updated_test.p""
file = open(filename, 'rb')
X_test = pickle.load(file)

filename = ""AugmentedData_X.p""
file = open(filename, 'rb')
X_train = pickle.load(file)
filename = ""AugmentedData_Y.p""
file=open(filename,'rb')
y_train= pickle.load(file)

filename = ""updated_valid.p""
file = open(filename, 'rb')
X_valid = pickle.load(file)","userComb.groupby('tier')['id','username'].count()",,Traffic Sign Classifier->Data Augementation,Traffic Sign Classifier->Data Augementation,User Analysis-Checkpoint->is_superuser
47602,"b""\xe5\xbd\xb9\xe3\x81\xab\xe7\xab\x8b\xe3\x81\xa4\xe6\x97\xa5\xe3\x81\x8c\xe6\x9d\xa5\xe3\x82\x8b\xe3\x81\x8b\xe3\x82\x8f\xe3\x81\x8b\xe3\x82\x8a\xe3\x81\xbe\xe3\x81\x9b\xe3\x82\x93\xe3\x81\x8c\xe3\x80\x81\n\xe3\x82\xa8\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3\x83\x90\xe3\x83\xa9\xe9\xa7\x85\xe3\x81\xae\xe9\x9b\xbb\xe8\xbb\x8a\xe3\x81\xae\xe9\xa7\x85\xe3\x81\xaf\xe3\x81\x93\xe3\x81\x93\xe3\x81\xaa\xe3\x81\xae\xe3\x81\xa7\xe3\x81\x99\xe3\x81\x8c\xe3\x80\x81\n\xe3\x82\xa2\xe3\x83\x8a\xe3\x82\xa6\xe3\x83\xb3\xe3\x82\xb9\xe3\x81\xa7\xe3\x82\xa8\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3\x83\x90\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xa7\xe3\x83\x90\xe3\x83\xaa\xe3\x83\xbc\xe3\x81\xa3\xe3\x81\xa6\xe9\xa2\xa8\xe3\x81\xab\xe8\x81\x9e\xe3\x81\x93\xe3\x81\x88\xe3\x82\x8b\xe3\x81\xae\xe3\x81\xa7\xe3\x81\x88\xe3\x81\xa3\xef\xbc\x81\xe3\x81\x93\xe3\x81\x93\xe3\x82\xa8\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3\x83\x90\xe3\x83\xa9\xe9\xa7\x85\xe3\x81\xa7\xe3\x81\x82\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\xa8\xe4\xbd\x95\xe5\x9b\x9e\xe3\x81\x8b\xe9\xa9\x9a\xe3\x81\x8d\xe3\x81\xbe\xe3\x81\x97\xe3\x81\x9f\xe3\x80\x82\n\n\xe3\x82\xa8\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3\x83\x90\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xa7\xe3\x83\x90\xe3\x83\xaa\xe3\x83\xbc\xe9\xa7\x85\xe3\x81\x8c\xe3\x82\xac\xe3\x82\xa4\xe3\x83\x89\xe3\x83\x96\xe3\x83\x83\xe3\x82\xaf\xe3\x81\xaa\xe3\x81\xa9\xe3\x81\xab\xe3\x81\xaf\xe3\x82\xa8\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3\x83\x90\xe3\x83\xa9\xe9\xa7\x85\xe3\x81\xab\xe3\x81\xaa\xe3\x81\xa3\xe3\x81\xa6\xe3\x82\x8b\xe3\x81\xae\xe3\x81\xa7\xe5\xa4\xa7\xe4\xb8\x88\xe5\xa4\xab\xe3\x81\xa7\xe3\x81\x99\xe3\x80\x82\xe6\x81\x90\xe3\x82\x89\xe3\x81\x8f\xe3\x80\x82\xe8\xbf\x91\xe3\x81\x8f\xe3\x81\xae\xe3\x83\x98\xe3\x82\xa4\xe3\x83\x9e\xe3\x83\xbc\xe3\x82\xb1\xe3\x83\x83\xe3\x83\x88\xe9\xa7\x85\xe3\x81\x8b\xe3\x82\x89\xe3\x81\x93\xe3\x81\xae\xe9\xa7\x85\xe3\x81\xbe\xe3\x81\xa7\xe3\x81\xaf\xe6\xad\xa9\xe3\x81\x84\xe3\x81\xa6\xe3\x82\x82\xe6\xa5\xbd\xe3\x81\x97\xe3\x82\x81\xe3\x82\x8b\xe9\x81\x93\xe3\x81\xab\xe3\x81\xaa\xe3\x81\xa3\xe3\x81\xa6\xe3\x81\xbe\xe3\x81\x99\xe3\x82\x88\xe3\x80\x82\n\n\xe6\x81\x90\xe3\x82\x89\xe3\x81\x8f\xe3\x80\x81\xe5\xa4\x9a\xe5\x88\x86\xe3\x80\x81\xe3\x82\xa8\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3\x83\x90\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xa7\xe3\x83\x90\xe3\x83\xaa\xe3\x83\xbc\xe9\xa7\x85\xe3\x81\x8c\xe3\x81\x82\xe3\x81\xaa\xe3\x81\x9f\xe3\x81\xae\xe3\x81\x8a\xe3\x82\x8a\xe3\x81\x9f\xe3\x81\x84\xe3\x82\xa8\xe3\x83\x87\xe3\x82\xa3\xe3\x83\xb3\xe3\x83\x90\xe3\x83\xa9\xe9\xa7\x85\xe3\x81\xa0\xe3\x81\xa8\xe6\x80\x9d\xe3\x81\x84\xe3\x81\xbe\xe3\x81\x99\xe3\x80\x82\n\n\xe5\xbf\xb5\xe3\x81\xae\xe3\x81\x9f\xe3\x82\x81\xe7\xa2\xba\xe8\xaa\x8d\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x8f\xe3\x81\xa0\xe3\x81\x95\xe3\x81\x84\xe3\x81\xadw"".decode('utf-8')","X90 = ptm.rotate_x_ptm(angle=np.pi/2)

print(X90.round(3))","alg.coef_, alg.intercept_",,Reviews-Data-Exploration->Exploration,Tutorial->A: Introduction->Everything easier: Circuit->Example: Bell state preparation again with circuits,"Benchmark->Load data using pandas->Well, not so far from the constant solution... Let's try to understand why."
296407,"import processImage
import matplotlib.pyplot as plt

image_path='./test_images/whiteCarLaneSwitch.jpg'

img=processImage.detect(mpimg.imread(image_path))
plt.imshow(img)","image = mpimg.imread('test_images/test3.jpg')
fin_img = process_image(image)
plt.imshow(fin_img)","df_counted = df[['countryname','project_name']].groupby('countryname').count()",,P1->Test one picture,Untitled->Detected Window Visualization,Sliderule Dsi Json Exercise->1. Find the 10 countries with most projects
19610,"from sklearn.base import BaseEstimator, TransformerMixin


class MyFeatureAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_relatives=True, add_title=True, add_age_band=True):
        self.add_title = add_title
        self.add_relatives = add_relatives
        self.add_age_band = add_age_band
    
    def fit(self, X, y=None):
        return self  # nothing to do

    def transform(self, X, y=None):
        if self.add_title:
            pd.options.mode.chained_assignment = None  # creates slice-copy assignment warning
            X['Title'] = X.Name.str.extract(r' ([A-Za-z]+)\.', expand=False)
            X['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',
                                'Don', 'Dr', 'Major', 'Rev', 'Sir',
                                'Jonkheer', 'Dona'],
                                'Rare', inplace=True)
            X['Title'].replace('Mlle', 'Miss', inplace=True)
            X['Title'].replace('Ms', 'Miss', inplace=True)
            X['Title'].replace('Mme', 'Mrs', inplace=True)
            pd.options.mode.chained_assignment = 'warn'

        if self.add_relatives:
#             X.loc[:, 'Family'] = X['SibSp'] + X['Parch']  # creates slice-copy assignment warning
            Xtmp = pd.DataFrame(X['SibSp'] + X['Parch'], columns=['Family'])
            X = pd.concat([X, Xtmp], axis=1)
        
        if self.add_age_band:
            pd.options.mode.chained_assignment = None  # creates slice-copy assignment warning
            def age_band(row, quantiles):
                for i in range(len(quantiles)):
                    if row['Age'] < quantiles[i]:
                        return i-1
                return len(quantiles)-1
            nBins = 4
            quantiles = [X['Age'].quantile(1.0 * q / nBins) for q in range(0, nBins+1)]
            X['Age'] = X.apply(lambda row: age_band(row, quantiles), axis=1)
            pd.options.mode.chained_assignment = 'warn'
        
        X = X.drop('Name', axis=1)
        return X

# feat_adder = MyFeatureAdder()
# X_new = feat_adder.fit_transform(X_full)
# print(X_new.head())
# X_new.describe()","class CustomScaler(TransformerMixin): 
    def __init__(self, ignore_vars):
        self.scaler = StandardScaler()
        self.ignore_vars = ignore_vars

    def fit(self, X, y=None):
        self.scale_vars = X.drop(self.ignore_vars, axis=1).columns
        self.scaler.fit(X[self.scale_vars], y)
        return self

    def transform(self, X, y=None, copy=None):
        init_col_order = X.columns
        X_scaled = pd.DataFrame(self.scaler.transform(X[self.scale_vars]), columns=self.scale_vars, index=X.index)
        X_not_scaled = X[self.ignore_vars]
        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","_agenda_item.add_subject(""HCR"")

_agenda_item['notes'].append(""Regulation of complex large-molecule drugs by the Food and Drug Administration"")",,Titanic->The Titanic data challenge->Data preparation->Feature engineering,Customer Defection Classification With Tree Ensembles And Svm->Support Vector Machine,Getting Started->Agenda Items
68457,"# get index of predicted dog breed for each image in test set
VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]

# report test accuracy
test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)
print('test accuracy: %.4f%%' % test_accuracy)","# get index of predicted dog breed for each image in test set
VGG16_predictions = [
    np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) 
    for feature in test_VGG16]

# report test accuracy
test_accuracy = (
    100 * np.sum(
        np.array(VGG16_predictions) == np.argmax(test_targets, axis=1)) /
    len(VGG16_predictions))
print('Test accuracy: %.4f%%' % test_accuracy)","plt.plot(X[y==0,1], X[y==0,2], 'o')
plt.plot(X[y==1,1], X[y==1,2], 's')",,Dog App->Artificial Intelligence Nanodegree->Test the Model,Dog App->Test the Model,4A Logistic Regression->Logistic regression (from scratch!)
228225,"fig = plot_decision_regions(X=X, y=y, clf=rf)
plt.title(""Random Forest"")","#Let's look at average wait time by region
avgDaysRegion = totalDF.groupby(['CA Region'])['timePassed'].mean()
fig = plt.figure(figsize=(10,5));
avgDaysRegion.plot(kind = 'bar', color = 'g');
plt.ylabel('Avg number of days');
plt.title('Avg wait time in days for pothole repair request to be filled by region');","dates_train = dates[dates <  (last_block)]
dates_test  = dates[dates == (last_block)]

X_train = sales.loc[dates <  (last_block)].drop(cols_to_drop, axis=1)
X_test =  sales.loc[dates == (last_block)].drop(cols_to_drop, axis=1)
print(X_train.shape)
print(X_test.shape)

y_train = sales.loc[dates <  (last_block), 'item_cnt_day'].values
y_test =  sales.loc[dates == (last_block), 'item_cnt_day'].values  # y_test is what we need to predict for the contest
print(""y_train head: "", y_train)
print(""y_test head: "", y_test)",,Ensemble->Bagging->Random Forest,Capstone1 Chicago Pothole Analysis->PCA & Clustering->Pulling in Census Data->Number of requests by CA,Predict Sales Kaggle Contest Grid Search->Prep for training
299994,"print(""commentdata.shape = "",commentdata.shape)
commentdata.head()","# group all scores by comment ID for each text sample, add mean and median score columns to comment data 
commentdata[""mean_score""] = pd.Series(ratingdata.groupby(""rev_id"",as_index=False).mean()[""toxicity_score""])
commentdata[""median_score""] = pd.Series(ratingdata.groupby(""rev_id"",as_index=False).median()[""toxicity_score""])

# Add columns for comment length and punctuation count
commentdata[""com_len""] = pd.Series(commentdata[""comment""].str.len())
commentdata[""pun_per""] = pd.Series(commentdata[""comment""].apply(punc_count))

# create catgorical variable toxicity: if median score < 0, toxicity=1, otherwise 0
commentdata[""toxicity""] = (commentdata[""median_score""] < 0).astype(int)","import pandas as pd
pd.options.display.encoding = 'ascii'",,"Language Analysis-Checkpoint->Language Analysis <br> <font color=red>Toxic comments in game review forums</font>->4. Check Predictability (Machine Learning, Classifiers, etc) <a href='#top'> (top)</a><br>->Naive Bayes Multinomial",Language Analysis-Checkpoint->Code for Language Analysis of toxic comments in game review forums->Add and manipulate data columns:,Sentiment Analysis Of Weather Related Tweets->Sentiment analysis of weather related tweets->Retrieving and basic cleansing of the data
95008,"p_A_given_B = None
p_A_given_B # correct answer: 0.625","p_B_given_A = None
p_B_given_A # correct answer: 0.5","# Distance between samples
step = 1/8.
cutoff = 5

# Sample points on positive h-axis
h = np.arange(0, cutoff, step) + step
# Add sample points on negative axis and at the origin
h = np.concatenate((h[::-1], np.zeros(1), -h),axis=0)

# Use the same points for the k-axis
k = h",,Index->Dependent events->Exercise 4->Solution->- $P(A \mid B)$,Index->Dependent events->Exercise 4->Solution,Chapter 1->Chapter 1. Projection images->1.1 Setting the scene
197731,"with tf.Session() as sess:
    saver.restore(sess, save_file)
    
    test_accuracy = sess.run(
        accuracy_operation,
        feed_dict={x: newTestImages_grayscale, y: newTestLabels, keep_prob: 1.0})
    print(test_accuracy)","with tf.Session() as sess:
    saver.restore(sess, save_file)

    test_accuracy = sess.run(accuracy_operation, feed_dict={x: X_test, y: y_test, keep_prob : 1.0})

print('Test Accuracy: {}'.format(test_accuracy))","with pm.Model() as uncertain_vacc_model:
    
    # Confirmation sub-model
    σ = pm.HalfNormal('σ', 5)
    y = pm.GaussianRandomWalk('y', sd=σ, shape=n_knots)
    α = interpolate(knots, y, age)
    π = pm.invlogit(α)
        
    confirmation = pm.Bernoulli('confirmation', π, observed=confirmed)
    
    # Calculate age group probabilities of confirmation
    α_group = interpolate(knots, y, age_midpoints)
    p_confirm = pm.Deterministic('p_confirm', pm.invlogit(α_group))
    
    # Basic reproduction number centered on the dominant eigenvector of the contact matrix
    R_0 = pm.Normal('R_0', max(evs[np.isreal(evs)]), sd=3.5)
    
    # Age-specific efficacy of immunization
    p_eff = pm.Beta('p_eff', 12, 1, shape=n_age_classes, testval=np.ones(n_age_classes)*0.1)
    # Susceptibility prior to outbreaks
    susceptibility = tt.prod(1 - group_coverage.T*p_eff, axis=0)
    # Initial susceptibles
    S_0 = pm.Deterministic('S_0', N * susceptibility)
    
    # Setting efficacy to a constant
    p_vacc = efficacy
    
    # Susceptibles over time, removing individuals vaccinated by SIA
    S = S_0 - sia_vacc*p_vacc - shared(I.cumsum(axis=0))
    
    # Force of infection
    λ = S * (I.dot(B_prem_exp) / N) + 0.001
    
    # Effective reproductive number
    R_t = pm.Deterministic('R_t', S.sum(1) * R_0 / N.sum())
    
    # Adjust for confirmation bias
    λ_apparent = λ / p_confirm
    
    # Infections likelihood
    infections = pm.Poisson('infections', λ_apparent[:-1], observed=I[1:])",,Traffic Sign Classifier->Analyze Performance->Load the Saved model and running on the new test images downloaded from the web,"Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Train, Validate and Test the Model
Traffic Sign Classifier->Train, Validate and Test the Model",Measles Dynamics->Stochastic Disease Transmission Model->Adding uncertainty in vaccine efficacy
137260,"# install required libraries
# !pip install numpy
# !pip install pandas

# import libraries
import numpy as np
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt
%matplotlib inline","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
%matplotlib inline","for i in range(num_clusters1):
    counts_injury[i]= cluster_injury[i]['CDESCR'].str.split(expand=True).stack().value_counts().head(10)",,Lab3,"Challenge1
2 Exploratory Data Analysis->Exploratory data analysis",Vehicle Clusters->Table: The most frequent terms in each cluster identified in Fatal Incidents
399610,n_non_unique_columns,non_2017_column.shape,"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)    #pcut
train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)",,Feature Selection->Featuretools Features->Missing Values,"Week 3 Exercises->Part 2: Working with data (DSFS Chapter 10)->Exercise: The types of crime and their popularity over time. The first field we'll dig into is the column ""Category"".",Kaggle - Titanic->Model Evaluation
262276,[i ** 2 for i in range(20) if i % 5 == 0],"iterations = 25
j=0
while j != iterations:
    rang = range(1,len(list(T))-2)
    for i in rang:
        temp = T[:]
        value = list(T)[i]
        del temp[value]
        Score_temp = cross_val_score(linreg,temp,Y,cv=10,scoring='neg_mean_squared_error')
        Score_all = cross_val_score(linreg,T,Y,cv=10,scoring='neg_mean_squared_error')
        S_temp = sqrt(-Score_temp.mean())
        S_all = sqrt(-Score_all.mean())
        if S_temp < S_all:
            T = temp[:]
            rang.remove(rang[-1])
    j += 1","url_image = 'http://farm8.staticflickr.com/7333/10045949924_865301c2bb_z.jpg'

plt.imshow(image_from_url(url_image))
plt.show()",,Example->Example of a published notebook,006->Using the Data set that had 2 Remnant Masses and multiple omega values removed,Show Tell-Demo->Test1
322447,"sns.jointplot(attacker_average_merge_df[""value/age""], attacker_average_merge_df[""points""], kind=""reg"")
plt.subplots_adjust(top=0.9)
plt.suptitle(""All clubs"")","plt.figure(figsize=(12,12))
sn.jointplot(x=np.log1p(train.full_sq.values), y=np.log1p(train.price_doc.values), size=10,kind=""hex"")
plt.ylabel('Log of Price', fontsize=12)
plt.xlabel('Log of Total area in square metre', fontsize=12)
plt.show()","import warnings
warnings.filterwarnings('ignore')

from datetime import datetime, date, time
import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import copy
import math

%matplotlib inline
#This is graph line style, from Nate silver
plt.style.use('fivethirtyeight')

#_DATA_DIR = r'/Users/administrator/Desktop/New Git Diabetes/Input/input'
#_FIG_DIR = r'/Users/administrator/Desktop/New Git Diabetes/figures'
_DATA_DIR = r'C:\Users\engadmin\Desktop\Diabetes  Project Code\Input\input'
_FIG_DIR = r'C:\Users\engadmin\Desktop\Diabetes  Project Code\figures'


if not os.path.exists(_FIG_DIR):
    os.makedirs(_FIG_DIR)
    
_GULCOSE_DATA_PATH = os.path.join(_DATA_DIR, 'P1.csv')
fields = [""Date"", ""mg/dL"", ""CHO (g)"", ""Fat (g)"", ""Protein (g)""]
df_P1 = pd.read_csv(_GULCOSE_DATA_PATH, parse_dates=['Date'], usecols = fields) #read and prase Date col as dataTimes object
df_P1 = df_P1.sort(['Date'])

## Matplotlib Variables
_FIG_SIZE = (16, 8)
_FIG_FORMAT = 'png'
_FIG_DPI = 200

def _file_format(string_):
    string_ = string_.replace('-', '_').replace(' ', '_').replace('$', '')
    string_ += '.' + _FIG_FORMAT
    return string_

def print_full(x):
    pd.set_option('display.max_rows', len(x))
    print(x)
    pd.reset_option('display.max_rows')

print (""\nTotal number of glucose readings is:"", df_P1['mg/dL'].size)
print (""\nSome of points may be missing glucose readings, the above number represents the collected readings from start to end date"")
#Removing NaN from a specific column of the dataframe, here it's mg/dL
df_P1_wonan_mg = df_P1.dropna(subset=['mg/dL'])
print (""\nRemove NaN from the mg/dL column..."", ""\nTotal actual number of glucose readings is:"", df_P1_wonan_mg['mg/dL'].size)
###################
#Converting object type to float for all of the dataframe
df_P12= pd.DataFrame(df_P1).convert_objects(convert_numeric=True)
#print df_P12.dtypes
print (""\nRemove NaN from the CHO so we can plot the points of meals..."")
#Removing NaN from a specific column of the dataframe, here it's CHO. isfinite can work for series, but not for dataframe
df_P12 = df_P12.dropna(subset=['CHO (g)'])
#print ""\n"", df_P12.head()
print (""Total number of meal times:"", df_P12['CHO (g)'].size)
print (""\nSome of the meal times do not have corresponding glucose reading, as it was not recorded at that time."")
#Removing NaN from a specific column of the dataframe, here it's mg/dL
df_P12_wonan_mg = df_P12.dropna(subset=['mg/dL'])
print (""Thus, total actual number of meal times with glucose readings is:"", df_P12_wonan_mg['mg/dL'].size)
print (""\nSome of the successive meal readings are redundant and stand for one meal only"")
########################
#resetting dataframe index
df_P13 = df_P12_wonan_mg.reset_index(drop=True)
#print df_P13.head()

df_P14 = pd.DataFrame(df_P13)   
#print df_P14.head()

#print_full(df_P14)  

print ('\nRemove redundant readings of same meal...\n') 
row = 0
for i, x in enumerate(df_P13['CHO (g)']):
        if i > 0:
            row = row + 1
            #if content of current element is same as previous, then delete row      
            #resetting dataframe index
            if x == df_P13['CHO (g)'][i-1]:
                #print '\nRemove redudant readings rows', i, x
                #drop ith row
                df_P14.drop(df_P14.index[row], axis=0, inplace=True)
                row = row - 1
#print_full(df_P14)  
                
#resetting dataframe index (1,2,3...)
df_P14 = df_P14.reset_index(drop=True)
print (""There are"", df_P14['CHO (g)'].size, "" unredundant recorded glucose readings with meal times.\n"")

print (df_P14.head())
#print df_P14

#reindexing the dataframe with the Date column-- This will remove Date column but that is needed for graphing
#df_P14 = df_P14.set_index('Date')

plt.figure(figsize=_FIG_SIZE)
plt.plot_date(df_P1.Date, df_P1['mg/dL'], '-', label=""Glucose Level"");
plt.plot_date(df_P14.Date, df_P14['mg/dL'], 'ro', label=""Meal Starting Time"")
plt.xlabel('Date')
plt.ylabel('Glucose Reading (mg/dl)')
plt.legend(loc='upper right')
title = 'Glucose Readings with the meal times'
plt.title(title);
#change title to include date of first index and last index
#title = 'Glucose readings for P1 between {} - 2008-07-27'.format(_BEGINNING_DATE)
fig_path = os.path.join(_FIG_DIR, _file_format(title))
plt.savefig(fig_path, dpi=_FIG_DPI)",,Premier League 17 18 Analysis->Which Feature Contributes The Most to The Standing of Clubs->Is the new signing player average market values a better evaluation?->Normalise the club names,Script1195->Bivariate Analysis->Full Square Vs Price Doc,Smoothing And Filtering
78516,"from keras import optimizers

model.compile(loss='binary_crossentropy',
              optimizer=optimizers.RMSprop(lr=1e-4),
              metrics=['acc'])","optimizerD = optim.RMSprop(netD.parameters(), lr = 1e-4)
optimizerG = optim.RMSprop(netG.parameters(), lr = 1e-4)","from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
species_one_hot = encoder.fit_transform(species_encoded.reshape(-1,1))
species_one_hot",,"Main->Model
5->Building our network
10 - Convolutional Neural Networks->Advanced Neural Networks->Visualizing class activation->Training
Dlai2018 D5L Transfer
Convnets-With-Small-Datasets->Building our network
Cats-Dogs-Revisited->2. Building our network->2.3 Configuring our model for training
Case 2->3. Building network
Case 2->Case 2 - Diabetic Retinopathy Analysis->Building network
Case 2 -  5->3. Building network
Cats And Dogs1->Using Covnets->Set Up the Files->Build the Model","Wgan-Pytorch->Wasserstein GAN in Pytorch->Create model
Wgan-Pytorch
Wgan-Pytorch-Lsun-Dataset->Create model->Continue Training (custom codes)
Wgan-Pytorch-Lsun-Conv5->Wasserstein GAN in Pytorch",Feature Encoding->Loading the Iris dataset
152169,groups = BWGroups(project),signals = BWSignals(project),save({name}),,Demo->SDK for the Brandwatch API: Demo->Groups,Demo->SDK for the Brandwatch API: Demo->Signals,Ch09 Working With Rasters->Chapter 9: Working with rasters->9.4 Working with raster objects
258673,"# An implementation of Gradient Descent for solving linear a system

left = -5
right = 25
bottom = -4
top = 6

# Setup the vandermonde matrix
N = len(x)
A = np.hstack((np.ones((N,1)), x))

# Starting point
w = np.matrix('[15; -6]')

# Number of iterations
EPOCH = 5000

# Learning rate: The following is the largest possible fixed rate for this problem
eta = 0.000696

Error = np.zeros((EPOCH))
W = np.zeros((2,EPOCH))

for tau in range(EPOCH):
    # Calculate the error
    e = y - A*w    
    
    # Store the intermediate results
    W[0,tau] = w[0]
    W[1,tau] = w[1]
    Error[tau] = (e.T*e)/2
    
    # Compute the gradient descent step
    g = -A.T*e
    w = w - eta*g

    #print(w.T)
    
w_star = w    
plt.figure(figsize=(8,8))
plt.semilogy(Error)
plt.xlabel('Iteration tau')
plt.ylabel('Error')
plt.show()


plt.figure(figsize=(8,8))
plt.imshow(ErrSurf, interpolation='nearest', 
           vmin=0, vmax=1000,origin='lower',
           extent=(left,right,bottom,top))
plt.xlabel('w0')
plt.ylabel('w1')

ln = plt.Line2D(W[0,:300:1], W[1,:300:1], marker='o',markerfacecolor='w')

plt.gca().add_line(ln)
plt.show()","def draw_errors(original, projection, fname=None, title='Projection with Errors'):
    plt.scatter(original[0,:], original[1,:], s=2, label='original')
    plt.scatter(projection[0,:], projection[1,:], s=2, color='black', label='projection')
    n = original.shape[1]
    orig_x = original[0,:]
    orig_y = original[1,:]
    trans_x = projection[0,:]
    trans_y = projection[1,:]
    for i in range(n): 
        line = plt.Line2D((orig_x[i], trans_x[i]), (orig_y[i], trans_y[i]), 
                          lw=1., alpha=.5, color='r') 
        plt.gca().add_line(line) 
    plt.axis('scaled')
    plt.title(title)
    plt.legend()
    plt.savefig('/Users/kailinlu/Documents/kailin-lu/images/pca_and_probabilistic_pca/'+fname)
    plt.show()","Kaggle_Data = pd.read_csv('multipleChoiceResponses.csv',encoding='ISO-8859-1')",,Regression->Line Fitting->Vector Notation->Alternative Solution: Gradient Descent (GD),Pca->Linear PCA,Ensemble+Code+Notebook->1. Data Preparation & Exploration:->1.1 KAGGLE->Combining Resume with Job Data to One DataFrame
302667,run(succeed) # a goal that always succeeds,"user_constraint_query = """"""
CREATE CONSTRAINT ON (u:User) 
ASSERT u.id IS UNIQUE
""""""

review_constraint_query = """"""
CREATE CONSTRAINT ON (r:Review) 
ASSERT r.id IS UNIQUE
""""""


import_query = """"""
USING PERIODIC COMMIT 10000
LOAD CSV WITH HEADERS FROM $reviewsFile AS row

// User
MERGE (u:User {id: row.reviewer_id})
SET u.name = row.reviewer_name

// Review
MERGE (r:Review {id: row.id})
SET r.date     = row.date,
    r.comments = row.comments
WITH row, u, r
MATCH (l:Listing {id: row.listing_id})
MERGE (u)-[:WROTE]->(r)
MERGE (r)-[:REVIEWS]->(l);
""""""

display(graph.run(user_constraint_query).summary().counters)
display(graph.run(review_constraint_query).summary().counters)
display(graph.run(import_query, {""reviewsFile"": reviews_file}).summary().counters)","die = list(range(1, 6+1))
P = BoxModel(die, size = 2)
X = RV(P, sum)
Y = RV(P, max)",,Presentation->microkanren,01 Data Import->Short Term Rentals - Data Import->Reviews,Joint->Symbulate Documentation: Multiple Random Variables and Joint Distributions->Multiple random variables
115010,"n_true = len(df.loc[df['diabetes'] == True])
n_false = len(df.loc[df['diabetes'] == False])

perc_true = ((n_true * 1.0)/(n_true + n_false)) * 100
perc_false = ((n_false * 1.0)/(n_true + n_false)) * 100

print 'True {0} : {1: 2.2f}%'.format(n_true, perc_true)
print 'False {0} : {1: 2.2f}%'.format(n_false, perc_false)","n_true  = len(df.loc[df[""diabetes""] == True])
n_false = len(df.loc[df[""diabetes""] == False])
print(""Number of True cases:  {0} ({1:2.2f}%)"".format(n_true, n_true / (n_true + n_false) * 100))
print(""Number of False cases: {0} ({1:2.2f}%)"".format(n_false, n_false / (n_true + n_false) * 100))
print(""Total:                 {0}"".format(n_true + n_false))","from scipy.stats import skew

#Check skewness
def check_skew(df, quant_col):
    #filter out columns with discrete values 
    quant_col = [col for col in quant_col if df[col].nunique()>100]
    #calculate skewness
    high_skew_col = [col for col in quant_col if abs(skew(df[col]) > 1)]
    return high_skew_col

#log transform skewed columns
def take_log(df, skewed_col):
    for col in skewed_col:
        df.loc[:,col] = df[col].map(np.log1p)
    return df",,1->New algorithm : Random forest,Pima Indian Diabetes Prediction->Diabetes Prediction->Plot Correlation->True False ratio:,Blog Post2->Other Models->Log-Transform
332743,"df[""stop_sequence""] = df.stop_sequence.astype(int)",df.StopType.unique(),"# define the function for z
def z(X,Y):
    r=np.sqrt(X**2+Y**2)
    return np.cos(2.*np.pi*r/8.)*np.e**(-r/10.)
# make a grid of X,Y data
X,Y=np.mgrid[-15.:15:.1,-15:15:0.1]
# plot the surface using mayavi's mlab
mlab.surf(X,Y,z,warp_scale=""auto"")",,Crash Course->Cleaning up,Manage Spatial Data Using Geopandas And Shapely In Python->Manage spatial data using Geopandas and Shapely in Python->Spatial searches,"Gmt Chapter 13 Examples->3-D illuminated surface (GMT Docs Example 13.5)
Gmt Chapter 13 Examples->Color images from gridded data (GMT Docs Example 13.2)"
380666,hdu_ffi[10].name,hdu_ffi = fits.open('/Volumes/Truro/ffi/ktwo2017079075530-c13_ffi-cal.fits'),"import os
import matplotlib.pyplot as plt

%pylab inline


import numpy as np
from astropy.io import fits
from astropy.table import Table
import desispec.io
import desisim.io
from desisim.obs import new_exposure
from desisim.scripts import quickgen
from desispec.scripts import group_spectra",,02->C13 example: How many times has each K2 pixel been telemetered?,"02->How many times has each K2 pixel been telemetered?
02->C13 example: How many times has each K2 pixel been telemetered?",Lya Quasar Sims And Redrock Mb->Quasar Redshift Estimates with Redrock. DESI simulated Spectra
119279,"sample_class=os.listdir(trainset)[6]
sample_img=os.listdir(os.path.join(trainset,sample_class))[0]
sample_img_path=os.path.join(trainset,sample_class,sample_img)
img=image_process(sample_img_path,resize=True,size=(200,100))
plt.gray()
plt.imshow(img);","sample_class=os.listdir(trainset)[6]
sample_img=os.listdir(os.path.join(trainset,sample_class))[0]
sample_img_path=os.path.join(trainset,sample_class,sample_img)
img=image_process(sample_img_path,resize=True,size=(200,100))
plt.gray()
plt.imshow(img);","print( ""Best C was: "", results.best_params_)
df_score.plot(x=""param_C"", y=""mean_test_score"", title=""C / Mean Scores"")

#plt.xscale('log')

df_score.plot( x=""param_C"", y=""mean_test_score"", title=""C / Mean Scores"")
plt.xscale('log')
plt.show()",,"1->demo of  image samples
Module-Checkpoint->demo of  image samples","1->demo of  image samples
Module-Checkpoint->demo of  image samples",Yijie Final Project Pt3 Final->Customer Churn Prediction->4.Implementing the model with final selection of features->Plot the model scores obtained for the different C
268111,"for key in data_dict:
    # Create from_ratio. Set from_ratio to zero if NaN appears
    if (data_dict[key]['from_this_person_to_poi'] == 'NaN') or (data_dict[key]['from_messages'] == 'NaN'):
        data_dict[key]['from_ratio'] = 0.0
    else:
        data_dict[key]['from_ratio'] = (1.0*data_dict[key]['from_this_person_to_poi']/data_dict[key]['from_messages'])
    # Create to_ratio. set to_ratio to zero if NaN appears
    if (data_dict[key]['from_poi_to_this_person']=='NaN') or (data_dict[key]['to_messages']=='NaN'):
          data_dict[key]['to_ratio'] = 0.0
    else:  
        data_dict[key]['to_ratio'] = (1.0*data_dict[key]['from_poi_to_this_person']/data_dict[key]['to_messages'])","data_dict[""currents""][-1]","##### capture the other data for later training #####
x_train_df = x_train_df[y_train_df['Negative'] == 0]
x_train = x_train_df.values.tolist()
            
##### capture the other data for later training #####
y_train_df = y_train_df[y_train_df['Negative'] == 0].drop(['Negative'],axis = 1)
y_train = y_train_df.values.tolist()
Category =  y_train_df.columns.tolist()           
##### capture the other data for later training #####
x_test_df = x_test_df[y_test_df['Negative'] == 0]
x_test = x_test_df.values.tolist()
            
##### capture the other data for later training #####
y_test_df = y_test_df[y_test_df['Negative'] == 0].drop(['Negative'],axis = 1)
y_test = y_test_df.values.tolist()
            
##### splitting #####
y_GP_train = choosing_data(y_train, 0)        #GP / others
y_GN_train = choosing_data(y_train, 1)        #GN / others
y_Fungus_train = choosing_data(y_train, 2)     #Other / others
y_Other_train = choosing_data(y_train, 3)  #Negative / others

train_list = [y_GP_train,y_GN_train,y_Fungus_train,y_Other_train]
            
##### splitting #####
y_GP_test = choosing_data(y_test, 0)         #GP / others
y_GN_test = choosing_data(y_test, 1)         #GN / others
y_Fungus_test = choosing_data(y_test, 2)      #Other / others
y_Other_test = choosing_data(y_test, 3)   #Negative / others

test_list = [y_GP_test,y_GN_test,y_Fungus_test,y_Other_train]",,Data Exploration->Feature Creation and Pre-processing,"02 Data Structures->Basic Python Containers: Lists, Dictionaries, Sets, Tuples->Assigning Variables to Other Variables->Length of Lists",Validation->Drop Negative Data->This  make sense since doctor need to know which paitient should take treatment
427381,arr3d[0],arr3d.ndim,"f""Total rows: {df.shape[0]} | columns: {df.shape[1]}""",,Ch4 - Num Py Basics->3. Array Oriented Programming with Arrays,Num Py Practice->NumPy Practice->Indexing with slices->Three-dimensional array slicing and indexing,Summarise->Summary of apprenticeships data->Overall summary
472190,"p = sns.heatmap(df.corr(), annot=True)
plt.xticks(rotation=90)","# show correlation
_ , axsn3 = plt.subplots(figsize=(8,8))
sn.heatmap(df_cpy.corr()*100, ax=axsn3, square=False, fmt='.0f', annot=True, cbar=False)
# rotation is recognized for matplotlib version 2.1.X and above
axsn3.tick_params(axis = 'x',rotation = 90.0)
axsn3.tick_params(axis = 'y',rotation = 0.0)
plt.tight_layout()","N = 50
mu = 0.0
sigma = 1.0
X = np.random.normal(loc=mu, scale=sigma, size=N)
plt.scatter(X, np.zeros(N))",,Homework 2->Question 6,02 Feature Extraction->Strategy->Correlation,Lecture18->Sampling from the normal distribution
317397,"import numpy as np
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
import scipy.interpolate as sc
import scipy.special
import random
from PIL import Image","import math
import numpy as np
from scipy import special as special
%matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.mlab as mlab
from prettytable import PrettyTable","pipeline = Pipeline([
        ('vect', CountVectorizer()),
        ('tfidf', TfidfTransformer()),
        ('clf', LogisticRegression())
    ])",,"Midterm->PART 1. THEORY
Final Lab->QUESTION 2",Exercise-1->**Data Analysis in High Energy Physics: Exercise 1.5 $p$-values**,Pipelines Lab - Starter->Logistic Regression Lab with pipelines->1. Model Pipeline
119810,"# train your network
n_epochs = 50 # start small, and increase when you've decided on your model structure and hyperparams

loss_his = train_net(n_epochs)","# train your network
n_epochs = 1000 # start small, and increase when you've decided on your model structure and hyperparams

# call train and record the loss over time
training_loss = train_net(n_epochs)",fm_modulation(),,1->Training and Initial Observation,2->Training and Initial Observation,03-Exploration-With-Containers->Declaring elements in a function
270447,"physical.loc[idx['TransGrid', 'transmission system capacities', 'installed transmission system transformer capacity',:],'value'
               ].pipe(standard_format).sort_index().fillna('-')","physical.loc[idx['TransGrid', 'transmission system capacities', 'cold spare capacity',:],'value'
               ].pipe(standard_format).sort_index().fillna('-')","lda.save_project('results/beer_analysis.project', 'Initial test + bugfix by Joe')",,0->Historical Physical Network Assets Data->TransGrid->Transformer Capacity->installed transmission system transformer capacity,0->Historical Physical Network Assets Data->TransGrid->Transformer Capacity->cold spare capacity,Synthetic Data Test-Checkpoint
446885,"lines= []
with open('data1/driving_log.csv') as csvfile:
    reader = csv.reader(csvfile)
    for line in reader:
        lines.append(line)
        
train_samples, validation_samples = train_test_split(lines, test_size=0.05)
        
#np.random.shuffle(lines)
#split_i = int(len(lines) * 0.9)
#X_train, y_train = list(zip(*lines[:split_i]))
#X_valid, y_valid = list(zip(*lines[split_i:]))


#X_train, y_train = np.array(X_train), np.array(y_train)
#X_valid, y_valid = np.array(X_valid), np.array(y_valid)","lines = []

with open('../data/driving_log.csv') as csvfile:
    reader = csv.reader(csvfile)
    for line in reader:
        lines.append(line)

#print(lines[0])
#print(lines[1])
#print(lines[2:4])

# This is required since the first line is comprised of headers.
lines = lines[1:]

train_samples, validation_samples = train_test_split(lines, test_size=0.2)

print(len(train_samples))
print(len(validation_samples))","#-- Imputed age distribution
fig, ax = plt.subplots(1, 4, figsize=(15, 4))
sns.distplot(dataC.Age[dataC.Age.notna()], ax=ax[0])
dataC_ = dataC.copy(); utdata.imputeFeature(dataC_, 'Age', 'mean')
sns.distplot(dataC_.Age, ax=ax[1])
dataC_ = dataC.copy(); utdata.imputeFeature(dataC_, 'Age', 'linear', methodExclude=['Survived', 'EmbarkedS', 'EmbarkedC'])
# dataC_.loc[dataC_.Age<0,'Age'] = 0
sns.distplot(dataC_.Age, ax=ax[2])
dataC_ = dataC.copy(); utdata.imputeFeature(dataC_, 'Age', 'tree', methodExclude=['Survived', 'EmbarkedS', 'EmbarkedC'])
sns.distplot(dataC_.Age, ax=ax[3])
ax[0].set_title('Original')
ax[1].set_title('Imputed: Mean')
ax[2].set_title('Imputed: Linear regression')
ax[3].set_title('Imputed: Random forest')",,Test->Load the images and data,"P3->Self-Driving Car Engineer Nanodegree->Read in Data
P3 Generator->Self-Driving Car Engineer Nanodegree->Read in Data",Data Analysis->Data analysis->Missing values and dummy variables->Name
344873,"predicted_values = list(predicted)
actual = dev_set.values.tolist()

for (z,y) in zip(actual, predicted_values):
    if (str(z[2]) == str(y)):
        continue
    else:
        print(""{}"".format(z[1]))
        print(""Graded as: {}"".format(str(y)))
        print(""Actual grade is {}"".format(str(z[2])))
        print()","listOfGrades = list(set(dfPropertyClean['Grade'].tolist()))
sortedGrade = sort_by_median('Grade', listOfGrades)

sns.boxplot(x='Grade', y='CurrentBuildingValue',
            data=dfPropertyClean, order=sortedGrade)
plt.ylim(ymax = 1500000, ymin = 10000)
plt.axhline(y=valueMed, color='r', linestyle='-.', alpha=0.7)
plt.title('Building Value over Grade', fontsize=15)
plt.show()","#show the images for slide 2 (Capture 53 - 100)

imPaths = glob('2nd Images/Slide 2/*.bmp')
imPaths = natural_sort(imPaths)

images = []
filenames = []

# read the images and extract each file name associated
for i, path in enumerate(imPaths):
    images.append( io.imread(path) )
    filenames.append( os.path.basename(imPaths[i]) )

half = math.ceil(len(images)/2)
fig, ax = plt.subplots(half, 2, figsize=(15, half*4), squeeze=False)
ax = ax.ravel()

start_time = time.time()

for i, img in enumerate(images):
    ax[i].imshow(img)
    ax[i].set_title(filenames[i])
    ax[i].axis('off')

plt.show()",,"Prediction Classifiers->Question 4 Classifier->Question 5 Classifier
Prediction Classifiers->Question 4 Classifier->Question 5 Classifier",Burlington Property Analysis->Burlington Property Analysis,Image Sharing Notebook->Slide 2
284192,df_awb['patient_count'].sum(),"df_awb.sort_values('patient_count', ascending=False).head(10)[awbv]","import numpy as np
import pandas as pd
from pandas import DataFrame, Series
import matplotlib.pyplot as plt
%matplotlib inline",,"Awb Meldungen->AWB (Anwendungsbeobachtungen, Observational Studies)->Numbers of patients","Awb Meldungen->AWB (Anwendungsbeobachtungen, Observational Studies)->Highest patient count",01 Exploring Data->Exploring the dataset->Exploring the columns->Team (team_id and team_name)
261998,"random_experiment_data = perform_experiment(
    X, y, 
    base_estimator=clone(base_clf), 
    query_strat=random_sampling,
    n_queries=n_queries,
    init_L_size=init_L_size,
    random_state=random_state
)","import numpy as np

def markov1(n,h_h,l_l):
    
    # States:
    states = [1, 0]  # 1 for high, 0 for low
    
    # Initial state
    init_state = states[0]
    
    # Empty array to store sequence of states:
    seq_states = np.empty(n)
    seq_states[0] = init_state
    x = np.random.uniform(0,1, size = n)

    # Main loop
    i = 0 
    while i < n-1:
        if seq_states[i] == 1:
            if x[i] <= h_h:
                seq_states[i+1] = states[0]
            else:
                seq_states[i+1] = states[1]         
        else:
            if x[i] <= l_l:
                seq_states[i+1] = states[1]
            else:
                seq_states[i+1] = states[0] 
        i += 1
    return(seq_states)","import scipy.optimize as optimize

help( optimize )

help( optimize.fmin )

help( optimize.fminbound )

help( optimize.minimize )",,Example->Training Models->Random Sampling,Markov Chains1->Introduction to Simulating Markov Chains,Hw4 Python->Problem 2: Earthquakes->4.6 Actually Optimizing Generalized Cross-Validation
144384,"y_predict_test2 = lm.predict(X_test)
lm.score(X_test, y_test)","y_predict_test2 = lm.predict(X_test)
lm.score(X_test, y_test)","PI   = np.pi
npts = 101
x    = np.linspace(-2*PI, +2*PI, npts)
y    = np.sin(x)",,Full Project->Data Cleaning Pt.1->Combine Repeated Columns->Logistic Regression,Full Project->Data Cleaning Pt.1->Combine Repeated Columns->Logistic Regression,Test Notebook
374676,"# Importing libraries
import numpy as np
import pandas as pd
import scipy.optimize as opt
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt
import seaborn as sns
from warnings import filterwarnings

%matplotlib inline
filterwarnings('ignore')
sns.set_context('notebook')
plt.style.use('fivethirtyeight')

# Plot sigmoid function
x = np.linspace(-10, 10, 100)
y = 1 / (1 + np.exp(-x))
fig, ax = plt.subplots(figsize = (10, 6))
ax.plot(x, y, c = 'r')
ax.set_xlabel('x', fontsize = 14)
ax.set_ylabel(r'$y = \frac {1} {1+ \exp^{-x}}$', fontsize = 14)
ax.set_title('Sigmoid Function', fontsize = 18);","%matplotlib nbagg
import seaborn as sns
import matplotlib.pyplot as plt
import multiprocessing
import pickle
import tmevaluator
import bluepyopt as bpop
import numpy as np

# Set plotting context and style
sns.set_context('talk')
sns.set_style('whitegrid')

# Load and display in vitro trace
trace = pickle.load(open('trace.pkl'))
fig, ax = plt.subplots()
ax.plot(trace['t'], trace['v'], label='in vitro')
ax.legend(loc=0)
ax.set_xlabel('time (s)')
ax.set_ylabel('soma voltage (V)')","df['day_of_week'] = df['NewDate'].dt.dayofweek
df.day_of_week.describe()",,Logistic-Regression,Tsodyksmarkramstp->Tsodyks-Markram model of short-term synaptic plasticity,Detective->AN ANALYTICAL DETECTIVE->UNDERSTANDING DATES
213434,tdf_remove_outlier = tdf[tdf != 100.8],"missingvalues_prop = (after_remov_similar_col_df.isnull().sum()/len(after_remov_similar_col_df)).reset_index()
missingvalues_prop.columns = ['fields','proportion']
missingvalues_prop = missingvalues_prop.sort_values(by = 'proportion', ascending = False)
print(missingvalues_prop)","plt.title('GC_MARK_MS')
(aggregated_content + aggregated_parent).plot(kind='bar', figsize=(15, 7))",,Sliderule Dsi Inferential Statistics Exercise 1->1.  Is the distribution of body temperatures normal?->Is the difference in male and female mean body temperatures statistsically significant?,Zillow Estimate->Improve Zillow's Home Value Prediction Algorithm->Tuning the default settings for easy reading,Telemetry Hello World->Histograms
300955,"sns.barplot(x='Pclass', y='Survived', data=train)","sn.barplot( y = 'mpg',x = 'cylinders',data = mpg_cylinders_df )","%cd $workdir

###              ###
#SET OF PARAMETES#
###              ###

#Energy
epsr=78.7
mindist=0    # minimum allowed COM-COM distance (angstrom)
maxdist=200  # maximum allowed COM-COM distance (angstrom)    

#System
cylradius=55 # cylinder radius (angstrom)
cyllen=400   # cylinder length (angstrom)


#Moves
dp=15        # COM translational displacement parameter (angstrom)

#Moleculelist
offset=40    # initial COM-COM separation (angstrom)

#Analysis  
nstep_xtc=100  # frequency for saving frames to xtc trajectory file

#Atomlist
ljeps=0.12350 # LJ epsilon (kT)

#Number of simulation steps 
micro = 50000
macro = 100

#Function that determines the Debye Length corresponding to the salt concentration

def debye(conc):
    ''' Convert salt concentration (in M) to Debye lenght. 
        Valid at room temperature (25 °C) for 1:1 electrolytes.'''
    return 3.04/np.sqrt(conc)


#Function for input file
def mkinput():
    js = {
              ""energy"" : {
                  ""___eqstate"": { ""processfile"": ""twobody.json"" },
                  ""nonbonded"" : { ""epsr"" : 78.7, ""coulombtype"": ""yukawa"", ""cutoff"":2*D, ""debyelength"": D},
                  ""cmconstrain"" : {""0sphere 1sphere"" : { ""mindist"": mindist, ""maxdist"": maxdist }}
                         },
                
               ""system"" : {
                  ""temperature"" : 298.15,
                  ""geometry"" : { ""length"" : cyllen, ""radius"" : cylradius },
                  ""mcloop""   : { ""macro"" : macro, ""micro"" : micro }
                          },
                 
                ""moves"" : {
                  
                    #1
                    ""titrate"" : {
                      ""prob"":0.2,
                      ""processes"" : {
                            ""H-Asp"" : { ""bound"":""HASP"" , ""free"":""ASP"" , ""pKd"":4.0  , ""pX"":pH },
                            ""H-Ctr"" : { ""bound"":""HCTR"" , ""free"":""CTR"" , ""pKd"":2.6  , ""pX"":pH },
                            ""H-Glu"" : { ""bound"":""HGLU"" , ""free"":""GLU"" , ""pKd"":4.4  , ""pX"":pH },
                            ""H-His"" : { ""bound"":""HHIS"" , ""free"":""HIS"" , ""pKd"":6.3  , ""pX"":pH },
                            ""H-Arg"" : { ""bound"":""HARG"" , ""free"":""ARG"" , ""pKd"":12.0 , ""pX"":pH },
                            ""H-Ntr"" : { ""bound"":""HNTR"" , ""free"":""NTR"" , ""pKd"":7.5  , ""pX"":pH },
                            ""H-Cys"" : { ""bound"":""HCYS"" , ""free"":""CYS"" , ""pKd"":10.8 , ""pX"":pH },
                            ""H-Tyr"" : { ""bound"":""HTYR"" , ""free"":""TYR"" , ""pKd"":9.6  , ""pX"":pH },
                            ""H-Lys"" : { ""bound"":""HLYS"" , ""free"":""LYS"" , ""pKd"":10.4 , ""pX"":pH }
                                    }
                                },
                    #2
                    ""moltransrot2body"" : {
                    ""0sphere"" : { ""dp"":dp, ""dprot"":0.5 }, 
                    ""1sphere"" : { ""dp"":dp, ""dprot"":0.5 } 
                                         }
                         },
                
                ""moleculelist"": {
                  ""0sphere"":  { ""structure"":xyzfile, ""Ninit"":1, ""insdir"":""0 0 0"", ""insoffset"":""0 0 ""+str(offset)},
                  ""1sphere"":  { ""structure"":xyzfile, ""Ninit"":1, ""insdir"":""0 0 0"", ""insoffset"":""0 0 -""+str(offset)}
                                },
        
                 ""analysis"" : {
                   ""pqrfile"" : { ""file"": ""confout.pqr""  }, 
                   ""chargemultipole"": {""mollist"" : [""0sphere"",""1sphere""], ""nstep"":10 },
                   ""molrdf"" : { ""nstep"":1, ""pairs"" : 
                            [ { ""name1"":""0sphere"", ""name2"":""1sphere"", ""file"":""rdf.dat"", ""dr"":0.1, ""dim"":1 }]
                              },
                   ""xtcfile"" : { ""file"": ""traj.xtc"", ""nstep"": nstep_xtc },

                              },
            
                 ""atomlist"" : {
                    ""ASP"":  dict(q=-1, sigma=5.94, eps=ljeps, mw=110.05),
                    ""HASP"": dict(q=0,  sigma=5.94, eps=ljeps, mw=110.05),

                    ""GLU"":  dict(q=-1, sigma=6.16, eps=ljeps, mw=122.06),
                    ""HGLU"": dict(q=0,  sigma=6.16, eps=ljeps, mw=122.06),
                    
                    ""HIS"":  dict(q=0,  sigma=6.28, eps=ljeps, mw=130.08),
                    ""HHIS"": dict(q=1,  sigma=6.28, eps=ljeps, mw=130.08),
                    
                    ""ARG"":  dict(q=0,  sigma=6.66, eps=ljeps, mw=155.18),
                    ""HARG"": dict(q=1,  sigma=6.66, eps=ljeps, mw=155.18),
                     
                    ""BCYS"": dict(q=0, sigma=5.72, eps=ljeps, mw=98.10),
                    ""CYS"":  dict(q=-1, sigma=5.72, eps=ljeps, mw=98.10),
                    ""HCYS"": dict(q=0,  sigma=5.72, eps=ljeps, mw=98.10),
                    
                    ""TYR"":  dict(q=-1, sigma=6.66, eps=ljeps, mw=154.10),
                    ""HTYR"": dict(q=0,  sigma=6.66, eps=ljeps, mw=154.10),
                    
                    ""LYS"":  dict(q=0,  sigma=6.06, eps=ljeps, mw=116.08),
                    ""HLYS"": dict(q=1,  sigma=6.06, eps=ljeps, mw=116.08),
                    
                    ""NTR"": dict(q=0,  sigma=4.70, eps=ljeps, mw=54.03), #gly was N terminal
                    ""HNTR"": dict(q=1,  sigma=4.70, eps=ljeps, mw=54.03),
                    
                    ""CTR"": dict(q=-1,  sigma=4.70, eps=ljeps, mw=16.00), #TYR (tritable) is C terminal
                    ""HCTR."": dict(q=0,  sigma=4.70, eps=ljeps, mw=16.00),
                     
                    ""MET"":  dict(q=0,  sigma=3.8, eps=ljeps,  mw=122),
                     
                    ""GLN"":  dict(q=0,  sigma=3.8, eps=ljeps,  mw=120),

                    ""ASN"":  dict(q=0,  sigma=5.58, eps=ljeps, mw=112.09),
                     
                    ""GLY"":  dict(q=0,  sigma=4.70, eps=ljeps, mw=54.03),

                    ""PHE"":  dict(q=0,  sigma=6.42, eps=ljeps, mw=138.10),

                    ""PRO"":  dict(q=0,  sigma=5.56, eps=ljeps, mw=90.06),

                    ""TRP"":  dict(q=0,  sigma=6.96, eps=ljeps, mw=176.13),

                    ""LEU"":  dict(q=0,  sigma=5.58, eps=ljeps, mw=111.14), 
                    
                    ""SER"":  dict(q=0,  sigma=5.40, eps=ljeps, mw=82.04),

                    ""ILE"":  dict(q=0,  sigma=5.80, eps=ljeps, mw=102.07),

                    ""ALA"":  dict(q=0,  sigma=5.02, eps=ljeps, mw=66.04),

                    ""VAL"":  dict(q=0,  sigma=5.56, eps=ljeps, mw=90.06),   
                }
        
    }


    json_str = json.dumps(js)


    with open('twobody.json', 'w') as f:
        json.dump(js, f, indent=4)",,"Titanic Survivors->Data visualisation->Pclass feature
Titanic Solution->Visualization->Pclass vs. Survival",Mpg Milestone Report->Average mpg by cylinders,Jupy Project->Supporting Information:->Input for Faunus
10898,"gdf_2015_world = gp.read_file(os.path.join(DATA_FOLDER, '2015-world-grid', 'filtered.shp'))
gdf_2010_world = gp.read_file(os.path.join(DATA_FOLDER, '2010-world-grid', 'filtered.shp'))
W_2015 = pysal.open(os.path.join(DATA_FOLDER, '2015-world-grid', 'filtered_q.gal')).read()
W_2010 = pysal.open(os.path.join(DATA_FOLDER, '2010-world-grid', 'filtered_q.gal')).read()","lisa_2015 = pysal.Moran_Local(gdf_2015_world['raster_val'].values, W_2015)
lisa_2010 = pysal.Moran_Local(gdf_2010_world['raster_val'].values, W_2010)","# first we collect all protein coding genes from all_text.txt
selected_genes = set(all_genes.keys())
# next, we update the list with genes in genes_for_network.txt
selected_genes.update(genes_for_network.keys())
# finally, we build a condition-specific network using the selected genes
network = get_interactions(complete_network, selected_genes)
print('Number of selected network:', len(network))

# get the interactions for genes_for_network, this will be used as the test
# statistics for randomization test
selected_interactions = get_interactions(network, genes_for_network)
print('Number of selected interactions:', len(selected_interactions))",,Analysis->Local Indicators of Spatial Association (LISA) clusters,Analysis->Local Indicators of Spatial Association (LISA) clusters,Network Randomization Test->Network randomization test->Build a protein-protein interaction network based on all_genes for randomization test
326559,"def change_shape(X, shape='2d'):
    '''
    Converts from a 1D row of 784 samples
    to a 28x28 2d matrix
    '''
    if shape == '1d':
        return X
    elif shape == '2d':
        return X.reshape(X.shape[0], 28, 28, 1).astype('float32')","# Reshaping the Image for CNN 2-dimesional input in [samples][pixels][width][height]
x_train = x_train.reshape(x_train.shape[0], 1, 28, 28).astype('float32')
x_test = x_test.reshape(x_test.shape[0], 1, 28, 28).astype('float32')
x_val = x_val.reshape(x_val.shape[0], 1, 28, 28).astype('float32')

print (num_pixels, x_train.shape, x_test.shape, x_val.shape)","def trainMyNw(nb_epoch,fold,folder):
    
    myGen=image.ImageDataGenerator()
    best_loss_metric = 999

    for e in tqdm(range(nb_epoch)):
        
        start = time.clock()
        f = open('/cp/home/ubuntu/intel/results/log.txt', 'a')       
        featureFiles = glob(folder + '/' + 'fold_*_trainX_*.dat')        
        chunkCount=0
        
        for files in featureFiles:
            if files.find(fold) == -1:
                features = load_array(files)
                labels = load_array(files.replace('trainX','labels'))
                chunkCount += 1
                numImages = features.shape[0]
                bn_model.fit_generator(generator=myGen.flow(features,labels),samples_per_epoch = numImages,nb_epoch=1,verbose=0)  
                del(features)
                del(labels)
                gc.collect()
            
        featureFiles = glob(folder + '/' + fold + '_trainX_*.dat')        
        accuracy = 0
        loss = 0
        avg_accuracy = 0
        avg_loss = 0
        chunkCount = 0
        totalImages = 0

        for files in featureFiles:
            features = load_array(files)
            labels = load_array(files.replace('trainX','labels'))
            chunkCount += 1
            numImages = features.shape[0]
            loss_metrics = bn_model.evaluate_generator(generator=myGen.flow(features,labels),
                                                       val_samples = numImages) 
            accuracy += (loss_metrics[1]*numImages)
            loss += (loss_metrics[0]*numImages)
            totalImages += numImages
            del(features)
            del(labels) 
            gc.collect()            
            
        if chunkCount>0: 
            avg_accuracy = accuracy/totalImages
            avg_loss = loss/totalImages
            
        print bn_model.metrics_names,avg_loss,avg_accuracy, ""time :"", time.clock() - start
        f.write(str(bn_model.metrics_names))
        f.write("" : "")
        f.write(str(avg_loss))
        f.write("" : "")
        f.write(str(avg_accuracy))
        f.write("" : time :"")
        f.write(str(time.clock() - start))
        f.write(""\n"")        
        f.close() 
        if (avg_loss<best_loss_metric):
            bn_model.save_weights('/cp/home/ubuntu/intel/results/best.hdf5') 
            best_loss_metric = avg_loss
            
    return",,Mnist->Create Function Transformers (transforms data in your pipe),Digit Cnn->Normalizing the Input,Mobile Odt->Train the Network
50817,data_set=Data(),set(data[25]) #Missing values disguised as ' ?',"pYPKa_Z_RPL36A = (pYPKa_ZraI  + prd).looped().synced(pYPKa)
pYPKa_E_RPL36A = (pYPKa_EcoRV + prd).looped().synced(pYPKa)",,2,Data Science Test-Us Census Data->Data Science Test-Dataiku->Part 1. Exploratory Data Analysis and Visualization,P Yp Ka Ze Rpl36A-Checkpoint
441083,"x1s, x2s = np.meshgrid(np.linspace(-100., 100., 100), np.linspace(-100., 100., 100))
fs = func(x1s, x2s)
np.shape(fs)","m0_1 = -0.2
m0_2 = 0.8
sigma0_1 = 1
sigma0_2 = 0.2

## TODO (Question 5)
x1 = np.arange(-1,1,0.01)
x2 = np.arange(-1,1,0.01)
X1,X2 = np.meshgrid(x1,x2,sparse=False)

vfunc = np.vectorize(probability_func)
function_values = vfunc(X1,X2,m0_1,m0_2,sigma0_1,sigma0_2)

#print(A)
visualization_func(X1, X2,function_values ,contour_values=[0.1, 0.2])
## /TODO","print(table_decoder.find_all(""td"")[10])",,Optimisation->Multivariate functions,Visualization->Decision boundary of a classifier->Visualize the distribution of the first class,Parsertest
245823,"for row in movielist:
    for i, entry in enumerate(row):
        try:
            row[i] = float(entry)
        except ValueError:
            pass

print(movielist)","# A:
clean = []
corrupted = ['!1', '23.1', '23.4.5', '??12', '.12', '12-12', '-11.1', '0-1', '*12.1', '1000', ]
for i in corrupted:
    try:
        clean.append(float(i))
    except ValueError:
        continue
print (clean)",print(U_f.shape),,7->JSON,Iteration-Control-Flows-Functions,Unsupervised Learning->Unsupervised Learning->PCA on faces->Implement PCA
42767,"park_codes_df = pd.read_csv('park_codes.csv')
park_codes_df.shape","CentralPark_df = pd.read_csv('weather_CentralPark.csv', index_col=0)",dt_frame1.columns,,Basics->Designing and Creating a Database - Major League Baseball->Import data into SQLite,Incorporate Weather Data->Stop here if you have not yet scraped Central Park weather data!,"107 Pandas->TUTORIAL->2. Create, Save and Load DataFrame->3.3 Name of columns"
194576,"# We should group by label id to understand the distribution
X_valid_group_by_label_count = group_img_id_to_lb_count(X_valid_id_to_label)
X_valid_group_by_label_count.head(n=3)","# We should group by label id to understand the distribution
X_train_group_by_label_count = group_img_id_to_lb_count(X_train_id_to_label)
X_train_group_by_label_count.head(n=5)","result = True
for  n in range(1, 8):
    fileName = ""new_"" + str(n) + ""_"" + str(2) + ""_"" + str(2) + ""_ special.txt""
    newFileGenerator = MaxPushdownGeneratorSpecialCase(n, path + fileName)
    #print (newFileGenerator.theoryEstimate())
    #print(pushdown.calculatePeriod(0))
    pushdown = Pushdown(path + fileName)
    if pushdown.calculatePeriod(0) != newFileGenerator.theoryEstimate():
        print(n)
        result = False
if result:
    print(""Test passed: theoretical solution is correct"")",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Data Pre-Processing->Non-Normalised Images->Group Images In Validation Set,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Data Pre-Processing->Non-Normalised Images->Color Full Dataset,Examples->Pushdown special case m = 2 and k = 2
155838,"X_train, X_test, y_train, y_test = train_test_split(
    X, y, random_state=100
)","# evaluate the model by splitting into train (70%) and test sets (30%)
# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html
# name your model as ""dt""
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","criterion = nn.BCELoss()
D = Discriminator()
G = Generator()
optimizer_d = optim.Adam(D.parameters(), lr=0.0002)
optimizer_g = optim.Adam(G.parameters(), lr=0.0002)",,10->Combining Decision Trees Into a Random Forest->Implementing extremely randomized trees,Cummings S Hw Regression Tutorial Ipynb->Data wrangling->Model Building,Gan->Use Binary Cross Entropy to compute Loss
419278,"fig, ax = plt.subplots(figsize=(8,5))

# plotting distribution
x = np.linspace(0, 1, 1000)

a = [0.5, 5, 1, 2, 2, 1]
b = [0.5, 1, 3, 2, 5, 1 ]
legend = []

for a, b in zip(a,b):
    y = beta.pdf(x, a, b)
    plt.plot(x, y, lw=2)
    legend.append(f""a = {a}, b = {b}"")

plt.xlabel(r'$\theta$')
plt.ylabel('Probability Density')
plt.title('Beta Distribution')
plt.legend(legend, bbox_to_anchor=(1, 1))
plt.ylim(0,2.6)
plt.show()","fig = plt.figure(figsize=(10,6))
means = [0, 0, 0, -2]
variances = [0.3, 1, 3, 0.5]
x_axis = np.arange(-5, 5, 0.001)
legend = []
for mu, var in zip(means, variances):
    plt.plot(x_axis, norm.pdf(x_axis,mu,var))
    legend.append(f'$\mu$ = {mu}, $\sigma^2$ = {var}')
plt.xlabel('X')
plt.ylabel('Probability Density')
plt.title('Normal/Gaussian Distribution')
plt.legend(legend)

plt.show()","btonez = False # set to True to turn tone generator on
newdata = False # for sychronization

def callback(in_data, frame_count, time_info, status):
    global yaudio, btonez, newdata

    # See if we're getting data faster than we can handle it
    
    if frame_count > block_length:
        print('overrun', frame_count)

    # Store as global, for use by main program
    
    yaudio = np.fromstring(in_data, np.int16)
    newdata = True

    # Audio is either on or off, depending on GUI selection
    
    if btonez:
        return (block, pyaudio.paContinue)
    else:
        return (block0, pyaudio.paContinue)

# Set up the audio data stream, basic settings supported by most PC's

p = pyaudio.PyAudio()

stream = p.open(format = pyaudio.paInt16,
                channels = 1,
                rate = sample_rate,
                output = True,
                input = True,
                frames_per_buffer = block_length,
                stream_callback = callback,
                start = False)

stream.start_stream()",,08-Bayesian Machine Learning-02-Bayesian-Ab-Testing->2. Bayesian A/B Testing->3. Bayesian Adaptive Learning,06-Functions-01-Composition-Of-Functions->1. Composition of Functions,Bass Amp Damping Factor->Audio analysis in a Jupyter notebook->Start audio data collection
340600,"plt.imshow(bw_photo,cmap='gray')
plt.colorbar()
plt.show()","plt.imshow(xtest, aspect='auto'); plt.colorbar(); plt.show()","# create dummy variables

rank_dummy = pd.get_dummies(df['rank'], prefix='rank')
rank_dummy.head(10)",,Python Tutorial->Python tutorial/refresher (ICON2017)->1. Jupyter Notebooks->1.1. Code cells,"Hw1-Solutions->Question 4: Learning Curves, Overfitting, and Machine Learning!->4.2 (4 points)",Project 3
241371,"# directly from the getting started example...
import plotly
print('Plotly:', plotly. __version__)

plotly.offline.init_notebook_mode() # run at the start of every notebook
plotly.offline.iplot({
    ""data"": [{
        ""x"": [1, 2, 3],
        ""y"": [4, 2, 5]
    }],
    ""layout"": {
        ""title"": ""hello world""
    }
})","import plotly
plotly.offline.init_notebook_mode(connected=True) 
plotly.offline.iplot(fig)","%%writefile ex1/demo.py
import sys, os, json, pickle
import pandas as pd
from sklearn.svm import SVC

TRAINING_DATA = os.getenv(""HUME_TRAIN_DATA"")
TARGET_LABEL = os.getenv(""HUME_TARGET_LABEL"")
with open(os.getenv(""HUME_PARAMS_FILE"")) as f:
    SVM_PARAMS = json.load(f)",,"03->Loading the Titanic Data for Example Visualizations->Grouping the Data
03->Update: Using the now open source version of Plotly",Practice - Connect Web Endpoint->An advanced candlestick chart,Hume Demo->Creating Your Own Hume Containers
225896,"import os
import csv
import time
import math
from thread import start_new_thread

import cv2
import rospy
import numpy as np
from cv_bridge import CvBridge
import hbp_nrp_cle.tf_framework as nrp

from gazebo_msgs.msg import ModelState
from std_srvs.srv import Trigger, TriggerResponse
from gazebo_msgs.srv import GetModelState, SetModelState
from std_msgs.msg import UInt32MultiArray, MultiArrayDimension, Float64

import thimblerigger_config as tc","import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import cv2
import numpy as np
import os
import glob
import csv
import random
import tensorflow as tf
from tensorflow.contrib.layers import flatten
import math
import time
import datetime
import logging
import sklearn.preprocessing
tf.logging.set_verbosity(tf.logging.INFO)

from sklearn.utils import shuffle

ts = time.time()
st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
log_file_ = ""../log/log""+st+"".log""
logging.basicConfig(filename=log_file_,level=logging.DEBUG,format='%(message)s')


seed = 21
random.seed(seed)
np.random.seed(seed)","print ""Training data: %d"" % data.count()",,Solution->Solving the thimblerigger,Traffic Sign Recognition,Email Spam Classifier Pipeline Demo->Email Spam Classification Pipeline->1. Read labeled email data
76491,"with tf.Session(graph=graph) as session:
    saver = tf.train.Saver()
    saver.restore(session, ""lenet5.ckpt"")
    offset=0
    iters=X_test.shape[0]//batch_size
    s=0
    for i in range(iters):
        preds=predict.eval(feed_dict={dataset:X_test[offset:offset+batch_size],labels: Y_test[offset:offset+batch_size],keep_prob:1.0})
        s=s+np.sum(np.argmax(preds, 1) == np.argmax(Y_test[offset:offset+batch_size], 1))
        offset=offset+batch_size
    acc=(s/X_test.shape[0])*100
    print('Test accuracy: %f%%' % acc)","num_steps = 5000
batch_size=512
with tf.Session(graph=graph) as session:
  tf.global_variables_initializer().run()
  saver = tf.train.Saver()
  saver.restore(session, ""lenet5.ckpt"")
  print('Initialized')
  for step in range(num_steps):
    offset = (step * batch_size) % (X_train.shape[0] - batch_size)
    batch_data =X_train[offset:(offset + batch_size), :, :, :]
    batch_labels = Y_train[offset:(offset + batch_size), :]
    feed_dict = {dataset : batch_data, labels : batch_labels,keep_prob:0.5}
    _, l, predictions = session.run(
      [optimizer, loss, predict], feed_dict=feed_dict)
    if (step % 100== 0):
      print('Minibatch loss at step %d: %f' % (step, l))
      print('Minibatch accuracy: %f%%' % accuracy(predictions, batch_labels))
      print('Val accuracy: %f%%' % accuracy(predict.eval(feed_dict={dataset:X_val,labels: Y_val,keep_prob:1.0}),Y_val))
    if((step+1)%1000==0):
        save_path = saver.save(session, ""lenet5.ckpt"")
        print('Model Saved')","from sklearn.metrics import precision_score
soft_svc_precision_score = precision_score(test_target_group, soft_svc_predicted)
print (soft_svc_precision_score)",,Main->Cifar 100 Dataset: Testing different CovNet Architectures->LeNet 5 CovNet Architecture,Main->Cifar 100 Dataset: Testing different CovNet Architectures->LeNet 5 CovNet Architecture,"Project2 V8->Problem(h) && Problem(i)->l-2 norm Logistic Regression->Soft Margin SVM---Accuracy, Recall and Precision"
42669,"# Let's start by analyzing the normalized movie rating distribution across different websites  
ax_1 = rating_comp.boxplot([""fandango_ratingvalue"", ""imdb_norm"", ""metacritic_norm"", ""rt_norm""], whis = 4)
ax_1.set_xticklabels([""Fandango"", ""IMDb"", ""Metacritic"", ""Rot.Tom.""], rotation = 45, fontsize = 12, fontweight = ""bold"")
ax_1.set_ylabel(""Normalized Ratings"", fontsize = 12, fontweight = ""bold"")
ax_1.set_title(""ax_1: Normalized Ratings Distribution Across Different Websites"")","ax = df_train.boxplot(column='area', by='label_name', figsize=(8,8), fontsize=15)

ax.set_yscale('log')
ax.set_ylabel(""km $^2$"", fontsize=20)","C = 0.08
y = 0.09
t_cash_flow = [3/12, 6/12, 9/12, 12/12, 15/12, 18/12, 21/12, 24/12]
n = len(t_cash_flow)
v_cash_flow = get_value_cash_flows(n, C/4)

# Bond price given zero rate curve
def risk_zero_curve(t):
    return 0.015 + t/(100+np.sqrt(1+t**2))

bond_price(t_cash_flow, v_cash_flow)",,Basics->Investigating Fandango Movie Ratings,Testing And Training Data->Distribution of area values for each label,Final Exam Scripts->Examples->Bonds
240972,"%matplotlib inline

from IPython.display import display",from sklearn.metrics import brier_score_loss,"locations_df = sqlCtx.sql('SELECT country, location FROM africa GROUP BY location, country')

top_conflicting_locations = africa_df.select('location')\
                                    .groupBy('location')\
                                    .agg({'location':'count'})\
                                    .orderBy('count(location)', ascending=False)\
                                    .limit(10)

joined_df = top_conflicting_locations.join(locations_df, top_conflicting_locations.location == locations_df.location, 'inner')\
                                    .orderBy('count(location)', ascending=False)\
                                    .drop(top_conflicting_locations.location)
joined_df.toPandas()",,03->More on Python,Skeleton->Brier score,Africa Conflicts->Analyzing the ACLED data with Spark->Exploratory analysis
236466,"tf.reset_default_graph()

n_steps = 2
X = tf.placeholder(tf.float32,[None,n_steps,n_inputs])
X_seqs = tf.unstack(tf.transpose(X,perm=[1,0,2]))
basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
output_seqs,states = tf.contrib.rnn.static_rnn(basic_cell,X_seqs,dtype=tf.float32)

outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])","tf.reset_default_graph()

n_inputs = 3
n_neurons = 5

X0 = tf.placeholder(tf.float32, [None, n_inputs])
X1 = tf.placeholder(tf.float32, [None, n_inputs])

basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, [X0, X1], dtype=tf.float32)
Y0, Y1 = output_seqs

init = tf.global_variables_initializer()","# Padding and Aligning
#   Left Alignment
print (' 0123456789012')
print ('|{:<12}|'.format('Message'))

#   Left Alignment - Default
print (' 0123456789012')
print ('|{:12}|'.format('Message'))

#   Right Alignment
print (' 0123456789012')
print ('|{:>12}|'.format('Message'))

#   Left Alignment with Padding with underscore
print (' 0123456789012')
print ('|{:_<12}|'.format('Message'))

#   Left Alignment with Padding with hyphen
print (' 0123456789012')
print ('|{:-<12}|'.format('Message'))

#   Center Alignment  (Note extra padding space on right side for uneven character count match)
print (' 0123456789012')
print ('|{:^12}|'.format('Message'))
print (""#"" + 65*'-')

#   Padding and aligning for clarity
print ()
for i in range(1,12):
    print(""{:3d} {:4d} {:5d}"".format(i, i*i, i*i*i))",,"Rnn->1. Recurrent Neurons <a class=""anchor"" id=""recurrent""></a>->Static Unrolling Through Time",M 14 Recurrent Neural Networks->Basic RNNs->Using `rnn()`,Learning Python 2 - Types And Operations->Learning Python - Notebook 2 - Types and Operations->Dictionaries in Python 3->String Formatting->Padding and Aligning
99199,df.train_type.value_counts(),"class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        # TODO:  Create two empty lists, self.loss and self.val_acc
        self.loss = []
        self.valu_acc = []
 
    def on_batch_end(self, batch, logs={}):
        # TODO:  This is called at the end of each batch.  
        # Add the loss in logs.get('loss') to the loss list
        self.loss.append(logs.get('loss'))
        
    def on_epoch_end(self, epoch, logs):
        # TODO:  This is called at the end of each epoch.  
        # Add the test accuracy in logs.get('loss') to the val_acc list
        self.valu_acc.append(logs.get('acc'))
        
# Create an instance of the history callback
history_cb = LossHistory()","X_train, X_valid, Y_train, Y_valid = train_test_split(train, y,
                                                      test_size=0.2,
                                                      random_state=42,
                                                      stratify = y)
# Class relations
print(Y_train.value_counts()/Y_train.count())
print(Y_valid.value_counts()/Y_valid.count())",,Project->Train Accidents in India,Instrument Classifier 1->Machine Learning Project:->Building the neural network classifier,Titanic 5 Models->Applied Data Science Solutions for first Kaggle competition.->**Split dataset to Train and Validation set**
290039,"df[""class""].unique()","ind = class_df[(class_df.predict == 'hip hop') &(class_df.real == 'reggae')].head().index
print(test.loc[ind][['artist','genre_x', 'genre_y', 'lyrics']])","from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X = LabelEncoder()
X[:, 3] = labelencoder_X.fit_transform(X[:, 3])
onehotencoder = OneHotEncoder(categorical_features = [3])
X = onehotencoder.fit_transform(X).toarray() 
# I believe that after it encodes the categorical var (State)
# that it will remove that column and replace with Dummy Var columns",,"Preprocessing->Pre-processing - making the dataset->Reading the image data
Akanksha Verma->Data Pre-processing->Changing class variable from text to numerical categories",Genre Prediction With Lyrics->7. Classification->7.2 Classify different genres,3 Multiple Linear Regression->Best fit model->Encoding categorical data
465961,"model_w2v = gensim.models.Word2Vec(sentences=text_w2v, size=300, sg=1, hs=1)
if not os.path.exists('./WE_models'):
    os.mkdir('./WE_models')
model_w2v.save('WE_models/w2v_sg_300D')","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
corpus = cleaned_sents[:]
# Reading Word2Vec Model
fname = 'wiki_corpus_300.model'
is_overwrite = 0 # <---- You can choose to load or re-build the model
if os.path.exists(fname) and is_overwrite == 0:
    # load the file if it has already been trained, to save repeating the slow training step below
    model = gensim.models.Word2Vec.load(fname)
else:
    # can take a few minutes, grab a cuppa
    model = gensim.models.Word2Vec(corpus, size=300, min_count=1, workers=4, iter=50) 
    model.save(fname)","state_names = fsm_spec_a['states']
state_names",,Word2Vec->1. Word2Vec Model->1.1. Train Word2Vec Model->Skip-Gram Version,Gkb Text Classification Word2Vec->2 - Loading and Building Word Models,Fsm Debug1-Checkpoint->FSM->FSM_BRAM->List of state names
284989,"fig, axs = plt.subplots(1,3)
df[df.SibSp < 1].Survived.value_counts().plot(kind='bar',ax=axs[0], title =""No SibSP"")
df[(df.SibSp>=1) & (df.SibSp<2)].Survived.value_counts().plot(kind='bar',ax=axs[1],  title =""A SibSP"")
df[(df.SibSp>=2)].Survived.value_counts().plot(kind='bar',ax=axs[2],  title =""Multi SibSP"")","fig, axs = plt.subplots(1,3)
df[df.Parch < 1].Survived.value_counts().plot(kind='bar',ax=axs[0], title =""No Parch"")
df[(df.Parch>=1) & (df.Parch<2)].Survived.value_counts().plot(kind='bar',ax=axs[1],  title =""A Parch"")
df[(df.Parch>=2)].Survived.value_counts().plot(kind='bar',ax=axs[2],  title =""Multi Parch"")","##rf Task 1
rf1clf = RandomForestClassifier(n_estimators=50,max_depth=5,random_state=1)
rf1clf.fit(X_train_scaled,y_train) 
rf1_y_hat = rf1clf.predict(X_test) # get test set precitions
names = df.columns


names = df.columns
importances = rf1clf.feature_importances_
indices = np.argsort(importances)


plt.figure(figsize=(10,20))
plt.title('Feature Importances Mileage')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), names[indices]) ## removed [indices]
plt.xlabel('Relative Importance')
plt.show()

##Rf task 2

rf2clf = RandomForestClassifier(n_estimators=50,max_depth=5,random_state=1)
rf2clf.fit(X2_train,y2_train) 
rf2_y_hat = rf2clf.predict(X2_test) # get test set precitions
names = df2.columns

names2 = df2.columns
importances2 = rf2clf.feature_importances_
indices2 = np.argsort(importances2)


plt.figure(figsize=(10,20))
plt.title('Feature Importances Mileage')
plt.barh(range(len(indices2)), importances2[indices2], color='b', align='center')
plt.yticks(range(len(indices2)), names2[indices2]) ## removed [indices]
plt.xlabel('Relative Importance')
plt.show()",,Titanic Eda->EDA Pclass,Titanic Eda->EDA Parch,"Lab2->Modeling and Evaluation 4 - Analyze Results
Lab2-Copy2->Random Forest Classifier Feature Importances"
409651,"stop = stopwords.words('english')
stop.append('KSAND')
stop.append('ALink')
vectorizer = TfidfVectorizer(stop_words=stop, lowercase=True, use_idf=True, max_features=150,
                             preprocessor=format_text, norm= 'l2', ngram_range=(1,1), sublinear_tf=True)","stop = stopwords.words('english')
stop.append('KSAND')
stop.append('ALink')
vectorizer = TfidfVectorizer(stop_words=stop, lowercase=True, use_idf=True, max_features=150,
                             preprocessor=format_text, norm= 'l2', ngram_range=(1,1), sublinear_tf=True)","joined_accuracy_count  = int()

joined_accuracy_count = true_avengers[""URL""][(true_avengers[""Years since joining""] == 2015 - true_avengers[""Year""])].count()",,Homework4->04 - Applied ML->Question 2: Applied ML->Answer->3. A propensity score model,Homework4->04 - Applied ML->Question 2: Applied ML->Answer->3. A propensity score model,Avengers
310326,"ttl_neg_examples = neg_examples_nobias.shape[0]
print(""Total negative examples: "", ttl_neg_examples)","ttl_features = dataset['neg_examples_nobias'].shape[1]
print(""Total features: "", ttl_features)","# Defining a list here
list1 = [1,2,3]

#Shallow copy which is used by python in default mode
list2 = list1
print(""List 2 is just been copied from list1 having values --> {new_list}"".format(new_list=list2))
# Now we will see that what happens when we append new element in list2
list2.append(4)

#Let's check the output of both list
print(""We have added in list2 --> 4, output of list1 --> {list1_after}"".format(list1_after=list1))
print(""We have added in list2 --> 4, output of list1 --> {list2_after}"".format(list2_after=list2))",,Perceptron->Plots,Perceptron->Dataset->Features,Basic->This is my basic python practice (Office)->In this we will demostrate the shallow copy
394720,combo_num.head(),"combo[combo.TimePeriod == ""2015""].head(10)","path_to_file = geoplot_data.contiguous_usa()
gpd.read_file(path_to_file).head()",,Final->Analyzing SAT performance of public shcools in New York->Mapping SAT Scores->Looking at other variables that influence SAT Performance,Final Proposal Data Report->Final Proposal: The Dynamics of Economic Activity at the Community Level.,Quickstart->Quickstart->More on geospatial data
44163,"# a array와 같은 모양의 0 array
np.zeros_like(a)","Mus.resize(N,1)

# Useful vectors / unit arrays
one = np.ones(1).reshape(1,1)
ones = np.ones_like(Mus)
zeros = np.zeros_like(Mus)
zero = np.zeros(1).reshape(1,1)


A = np.vstack((
                np.hstack(( 2*Q,    -ones,  -(Mus-r_f), zeros  )),
                np.hstack((-ones.T,   zero,  zero, one )),
                np.hstack((-(Mus-r_f).T,   zero,  zero, zero )),
                np.hstack((zeros.T,   one,  zero, zero ))
              ))

b = np.zeros((N+3,1))
b[N+1]=1",%time kepler_dw = kepler_fetcher.output(),,Basics->Array Creation->Array creation functions,Solution->Part 3: Maximise the Sharpe Ratio to Find Global Optimum,Demo Kepler Light Curves All Exoplanets Pdcsap Relative Flux->Plot all Kepler Exoplanet Light Curve Time Series (1652)
257762,"df = pd.read_csv('./ecommerce-customers.csv')
df.head()","df=pd.read_csv('/home/xyou2/Team7FinalProject/white.csv')
df.head()","report_file = '/Users/bking/IdeaProjects/LanguageModelRNN/reports/encdec_noing_250_512_040dr_2.json'
log_file = '/Users/bking/IdeaProjects/LanguageModelRNN/logs/encdec_noing_250_512_040dr_2.json'

import json
import matplotlib.pyplot as plt
with open(report_file) as f:
    report = json.loads(f.read())
with open(log_file) as f:
    logs = json.loads(f.read())
print'Encoder: \n\n', report['architecture']['encoder']
print'Decoder: \n\n', report['architecture']['decoder']",,Regression->Regression->Show the first 5 rows of the dataset,Analysis Of Wine Quality Data->Data Exploration,Encdec Noing 250 512 040Dr->Encoder-Decoder Analysis->Model Architecture
452392,"from sklearn.svm import LinearSVC

X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state = 0)

clf = LinearSVC(C=5, random_state = 67).fit(X_train, y_train)
print('Coefficient:\n', clf.coef_)
print('Intercepts:\n', clf.intercept_)","from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state=0)
# Linear SVC is smart in detecting number of classes (Apple, lemon, orange, mandarin) and their input feature.
clf = LinearSVC(C=5.0, random_state=67).fit(X_train, y_train)
# output Coef will have 4 rows corresponding to each class and each row has a tuple.
print(""clf coef_ {}"".format(clf.coef_))
print(""clf intercepts {}"".format(clf.intercept_))",example_wanderer_median.mp_compute_flux_over_time_betaRad(),,Supervised->Multi-Class classification with linear models,Week2-Checkpoint->Linear SVC with M classes,"Exoplanet Tso Pipeline Median->Load Wanderer Class
Exoplanet Tso Pipeline Median->I made up this loop and did not test it->The following code is a copy/paste from a different notebook of mine."
180020,"# Drop all columns that relate to 
retweet_cols = ['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp']
df_clean_final = df_clean_final.drop(retweet_cols, axis=1)","from sklearn.model_selection import train_test_split

x = rd_clean_final.drop(['SalePrice','TotRmsAbvGrd'],1)
y = rd_clean_final['SalePrice']

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)","stopping_variance = 0.95 # stopping criteria
for n_comps in range(X.shape[1], 0, -1):
    pca = PCA(n_components = n_comps)
    pca.fit(X)
    if sum(pca.explained_variance_ratio_) >= stopping_variance:
        pca_sv = pca
    
    print('======================== ', n_comps, ' components =========================')
    print('===== explained variance ratio: ===========================================')
    print(pca.explained_variance_ratio_)
    print('\n')
    print(""total variance explained: {:0.3f}"".format(sum(pca.explained_variance_ratio_)))
    print('\n')",,Wrangle Act->Project: Wrangle and Analyze Data->Gather Data->`df_clean_final`->Define,Boston House Pricing,3 Ex2
329912,"uniques, idx = np.unique(rest_rvws.name, return_index=True)
rest_features = pd.DataFrame(X[idx, :], index=rest_rvws.name[idx])
print rest_features.shape
rest_features.head(2)","# Load data
data_00 = np.loadtxt('datasets/dataset_1_year_2000.txt')
print 'n =', len(data_00)

# Split predictors and response
x_00 = data_00[:, 0:2]
y_00 = data_00[:, 2]

pd.DataFrame(data_00).head()","# essential line for notebooks - variables declarations will clash with existing ones when re-run cells
# without this
tf.reset_default_graph()

# using get_varible() on it's own checks if we have double used a variable
# differnce with Variable() constructor is constructor will duplicate if already created 
# calling it name, name_1, name_2, etc under the hood
# to have get_variable auto reuse:
# with tf.variable_scope(""processing_block_1"", reuse=tf.AUTO_REUSE):
#    t_1 = tf.get_variable(""t_1"", shape=(), initializer=tf.constant_initializer(1.0))
# otherwise ""tf.reset_default_graph()"" is needed in notebooks ^
t_1 = tf.get_variable(""t_1"", shape=(), initializer=tf.constant_initializer(2.0))

# we first need to have an init (operation), and put it in the session.run(init) before other runs
# else no variables will be initialised
init = tf.global_variables_initializer()

vec_1 = tf.placeholder(dtype=tf.float32, name=""vec_1"", shape=(1,3))
res = tf.multiply(t_1, vec_1)
res2 = tf.multiply(res, res)

with tf.Session() as sess:
    # name is vec_1:0 when created as an instance of Varible, held in vec_1.name attribute
    # intermediate names produced by operations are operation_name:0, operation_name:1 etc
    # for operations that produces multiple tensors. Duplucated operations are Mul, Mul_1, etc...
    # To print all of op produced intermediate tensors:
    # for each_op in sess.graph.get_operations():
    #     print(each_op.name, each_op.values())
    #     # or equivalently res.op.name etc. using <variable name>.op.name will also work 
    sess.run(init)
    res_ = sess.run([res], feed_dict={'vec_1:0': np.array([[1.0, 2.0, 3.0]])})
    print(res_)
    
#     # specifying op:index_of_result also works:
#     print(sess.run(['Mul:0'], feed_dict={'vec_1:0': np.array([[1.0, 2.0, 3.0]])}))",,Collaborative Filtering-Checkpoint->Project: Recommendation System for Yelp Users->Content-Based Flitering->Dot-product method,Halperin Keyan Stat121A Hw6->CS 109A/AC 209A/STAT 121A Data Science: Homework 6->Problem 2: Predicting Urban Demographic Changes,Useful Tf Snippets->Graph definition and execution
310125,"# Get the shape, # of elements, and # of dimensions
print(np.shape(b))
print(np.size(b))
print(np.ndim(b))","print ('Number of dimensions')
print(np.ndim(C))
print(np.ndim(D))

print ('Shape of the arrays')
print(np.shape(C))
print(np.shape(D))","dataset_path = ""C:\\dataset""
class_names = [""dog"",""bird"",""rain""]
dirpath_1 = os.path.join(dataset_path, class_names[0])
dirpath_2 = os.path.join(dataset_path, class_names[1])
dirpath_3 = os.path.join(dataset_path, class_names[2])",,04 Exploratory Data Analysis With Num Py And Pandas->Exploratory Data Analysis with NumPy and Pandas->Exercise: Baby Names,Numpy Introduction->Array creation,Tutorial Notebook->classifier.py
229123,"from __future__ import unicode_literals
import spacy
import string
from spacy.en import English
# this is included because spacy is written in python 2 and python 3 but sometimes
# for strings it uses python 3 therefore, we need to include this line 
# to avoid writing ""u"" for unicode before writing text
 
nlp = spacy.load('en')  # load english language module","import spacy
import string
from spacy.en import English
from __future__ import unicode_literals 
nlp = spacy.load('en')  # load english language module",df_loan['issue_d'].min(),,"Tokenization-Checkpoint->Tokenization
Tokenization->Tokenization",Text Normalization->Text Normalization using Spacy->Steps for Text Normalization,Lending-Club-Exploration->Analysis of Lending club loan dataset->Transformations->Exploring the data
493003,"A = np.array([1,2,3])
B = np.array([4,5,6])

# Dot product
print A.dot(B)
print np.dot(A,B)","# Code goes here
print('Inner product')
print(np.dot(A.T,B))","# set up the data, prep for processing
ilabs_data = nlit.Dataset(data_folder = os.path.join(nlit.__path__[0],'data'), selected_metalabels='CTOPP'
                                        , metalabel_files='readingdata_metalabels.csv'
                                        , selected_features = None
                                        , outcome_variable = 'Dyslexia Diagnosis'
                                        , missingness_threshold = 0.4
                                        , max_missing_count = 1
                                        , token_file = 'neurolit_api_token.txt')",,Numpy->Matrix Multiplication,Admission Test + Answers->Python requirements for MLiFC->Numpy->Dot products,Predict Example->NeuroLit Prediction Notebook->Initialize dataset
38906,"model = Net(2, 2, 1)
criterion = nn.MSELoss()","# Define the loss and optimization
import torch.optim as optim

criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters())",titanic_data.count(),,Neural Networks->Neural Networks for Regression,2 Define Network Architecture->Facial Keypoint Detection - Define the CNN->Batching and loading data->Visualize the predicted keypoints,"Project->Project 2: Titanic Data Set->2. Wrangle->i. Investigating NaN values in the data
P2 Titanic Dataset->Load Dataset and Data Wrangling
Project4-Starter
Preprocesamiento - Titanic Dataset->[Titanic3D](https://www.tinkercad.com/things/8FKovaXaBOe-titanic)->titanic_data is now a [DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html ""Go to the DataFrame documentation!"") powered by Pandas, so now we can do whatever we want with this object."
26319,"#Cabin
combinedTitanic['Cabin_Type']=combinedTitanic['Cabin'].str.extract('([A-Za-z])',expand=False)
combinedTitanic['Cabin_Type'] = combinedTitanic['Cabin_Type'].fillna('U')

print(combinedTitanic['Cabin_Type'].unique())","#score
print(titan_logit.score(T_test, z_test))
#coefficients
print(titan_logit.coef_)",print ames_gridmesh,,Titanic->Project Titanic->Fill Missing Values->Cabin :,Try Titanic Perso->Testing a simple model,2 Environment Representation->3. Other Representations: TriMesh
451791,df[df['origin'] == 'Asia'].origin.count(),"# Cluster distribution in all origins
df_origin = df_merge1.groupby([""cluster_id"", ""origin""])[""n""].count().unstack(""cluster_id"")
df_origin",idx = dv.vocabulary_['tag_tiltbrush'] + item_features.shape[0],,2019-01-24 Pandas Dataframes->pandas DataFrames->Data ingestion & inspection->Exercises->Filtering and counting,Mini Project Clustering->Visualizing Clusters using PCA,2016-11-7-Implicit-Mf-Part-2->Optimizing Hyperparameters with ```scikit-optimize```->Feature Sorting
202806,"df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])    
df['Iu'] = df.uid.apply(get_Iu)
df['Ui'] = df.iid.apply(get_Ui)
df['err'] = abs(df.est - df.rui)
df.head()","# Let's build a pandas dataframe with all the predictions
# ur : The users ratings. This is a dictionary containing lists of tuples of the form 
# (item_inner_id,rating). The keys are user inner ids.
# ir : The items ratings. This is a dictionary containing lists of tuples of the form 
# (user_inner_id,rating). The keys are item inner ids.
def get_Iu(uid):
    #Returns: The number of items rated by the user.
    
    try:
        return len(trainset.ur[trainset.to_inner_uid(uid)])
    except ValueError:  # user was not part of the trainset
        return 0
    
def get_Ui(iid):
    #Returns: The number of users that have rated the item.
    
    try:
        return len(trainset.ir[trainset.to_inner_iid(iid)])
    except ValueError:  # item was not part of the trainset
        return 0

df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])    
df['ItemWatchedByUser'] = df.uid.apply(get_Iu)
df['UserGaveRatingToItem'] = df.iid.apply(get_Ui)
df['err'] = abs(df.est - df.rui)

df1 = pd.DataFrame(predictions1, columns=['uid', 'iid', 'rui', 'est', 'details'])    
df1['ItemWatchedByUser'] = df1.uid.apply(get_Iu)
df1['UserGaveRatingToItem'] = df1.iid.apply(get_Ui)
df1['err'] = abs(df1.est - df1.rui)","for k in array_dict:
    cand_matrix = np.concatenate((np.transpose(M1_c1),np.transpose(array_dict[k])), axis=1)
    print k
    print array_dict[k]
    print ''
    print np.dot(np.transpose(cand_matrix),cand_matrix)
    print ''",,Knn->train data and test data->i created dataframe with all prediction and errors.....,Recommendation Using Surprise->Creating DataFrame with all prediction,Hw4B Orthonormal Pca->Question 1
264564,"Audio(data = sound, rate = 44100)","from IPython.display import Audio
sound_file = 'hello.mp3'

Audio(url=sound_file, autoplay=False)","import pprint
import subprocess
import sys 
sys.path.append('../')

import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import matplotlib.gridspec as gridspec
from mpl_toolkits.axes_grid1 import make_axes_locatable
import seaborn as sns

%matplotlib inline

np.set_printoptions(suppress=True, precision=2)

sns.set(font_scale=2.0)",,Homework2->Build the sound,Recruitment Bot->Scenario 3->Text to speech,"2017-06-04(Study Of Connectivity Matrix I)->Study of connectivity matrix
2017-09-2017(Anders Experiments)->Anders exepriments
2017-06-01(Artificial Matrix Study Ii)->Artificial matrix study II
2017-04-13(Artificial Matrix Study)->Artificial matrix study
2017-05-28(Examples Of Random Sequence Generation)->Example of random sequence generation
2017-06-15(Overload And Overlap Study)->Overload and overlap study
2017-05-29(Recall Study Ii)->Recall study II
2017-06-25(Dynamics Of Recall - Buffer Memory)->Dynamics of recall buffer memory.
2017-08-17(Excitation-Inhibition Balance On The Artificial Matrix)->Excitation-Inhibition balance on the artificial matrix
2017-08-15(Nmda - Ampa Ratio)->NMDA / AMPA ratio
2017-06-02(Effects Of Training On Beta)->Effects on training on Beta
2017-08-16(Overlap Artificial Vs Real)->Overlap capacity: artificial vs real"
167766,"# Test In-Sample and OutofSample RSS and R^2
y0_pred_IS = lm0.predict(X0_train)
err_IS = (y0_pred_IS - np.asarray(y0_train).T).T
RSS_IS = sum(err_IS**2)
R_2_IS = 1 - np.var(err_IS)/np.var(y0_train)
print(""The Residual sum of square for OLS regression is: {0}\nThe R-squared is: {1}"".format(RSS_IS, R_2_IS))

y0_pred_OS = lm0.predict(X0_test) #compute the prediction for the test sample 
err_OS = (y0_pred_OS - np.asarray(y0_test).T).T
RSS_OS = sum(err_OS**2)
R_2_OS = 1 - np.var(err_OS)/np.var(y0_test)

print(""\nThe Residual sum of square for OLS regression is: {0}\nThe R-squared is: {1}"".format(RSS_OS, R_2_OS))","ridge = 40000
Ridge = linear_model.Ridge(fit_intercept=True, alpha=ridge)

Ridge.fit(X_train,y_train)

p_IS=Ridge.predict(X_train)
err_IS=p_IS-y_train
R_2_IS_Ridge=1-np.var(err_IS)/np.var(y_train)
print(""The R-squared for In-Sample Ridge is: {0}"".format(R_2_IS_Ridge))

Ridge_coef=Ridge.coef_

p_OS=Ridge.predict(X_test)
err_OS=p_OS-y_test
R_2_OS_Ridge=1-np.var(err_OS)/np.var(y_test)
print(""The R-squared for Out-Of-Sample Ridge is: {0}"".format(R_2_OS_Ridge))","os.chdir('..')
healthSurvey_df = pd.read_csv('DailyCheck.csv')
healthSurvey_df['Date'] = pd.to_datetime(healthSurvey_df['Date'])
healthSurvey_df['day'] = healthSurvey_df['Date'].dt.date
healthSurvey_df['day'] = healthSurvey_df.Date.apply(lambda x: x.strftime('%Y%m%d')).astype(int)",,3->1. Possible Pattern in Data->b. Run multivariate regression->Model Evaluation,Bayesian Inference->(d),P180625 Personal Health Random Forest
179453,archive[np.isnan(archive['in_reply_to_status_id']) != True].sample(5),np.isnan(df_FileID_ProductID_features).sum(axis=0),"fig = plt.figure(figsize=(15,5))
ax1 = fig.add_subplot(211)
fig = sm.graphics.tsa.plot_acf(diff_ts1_test.values.squeeze(), lags=12, ax=ax1)
ax2 = fig.add_subplot(212)
fig = sm.graphics.tsa.plot_pacf(diff_ts1_test.values.squeeze(), lags=12, ax=ax2)",,Wrangle Act->Wrangling Doggos->Assess->Recode,Trend->Appendix->1. get spicifit index values->df_FileID_ProductID_features has no NaN,Arima Research->4) plot ACF/PACF for the transformed train TS in order to understand whether  AR(p)/MA(q)AR(p)/MA(q)  may be used.
466680,"holomap.select(Hour={3,6,9}).layout()","from ipywidgets import widgets, Layout


selection = widgets.Select(
    options=['Read df_retracted_retraction from stored files', 'Generate df_retracted_retraction by searching into the PMC OAS (It will take several hours)'],
    value='Read df_retracted_retraction from stored files',
    layout=Layout(width='50%', height='70px', margin = '70px'),
    description = 'Select: ',
    disabled=False
)

selection","# To make it more readable, lets store the OAuth credentials in strings first

access_token =  ""Your token here""
access_token_secret = ""Your token secret here""
consumer_key = ""Your consumer key here""
consumer_secret = ""Your consumer secret here""",,1-Introduction->Introduction->Effortlessly exploring data,Select Sentences Citing And Model3->4. Word embedding approaches->4.1  word2vect,"Smm Process Module->Loading the access tokens to connect to twitter
Smm Process Module->Function to load csv files of already collected tweets and related data"
204552,"for i in range(3):
    companies[i][""Volume""].plot(figsize=(15, 4), grid=True)
    plt.title(abbr[i], fontsize=20, fontweight=""bold"")
    plt.ylabel(""Volume"")
    plt.show()","jaro_quer_titl = []
for i in (range(len(data['title']))):
    jaro_quer_titl.append(jaro(data['title'][i].split(),data['search_query'][i].split()))","mapping = {
    ""Asian/Pacific Islander"": 15159516 + 674625,
    ""Black"": 40250635,
    ""Native American/Native Alaskan"": 3739506,
    ""Hispanic"": 44618105,
    ""White"": 197318956
}

race_per_hundredk = {}

for race in race_counts:
    race_per_hundredk[race] = round((race_counts[race] / mapping[race]) * 100000, 3)

race_per_hundredk",,Code-><u>Stock Market data</u>->Line graph for Volume,Model V4->Length of search_query,Basics
192693,"def outputFeatureMapLayer(image_input,layer):
    with tf.Session() as sess:
        saver.restore(sess, tf.train.latest_checkpoint('.'))

        conv_layer = tf.get_default_graph().get_tensor_by_name(""conv{}:0"".format(layer))
        print(""conv layer is:"")
        print(conv_layer)
        outputFeatureMap(sess,image_input,conv_layer)","with tf.Session() as sess:
    saver.restore(sess, tf.train.latest_checkpoint('.'))
    activation = tf.get_default_graph().get_tensor_by_name(""conv1_relu:0"")
    
    reshaped_image = np.reshape(prepared[4], (1, 32, 32, 3)) 
    
    outputFeatureMap(reshaped_image, activation)",tuned_clf=tuned_models['gb'][-1],,Traffic Sign Classifier->Traffic Sign Recognition Classifier->Output Each Layer,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Output Top 5 Softmax Probabilities For Each Image Found on the Web,"Eda, Sentiment Analysis, Extracting Representative Sentences->Sentiment Analysis->Classifying Opinions->III. Evaluate performance of tuned classifiers"
357374,"def find_testings(pose, savename, savepath):
    
    path='/Users/juliannefreeman/Documents/Insight/Project/imgs/imgs_pract5/model_accuracy/videos/'
    
    count=0
    dirs=os.listdir(path)
    for currdir in dirs: #video folders
        #print(currdir)
        if currdir != '.DS_Store':
            folders=os.listdir(path+currdir)
            for folder in folders: #pose folders
                if pose in folder:
                    #print(folder)
                    os.chdir(path+currdir+'/'+folder)
                    files=os.listdir() #imgs
                    for file in files:
                        os.chdir(path+currdir+'/'+folder)
                        if file.split(""."")[-1] == ""jpg"":
                            count+=1
                            im = Image.open(file)
                            if count<10:
                                newfile= savename + '_000' + str(count) + '_' + currdir + '.jpg'
                            elif count<100 and count>=10:
                                newfile= savename + '_00' + str(count) + '_' + currdir + '.jpg'
                            elif count<1000 and count>=100:
                                newfile= savename + '_0' + str(count) + '_' + currdir + '.jpg'
                            elif count>1000:
                                newfile= savename + '_'+ str(count)+ '_' + currdir + '.jpg'
                            os.chdir(savepath)
                            try:
                                im.save(newfile,""JPEG"")
                            except:
                                im.convert('RGB').save(newfile,""JPEG"")
    return count","import os
from PIL import Image
import matplotlib.pyplot as plt
%matplotlib inline

categories = [""Animal"",""Fungus"",
              ""Geological Formation"",
              ""Person"",
              ""Plant, flora, plant life"",
              ""Sport""]

# X_RGB = []
# X_BW = []
label = []
X_raw = []
X_RGB_64 = []
X_BW_64 = []

for category in categories:
    folder = '/Users/Madhu/Documents/Courses/WSTM/hw2/'
    folder += category
    file_count  = 0
    file_names  = os.listdir(folder)
    file_count = 0
    
    for file_name in file_names:
        image = Image.open(os.path.join(folder, file_name))
        image = image.resize((64, 64))
        X_raw.append(image)
        image_ = image.convert(""RGB"")
        X_RGB_64.append(image_)
        image_ = image.convert(""L"")
        X_BW_64.append(image_)
        label.append(category)     
        file_count += 1","import ee
from geetools import ui, tools, composite, cloud_mask, indices, algorithms",,Part 1->Compile all images into folders,Hw2 P3->Problem 3,Medoid->Medoid Compositing
103777,"features = np.array(df.iloc[:, 1:])
target = np.array(df.iloc[:, 0])
labels = np.array(list(df))","target_label = 'poi'

features_list = ['poi',
                 'salary',
                 'total_payments',
                 'bonus',
                 'total_stock_value',
                 'expenses',
                 'exercised_stock_options',
                 'other',
                 'long_term_incentive',
                 'restricted_stock',
                 'to_messages',
                 'from_poi_to_this_person',
                 'from_messages',
                 'from_this_person_to_poi',
                 'shared_receipt_with_poi',]","# Plot the yearly mean, three-year simple rolling mean, and three year centered rolling mean
fig, ax = plt.subplots()
unemp.loc[:, 'unemployment_rate'].plot(ax=ax, label=""quaterly"")
yearly_mean.plot(ax=ax, label=""yearly"")
three_year_rolling_mean.plot(ax=ax, label=""three-year rolling"");
three_year_centered_rolling_mean.plot(ax=ax, label=""three-year centered rolling"");
ax.legend();",,"Decision Tree->Split features, target and labels",Final Project Report->Machine Learning Project- Investigating the Enron Dataset for Fraud->Question 1->Data Exploration,02 Rolling Statistics
417074,"import underworld as uw
from underworld import function as fn
mesh = uw.mesh.FeMesh_Cartesian()
tempVar = mesh.add_variable(1)

positiveTemp = fn.misc.min(0.,tempVar)
try:
    positiveTemp.evaluate((0.1,))
except RuntimeError as e:
    print(""RuntimeError: ""+str(e))","import underworld as uw
mesh = uw.mesh.FeMesh_Cartesian(elementRes=(64,64), maxCoord=(2.,1.))
var = mesh.add_variable(1)
# first evaluate directly using mesh","#Comparison of week daily uses for different user type.
Return_to_same_users.groupby([Return_to_same_users.Starttime.dt.weekday_name,""Usertype""]).count()[""Tripduration""].unstack(level=1).plot(kind='bar',figsize=(10,8),logy=True)
plt.xlabel(""weekday"")
plt.title(""Daily usage throughout a week for Same return users"")
Rentals.groupby([Rentals.Starttime.dt.weekday_name,""Usertype""]).count()[""Tripduration""].unstack(level=1).plot(kind='bar',figsize=(10,8),logy=True)
plt.title(""Daily usage throughout a week for All users"")
plt.ylim(0,100000)
plt.xlabel(""weekday"");",,04 Functions->Conformal input & output checking,04 Functions->The `evaluate()` Method,3-Pandas-Your-Data->Assignment 3 - Pandas your Data->Part 1 & 2: Load and parse your data into Python->Part 3: Processing your data
438851,"bestcv, Xtrain, ytrain, Xtest, ytest = do_classify(KNeighborsClassifier(), {""n_neighbors"": np.arange(1,40,2)}, df, ['pc1','pc2'], 'label', 'check' )","from sklearn.neighbors import KNeighborsClassifier 
clf_knn, Xtrain_k, ytrain_k, Xtest_k, ytest_k = do_classify(KNeighborsClassifier(), 
                                                            {""n_neighbors"": range(1,40,2)}, 
                                                            df,pc_name, 'label', 'T', mask=mask)",df.max_funding.describe(),,"Classification->As before, cross-validation","Cs205 Final Report - December 10->1. Image Processing for Training Data set->Coloring and Labeling->Read the ""Blue"" color RGB matrix",Summarise->Summary of apprenticeships data->Max funding available
339378,"INPUT_PRICE = 25
INPUT_COST = 10
if INPUT_PRICE - INPUT_COST <= 0:
    print(""Invalid input"")","Cost = [q * u for q,u in zip(Quantity,UnitPrice)]
print(Cost)","features = ['hour', 'weekday', 'svm_encoded_features',#]
#svm_enc_features = [
#                 'url',
#                 'domain',
                'user_agent',
                'ad_slot_id',
                'ad_exchange',
                'ad_slot_width',
                'ad_slot_height',
                'ad_slot_format',
                'ad_slot_visibility',
                'ad_slot_floor_price']
svm_enc_features = [
                'url',
                'domain']
onehot, _ = create_onehot_features(trainset, svm_enc_features, 100, 'svm_encoded_features')
trainset = apply_feature_eng(trainset, onehot, 'svm_encoded_features')
validationset = apply_feature_eng(validationset, onehot, 'svm_encoded_features')
test_set = apply_feature_eng(test_set, onehot, 'svm_encoded_features')",,Hw4-Checkpoint,Python-Patterns->Programming Patterns in Python->Search->The zip function,Gbm And Svm->*/->Just for SVM
66631,"## TODO: Execute your algorithm from Step 6 on
## at least 6 images on your computer.
## Feel free to use as many code cells as needed.
predictDogOrHuman('sample_images/me.jpg')",predict_resembling_human_or_dog_breed('lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg'),lc_corr = lc_net.correct(restore_trend=True),,Dog App->(IMPLEMENTATION) Predict Dog Breed with the Model,Dog App->(IMPLEMENTATION) Assess the Human Face Detector,02-How-To-Make-A-Supernova-Lightcurve->How to make a supernova lightcurve?
492024,"model = make_pipeline(ensemble.ExtraTreesClassifier())
model.get_params()


parameters = {'extratreesclassifier__n_estimators': list(range(1,400,100)),
              'extratreesclassifier__min_samples_leaf': list(range(1,4,1))}

gs_extraTree = GridSearchCV(model, parameters, cv=3, n_jobs=-1) 
gs_extraTree.fit(X.values,list(Y.values))

print(gs_extraTree.best_score_)
print(gs_extraTree.best_params_)","from sklearn.pipeline import make_pipeline
from sklearn import tree, ensemble
from sklearn.grid_search import GridSearchCV

model = make_pipeline(ensemble.RandomForestClassifier())
model.get_params()

parameters = {'randomforestclassifier__n_estimators': list(range(1,400,100)),
              'randomforestclassifier__min_samples_leaf': list(range(1,4,1))}

gs_forest = GridSearchCV(model, parameters, cv=3, n_jobs=-1) 
gs_forest.fit(X.values,list(Y.values))

print(gs.best_score_)
print(gs.best_params_)","plt.scatter(df.movie_title.apply(lambda title: len(title.split())), df.imdb_score, lw=0, alpha=0.3)
plt.xlabel('number of words in movie title')
plt.ylabel('imdb score');",,Capstone Atp Week5->ATP tournament data->A simple decision tree,Capstone Atp Week5->ATP tournament data->A simple decision tree,01 - Exploration->Study Features->Aspect ratio
272305,"from bokeh.palettes import Blues9, Reds9, Greens9
ranges = dict(x_range=(-.5, 1.6), y_range=(-.5, 1.6), width=800, height=800)
protocols = [('tcp', Blues9), ('udp', Reds9), ('icmp', Greens9)]
shaded = hv.Overlay([datashade(forceatlas.select(protocol=p), cmap=cmap, **ranges)
                     for p, cmap in protocols]).collate()
stack(shaded * dynspread(datashade(forceatlas.nodes, cmap=['white'], **ranges)), link_inputs=True)","from bokeh.transform import factor_cmap

p = figure(x_range=FactorRange(*x), plot_height=250, title=""Fruit Counts by Year"")

p.vbar(x='x', top='counts', width=0.9, source=source, line_color=""white"",

       # use the palette to colormap based on the the x[1:2] values
       fill_color=factor_cmap('x', palette=['firebrick', 'olive', 'navy'], factors=years, start=1, end=2))

p.y_range.start = 0
p.x_range.range_padding = 0.1
p.xaxis.major_label_orientation = 1
p.xgrid.grid_line_color = None

show(p)","# function for getting average grade over all 3 grade periods (G1, G2, G3)
def avg_grade(stud):
    x = (stud['G1']+stud['G2']+stud['G3'])/3 
    return x
# import for generating legend
import matplotlib.patches as mpatches
# get grade averages for all students
stud_math['avg'] = stud_math.apply(lambda row: avg_grade(row), axis = 1)
stud_port['avg'] = stud_port.apply(lambda row: avg_grade(row), axis = 1)
# plot averages as density plot for both classes
stud_math['avg'].plot(kind = ""density"", color = 'yellow')
stud_port['avg'].plot(kind = ""density"", title = 'Average of all grades', color = 'black')
# generate legend
math = mpatches.Patch(color='yellow', label='Math')
port = mpatches.Patch(color='black', label='Portuguese')
plt.legend(handles=[math, port])
plt.show()",,Network Packets->Coloring by protocol,"07->Grouped Bar Charts
07 - Bar And Categorical Data Plots->Grouped Bar Charts
07 - Bar And Categorical Data Plots->Grouped Bar Charts",Pmk3796->Exploratory Data Analysis->Student Academic Performance
429965,from pandas.tools.plotting import andrews_curves,"andrew_mod = sm.RLM(data.endog, data.exog, M=sm.robust.norms.AndrewWave())
andrew_results = andrew_mod.fit(scale_est=sm.robust.scale.HuberScale(), cov=""H3"")
print('Parameters: ', andrew_results.params)","n = 1000
p = 0.009
GERR=nx.fast_gnp_random_graph(n,p)
theor_mean_degree_GERR=(n-1)*p",,"Pandas Data Munging->Data Munging with Pandas
Brains From Scratch - Machine Learning For Dummies->Problems->Support Vector Machines (SVM)->2. Andrew's Plot
Mod3Ass1->Assignment 5",Robust Models 0->Robust Linear Models->Estimation,Assignment1 Code->Question 7 (c)
224115,"# PROPERTY ID - DO NOT EDIT !  
DOC.set_id('cmip6.ocean.key_properties.resolution.name')  

# PROPERTY VALUE: 
# Set as follows: DOC.set_value(""value"")  
# TODO

# PROPERTY QUALITY CONTROL STATUS:  
# 0=draft, 1=in-review, 2=reviewed  
DOC.set_qc_status(0)","# PROPERTY ID - DO NOT EDIT !  
DOC.set_id('cmip6.toplevel.radiative_forcings.aerosols.tropospheric_volcanic.additional_information')  

# PROPERTY VALUE: 
# Set as follows: DOC.set_value(""value"")  
# TODO

# PROPERTY QUALITY CONTROL STATUS:  
# 0=draft, 1=in-review, 2=reviewed  
DOC.set_qc_status(0)","from ndreg import *
import matplotlib
import ndio.remote.neurodata as neurodata",,Ocean->8. Key Properties --> Resolution->8.1. Name,Toplevel->5. Key Properties --> Conservation --> Heat->22.4. Additional Information,"Branch Registration->Volumetric Registration and Analysis
3D Clarity Registration And Analysis->Volumetric Registration and Analysis"
27507,"s = mcq.apply(lambda x: pd.Series(x['BlogsPodcastsNewslettersSelect']),axis=1).stack().reset_index(level=1, drop=True)
s.name = 'platforms'","t = mcq.apply(lambda x: pd.Series(x['CoursePlatformSelect']),axis=1).stack().reset_index(level=1, drop=True)
t.name = 'courses'","#Store spend modeled by lognormal distribution
cust_df['store_spend']=np.exp(np.random.normal(3.5, 0.4, ncust))*cust_df['store_trans']",,"Notebook->Frequently Asked Questions->Q6. Can you recommend some blogs, podcasts, courses, etc. that I can follow?",Notebook->Data Science FAQ->Survey Demographics->Gender,R To Python Chapter 4->STEP 2: Creating the dataset for customer data->Adding horizontal and vertical lines to a basic scatterplot
235131,"nei = df[df['ratio_MW'] == min(df['ratio_MW'])]['Neighbou_1']
rat = df[df['ratio_MW'] == min(df['ratio_MW'])]['ratio_MW']
print(nei + ' - ' + str(rat))","print(""minimum age in test dataset = "" + str(test_df['age'].min()))
print(""maximun age in test dataset = "" + str(test_df['age'].max()))","def false_position(interval, f):
    """"""
    Input: Interval  --> [a, b] array where f(a)f(b) < 0
                  f  --> function to find the root
                tol  --> stopping criteria
    Output:       c  --> c-value where f(c) = 0
    
    """"""
    a = interval[0]
    b = interval[1]
    
    error = 1
    
    while True:
        c = (b*f(a) - a*f(b))/(f(a) - f(b))
        if f(c) == 0:
            break
        if f(a)*f(c) < 0:
            b = c
        else:
            a = c
    return c",,Python->Start the processing: your part,"Analysis-># # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #->program_id-><font id='dv6' color='blue'>program_duration</font><a href='#toc' style=""text-decoration: none;margin-left:10px""><font size='3px' color='blue' title='Table of Content'>&#8624;</font></a>-><font color='green'>From <a href='#program_duration-ttdc'>here</a> we know that 'program_duration' has 10 distinct int values, same in both train and test dataset.<br>So, we can consider it as a categorical variable. </font>",Method False Position->Method of False Position
164448,"# Let's see put it all together

Z = np.random.randint(100, size=(10,2))

plot_heatmap(Z)
print(get_nearest_neighbour(Z, 2))","#Angles in degree
azimuth = 34;
elevation = 90;


#Get the record of the nearest neighbour

record6dB, = daff6dB.GetNearestNeighbourRecord(34,65)
recordHRIR = daffHRIR.GetNearestNeighbourRecord(azimuth, elevation)
record3dB, = daff3dB.GetNearestNeighbourRecord(angle2=0,angle1=65)

#daff objects with spectral content types contain the property frequencies
frequencies3dB = daff3dB.Properties.Frequencies
frequencies6dB = daff6dB.Properties.Frequencies

#The number of channels and records
numChannels = daffHRIR.Properties.NumChannels
numRecords = daffHRIR.Properties.NumRecords


print('The object daffHRIR contains ' + str(numRecords) + ' records, each containing ' + str(numChannels) + ' channels.')

print('The magnitude of record3dB at ' + str(frequencies3dB[1]) + ' Hz is ' + str(record3dB[1]) + 
     ' and the magnitude of record6dB at ' + str(frequencies6dB[1]) + ' Hz is ' + str(record6dB[1]) + '. ')",fig.canvas.get_supported_filetypes_grouped(),,3->K - Nearest Neighbours->Showtime,Daff Example->DAFF example->Accessing the record data of the DAFF objects,Matplotlib->Getting Started With Matplotlib->Plotting Step by Step->Plotting to a File
53626,"tci = pd.merge(tci, prop[['totbldgs']], \
               how='left', left_on='parcel', right_index=True)
# tci.loc[tci.totbldgs.isnull(),'totbldgs'] = np.median(tci.loc[tci.totbldgs.notnull(),'totbldgs'])
tci.loc[tci.totbldgs.isnull(),'totbldgs'] = 1
print(tci.shape)","tci = pd.merge(tci, prop[['tbval']], how='left', left_on='parcel', right_index=True)
tci.loc[tci.tbval.isnull(),'tbval'] = np.median(tci.loc[tci.tbval.notnull(),'tbval'])

median_val = dict(tci[tci.tbval>0].groupby('SPA_NAME').agg(np.median)['tbval'])
tci.loc[tci.tbval==0, 'tbval'] = tci[tci.tbval==0].apply(lambda x: median_val[x.SPA_NAME], axis=1)
print(tci.shape)","import pycountry as pc

#Generate dict to translate Alpha2 Code to Country Name
alpha2_code = dict()
for country in list(pc.countries):
    alpha2_code[country.alpha2] = country.name",,2->2.0 Combine Data from ```prop.csv```->Residential characteristics->Total buildings,2->2.0 Combine Data from ```prop.csv```->Residential characteristics->Total market value,Apm Before 2Nd Meeting->3. Geo Info
346638,"plt.subplots(figsize = (15,5))
plt.title('Attack by Type1')
sns.boxplot(x = ""TYPE 1"", y = ""ATTACK"",data = df)
plt.ylim(0,200)
sns.plt.show()","cols = palettable.tableau.ColorBlind_10.hex_colors
sn.set_style(""whitegrid"")

plt.clf()
fig = plt.figure(figsize=(20,8))
plt.subplot(1,3,1)
sn.boxplot(x=""N"", y=""bdice"", hue='pipeline', hue_order=['fmriprep', 'fslfeat'],
           data=dataframe, palette=cols, linewidth=0.6)

plt.ylabel(""Binary Dice"")
plt.xlabel(""Sample size $N$"")

plt.subplot(1,3,2)
sn.boxplot(x=""N"", y=""fdice"", hue=""pipeline"", hue_order=['fmriprep', 'fslfeat'],
           data=dataframe, palette=cols, linewidth=.6)
plt.ylabel(""Fuzzy Dice"")
plt.xlabel(""Sample size $N$"")
plt.subplot(1,3,3)
sn.boxplot(x=""N"", y=""correlation"", hue=""pipeline"", hue_order=['fmriprep', 'fslfeat'],
           data=dataframe, palette=cols, linewidth=.6)
plt.ylabel(""Correlation"")
plt.xlabel(""Sample size $N$"")",p.keys(),,"Pokemon Analysis->Other statistical visualizations
Pokemon Stats Analyses And Visualization->All stats analysis of the pokemons
Pokemon->POKEMON STATS ANALYSIS->Let's get started with some Basic Analysis",02 Evaluation - Feat Comparison (5Mm Smoothing)->3. Second level analysis->3.3. Agreement between group activations,"Scoring->Ignore all this
Eval And Dataset Expts->Load the ladder model trained on MNIST->New Emboot dataset with 80/20 split"
132084,"fig, ax = plt.subplots(2, 1, figsize=(10,8))
sns.boxplot(data=utdata.normFeatures(dataC, ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], 'std'), ax=ax[0])
sns.boxplot(data=utdata.normFeatures(dataC, ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], 'maxmin'), ax=ax[1])
ax[0].set_title('Box plot: STD = 1')
ax[1].set_title('Box plot: MAX-MIN = 1')
fig.tight_layout()
fig.subplots_adjust(top=0.9)","fig = plt.figure(figsize=(16,40))

for i in range(0,40):

    ax = fig.add_subplot(10,4,i+1)

    data = [yes[i], no[i]]
    
    bp = ax.boxplot(data, patch_artist = True)
    bp['boxes'][0].set(color='#000000', linewidth=1)
    bp['boxes'][0].set( facecolor = '#66B2FF' )
    bp['boxes'][1].set(color='#000000', linewidth=1)
    bp['boxes'][1].set( facecolor = '#FF9999' )

    bp['medians'][0].set(color='#000000', linewidth=2)
    bp['medians'][1].set(color='#000000', linewidth=2)
        # get position data for median line
    ax.grid(alpha=.4,linewidth=.5,linestyle='--')
    ax.set_xticklabels(['valence ', 'no valence'])
    plt.title(i)

fig.tight_layout()
plt.show()","opt = L4.L4Adam(fraction=0.25) # (L4Adam)
#opt = L4.L4Adam() # default value (L4Adam*)
#opt = L4.L4Mom(fraction=0.25) # (L4Mom)
#opt = tf.train.AdamOptimizer(0.001, epsilon=1e-4) # (Adam)
#opt = tf.train.MomentumOptimizer(learning_rate=0.05, momentum=0.9) # (mSGD)
#opt = tf.train.GradientDescentOptimizer(learning_rate=0.7) # (SGD)

train_op = opt.minimize(cross_entropy)",,Data Analysis->Data analysis->Distribution characteristics->Box plots,Finalterm 2017572006 2017572012->Final term project->3.1 Threshold,L4 Mnist->L4 stepsize adaptation performance on MNIST->Optimizer choice
169896,"import numpy as np
import pandas as pd
import scipy
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn import datasets, preprocessing
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import accuracy_score
%matplotlib inline
from sklearn.svm import SVC, SVR
import operator","import sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.svm import LinearSVC
import seaborn as sns
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cross_validation import train_test_split
%matplotlib inline","# we will use only the first two (of four) features, so we can visualize
X = X_train[:, :2] 
h = .02  # step size in the mesh
dtc.fit(X, y_train)
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                      np.arange(y_min, y_max, h))
Z = dtc.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=y_train, cmap=cmap_bold)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
_ = plt.title(""3-Class classification (k = {})"".format(k))",,3->Thinkful Support Vector Machine Challenge,Sales Classification Sadat,12B-Classification-Ii-Demo->Classification in Practice->$k$- Nearest Neighbors
133491,"deliveries_df.query(""dismissal_kind == 'stumped'"")['batsman'].value_counts().head()","batsman_runs_counts = ipl_dataframe_delv[['total_runs',
                                          'batsman']
                                        ].groupby(['batsman']).total_runs.count().sort_values(ascending=False)
batsman_names = batsman_runs_counts.index.get_values()
max_runs_batsman_scored = batsman_runs_counts.max()
print('Best Batsman is '
      + batsman_names[0]
      + ' - '
      + str(max_runs_batsman_scored)
     )
batsman_runs_counts.head(50).plot(kind='pie',
                                  figsize=(10,10)
                                )","df = dfs.loc[:pd.to_datetime('2016-11-16 15:59:59')]
df.head()
df.tail()",,Data Analysis->Which batsman got stumped out maximum number of times?,Pandas-2-Ipl-Story->IPL Data Analysis - 2->6.Best Player - won max player of match title,Time Series With Gmmhmm->Data:
317963,e_car.accelerate(50),"slope2, intercept2 = np.polyfit(EUalcoholYear.alcohol, EUcarYear.car, 1)
CorrCarAlc=np.corrcoef(EUalcoholYear.alcohol,EUcarYear.car)[0,1].round(3).astype(float)

plt.scatter(EUalcoholYear.alcohol, EUcarYear.car)
plt.plot(EUalcoholYear.alcohol, EUalcoholYear.alcohol*slope2 + intercept2)
plt.title(""Alcohol consumption vs mortality in car accidents, selected EU countries"", loc='center', fontsize=12, fontweight=0, color='blue')
plt.xlabel(""Alcohol consumption per adult (15+), litres"")
plt.ylabel(""Mortality in car accidents (deaths/100.000 inh.)"")
plt.text(9.8,13, ""R: "" + str(CorrCarAlc),style='italic', size= 13,
        bbox={'facecolor':'white', 'alpha':0.5, 'pad':10})","x_mc = surrogate_mc(p_hat, T_hat, n_maps, len(x))
p_surr = p_empirical(x_mc, n_maps)
T_surr = T_empirical(x_mc, n_maps)
print(""\n\t\tSurrogate symbol distribution:\n"")
for i in range(n_maps): print(""\t\tp_{:d} = {:.3f}"".format(i, p_surr[i]))
print( ""\n\t\tSurrogate transition matrix:\n"" )
print_matrix(T_surr)",,Object Oriented Programming->Using objects,"Udacity Project 2->Project: Car mortality, alcohol consumption and gasoline prices in selected EU countries. Historical trends and relations->Data cleaning - Selection",Eeg Microstates->Information-theoretic analysis of EEG microstates in Python->(10) Markov surrogate example
140803,"# To make the comparison in time, I use 2015 first half year combined with 2014 last half year data to compare with age_gender_bkts 2015 year data
first_booking=train_users[train_users.date_first_booking>(train_users.date_first_booking.max()-pd.DateOffset(years=1))]
first_booking.date_first_booking.min()","# Load the data from the raw CSV files and combine it into a unified data frame
vsa05 = pd.read_csv('VSA05.csv', index_col=0, skiprows=2, na_values='..')
vsa10 = pd.read_csv('VSA10.csv', index_col=0, skiprows=2, na_values='..')
baby_names = vsa05.combine_first(vsa10)

# Initially, the rows are the names and the columns are the dates, so swap these
baby_names = baby_names.T

# Set the index, so that each year in the period 1964-2015 is represented (some are missing)
baby_names.index = baby_names.index.map(pd.to_datetime)
baby_names = baby_names.reindex(pd.date_range(baby_names.index.min(), baby_names.index.max(), freq='AS'))
assert baby_names.shape[0] == 1 + (2015 - 1964)","c = (x**3 for x in integers() if x > 0)
list(next(c) for _ in range(10)) # confirm the first 10 perfect cubes",,Eda->Inferential Statistics->Explore age_gender_bkts data,Analysis->Called Kian? It's probably because of this guy.->Irish baby name data,"Davidguo Hw5->Stats701 Homework 5, Winter 2018->2->2.2"
349622,"pca = decomposition.PCA(whiten=True, copy=True)
pca.fit(reshaped_array)
diffs = np.ediff1d(pca.explained_variance_ratio_)","dx = np.ediff1d(x)
dx","def generate_and_save_plot(plot_fname, with_country_labels=True):
    # CORE PLOTTING: http://ramiro.org/notebook/basemap-choropleth/
    # LABELS: http://introtopython.org/visualization_earthquakes.html
    # MINIMAL MAP: https://github.com/onenorth/interactive-svg-map/tree/master/src/data

    shapefile = ""data/shapefiles/ne_110m_admin_0_countries_lakes""
    fig = plt.figure(figsize=(22, 12))

    ax = fig.add_subplot(111, axisbg='w', frame_on=False)

    m = Basemap(lon_0=0, projection='robin')
    m.drawmapboundary(color='w')
    m.readshapefile(shapefile, 'units', color='#ededed', linewidth=.2)

    for info, shape in zip(m.units_info, m.units):    
        iso3 = info['adm0_a3']

        try:
            bin = df_countries.loc[df_countries['ISO3']==iso3,'bin'].iloc[0]
        except IndexError:
            bin = -1

        if bin < 0 :
            color = '#fcfcfc'
        else:
            color = scheme[bin]

        patches = [Polygon(np.array(shape), True)]
        pc = PatchCollection(patches)
        pc.set_facecolor(color)
        ax.add_collection(pc)

    # these coordinates are used to place the data for european collaborators
    eun_lat, eun_lon = -12.125004, 80.372757  # somewhere in the norwegian sea
    euw_lat, euw_lon = -36.001956, 59.468125  # somewhere in the north atlantic
    jpn_lat_lon      = (139, 30) # somewhere in the pacific

    for d in df_countries.loc[df_countries['collab_counts']>0][['ISO3','lat', 'lon', 'Country', 'collab_counts']] \
                .sort_values(by='collab_counts', ascending=False).itertuples(): 
        # require to project onto map with annotate
        lon, lat = m(d.lon, d.lat)

        eun_lat_lon_m = m(eun_lat, eun_lon)
        euw_lat_lon_m = m(euw_lat, euw_lon)

        title = d.Country.title()
        
        if with_country_labels:
            title_format_sm , title_format_lg = ""{} {}"", ""{}\n{}""
        else:
            title_format_sm , title_format_lg = """", """"

        m.plot(lon, lat, 'o', markersize=4, latlon=False)

        if d is False or title.lower() in eu_north:
            plt.annotate(title_format_sm.format(title, int(d.collab_counts)), \
                         xy=(lon, lat), xytext=eun_lat_lon_m, 
                         size='small', multialignment='center'),
    #                      arrowprops=dict(arrowstyle=""->"",
    #                                      connectionstyle=""arc3""))
            eun_lon -= 3
            eun_lat += 0.4
        elif d is False or title.lower() in eu_west:
            plt.annotate(title_format_sm.format(title, int(d.collab_counts)), \
                         xy=(lon, lat), xytext=euw_lat_lon_m, 
                         size='small', multialignment='center'),
    #                      arrowprops=dict(arrowstyle=""->"",
    #                                      connectionstyle=""arc3""))
            euw_lon -= 3
            euw_lat += 0.65
        elif title.lower() == 'japan': # japan and s. korean are overlapping, so we move it out into the pacific a bit
            jpn_lat_lon_m = m(*jpn_lat_lon)
            plt.annotate(title_format_lg.format(title, int(d.collab_counts)), \
                         xy=(lon, lat), xytext=m(139, 30), \
                         size='medium', multialignment='center')        
        else:
            plt.annotate(title_format_lg.format(title, int(d.collab_counts)), \
                         xy=(lon, lat), xytext=(lon, lat), \
                         size='medium', multialignment='center')

    plt.savefig(plot_fname, dpi=300, bbox_inches='tight', pad_inches=.2)
    plt.show()",,5->Final-project related fuckery->PCA,101 Experiement2->PHYS 101/121 Experiment 2 Analysis,"Implementation Note->Now that we have the countries, counts and centroids ... we can get some maps drawn!->SOLUTION 2: BASEMAP"
186706,"### Calculate the accuracy for these 5 new images. 
### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images.
tf.reset_default_graph()

x = tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], 1], name='x')
y = tf.placeholder(tf.int32, shape = [None], name = 'y')
keep_prob = tf.placeholder(tf.float32)
one_hot_y = tf.one_hot(y, n_classes)

mu = 0
sigma = 0.01

logits, _ = Convnet(x, keep_prob, mu, sigma)
predictions = tf.nn.softmax(logits)
saver = tf.train.Saver()

with tf.Session() as sess:
    saver.restore(sess, ""/tmp/model.ckpt"")
    web_accuracy = evaluate_accuracy(sess, X_web_normalized, y_web, predictions, one_hot_y)
    print(""Web Accuracy = {:.3f}"".format(web_accuracy))","tf.reset_default_graph()

x = tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], 1], name='x')
y = tf.placeholder(tf.int32, shape = [None], name = 'y')
keep_prob = tf.placeholder(tf.float32)
one_hot_y = tf.one_hot(y, n_classes)

mu = 0
sigma = 0.01

logits, _ = Convnet(x, keep_prob, mu, sigma)
predictions = tf.nn.softmax(logits)
saver = tf.train.Saver()

with tf.Session() as sess:
    saver.restore(sess, ""/tmp/model.ckpt"")
    test_predictions = sess.run(tf.argmax(predictions, 1), feed_dict = {x: X_test_normalized, y: y_test, keep_prob: 1.0})
    test_accuracy = evaluate_accuracy(sess, X_test_normalized, y_test, predictions, one_hot_y)
    print(""Test Accuracy = {:.3f}"".format(test_accuracy))
    index = random.randint(0, len(X_test))
    predicted_class = test_predictions[index]
    predicted_label = signnames_df.loc[signnames_df['ClassId'] == predicted_class]
    print(predicted_label)
    actual_class= y_test[index]
    image = X_test[index].squeeze()
    plt.figure(figsize = (1, 1))
    plt.imshow(image)
    print(""Actual Class is {0} & Predicted Class {1}"".format(actual_class, predicted_class))","for ix, col in enumerate(train.columns):
    print(ix,col)",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Analyze Performance,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Lets visualize how cost has changed over time,Classification Of Loan Data->Some observations:->Basic MLP Neural Network->A list of all variables and their index numbers to be used when slicing data in the following steps
484244,"# confidence interval for SAT Participation, default alpha = 0.05
sms.DescrStatsW(sat['Participation']).tconfint_mean()","sms.DescrStatsW(patient_data[""amount""]).tconfint_mean()","knn_clf.fit(X_train_scaled,y_train)
y_test_pred=knn_clf.predict(X_test)",,Starter-Code->Step 3: Visualize the data->27. Generate and interpret 95% confidence intervals for SAT and ACT participation rates.,Code And Analysing Process->5. Analysing correlations and visualization->(2). Visualizing some features to verify assumptions->Visualize total bill amount distribution,Mnist->Exersice 1
140081,"#what's the percentage that the new user convert
full_data[full_data.new_user == 1].converted.value_counts() / full_data[full_data.new_user == 1].shape[0] * 100","tmp = full_data[full_data.converted == 1].groupby(['source', 'country']).converted.count() / full_data.groupby(['source', 'country']).converted.count() * 100","ax = graphical_iteration(I, logistic(2.8), 0.1, final=1, do_slope=True)",,Eda->Objectives->New User->Age,Eda->Objectives->New User->Age,Feigenbaum Period Doubling->Part A. The period doubling cascade in unimodal maps->Zoom on the birth of the period-$3\cdot 2^n$ period-doubling cascade
438555,"from sklearn.metrics import precision_score, recall_score


precision_score(y_train_5, y_train_predict)
recall_score(y_train_5, y_train_predict)","rf_score = cross_val_predict(rf_clf, X_train, y_train_5, cv=3, method='predict_proba')","np.char.center(states_array, 15, '*')",,Classification->Performence Measures,Chapter3->MNIST,Num Py-String-Functions
167174,"# 1. What is the proportion of coffee drinkers in the sample? What is the proportion of individuals that don't drink coffee?
coffee_red.drinks_coffee.mean()",kitty.drink(),"#Logistic Regression
from sklearn.linear_model import LogisticRegression
logmodel = LogisticRegression()
logmodel.fit(x_train,y_train)
log_val_pred=logmodel.predict(x_val)
print('The validation accuracy is',np.mean(log_val_pred==y_val))",,3->We just use a bootstrapping to build a confidence interval !,4->Class->2. Linked List->B. Sort in Class->Array,Data Science London
302058,"# good fit - simulate directly against expected
synthExp_gf = expStd.process_synthExp(expStd.buildSynthPop(10000, 5000, 1000000, 1, 20,0))

# not good fit - add distortion to simulation 
synthExp_ngf = expStd.process_synthExp(expStd.buildSynthPop(10000, 5000, 1000000, 1, 20,0.1))","#Building the test dataset
X_synth_test,y_synth_test = build_synth_data(X_test,y_test,10000)","n_b = len(data[data.race=='b']) # number of blacks applied for job
n_w = len(data[data.race=='w']) # number of whites applied for job
n_bcald = sum(data[data.race=='b'].call) # number of blacks called for interview
n_wcald = sum(data[data.race=='w'].call) # number of whites called for interview
p_b = n_bcald/n_b #Probability of calling a black for interview = Mean of corresponding bernouli distribution
p_w = n_wcald/n_w #Probability of calling a white for interview = Mean of corresponding bernouli distribution
    
sd_b = np.sqrt(p_b*(1-p_b)) #Standard deviation of blacks distribution
sd_w = np.sqrt(p_w*(1-p_w)) #Standard deviation of whites distribution

n_b,p_b,sd_b,n_w,p_w,sd_w",,Advanced Analytics Frame->Prescriptive analytics and beyond->How was the data made,Digit Recognition-Mnist-Sequence->Deep Learning Project->Implementation,Sliderule Dsi Inferential Statistics Exercise 2-Mahesh Yerra->Examining Racial Discrimination in the US Job Market
46739,"def print_first_point(filename):
    """"""
    This function prints and returns the first data point (second row) from
    a csv file that includes a header row.
    """"""
    # print city name for reference
    city = filename.split('-')[0].split('/')[-1]
    print('\nCity: {}'.format(city))
    
    with open(filename, 'r') as f_in:
        ## TODO: Use the csv library to set up a DictReader object. ##
        ## see https://docs.python.org/3/library/csv.html           ##
        trip_reader = csv.DictReader(f_in)
        
        ## TODO: Use a function on the DictReader object to read the     ##
        ## first trip from the data file and store it in a variable.     ##
        ## see https://docs.python.org/3/library/csv.html#reader-objects ##
        first_trip = trip_reader.__next__()
        
        ## TODO: Use the pprint library to print the first trip. ##
        ## see https://docs.python.org/3/library/pprint.html     ##
        pp(first_trip)
        
    # output city name and first trip for later testing
    return (city, first_trip)

def pp(data):
    """"""
    Helper function for pretty printing. Rather than print, call 'pp([data])'
    to see data in prettified format.
    """"""
    return pprint.PrettyPrinter(indent=4).pprint(data)

# list of files for each city
data_files = ['./data/NYC-CitiBike-2016.csv',
              './data/Chicago-Divvy-2016.csv',
              './data/Washington-CapitalBikeshare-2016.csv',]

# print the first trip from each file, store in dictionary
example_trips = {}
for data_file in data_files:
    city, first_trip = print_first_point(data_file)
    example_trips[city] = first_trip

print(""\n"")
pp(example_trips)","def print_first_point(filename):
    """"""
    This function prints and returns the first data point (second row) from
    a csv file that includes a header row.
    """"""
    # print city name for reference
    city = filename.split('-')[0].split('/')[-1]
    print('\nCity: {}'.format(city))
    
    with open(filename, 'r') as f_in:
        ## Used the csv library to set up a DictReader object. ##
        
        trip_reader = csv.DictReader(f_in)
        
        ## Used a function on the DictReader object to read the     ##
        ## first trip from the data file and store it in a variable.     ##
        
        first_trip = trip_reader.__next__()
        
        ## Used the pprint library to print the first trip. ##
       
        pprint(first_trip)
        
    # output city name and first trip for later testing
    return (city, first_trip)

# list of files for each city
data_files = ['./data/NYC-CitiBike-2016.csv',
              './data/Chicago-Divvy-2016.csv',
              './data/Washington-CapitalBikeshare-2016.csv',]

# print the first trip from each file, store in dictionary
example_trips = {}
for data_file in data_files:
    city, first_trip = print_first_point(data_file)
    example_trips[city] = first_trip","def nnModel(num_layers, layer_units, X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,
          num_epochs = 1500, minibatch_size = 64):
    # allows the graph to run without overwriting the graph
    ops.reset_default_graph()
    
    # no. of features and training examples 
    (n_x, m) = X_train.shape
    # no. of minibatches
    num_minibatches = math.floor(m/minibatch_size)
    # no. of units in output layer
    n_y = Y_train.shape[0]
    
    # for stroing the costs 
    costs = []
    
    ## Set the computational graph in place 
    # create placeholders
    X, y = createPlaceholders(n_x, n_y)
    
    # initialize the parameters
    parameters = initializeParam(num_layers, layer_units)
    
    # do forward propagation
    Z = forwardPropagate(num_layers, layer_units, X, parameters)
    
    # for computing the cost
    cost = computeCost(Z, y)
    
    # setting the optimization technique
    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)
    
    # initilaze all the tf variables
    init = tf.global_variables_initializer()
    
    # start a tf session and start training the neural network
    with tf.Session() as sess:
        # initilaize variables
        sess.run(init)
       
        for epoch in range(num_epochs):
            epoch_cost = 0
            # get the shuffled mini batches
            minibatches = getRandomMiniBatches(X_train, Y_train, minibatch_size)
            
            # start with each mini batch
            for minibatch in minibatches:
                # curr mini batch
                (minibatch_X, minibatch_y) = minibatch
                (minibatch_cost, _) = sess.run([cost, optimizer], feed_dict = {X:minibatch_X, y:minibatch_y})
                
                epoch_cost += minibatch_cost
                
            epoch_cost = epoch_cost/num_minibatches
            
            # printing the cost to see progress
            if epoch % 50 == 0:
                print('Cost for epoch %i: %f' % (epoch, epoch_cost))
           
            # storing the epoch cost for plotting 
            if epoch % 5 == 0:
                costs.append(epoch_cost)

        print (""Training done"")    
        
        # save and return the parameters so that it can be used again
        parameters = sess.run(parameters)
        
        return parameters, costs",,Bike Share Analysis,P1 Bike Share Analysis,Deep L Layer Nn-><u>Preprocessing Step</u>-><u>Backward Propagation and Optimization
147177,foo(2),"foo(10), foo(20)","# define Deren's function to count # of unique species
def count_unique_values(series):
    ""counts the number of unique species""
    return np.unique(series).size

# call the function to count number of unique species in each state.
(eur.df[eur.df.species.notna()]
 .groupby(""stateProvince"")
 .species
 .apply(count_unique_values)
)",,Lecture 2->Restart Kernel,05-Sequence-Alignment->Where Are We Now?,Nehasavant Machine-Learning->Neha Savant's Machine Learning Assignment->My Question: Can you use the month that Eurycea salamanders are found to predict the state they were found in?
158424,"change_8 = get_change('eight_regions', mat_mort_regions, 1980, 2013).sort_values(by=['mean'])
change_8","df_region.sort_values(['Happiness Score'],ascending=False, inplace=True)","cars = glob.glob('./vehicles/**/*.png', recursive = True)
notcars = glob.glob('./non-vehicles/**/*.png', recursive = True)

print('There are {} images in total who contained a car. And,'.format(len(cars)))
print('there are {} images in total who contained no car.'.format(len(notcars)))",,Investigate A Dataset->Extending the data set: region variables->Region groups,Project 1->B. Indexing and grouping->B.1,Hog Classifier V3->Data Exploration
288297,"list_data = data.columns[data.isnull().sum() > 0].tolist()
list_test = test.columns[test.isnull().sum() > 0].tolist()
test[list(i for i in list_test if i not in list_data)].info()","projy = pd.DataFrame(data = df['label'], columns=['label'])

y_levels = []
column_names = []
for i in range(0, 28):
    y_levels.append(df.columns.values.tolist()[28*i+1:28*i+29])
    column_names.append('proj'+str(i))
    
for i in range(0,28):
    projy[column_names[i]] = df[y_levels[i]].sum(axis=1)
    
projy","from __future__ import print_function
from ipywidgets import interact, interactive, fixed, interact_manual
import ipywidgets as widgets

nb_menu = {
    '1. Arithmetic operations': 'arithmetic/SuaveArithmetic.ipynb',
    '4. Color Characteristics': 'colors/ColorCharacteristics.ipynb',
    '3. Classify Images': 'classify/ImageClassify.ipynb',
    '2. Predictive models': 'predict/PredictiveModel.ipynb',
    '5. SVM models': 'svm/SVMPredictiveModel.ipynb',
}

def f(notebooks_menu):
    return notebooks_menu
# out = interact(f, notebooks_menu=nb_menu);
out = interact(f, notebooks_menu=nb_menu.keys());",,"House Prices-><a name=""datprep"">Data preparation</a>",Y Project Predictions->Classification Under y-Axis Projection->Initial Handling of Data,"Suave Dispatch-Beautified->Welcome to the SuAVE Jupyter Notebook Server->4. Now, select a notebook to do some work"
482722,"#Check
print(my_dict)","my_dict['pie'] = 4
print(my_dict)","jobW_txt.concordance(""nltk"")",,Dictionaries->Dictionaries->Assigning New Keys to a Dictionary,Python Dri->Functions!->Type Examples,Lin127 Final Project->Text Mining
419476,"boston.keys()
boston.data.shape
boston.feature_names
boston.target",boston = load_boston() # load the boston data set,"res = []

with conn.cursor() as cur:
    for t in tables:
        cur.execute(""select count(1) from core_logic_2018.{}"".format(t))
        res.append({'table': t, 'count': cur.fetchone()[0]})",,Elastic Net->Print many statements same time,Ex10->ISLR: Chapter 2 Exercise 10,Parcel-Loading->Load into individual parcel tables in chunks
35347,"def costFunction(theta,X,y):
    theta = np.matrix(theta) 
    X = np.matrix(X)
    m = len(X)
    y = np.matrix(y)
    
    # vectorized implementation, computing model's predictions in one statement. 
    left = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    right = np.multiply((1-y), np.log(1 - sigmoid(X * theta.T)))
    # divide by m 
    return np.sum(left - right) / m","def cost(theta, X, y):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X * theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X * theta.T)))
    return np.sum(first - second) / (len(X))","tmdb_uniq_genres=tidy_split(tmdb_uniq_genres,'cast')
len(tmdb_uniq_genres)",,Logistic Regression,"01 - Logistic Regression - Uni Variate->Manual Implementation->Algorithm
Ml-Exercise2->Logistic regression",Tmdb Dataset Resubmit
174624,"# Calculate stress state at each atomic position
system.atoms.stress = stroh.stress(system.atoms.pos)","pos_test = uc.set_in_units(np.array([12.4, 13.5, -10.6]), 'angstrom')

disp = stroh.displacement(pos_test)
print ""displacement ="", uc.get_in_units(disp, 'angstrom'), 'angstrom'","import warnings
warnings.simplefilter('ignore')
    
from gensim import models
model = models.Doc2Vec.load(""doc2vec_dbow_d100_n5_w15_mc5_s1e_05_t6.model"")",,04->Introduction to atomman: Stroh class->5. Position-dependent calculations->5.3 stress,Atomman->Find the displacement at a point,"Part 6 Semantic Characterization Of Words, Posts, And Authors
Part 5 Temporal Analysis - Linguistic Divergence, Bigram Bow And Doc2Vec Model->2. Semantic distance using vector arithmetics"
315239,"sse = {}
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=99).fit(data_3rfm)
    cluster_labels = kmeans.labels_
    sse[k] = kmeans.inertia_
    # Inertia = Sum of squared distances of samples to their closest cluster center.

plt.title('Distortion Score Elbow for KMeans')
sns.pointplot(x=list(sse.keys()), y=list(sse.values()))
plt.show()","#Kmeans
sse_list = []

for k in range(2,20):
    sse_k = []
    
    for seed in [1,7,42,123,456,111,222,333,444,555]:
        kmean = KMeans(n_clusters=k, random_state=seed)
        cluster_labels = kmean.fit_predict(df6)
        sse_score = kmean.inertia_

        sse_k.append(sse_score)
        
    sse_list.append(np.mean(sse_k))
    
f, ax = plt.subplots(figsize=(7, 5))
fig = plt.plot(list(range(2,20)),sse_list)
ax.set_title('Elbow plot of WSS Vs K')
x_axis = plt.xlabel('No. of Clusters,K')
y_axis = plt.ylabel('Within Sum of Squares')
plt.show()","mod202 = sm.tsa.statespace.SARIMAX(global_temp['LandAverageTemperature'], order = (2, 0, 2), 
                                   seasonal_order=(0,1,1,12), enforce_stationarity = False, 
                                   enforce_invertibility = False).fit()
print('Model', 'AIC', 'BIC', 'HQIC')
print('ARIMA(2,0,2)x(0,1,1)_12', mod202.aic, mod202.bic, mod202.hqic)",,"Customer Segmentation Rfm Kmeans->4. Customer segmentation by clustering->4.1. Clustering with k-means on tier 1 metrics->Evaluate the number of clusters using Elbow Method
Customer Segmentation Rfm Kmeans->3. Create RFM table and analyze RFM segments->7-cluster solution",1->Part I : Unsupervised Learning->Deliverable 1: Complete the Clustering Case Study: Using PCA and clustering to uncover customer segments->K Means,Temperature Sarima->Exploring Historical Global Temperatures->Modeling the Data (Seasonal ARIMA)->Forecast Performance:
287657,comb = comb[comb['Id'] != 524],"comb(100, 10)",median_py = statistics.median(df['age']),,House Prices->House Prices: Advanced Regression Techniques->Removing outliers,Homework 2 Key->Answer 1.5,Unit 3 - Statistics For Data Science->Lesson 1 - Summarizing Data->Measures of Central Tendency->Median (using built-in Python functionality)
427330,walks,walk.max(),"cota_stops = cota_stops[(cota_stops.geometry.x >= -83.1) &
                                 (cota_stops.geometry.x <= -83.0) &
                                 (cota_stops.geometry.y >= 39.9) &
                                 (cota_stops.geometry.y <= 40.0)]
len(cota_stops)",,"Ch4 - Num Py Basics->1. The NumPy ndarray->7.1 Simulating Many Random Walks at Once
Pandas For Data Analysis->1. Numpy
Deep Walk - Walkthrough->Graph > Build_deepwalk_corpus",Random Walk->Random Walk,Geospatial->Geospatial Analytics->COTA Data
105252,"plt.scatter(df['gdp'], df['population'])
plt.suptitle('GDP by Population')
plt.xlabel('GDP')
plt.ylabel('Population')
plt.show;","plt.scatter(df['head-size'], df['brain-weight'])
plt.xlabel('Head size (cm^3)')
plt.ylabel('Brain weight (grams)');","sns.pairplot(df_best_fits.assign(colour_category=pd.cut(np.log(df_best_fits['dist_colour_kl']), bins=10)),
             hue='colour_category',
             palette=mycubehelix,
             x_vars=['M', 'ratio_conj', 'sigmax'],
             y_vars=['dist_colour_log', 'dist_colour_kl'],
             size = 6
            )",,Capstone->Capstone Final Project -  Olympic Medal Predictor->Discussion,1->1. Linear Regression->Visualize the Dataset,Model Fitting Interactive Try->Dual Recall data fitting->Pandas + Seaborn plots->=> Confirms what we saw before:
275952,"X.loc[1916, 'MSZoning'] = 'RM'
X.loc[2217, 'MSZoning'] = 'C (all)'
X.loc[2251, 'MSZoning'] = 'RM'
X.loc[2905, 'MSZoning'] = 'RL'
print('MSSubClass missing values filled.')","#-Multiple Conditions-#
if x > 0:
    print(""x=%s is > 0""%x)
elif x == 0:
    print(""x=%s is = 0""%x)
else:
    print(""x=%s is < 0""%x)","import numpy as np
import pandas as pd
from collections import defaultdict

import matplotlib.pyplot as plt
plt.rcParams[""figure.figsize""] = (10,5)",,Kernel->Modeling->Lasso->Out of fold residual analysis->Sidequest,Intro-To-Python-Two->Intro to Python (Part 2)->Conditional Statements,Data Exploration
346769,"v = np.random.randint(0,50,size=(6,6))
print(v)","v = np.random.randint(0,50,size=(6,6))
print(v)","def get_impfeature_names(indices, text, gene, var, no_features):
    gene_count_vec = CountVectorizer()
    var_count_vec = CountVectorizer()
    text_count_vec = CountVectorizer(min_df=3)

    gene_vec = gene_count_vec.fit(train_df['Gene'])
    var_vec  = var_count_vec.fit(train_df['Variation'])
    text_vec = text_count_vec.fit(train_df['TEXT'])
    
    fea1_len = len(gene_vec.get_feature_names())
    fea2_len = len(var_count_vec.get_feature_names())
    
    word_present = 0
    for i, v in enumerate(indices):
        if v < fea1_len:
            word = gene_vec.get_feature_names()[v]
            yes_no = True if word == gene else False
            if yes_no:
                word_present += 1
                print(i, ""Gene feature [{}] present in test data point [{}]"".format(word,yes_no))
        elif v < (fea1_len + fea2_len):
            word = var_vec.get_feature_names()[v-(fea1_len)]
            yes_no = True if word == var else False
            if yes_no:
                word_present += 1
                print(i, ""variation feature [{}] present in test data point [{}]"".format(word,yes_no))
        else:
            word = text_vec.get_feature_names()[v-(fea1_len+fea2_len)]
            yes_no = True if word in text.split() else False
            if yes_no:
                word_present += 1
                print(i, ""Text feature [{}] present in test data point [{}]"".format(word,yes_no))
                
    print(""Out of the top "",no_features,"" features "", word_present, ""are present in query point"")",,Numpy Assignment->find the closest value (to a given scalar) in a vector,Numpy Assignment->find the closest value (to a given scalar) in a vector,Personalized-Cancer-Diagnosis->4. Machine Learning Models
343771,"model = LinearRegression(normalize=True)
print(model)","regress = LinearRegression()
regress.fit(data[x].reshape(-1, 1), data[y])","precision_all = []
recall_all = []

probabilities = model.predict(test_data, output_type = 'probability')
for threshold in threshold_values:
    predictions = apply_threshold(probabilities, threshold)
    
    precision = graphlab.evaluation.precision(test_data['sentiment'], predictions)
    recall = graphlab.evaluation.recall(test_data['sentiment'], predictions)
    
    precision_all.append(precision)
    recall_all.append(recall)",,"Intro->Introduction to Scikit-Learn: Machine Learning with Python->The Scikit-learn Estimator Object
Introduction Machine Learning->Regresion->Linear Regresion",St Ds Us->Sexually Transmitted Diseases and Public Education->Types of Disease->Counts of Categories of Disease Reported->Total counts between 1984 and 2014,09 Precision And Recall->Precision-recall tradeoff->Precision-recall curve
162381,"raw_data = raw_data.drop(raw_data[(raw_data['rating']==0)].index)

raw_data['rating'].replace([1.25, 1.875, 2.5, 3.125, 3.75, 4.375, 5], [2, 3, 4, 5, 6, 7, 8], inplace=True)","raw_data_target = raw_data['Cancelled']
raw_data.drop('Cancelled', axis=1, inplace=True)","plt.plot(X_pacf, 'ro')
plt.xlabel('Lag')
plt.ylabel('Autocorrelation')
plt.title(""PACF"");",,3,Airlines Cancellation Flights Nikita Simonov->7. Data preprocessing,Notebook->Autoregressive (AR) Models->Testing for AR Behavior
461645,w.BayRidge(),"ridgReg.score(x_cv, y_cv)","spock_account = Account('Spock')
spock_account.deposit(100)",,"Starter Reg->BayesianRidge(n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)[source]","Ridge, Lasso And Elastic Net Regressions->Import test and train files->Ridge Regression",2->2.5.4 Class Attributes
316723,"# copy 'timestamp', 'favorite_count', and 'retweet_count'
tweet_counts_by_hour = tweet_archive_master[['timestamp', 'favorite_count', 'retweet_count']].copy()
# isolate hour of day
tweet_counts_by_hour.timestamp = tweet_counts_by_hour.timestamp.map(lambda x: x.hour)
# group by hour of day and concatenate total tweet count
tweet_counts_by_hour = pd.concat([tweet_counts_by_hour.groupby('timestamp').sum(), 
                                  tweet_counts_by_hour.groupby('timestamp').count()['favorite_count'].rename('tweet_count')],
                                 axis = 1)
tweet_counts_by_hour.index.names = ['hour of day']
tweet_counts_by_hour.plot(title='Favorite, Retweet, and Total Tweet Counts by Hour of Day');","tweet_counts_by_hour = pd.concat([tweet_counts_by_hour.groupby('timestamp').sum(), 
                                  tweet_counts_by_hour.groupby('timestamp').count()['favorite_count'].rename('tweet_count')],
                                 axis = 1)
tweet_counts_by_hour.index.names = ['hour of day']
tweet_counts_by_hour.plot()","def cleanSpaces(x):
    x.strip()
    return x",,Act Report->Historical analysis of WeRateDogs tweets->Analysis->Tweet counts by month->Tweet counts by hour of day,Wrangle Act->Table of Contents->Verify,Us Permanent Visa Applications->U.S. Permanent Visa - Certified or Denied?->Interesting Findings->Second Finding->Using a strip function to remove unwanted spaces & using map to feed data.
416278,"def add_bias(X):
    m,n = X.shape
    return np.append(np.ones(m)[...,None], X , 1)

def add_zero(X):
    m,n = X.shape
    return np.append(np.zeros(m)[...,None], X , 1)

def del_bias(X):
    return np.delete(X, 0, 1)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_gradient(z):
    return sigmoid(z) * (1 - sigmoid(z)) #   *: multiple elementwise

def initial_theta(s1, s2):
    epsilon = np.sqrt(6) / np.sqrt(s1+s2)
    return np.random.rand(s2, s1 + 1) * 2 * epsilon - epsilon

def cost_function(nn_params, s1, s2, s3, X, y, ld):
    J = 0 # cost value
    m, n = X.shape
    
    Theta1 = np.reshape(nn_params[:s2*(s1+1)], (s2, s1+1))
    Theta2 = np.reshape(nn_params[s2*(s1+1):], (s3, s2+1))

    Theta1_grad = np.zeros(Theta1.shape);
    Theta2_grad = np.zeros(Theta2.shape);
    # Forward propagation
    a1 = add_bias(X)
    z2 = np.dot(a1, Theta1.T)
    a2 = add_bias(sigmoid(z2))
    z3 = np.dot(a2, Theta2.T)
    a3 = sigmoid(z3)
    
    h = a3
    
    # Compute Costfunction
    Jred = ld * (sum(sum(np.square(del_bias(Theta1)))) +  sum(sum(np.square(del_bias(Theta2))))) / (2 * m)
    J = sum(sum(-y * np.log(h) - (1-y) * np.log(1-h))) / m + Jred
    
    return J

def cost_gradient(nn_params, s1, s2, s3, X, y, ld):
    Theta1 = np.reshape(nn_params[:s2*(s1+1)], (s2, s1+1))
    Theta2 = np.reshape(nn_params[s2*(s1+1):], (s3, s2+1))
    Theta1_grad = np.zeros(Theta1.shape);
    Theta2_grad = np.zeros(Theta2.shape);
    
    # Forward propagation
    a1 = add_bias(X)
    z2 = np.dot(a1, Theta1.T)
    a2 = add_bias(sigmoid(z2))
    z3 = np.dot(a2, Theta2.T)
    a3 = sigmoid(z3)
    
     # Compute Gradient
    delta3 = a3 - y
    delta2 = del_bias((np.dot(delta3, Theta2) * sigmoid_gradient(add_bias(z2))))
    
    Theta2_grad = np.dot(delta3.T, a2)/m + ld * add_zero(del_bias(Theta2))/m
    Theta1_grad = np.dot(delta2.T, a1)/m + ld * add_zero(del_bias(Theta1))/m

    grad = np.concatenate((Theta1_grad.reshape(-1), Theta2_grad.reshape(-1)))
    
    return grad","def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, no_labels, X, y, lam):
    
    #t1 = time.time()
    
    Theta1 = nn_params[:hidden_layer_size*(input_layer_size + 1)].reshape(hidden_layer_size, (input_layer_size + 1))
    Theta2 = nn_params[hidden_layer_size * (input_layer_size + 1 ) :].reshape(no_labels, (hidden_layer_size + 1))
    
    m = X.shape[0]
    
    #J = 0;
    #Theta1_grad = np.zeros_like(Theta1);
    #Theta2_grad = np.zeros_like(Theta2);

    #convert y to y matrix
    ymat = np.zeros((m, no_labels));
    for i in range(m):
        ymat[i, y[i]] = 1;

    #hypothesis
    hypo = np.zeros_like(ymat)
    
    # add ones to the X data matrix so the shape is 5000 * 401
    ones = np.ones((m, 1))
    X = np.concatenate((ones, X), axis = 1)
    n = X.shape[1]
    
    # cummulative theta derivatives initialization
    accuDELTA1 = np.zeros_like(Theta1)
    accuDELTA2 = np.zeros_like(Theta2)
    
    #For loop
    for i in range(m):
        a1 = X[i]
        z2 = np.dot(a1, Theta1.transpose())
        a2 = sigmoid(z2)

        #add bias
        a2 = np.append([1], a2)
        z3 = np.dot(a2, Theta2.transpose())
        a3 = sigmoid(z3);
        hypo[i] = a3
        
        #backprop
        #delta calculation, here the deltas are gradients or 
        # partial derivatives for activation nodes a
        
        delta3 = a3 - ymat[i];
        delta2 = np.dot(delta3, Theta2) * (a2 * (1 - a2))
        delta2 = delta2[1:]
        
        accuDELTA2 += delta3.reshape(delta3.shape[0], 1) * a2
        accuDELTA1 += delta2.reshape(delta2.shape[0], 1) * a1

    Theta1_grad = accuDELTA1 / m
    Theta2_grad = accuDELTA2 / m
    #add regularization term
    Theta1_grad[:, 1:] += lam/m * Theta1[:, 1:]
    Theta2_grad[:, 1:] += lam/m * Theta2[:, 1:]
    
    grads = np.append(Theta1_grad.flatten(), Theta2_grad.flatten())
    
    
    Junregularized = 1/m * ((-ymat) * np.log(hypo) - (1 - ymat) * np.log(1- hypo))
    Junregularized = np.sum(Junregularized)
    
    #add regularization term
    Jregularization = lam / (2 * m)*(np.sum(Theta1[:, 1:]**2) + np.sum(Theta2[:, 1:]**2))
    J = Junregularized + Jregularization;
    
    
    #print(""\ncost: "" + str(J))

    #t2 = time.time()
    #print(""time:"" + str(t2-t1))

    return(J, grads)
    #return(J)","# Print the precision, recall and f1 scores
print('Precision: ', np.mean(cross_val_score(log_reg, counts, train_list[CLASS], cv=4, scoring='precision_weighted')))
print('Recall:    ', np.mean(cross_val_score(log_reg, counts, train_list[CLASS], cv=4, scoring='recall_weighted')))
print('F1:        ', np.mean(cross_val_score(log_reg, counts, train_list[CLASS], cv=4, scoring='f1_weighted')))",,Neural Network->Digit recognition using neural network->2. Define function,Neural Networks->nnCostFunction,3First Classification->Investigate the errors by computing some scores
33229,"# X_train's variance, rounding the output to 3 decimal places
# ... YOUR CODE FOR TASK 8 ...
X_train.var().round(3)","round(np.var(x[:,1], ddof=1),3)","seq3 = [False, True]
list(zip(seq1, seq2, seq3))",,Notebook->8. Checking the variance,Linear Regression->Variance,"Ch03->Built-in Data Structures, Functions,->Data Structures and Sequences->Errors and Exception Handling->zip"
42355,bike_rentals['time_label'] = bike_rentals['hr'].apply(assign_label),"bike_rental['season'] = bike_rental.season.map({1: ""Spring"", 2 : ""Summer"", 3 : ""Fall"", 4 :""Winter"" })","%matplotlib inline

# Basic import(s)
import os
import sys
import numpy as np
import itertools
import matplotlib.pyplot as plt
from scipy.linalg import block_diag
from sklearn.metrics import roc_auc_score
from IPython.display import Image

# Set Keras backend
os.environ['KERAS_BACKEND'] = 'tensorflow'

# Keras import(s)
import keras
import keras.backend as K
from keras.optimizers import Adam
from keras.engine.topology import Layer
from keras.utils.vis_utils import plot_model

# TensorFlow import(s)
import tensorflow as tf

# Project import(s)
# -- Add module path
module_path = os.path.abspath(os.path.join('..'))
if module_path not in sys.path:
    sys.path.append(module_path)
    pass
from egamma.utils import *",,"Basics->Features->Hours
Project - Predicting Bike Rentals->Goal:->2. Exploratory Data Analysis:
Project - Predicting Bike Rentals->Predicting Bike Rentals
Predicting Bike Rentals->Calculating Features
Guided Project Predicting Bike Rentals->Data Analysis and Feature Engineering
Project- Predicting Bike Rentals->2. Exploratory Data Analysis->Study Correlation",Bike Rental Exploratory Data Analysis (Eda)->Get the Data and explore it,Toys->Training mutually independent classifiers->Initialisation
331777,"images_per_row = 16

# Now let's display our feature maps
for layer_name, layer_activation in zip(layer_names[:-1], activations[:-1]):
    # This is the number of features in the feature map
    n_features = layer_activation.shape[-1]

    # The feature map has shape (1, size, size, n_features)
    size = layer_activation.shape[1]

    # We will tile the activation channels in this matrix
    n_cols = n_features // images_per_row
    display_grid = np.zeros((size * n_cols, images_per_row * size))

    # We'll tile each filter into this big horizontal grid
    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0,
                                             :, :,
                                             col * images_per_row + row]
            # Post-process the feature to make it visually palatable
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype('uint8')
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    # Display the grid
    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, aspect='auto', cmap='magma')
    
plt.show()","# Code used from book https://www.manning.com/books/deep-learning-with-python
layer_names = []                                                          
for layer in model.layers[:6]:                                            
    layer_names.append(layer.name)                                        

images_per_row = 16

for layer_name, layer_activation in zip(layer_names, activations):
    if 'conv' in layer_name:
        n_features = layer_activation.shape[-1]                               

        size = layer_activation.shape[1]                                      

        n_cols = n_features // images_per_row                                 
        display_grid = np.zeros((size * n_cols, images_per_row * size))

        for col in range(n_cols):                                             
            for row in range(images_per_row):
                channel_image = layer_activation[0,
                                                 :, :,
                                                 col * images_per_row + row]
                channel_image -= channel_image.mean()                         
                channel_image /= channel_image.std()
                channel_image *= 64
                channel_image += 128
                channel_image = np.clip(channel_image, 0, 255).astype('uint8')
                display_grid[col * size : (col + 1) * size,                   
                             row * size : (row + 1) * size] = channel_image

        scale = 1. / size
        plt.figure(figsize=(scale * display_grid.shape[1],
                            scale * display_grid.shape[0]))
        plt.title(layer_name)
        plt.grid(False)
        plt.imshow(display_grid, aspect='auto', cmap='viridis')",dww=data,,Model->Constructing the model through Keras->V. Visualize the entire output of the activation model,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Project Writeup,Data Update - October 14Th
261866,io_model.viz_totals(),bokeh.io.show(bootcamp_utils.viz.bokeh_imshow(ims[0])),"# General Libraries 
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import os

# Scikit-Learn Libraries
from sklearn.metrics import log_loss
from sklearn.model_selection import StratifiedKFold,cross_val_score,train_test_split,learning_curve
from sklearn.preprocessing import MinMaxScaler
from sklearn import linear_model,svm,tree
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier

# Keras Libraries 
from keras.models import Model
from keras.layers import Input, Dense, Dropout

# Interactive Widgets
from ipywidgets import interact_manual
from IPython.display import display

# Print out the folders where our datasets live. 
print(""Datasets: {0}"".format(os.listdir(""../input"")))",,Example->Using the IO-Model-Builder,Segment Images->Behavioral Analysis Tools,Part2
405198,"# Set bucket, project, and region
BUCKET = 'eim-muse'
PROJECT = 'eim-muse'
REGION = 'us-east1'","BUCKET = 'ksalama-gcs-cloudml'
PROJECT = 'ksalama-gcp-playground'
REGION = 'europe-west1'",float(len(df[df['counter'] == 1]))/len(test_tokens_dict),,"Pca->""Hallelujah Effect"" Principal Components Analysis",Cloud Ml - Baby Weight Estimation - Basic->ML with Structured Data using Google Cloud->Housekeeping,Analyzing The Data->Analyzing the data->Lets get the data!
179804,"# Restart Kernel at here
archive_clean = pd.read_csv('archive_clean.csv')
images_clean = pd.read_csv('images_clean.csv')
popularity_clean = pd.read_csv('popularity_clean.csv')","clean_df = pd.read_csv('ttlp_test_clean.csv')
clean_df","questions_data_con = pd.merge(questions_data_full, questions_data, left_index=True, right_index=True)",,"Wrangle Act->Wrangle and Analyze Data-><a name=""clean""> Cleaning data </a>","Ttlp-Demographic-Breakdown->Post survey->Q: The content and language was easy to understand.
Ttlp-Comparison-Breakdown->Import cleaned data - Note: To get cleaned data, run clean before this point
Ttlp-Compare-Only-Non Ato->Import cleaned data - Note: To get cleaned data, run clean before this point
Ttlp-Demographic-Breakdown-Checkpoint->Import cleaned data - Note: To get cleaned data, run clean before this point",Analysis-Of-Mbti-Types->MBTI Types of the StackOverflow Users
459040,"plt.hist(nutrition_features_means[63:-4],bins=1000)
plt.show()","## show the range of TCPs at a given fraction number (after a given nominal dose)

plt.hist(TCPs[:,frac_of_interest-1], bins=50, color='grey')
plt.show()
#print(np.mean(TCPs[:,frac_of_interest-1]))

## want to calcualte how the proportion that have TCP = 1.
## Is this just the same as reading off the TCP_pop value?","merged_review_per_author = most_reviewed_ebooks_author.merge(most_reviewed_books_author,
                                                          left_index=True, right_index=True,
                                                         suffixes=('_ebook', '_book'))",,Data Visualization->DataSet,Survival Fraction + Tcp - New-Checkpoint->Calculation fo Survival Fraction and TCP for varying dose/frac->Histogram to display spread of TCP for all patients at dose of interest,Milestone 3->Books versus eBooks : The customer's choice->Data retrieval->Refined analysis on some specific authors
325356,"df1_company=df1.groupby('Company')
df1_company #groups all the numerical values(Sales) by company","print(df1['Company'].value_counts())
df1.boxplot(column='SalaryNormalized', by = 'Company')

print(df1['ContractType'].value_counts())
df1.boxplot(column='SalaryNormalized', by = 'ContractType')

print(df1['Category'].value_counts())
df1.boxplot(column='SalaryNormalized', by = 'Category')

print(df1['ContractTime'].value_counts())
df1.boxplot(column='SalaryNormalized', by = 'ContractTime')

print(df1['LocationNormalized'].value_counts())
df1.boxplot(column='SalaryNormalized', by = 'LocationNormalized')

print(df1['LocationRaw'].value_counts())
df1.boxplot(column='SalaryNormalized', by = 'LocationRaw')

print(df1['SourceName'].value_counts())
df1.boxplot(column='SalaryNormalized', by = 'SourceName')",b = float(ridge.intercept_),,Data Frames->GroupBy in pandas DataFrame,"Code - Moustapha,Shishir ,Shashank,->Cleanning Full Description->Let us see which variables we must regroup.",2->Ridge Regression using SciKit-Learn->Ridge Regression
17420,"# load some key packages:
import numpy as np                # a numerical package
import pandas as pd               # a data analysis package
import matplotlib.pyplot as plt   # a scientific plotting package

# to display the plots in the same document
%matplotlib inline","% matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd","# Getting some information on the categorical variables
rp.summary_cat(df[['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', \
                   'ca', 'thal', 'target']])",,Intro To Python->Intro to Python,"01-02-Simple-And-Multivariate-Linear-Regression->Linear Regression
Usa State Map",Comparing Machine Learning And Neural Network Performance On Heart Disease Data Set->Classification using Heart Disease Data Set->The Data Source
20720,"sns.pointplot(x= df_train['Pclass'], y=df_train['Survived'], hue=df_train['Sex'])","fig,ax = plt.subplots()
sn.pointplot(data=df[['mean_test_score',
                           'param_max_leaf_nodes',
                           'param_max_depth']],
             y='mean_test_score',x='param_max_depth',
             hue='param_max_leaf_nodes',ax=ax)
ax.set(title=""Effect of Depth and Leaf Nodes on Model Performance"")","# convert product_title & product_description to lower case
test_prepDF = test_full_df.select('id', 'product_uid', lower(test_full_df.product_title), \
                                         lower(test_full_df.search_term), lower(test_full_df.product_description)) \
                                 .withColumn(""tokenized_title"", udfTokenize(""lower(product_title)"")) \
                                 .withColumn(""tokenized_description"", udfTokenize(""lower(product_description)"")) \
                                 .withColumn(""tokenized_search"", udfTokenize(""lower(search_term)"")) \
                                 .select('id', 'product_uid', 'tokenized_title','tokenized_description','tokenized_search')
test_prepDF.cache()",,Titanic->Titanic Project->2. Visualization,Cs I - In Depth Analysis,Script->check label distribution->Preprocess test data
223182,"from moviepy.editor import VideoFileClip

l_line = Line()
r_line = Line()

video_output = 'project_video_output.mp4'
clip = VideoFileClip('project_video.mp4')

output_clip = clip.fl_image(pipeline)
%time output_clip.write_videofile(video_output, audio=False)","# process project video
l_line = Line()
r_line = Line()

output = 'project_video_result.mp4'
##clip1 = VideoFileClip(""test_videos/solidWhiteRight.mp4"").subclip(0,5)
clip = VideoFileClip(""project_video.mp4"")
# clip_result = clip.fl_image(process_image).subclip(0,20)
clip_result = clip.fl_image(process_image)",vocab[0:10],,P4->Test on Videos->Standard: `project_video.mp4`,Main->Apply lane detection to challenge video,Vocab-Checkpoint->Load data
481532,"def computeCost(X, y, theta = [[0]] * X.shape[1]):      
      
    m = y.size
    
    h = X.dot(theta)
    
    J = 1/ (2 * m) * np.sum(np.square(h - y))
    
    return J","def computeCost(X, y, theta = [[0]] * X.shape[1]):      
      
    m = y.size
    
    h = X.dot(theta)
    
    J = 1/ (2 * m) * np.sum(np.square(h - y))
    
    return J","flight_squeeze = [""2015-06-11 03:18:00"", ""2015-06-11 03:22:00""]
lonl = [-99., -97]
latl = [39.5, 41.]
radsweep = 2",,Linear Regression Demo->Machine Learning Exercise - Univariate and Multivariate Linear Regression->Univariate linear regression,Linear Regression Demo->Machine Learning Exercise - Univariate and Multivariate Linear Regression->Univariate linear regression,Pytda Turb 20150611 Uwka
11223,"mask = df_feature_M.keyword_alias_for_user_hash == '24workpng1' 
print '\n'.join(df_feature_M[mask].browser_type.unique())","map_misc_browser = {
    'NetFront': 'misc',
    'SeaMonkey': 'misc',
    'NokiaOvi': 'misc',
    'BrowserNG': 'misc',
    'OperaMobile': 'misc',
    'Dolfin': 'misc',
    'MAUI': 'misc',
    'Openwave': 'misc',
    'PhantomJS': 'misc',
    'MSEdge': 'misc',
    'TelecaBrowser': 'misc',
    'UPBrowser': 'misc',
    'Konqueror': 'misc',
    'Jasmine': 'misc',
    'NintendoBrowser': 'misc',
}

df_feature_M.browser_type = df_feature_M.browser_type.apply(lambda x: map_misc_browser.get(x) if map_misc_browser.get(x) else x)","import sys

def logistic_model(data, explanatory_variables, response_variable, 
                   maxiter = 35, verbose = True):
    explanatory_vars = ' + '.join(explanatory_variables)
    formula = response_variable + ' ~ ' + explanatory_vars

    try:
        model = smf.logit(formula = formula, data = data).fit(maxiter = maxiter)
    except:
        print('Error ""' + str(sys.exc_info()[1]) + '"" while processing model', formula)
        model = None
    
    if verbose and model != None:
        print()
        print('MODEL:', formula, '\n')
        print(model.summary())
        print()

        # odds ratios with 95% confidence intervals
        print (""Odds Ratios"")
        params = model.params
        conf = model.conf_int()
        conf['OR'] = params
        conf.columns = ['Lower CI', 'Upper CI', 'Odds Ratios']
        print (numpy.exp(conf))
        
    return(model)",,Analysis->Making a quick hash map with users as keys and browsers as values,Analysis->First step: extreme value analysis(value counts),Forest Fires - Week 4->Logistic regression
454095,_6.show_table(),"theta_1 = []
theta_2 = []
theta_3 = []
theta_4 = []
theta_5 = []
theta_6 = []

for k in range(0,1200):
    theta_1.append(nn.all_theta_1[k][0][0])
    theta_2.append(nn.all_theta_1[k][0][1])
    theta_3.append(nn.all_theta_1[k][0][2])
    theta_4.append(nn.all_theta_1[k][1][0])
    theta_5.append(nn.all_theta_1[k][1][1])
    theta_6.append(nn.all_theta_1[k][1][2])

plt.plot(range(0,1200),theta_1)
plt.plot(range(0,1200),theta_2)
plt.plot(range(0,1200),theta_3)
plt.plot(range(0,1200),theta_4)
plt.plot(range(0,1200),theta_5)
plt.plot(range(0,1200),theta_6)
plt.show()","# Modify these variables according to your needs.
application_name = ""Distributed Numpy Parsing""
using_spark_2 = False
local = False

if local:
    # Tell master to use local resources.
    master = ""local[*]""
    num_processes = 3
    num_executors = 1
else:
    # Tell master to use YARN.
    master = ""yarn-client""
    num_executors = 20
    num_processes = 1

# This variable is derived from the number of cores and executors,
# and will be used to assign the number of model trainers.
num_workers = num_executors * num_processes

print(""Number of desired executors: "" + `num_executors`)
print(""Number of desired processes / executor: "" + `num_processes`)
print(""Total number of workers: "" + `num_workers`)",,Program 02->Part (II) A pair of natural cubic splines->(a),Machine Learning,Distributed Numpy Parsing->Distributed Numpy Parsing
427253,"tws.reqAccountSummary(2,""All"",""NetLiquidation"")",me = tw.account.verify_credentials(),"df2 = df.fillna(method='pad', axis=0)
df2.head() # The price of row 3 is the same as that of row 2",,"Ib Py Demo V2018-04-05->Real Time Bars->Sending Account Summary Request
Ib Py Demo V2018-04-05->Orders->Limit Order->Checking on Open Order",02 Social->14.2. Analyzing a social network with NetworkX,L3 - Preprocessing->Data Preprocessing with Pandas->Finding and Handling Missing Data
167287,"bg_colors = {'present':'red',
             'absent': 'white'}

supports = {'black': [100,99],
            'gray': [99,80]}

pj.annotate('./images/',               # Path to write figs to
            
            'genus', 'Astrophorida',   # Set OTUs that have 'Astrophorida'
                                       # in their 'genus' qualifier
                                       # as outgroup
            
            ['source_organism', 'record_id'], # leaf labels
            
            node_bg_meta='porocalices', # The qualifier that
                                        # will determine bg colors
            node_bg_color=bg_colors,    # The colors assigned to 
                                        # each qualifier value
            
            node_support_dict=supports, 
            
            html='./images/figs.html'
           )

pj.clear_tree_annotations()","# Annotate the tree
pj.clear_tree_annotations()
pj.annotate('18S_tree/fasttree_figure', 'keep','Alyr',['source_organism','source_original_id'],
            fsize=18, scale=600)","# access the element in the last row and last column
b[-1, -1]",,3->3.10.1 Updating the metadata after the tree has been built,18S Tree->1. Phylogenetic reconstruction of an 18S tree for species with genomes->1.10 Make a figure of the preliminary large tree,1 Beginning Numpy->NumPy Basics - Part 1->Accessing Multi-Dimensional Array Elements Through Indices
311597,"for set in (strat_train_set, strat_test_set):
    set.drop([""income_cat""], axis=1, inplace=True);","for set_ in (strat_train_set, strat_test_set):
    set_.drop('income_cat', axis =1, inplace = True)","import pandas as pd
import numpy as np
import networkx as nx
from bqplot import pyplot as plt

# accepts models of 4 variables with variable bins

class var_set:
    def __init__(self):
        self.n_variables = 0
        self.n_states = 0
        self.n_bins = []
        self.state_list = []
        self.param = []
        self.adj = []

    def load(self, file, variable_names):
        data = pd.read_csv(file, sep=',')
        data = data.set_index(np.arange(1, len(data.index)+1))

        self.n_states = int(len(data.index))
        self.n_variables = int(len(data.columns)-3)
        self.n_bins = np.zeros(self.n_variables)

        for i in range(self.n_variables):
            self.n_bins[i] = int(len(data.iloc[:, i].unique()))

        self.state_list = data.iloc[:, 0:4].astype(str)
        self.state_list['combo'] = self.state_list[variable_names].apply(lambda x: ''.join(x), axis=1)
        self.state_list = np.asarray(self.state_list['combo'])

        self.param = data[['Odds', 'P', 'Frequency']]
        self.param = self.param.set_index(self.state_list)

    def  create_adj(self):
        adj = pd.DataFrame(np.zeros((len(self.state_list), len(self.state_list))), index=self.state_list, columns=self.state_list)
        for i in range(len(self.state_list)):
            for j in range(len(self.state_list)):
                adj.iloc[j, i] = sum ( self.state_list[i][k] != self.state_list[j][k] for k in range(self.n_variables) )
        self.adj = adj

    def trim_freq(self, G):
        crit = np.std(self.param.Frequency)/8.0
        new = []
        for i in range(len(self.state_list)):
            if graph1.node[str(self.state_list[i])]['Frequency']>crit:
                new = np.concatenate((new, [self.state_list[i]]))

        return nx.subgraph(G, new)


    def find_joint(self, N, thresh):
        joint = pd.DataFrame(np.zeros((self.n_states*4, self.n_variables)), columns=np.arange(1, self.n_variables+1), dtype=str)
        num_joint = 0
        joint_list = pd.Series(np.zeros(self.n_states*4), dtype=str)
        adj = pd.DataFrame(self.adj)
        param = pd.DataFrame(self.param)
        state_list = np.asarray(self.state_list)
        n_variables = self.n_variables
        n_states = self.n_states

        for i in range(n_states-1):
            for j in range(i+1, n_states):
                # if they have nth-order relationship
                if graph1.edge[str(state_list[i])][str(state_list[j])]['weight'] == N:
                    # if they have comparable parameter values
                    if (graph1.node[state_list[j]]['Odds']-thresh) < graph1.node[state_list[i]]['Odds'] < (graph1.node[state_list[j]]['Odds']+thresh):
                        num_joint += 2
                        print(N, (j, i))
                        print(state_list[j], state_list[i])
                        print(param.Odds.iloc[i], param.Odds.iloc[j])
                        for k in range(n_variables):
                            # if the kth variable in the states is equal
                            if state_list[i][k] == state_list[j][k]:
                                joint.iloc[num_joint-2, k] = str(state_list[i][k])
                                joint.iloc[num_joint-1, k] = str(state_list[i][k])
                            else:
                                joint.iloc[num_joint-2, k] = 'x'
                                joint.iloc[num_joint-1, k] = 'x'
                            joint_list.iloc[num_joint-2] = str(state_list[i])
                            joint_list.iloc[num_joint-1] = str(state_list[j])


        joint['joined'] = joint_list
        joint['joint'] = joint[np.arange(1, n_variables+1)].apply(lambda x: ''.join(x), axis=1)
        new_joint = joint[['joint', 'joined']]

        # trim the extra space out of the array
        for i in range(len(joint_list)):
            if joint_list.iloc[i] == 0:
                joint_list = joint_list[:i]
                new_joint = new_joint[:i]
                break

        # create a new graph, with new nodes (eg. '1x01') with OR of edges of both members

        graph3 = graph2.remove_nodes_from(np.asarray(new_joint.iloc[:, 1]))

        return graph3


    def as_graph(self):
        G = nx.Graph()

        # generate nodes
        G.add_nodes_from(self.state_list)
        for i in range(len(self.state_list)):
            G.node[self.state_list[i]]['Odds'] = self.param.Odds[self.state_list[i]]
            G.node[self.state_list[i]]['P'] = self.param.P[self.state_list[i]]
            G.node[self.state_list[i]]['Frequency'] = self.param.Frequency[self.state_list[i]]

        # self.adj[self.adj == 1]

        # generate edges
        k = 0
        edge_list = np.zeros(len(self.state_list)**2, dtype=tuple)
        adj = pd.DataFrame(self.adj)
        for i in range(self.n_states-1):
            for j in range(i+1, self.n_states):
                # edge_list[k] = [self.state_list[i], self.state_list[j], {'weight' : int(adj.iloc[i, j])}]
                edge_list[k] = [self.state_list[i], self.state_list[j], int(adj.iloc[i, j])]
                k += 1

        # trim empty spots
        for i in range(len(edge_list)-1):
            if edge_list[i] == 0:
                edge_list = edge_list[:i]
                break

        #G.add_edges_from(edge_list)
        G.add_weighted_edges_from(edge_list)

        # G = nx.minimum_spanning_arborescence(G)

        return G

def build_tree(G):
    # create list of nodes

    # determine which variables are represented

    # determine which values of those variables are represented

    # determine order of tree branchings

    # build new network, nodes for each branching
    # ie. 1, 2, 3; 10, 11, 20, 21, 30, 31; 100, 101, 200, 201, 210, 211...

    # name the nodes the variable branching at it, or assign as attribute \

    print('yey')

fourvar = var_set()
vars = ['Ageb', 'Nrb', 'Rku', 'Rro']
# fourvar.load('AgebNrbRkuRro.csv', vars)
fourvar.load('cecily.csv', vars)
fourvar.create_adj()

graph1 = fourvar.as_graph()
# graph2 = graph1(data='')

graph2 = graph1

# first_degree = np.zeros(len(fourvar.state_list))
# for (u,v,d) in graph1.edges(data='weight'):
#     if d==1:print('oik')

graph3 = fourvar.trim_freq(graph1)

graph4 = fourvar.find_joint(1, (np.std(fourvar.param.Odds)/4.0))
# second_order_joint = fourvar.find_joint(2, (np.std(fourvar.param.Odds)/4.0))
# first_order_joint = fourvar.find_joint(3, (np.std(fourvar.param.Odds)/4.0))
# zero_order_joint = fourvar.find_joint(4, (np.std(fourvar.param.Odds)/4.0))

#joined_items = np.unique(third_order_joint.joined)

# add in all leaves that weren't reduced
# for i in range(len(joined_items)):
#     state_list = fourvar.state_list[(fourvar.state_list != joined_items[i])]
# state_list = pd.DataFrame(fourvar.state_list)
# state_list = state_list[~(state_list.isin(joined_items))].dropna()

# create final state_list, aka leaves
# state_list = pd.Series(np.append(fourvar.state_list, np.unique(third_order_joint.joint)))
#state_list = pd.Series(np.append(state_list, np.unique(third_order_joint.joint)))

# then find the first branching point
# aka, the variable that is constant between all lowest-order states

# maybe I should be using set operations a la np.intersect1d() or np.setdiff1d()

# reduced_states = np.zeros(len(vars), dtype=object)
# states = np.zeros((len(state_list), len(vars)), dtype=str)
# for i in range(len(vars)):
#     for j in range(len(state_list)):
#         states[j, i] = state_list[j][i]
#     reduced_states[i] = np.unique(states[:, i])
# print(reduced_states)

graph4 = fourvar.as_graph()",,Housing->Get the Data->Load the Data Using Pandas,California Housing Price Prediction->California Housing->4. Prepare the Data for ML Algorithm->Evaluate Your System on the Test Set:,Occam Tree
289364,"from nltk.tokenize import RegexpTokenizer
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical

rxtokenizer = RegexpTokenizer(r'\w+')

data['tokens'] = data['response_text'].apply(rxtokenizer.tokenize)
data.head()","from nltk.tokenize import RegexpTokenizer
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical

rxtokenizer = RegexpTokenizer(r'\w+')

data['tokens'] = data['response_text'].apply(rxtokenizer.tokenize)
data.head()","# Plot the radars from given time.

times = get_radar_times_cpol_cfradial(start_year, start_month, start_day,
                                      start_hour, start_minute, end_year,
                                      end_month, end_day, end_hour, 
                                      end_minute, minute_interval=1)

times_berr = get_radar_times_berr_cfradial(start_year, start_month, start_day,
                                           start_hour, start_minute, end_year,
                                           end_month, end_day, end_hour, 
                                           end_minute, minute_interval=1)
print(times_berr)",,Preprocessing->Preprocessing and Visualization of Data,Preprocessing->Preprocessing and Visualization of Data,Cpol Dual Doppler Test->CPOL and Berrima Dual Doppler test
378635,hdu_ffi = fits.open('/Volumes/Truro/ffi/ktwo2017079075530-c13_ffi-cal.fits'),"#cffi
import cffi
ffi = cffi.FFI()
# Two stages, cdef reads the headers, then dlopen finds the functions in the shared object

with open('create_fractal.h', 'rt') as fid:
    header = fid.read()
    # clean up all preprocessor macros before calling this
    print('Contents of create_fractal.h\n------------\n')
    ffi.cdef(header)
    print(header)

dll = ffi.dlopen('./libcreate_fractal.so')","game = Game()
th = threading.Thread(target = game.mainloop)
th.start()",,"02->How many times has each K2 pixel been telemetered?
02->C13 example: How many times has each K2 pixel been telemetered?","C From Python->CFFI, Ctypes, Cython, Cppyy: The good, the bad, the almighty and the unknown->The looping function","02 - A Better Game->Hey, let's code a better game!
01 - Pygame Live Coding Test->Live coding with pygame and ipython"
202329,"import pandas as pd
import numpy as np
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import bisect
plt.style.use('seaborn')
%matplotlib inline","import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
from pandas.tools.plotting import scatter_matrix
import matplotlib
matplotlib.style.use('ggplot')
%matplotlib inline  

from sklearn.cross_validation import train_test_split
from sklearn import linear_model
from sklearn.metrics import mean_squared_error","plot_hit_ratio_variation(""128MB LRU Ehcache Hit Ratio Variation"", ehcache_lru_128_data, 1)",,Health Data->Analyzing health data with python,"Predicting House Prices-Python-Quiz2-Checkpoint->Fire up necessary modules
Predicting House Prices Pydemo-Checkpoint->Fire up necessary modules","Cachingrest Data Investigation->Hazelcast Data<a class=""anchor"" id=""hazelcast""></a>->Hazelcast LRU 128 MB"
115411,"gnb = GaussianNB()  # instantiate model
y_pred = gnb.fit(X_train, y_train).predict(X_test)  # train model and get predictions

# get classification metrics
report = model_report(y_pred=y_pred, y_true=y_test)
print report
plot_confusion_matrix(y_test, y_pred)","gnb = GaussianNB()

y_pred = gnb.fit(x_train, y_train).predict(x_test)
print('Testing classification accuracy: %.4f' % ((y_test == y_pred).sum()/x_test.shape[0]))","matrix = pd.DataFrame(0, index=words, columns = ['w2v_'+str(i) for i in range(0,300)])
for w in words:
    matrix.loc[w] =  model.word_vec(w)
    
matrix.head()",,1->Can we do better than Spotify?->Step 3 - Get song features->Gaussian naive Bayes,Iris Bayesian Decision Making->(4) bayesian decision making,"Word Clustering Pt 1->Hello everyone,->K-means
Tsne On Words Embedding->Hello everyone,"
412633,"clean_dataset, clean_labels = remove_duplicates(train_dataset, train_labels)
valid_dataset, valid_labels = remove_duplicates(valid_dataset, valid_labels)
test_dataset, test_labels = remove_duplicates(test_dataset, test_labels)
print(clean_dataset.shape, valid_dataset.shape, test_dataset.shape)","# All Data set
logreg = LogisticRegression() #C=1e5 C=0.001
# idx = random.sample((len(train_labels)))
logreg.fit(train_dataset_lin, train_labels)
              
res = logreg.predict(train_dataset_lin)
print(sklearn.metrics.accuracy_score(train_labels, res))

res = logreg.predict(test_dataset_lin)
print( sklearn.metrics.accuracy_score(test_labels_clean, res) )

res = logreg.predict(valid_dataset_lin)
print( sklearn.metrics.accuracy_score(valid_labels_clean, res) )","# evaluate combinations of p, d and q values for an ARIMA model
def evaluate_models(dataset, p_values, d_values, q_values):
    #dataset = dataset.astype('float32')
    best_score, best_cfg = float(""inf""), None
    for p in p_values:
        for d in d_values:
            for q in q_values:
                order = (p,d,q)
                try:
                    mse = evaluate_arima_model(dataset, order)
                    if mse < best_score:
                        best_score, best_cfg = mse, order
                    print('ARIMA%s MSE=%.3f' % (order,mse))
                except:
                    continue
    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))
    #return best_cfg, best_score",,1 Notmnist,Not Mnist - Logistic Regression,Afl-No Finals-><center> CHE Proximity Data Science Challenge</center>->4.2 Training ARIMA model
148261,"a = []

for x in range(0, len(hist)):
    b = hist[x]*np.ones(hist[x])
    a = np.append(a,b)","on_time_red = on_time.sample(len(late))
combined = late.append(on_time_red, ignore_index=True)
X = combined.drop('LateAircraftDelay',axis=1)
Y = combined['LateAircraftDelay']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)","# print the summary
print(regressor.summary())",,Creating-Sound-From-Data->Data and sounds - between science and art->Creating sound,Late Airline Arrivals->Late Airline Arrivals,03-Multiple Regression - Housing->Splitting Data 80 % training set and 20 % testing set
319930,len(sampleImages),"z_value= (mean_obs-mean_sampl_dist)/std_error
z_value",ts_df = t60_df[fields_above_thres],,Sat Image-Tutorial-Sampling->Load data in GeoTiff format->Sample images around locations,Sliderule Dsi Inferential Statistics Exercise 1->4.  Is there a significant difference between males and females in normal temperature?->Hypothesis test:,01 Jmg Exploration->1. Data collection->Initial exploration
66436,"%%time
from keras.models import load_model
model = load_model('saved_models/bestweights_facesmodel.hdf5')
for human_files_2 in human_files_short:
    if face_detector2(human_files_2, model):
        h_in_h_count += 1
    else:
        print(""no human detected at index i = "", np.where(human_files==human_files_short)[0][0])
        
for dog_files_2 in dog_files_short:
    if face_detector2(dog_files_2, model):
        h_in_d_count += 1

hcount_to_percentage = 100/len(human_files_short)
dcount_to_percentage = 100/len(dog_files_short)        
print(""percentage of detected humans in human images = "", '{:>2}'.format(h_in_h_count*hcount_to_percentage), ""%"")
print(""percentage of detected humans in dog images = "", '{:>2}'.format(h_in_d_count*dcount_to_percentage), ""%"")","## Test the performance of the face_detector2 (LBP) algorithm 
## on the images in human_files_short
count_hfaces_humans = 0
count_hfaces_dogs = 0

for human in human_files_short:
    if face_detector2(human):
        count_hfaces_humans += 1
        
for dog in dog_files_short:
    if face_detector2(dog):
        count_hfaces_dogs += 1    
        
# print statistics about the dataset
print('Haar cascades for face detection finds human faces in %d%% of the images in human_files_short.' % (count_hfaces_humans))
print('Haar cascades for face detection finds human faces in %d%% of the images in dog_files_short.' % (count_hfaces_dogs))","# Get rid of first <, then get rid of >
reformated_source = trump['source'].str.replace('<[^>]+>', '')

trump['source'] = reformated_source",,Dog App->(IMPLEMENTATION) Predict Dog Breed with the Model,Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION ResNet-50) Model Architecture,"Trump And Russian Bot Tweet Analysis->Trump, Twitter, Russian Bots and the 2016 Election->Tweet Source Analysis"
187580,"# CELL 13

# Define the placeholders and training hyperparameters

import tensorflow as tf
import selu

x = tf.placeholder(tf.float32, (None, 32, 32, n_channels))
y = tf.placeholder(tf.int32, (None))
one_hot_y = tf.one_hot(y, n_classes)
keep_prob = tf.placeholder(tf.float32)
training = tf.placeholder(tf.bool)

EPOCHS = 20
BATCH_SIZE = 128
rate = 0.001","import tensorflow as tf
import csv
NUM_CHANNEL = 1

with open( './signnames.csv', 'rt') as f:
    reader = csv.reader(f)
    label_name = list(reader)

x = tf.placeholder(tf.float32, (None, 32, 32, NUM_CHANNEL))
y = tf.placeholder(tf.int32, (None))
keep_prob = tf.placeholder(tf.float32)
one_hot_y = tf.one_hot(y, n_classes)","file = str(race) + '_' + raceName + '_' + gender + '.results.csv'
df = pd.read_csv( file, index_col=0 )
df = df.dropna() # remove DNF/DNS",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Model Architecture,Traffic Sign Classifier->Labeling,Dh Analysis-Checkpoint->UCI DH MTB Race Analysis->Config->Import Data
317604,"print(""Conversations Dataset : "")
conversation_df.columns = ['charId1', 'charId2', 'movieName', 'lineId']
conversation_df.head(5)",print(df.head(3)),"d,c=utils.get_data(type=1)
TSNE(d, max_iter = 20, labels = c, target_dimension = 2, intermediate_dimension = 50, perplexity=25,  early_exaggeration=0, seed=7, visualize = False)
TSNE(d, eta=1000, max_iter = 20, labels = c, target_dimension = 2, intermediate_dimension = 50, perplexity=25,  early_exaggeration=0, seed=7, visualize = False)",,Data Description And Preprocessing->Data Description and Preprocessing->Dataset Information->Sample of read conversations,Decision Tree->Part 1: Decision Tree->Dataset,Sheet 02 Jlmn->Part 2: Experimenting With t-SNE (30P)
112607,"# To complete
def normalDistribution(points, mean, std):
    """"""
        Computes the normal distribution of mean and std
        for the given points
    """"""
    values = 
    return values",normal[(normal['HP']-normal['HP'].mean())>=(3*normal['HP'].std())],"# Look for correlation between pm25 and temperature
fig, ax1 = plt.subplots()
color = 'tab:red'
df[['temp']].plot(figsize=(15,6), linewidth=2, fontsize=16, ax=ax1, color=color, legend=False)
ax1.set_xlabel('Year', fontsize=18)
ax1.set_ylabel('Temperature (C)', fontsize=18, color=color)
ax1.tick_params(axis='y', labelcolor=color)

ax2 = ax1.twinx()  
color = 'tab:blue'
df[['pm25']].plot(linewidth=2, fontsize=16, ax=ax2, color=color, legend=False)
ax2.set_ylabel('PM 2.5 (ug/m3)', fontsize=18, color=color)  
ax2.tick_params(axis='y', labelcolor=color)
fig.tight_layout();",,1->Square example->Visualization of distribution,Je4 Hypothesis,Beijing Air Quality 2-Eda->Forecasting Air Quality in Beijing
355436,"vectorizer = CountVectorizer(min_df=best_min_df, ngram_range =(2,2))
X, y = make_xy(critics, vectorizer)
xtrain=X[mask]
ytrain=y[mask]
xtest=X[~mask]
ytest=y[~mask]

clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)

#your turn. Print the accuracy on the test and training dataset
training_accuracy = clf.score(xtrain, ytrain)
test_accuracy = clf.score(xtest, ytest)

print(""Accuracy on training data: {:2f}"".format(training_accuracy))
print(""Accuracy on test data:     {:2f}"".format(test_accuracy))","# n=2
vectorizer = CountVectorizer(min_df=best_min_df, ngram_range=(2,2))
X, y = make_xy(critics, vectorizer)
xtrain=X[mask]
ytrain=y[mask]
xtest=X[~mask]
ytest=y[~mask]

clf = MultinomialNB(alpha=best_alpha).fit(xtrain, ytrain)

#your turn. Print the accuracy on the test and training dataset
training_accuracy = clf.score(xtrain, ytrain)
test_accuracy = clf.score(xtest, ytest)

print(""Accuracy on training data: {:2f}"".format(training_accuracy))
print(""Accuracy on test data:     {:2f}"".format(test_accuracy))","import requests

address = ""1 Brookings Dr, St. Louis, MO 63130""

response = requests.get('https://maps.googleapis.com/maps/api/geocode/json?address='+address)

resp_json_payload = response.json()

print(resp_json_payload['results'][0]['geometry']['location'])",,Mini Project Naive Bayes->1. n-grams instead of words,G->Aside: TF-IDF Weighting for Term Importance,T81 558 Class4 Training->Scikit-Learn Versions: model_selection vs cross_validation
133080,"# Weight fraudulent cases higher
weights = {0: 1., 1: 10.}
train_logReg(X_train_small,y_train_small,X_test_small,y_test_small,weights)","from sklearn import preprocessing

X_train_small_scaled = pd.DataFrame(preprocessing.robust_scale(X_train_small.values))
X_train_small_scaled[[5,6]] = X_train_small[[5,6]].values

X_test_small_scaled = pd.DataFrame(preprocessing.robust_scale(X_test_small.values))
X_test_small_scaled[[5,6]] = X_test_small[[5,6]].values


train_logReg(X_train_small_scaled,y_train_small,X_test_small_scaled,y_test_small)","lm.create_list(query,name=""my list 2"")",,Data Analysis->Paysim Dataset analysis (fraud detection)->50/50 split:,Data Analysis->Paysim Dataset analysis (fraud detection)->Scaling the data so that features have mean 0 and std 1,09-Tutorial
438533,"import time

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer



training_ground_truth = { '01': ['Aggressive'],
                         '02': ['Excited', 'Aggressive', 'Aimless'],
                         '03': ['Excited', 'Fun'],
                         '04': ['Cooperative'],
                         '05': ['Bored', 'Aimless'],
                         '06': ['Cooperative'],
                         '07': ['Dominant'],
                         '08': ['Bored', 'Fun'],
                         '09': ['Cooperative'],
                         '10': ['Cooperative', 'Dominant'],
                         '11': ['Cooperative', 'Dominant'],
                         '12': ['Aggressive', 'Aimless'],
                         '13': ['Excited', 'Aggressive', 'Aimless'],
                         '14': ['Aggressive'],
                         '15': ['Dominant'],
                         '16': ['Cooperative', 'Dominant'],
                         '17': ['Excited', 'Aggressive'],
                         '18': ['Aggressive', 'Dominant'],
                         '19': ['Dominant'],
                         '20': ['Excited']}

mlb = MultiLabelBinarizer()
mlb.fit(training_ground_truth.values())

def datasets(training=data, testing=None, cols=allQuestionsDiffSum, test_size=0.2, use_clip_id_as_label=False, random_labels=False):
    """"""Returns a training dataset and training labels, and a testing dataset and testing labels.
    
    If testing is None, it randomly splits the training dataframe (at test_size).
    """"""


    if testing is None:
        
        if use_clip_id_as_label:
            labels = list(training[""clipId""].map(int))
        else:
            labels = []
            for id in training[""clipId""]:
                labels.append(training_ground_truth[id])

        data = training[cols].values

        training_data, testing_data, training_labels, testing_labels = train_test_split(data, labels, test_size=test_size, random_state=int(time.time()))

        if not use_clip_id_as_label:
            
            training_labels, testing_labels = mlb.transform(training_labels), mlb.transform(testing_labels)
            
            if random_labels:
                for labels in training_labels:
                    np.random.shuffle(labels)                 
                np.random.shuffle(training_labels)             
            

        return training_data, testing_data, training_labels, testing_labels
    
    else:
        
        if use_clip_id_as_label:
            training_labels = list(training[""clipId""].map(int))
            testing_labels = list(testing[""clipId""].map(int))
        else:
            labels = []
            for id in training[""clipId""]:
                labels.append(training_ground_truth[id])

            training_labels = mlb.transform(labels)

            labels = []
            for id in testing[""clipId""]:
                labels.append(training_ground_truth[id])

            testing_labels = mlb.transform(labels)

            if random_labels:
                if random_labels:
                    for labels in training_labels:
                        np.random.shuffle(labels)                 
                    np.random.shuffle(training_labels) 

        
        training_data = training[cols].values
        testing_data = testing[cols].values

        return training_data, testing_data, training_labels, testing_labels","def load_reuters():
    reuters_data = {""name"" : ""reuters"", ""ovr"" : True}
    
    # The test and train sets are listed as IDs in the .fileids() member
    train_ids = list(filter(lambda x: x[:5] == ""train"", reuters.fileids()))
    test_ids = list(filter(lambda x: x[:4] == ""test"", reuters.fileids()))
    reuters_data[""X_train""] = list(map(lambda x: reuters.raw(x), train_ids))
    reuters_data[""X_test""] = list(map(lambda x: reuters.raw(x), test_ids))
    
    # The MultiLabelBinarizer will get you the 1s and 0s your model wants
    mlb = MultiLabelBinarizer(sparse_output=True)
    reuters_data[""y_train""] = mlb.fit_transform(list(map(lambda x: reuters.categories(x), train_ids)))
    reuters_data[""y_test""] = mlb.transform(list(map(lambda x: reuters.categories(x), test_ids)))
    
    return reuters_data
    
reuters_data = load_reuters()","d1 = {'a': 1, 'b': 2, 'c': 3}",,Classification->3.2 Automatic Labelling of Social Situations,5 Text Classification->Text classification->Recurrent neural networks->Reuters newswire dataset,R1-Lab Internal Hm->Pandas->Series->Question 1
116983,"s = ['a', 'b', 'c', 'd', 'e', 'f']
s[-1]","S(1, 2, 3)","# Calculating the confidence interval
conf_int = z * (std / n)
conf_int",,"1->1.3 Expressions, Operators, and Precedence->Sequence Operators",Gravi Py - Tutorial->GraviPy - tutorial->Predefined _Tensor_ Classes->The _Christoffel_ symbols,"Sliderule Dsi Inferential Statistics Exercise 1->5) At what temperature should we consider someone's temperature to be ""abnormal""?->Frequentist Approach"
329169,CLA_by_graphcat.head(),CLA_by_graphcat['WAR_TOTAL_CHG_AMT_NATIVE'].sum().round(2),"# merge them into one dataframe
data = pd.concat([data1, data2, data3])
data.reset_index(drop=True, inplace=True)
print data.shape",,"Cla Vs Rvms Check->Payment Reconciliation - Validating CLA data against RVMS Budget Summary Data->Execute Data Checks->Confirm that the total charge amount from the merged data set equals to the original CLA data set.  If they don't match, then we know something went wrong with the merging.",Cla Vs Rvms Check->Payment Reconciliation - Validating CLA data against RVMS Budget Summary Data->Confirm CLA data in SQL Server matches data in DB2:->First 5 rows of SQL Server CLA data:,Clean Data->load data
402236,a.shape[0]/float(beer_reviews.shape[0]),beer_reviews_data[beer_reviews_data.review_profilename.isna()],"import seaborn as sns
sns.set_style('white')
sns.set_context('paper', font_scale=1.5)
from glob import glob",,Deep Beers Blog Post Part 1->Explicit feedback Recommender System->Visualizing embeddings->Metadata,Beer Analys And Beer Bottles And Cans Production In Usa->Beers Analys and Beer Bottles and Cans Production in USA->Working with the beer raters,Cluster Advantage Plots
140765,"import sys
sys.path.append("".."")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use(""dark_background"") # comment out if using light Jupyter theme

dtypes = {""qid"": str, ""question_text"": str, ""target"": int}
train = pd.read_csv(""../data/train.csv"", dtype=dtypes)
test = pd.read_csv(""../data/test.csv"", dtype=dtypes)","import sys
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import random
import matplotlib.pyplot as plt
import seaborn as sns
import sys


#Ignore Warnings - save some confusion
import warnings
warnings.filterwarnings('ignore')

#Pandas more columns
pd.options.display.max_columns = None
pd.set_option('display.max_columns', None)

# Add input as import path
sys.path.insert(0,'../input')

# Plot style
plt.style.use('fivethirtyeight')

# Import the data from the dataset
train_data = pd.read_csv('../input/train.csv',index_col='id')
test_data = pd.read_csv('../input/test.csv',index_col='id')

train_data.sort_index()","# Create dataframe for each category of crime
data0 = df.loc[df['Category'] == 'VANDALISM']
data0 = data0.sort_values(by = ['date'])
data1 = df.loc[df['Category'] == 'ARSON']
data1 = data1.sort_values(by = ['date'])
data2 = df.loc[df['Category'] == 'STOLEN PROPERTY']
data2 = data2.sort_values(by = ['date'])
data3 = df.loc[df['Category'] == 'BURGLARY']
data3 = data3.sort_values(by = ['date'])
data4 = df.loc[df['Category'] == 'LARCENY/THEFT']
data4 = data4.sort_values(by = ['date'])
data5 = df.loc[df['Category'] == 'VEHICLE THEFT']
data5 = data5.sort_values(by = ['date'])",,Eda->Exploration of Quora dataset,"Session3->Week 2 - Let's get classifying->Step 0 - Library and imports, as always",Interactive Time Series Visualizations In Plotly->Time Series Visualization using plotly
175081,"for team in play_in_winners:
    win_probs.ix[win_probs.WS_team == team, 'WS_Seed'] = win_probs.WS_Seed.str[0:3]
    win_probs.ix[win_probs.SS_team == team, 'SS_Seed'] = win_probs.SS_Seed.str[0:3]","win_lose = data
win_lose['winner'] = data.apply(lambda row: winner(row), axis=1)
win_lose['loser'] = win_lose.apply(lambda row: lose(row), axis=1)
win_lose.sample(5)","import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from scipy.io import loadmat 
from scipy.optimize import fmin_cg
import random

mpl.rcParams['figure.figsize'] = (10,6)",,04->Remove the letter from the seed of the 4 play in game winners (in the win probs dataset),"Pivot Table Kernel->Who's Winning, Who's losing!",Ex5
280440,"widgets.interact(tree_plot_vars,
                 function=('ysq','yexp','ysin','ygenlog'),
                 treenum=widgets.IntSlider(min=1, max=10, step=1, continuous_update=False, value=1))","interact(f, x=widgets.IntSliderWidget(min=-10,max=30,step=1,value=10));","gamma = 0.2

x_min_better = gradient_descent_adaptive(func_der_2, cur_x, gamma, precision, previous_step_size)

print(""The local minimum occurs at %f"" % x_min_better)
draw_func_min(func_2, np.arange(-1, 9, 0.1), x_min_better)",,6->Classification and regression trees (CARTs)->Common purity functions (classification),"Using Interact->Widget abbreviations
Using Interact->Widget abbreviations",F-11 Gradient Descent->Adaptive learning rate
15113,"import umap

features = data[['d_ability_1', 'p_ability_1', 'p_ability_2', ""d_h2h_1"", ""travel_3""]].dropna()
feature_values = features.values
embedding = umap.UMAP().fit_transform(feature_values)","from bokeh.plotting import figure, output_file, show
from bokeh.models import CategoricalColorMapper, ColumnDataSource
from bokeh.palettes import Category10

import umap
from sklearn.datasets import load_digits

digits = load_digits()
embedding = umap.UMAP().fit_transform(digits.data)

output_file(""digits.html"")

targets = [str(d) for d in digits.target_names]

source = ColumnDataSource(dict(
    x = [e[0] for e in embedding],
    y = [e[1] for e in embedding],
    label = [targets[d] for d in digits.target]
))

cmap = CategoricalColorMapper(factors=targets, palette=Category10[10])

p = figure(title=""test umap"")
p.circle(x='x',
         y='y',
         source=source,
         color={""field"": 'label', ""transform"": cmap},
         legend='label')

show(p)",%timeit -n1 -r4 linear_complete_gsa(100),,Getting Started->Throne : Getting Started->Data Analysis,Try Umap->Given Example,Multiproc Gill->Multiprocessing Example with Stochastic Simulations->Linear Method
22009,"# So let's grab that letter for the deck level with a simple for loop

# Set empty list
levels = []

# Loop to grab first letter
for level in deck:
    levels.append(level[0])    

set(levels)","def add_literal_level(self, level): #S 레벨 추가
    """""" 
    add an S (literal) level to the Planning Graph

    :param level: int
        the level number alternates S0, A0, S1, A1, S2, .... etc the level number is also used as the
        index for the node set lists self.a_levels[] and self.s_levels[]
    :return:
        adds S nodes to the current level in self.s_levels[level]
    """"""
    # TODO add literal S level to the planning graph as described in the Russell-Norvig text
    # 1. determine what literals to add
    # 2. connect the nodes
    # for example, every A node in the previous level has a list of S nodes in effnodes that represent the effect
    #   produced by the action.  These literals will all be part of the new S level.  Since we are working with sets, they
    #   may be ""added"" to the set without fear of duplication.  However, it is important to then correctly create and connect
    #   all of the new S nodes as children of all the A nodes that could produce them, and likewise add the A nodes to the
    #   parent sets of the S nodes
    
    pre_a_level = self.a_levels[level - 1] # S - A - S로 연결되어야 하므로, 이전 A 레벨을 가져와야 한다.
    self.s_levels.append(set()) #초기화
    
    for node_a in pre_a_level: #이전 A 레벨의 노드들 가져오기
        for node_s in node_a.effnodes: #해당 A노드의 자식 노드가 될 수 있는 S노드들. Effect가 된다.
            node_s.parents.add(node_a) #S - A
            node_a.children.add(node_s) #A - S
            #S - A - S의 형태로 연결되어야 하므로.
            self.s_levels[level].add(node_s) #State 레벨 리스트에 추가(요소는 인덱스(레벨)의 셋으로 설정되어 있다.)","iterations = 1000
alpha = 0.01
initial_theta = np.zeros((X_norm.shape[1],1))
theta, thetahistory, jvec = descendGradient(X_norm, initial_theta)

# None, Bedrooms, Size
print('Theta found by gradient descent: ', theta)",,Titanic->Titanic,Planning->Part 2 - Domain-independent heuristics,Predicting House Price With Linear Regression-Checkpoint->Gradient Descent
47383,TSD = 3 * Population_std,import osm_popul_wrangle as popul,"my_dict = {""a"" : 4, ""b"": 10, ""12"": 3, ""d"": 33, ""1"": 22}
for a in sorted(my_dict):
    print(a)",,"Project-1-V2-Starter-Code->Via list comprehensions->Method 2: A 'for' loop through the player list dictionary->More efficient way, list comprehensions",Osm Project->OpenStreetMap Data Wrangling Project->Detailed Audit,Interview Questions - Core Python->Dictionary
8720,"import pandas as pd
import numpy as np
import matplotlib.pylab as plt
%matplotlib inline
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 15, 6","import numpy as np
import pandas as pd

#Pandas display options
# pd.set_option('display.max_colwidth', 200)
# pd.set_option('display.width', 200)
# pd.set_option('display.precision', 10)
# pd.set_option('display.max_rows', 500)

import matplotlib.pylab as plt
%matplotlib inline
font = {'family' : 'serif',
    'weight' : 'normal',
    'size'   : 15}
plt.rc('font', **font)","def exceptional_example(a, b):
    try:
        return a/b
    except Exception:
        print ""You tried dividing by zero""
    except TypeError:
        print ""Division does not make sense for %s and %s""%(a, b)",,"Analysis
Dataframes With Pandas (Complete)
0-Time-Series-Analysis->Time Series Analysis and Forecasting->Time Series Exploration With Pandas
Seasonal Arima->Using Seasonal ARIMA model to forcast short-time price
Time Series Forecast->loading and handling time series in pandas->__0.__import libraries
Stock-Prediction->Stock Price Prediction Problem->Loading and Handling Data into Pandas
Load Forecast Nyc Iso->ARIMA Model (Considering only the `capitl` region)
Aea1Assignment->Final assignment: Applied economics analysis 2017",Pack Inputs,Exception Handling->Exception Handling->Exception-handling with try-except can make this simpler
125802,"df_vets['state'] = df_vets.state.str.replace('*' , '')

print(df_vets.state.unique())","vet_pop_2005 = df_vets[df_vets['year'] == 2005]
vet_pop_2005 = vet_pop_2005.set_index('state')","import numpy as np
import pandas as pd #To read the dataset
import matplotlib.pyplot as plt #Plotting
import time
from IPython import display
%matplotlib inline",,Final Project->Data Cleaning/Pre-Processing->Visualize Veteran Suicide,Final Project->Data Cleaning/Pre-Processing->Visualize Veteran Suicide,Demo->The Math of Intelligence->Dependencies
99834,"books_bf_movies = merged_df[merged_df[""unixReviewTime_books""] < merged_df[""unixReviewTime_movies""]]
books_at_movies = merged_df[merged_df[""unixReviewTime_books""] > merged_df[""unixReviewTime_movies""]]
books_se_movies = merged_df[merged_df[""unixReviewTime_books""] == merged_df[""unixReviewTime_movies""]]

print(f""Review for book before review for movie : {100 * books_bf_movies.shape[0]/merged_df.shape[0]}%"")
print(f""Review for movie before review for book : {100 * books_at_movies.shape[0]/merged_df.shape[0]}%"")
print(f""Reviews the same day : {100 * books_se_movies.shape[0]/merged_df.shape[0]}%"")","#Only a couple rows drop due to NaN, I can proceed to assign daily climate data to my main data.
merg = big.copy()
print(merg.shape[0])
merg['DATEDAY']= merg['ADDDATE'].dt.date
noaa_all['DATE']= noaa_all['DATE'].dt.date
merg= pd.merge(merg, noaa_all, left_on='DATEDAY', right_on='DATE',
              how='left', sort=False)
print(merg.shape[0])
merg = merg.dropna()
print(merg.shape[0])","x= 5;y=5
x is y",,Project->Data collection->People buying all or a majority of products linked to the same franchise_id,Assignment-02->Exercise 23,"4->Python Operators->6. Special Operators->- Identity Operator
4->Python Operators->6. Special Operators->- Membership Operators"
14282,"def Xgboost(X_train,y_train,X_test,y_test):
    param_grid = {
        'silent': [False],
        'max_depth': [3,4,5,6,7],
        'subsample': [0.5, 0.6, 0.7,0.9],
        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8,],
        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, ],
        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],
        'n_estimators': [5,10,20,25]}
    fit_params = {
              'eval_set': [(X_train, y_train)]
              }
    clf = xgb.XGBRegressor()
    rs_clf = RandomizedSearchCV(clf, param_grid, n_iter=20,
                            n_jobs=1, verbose=2, cv=2,
                            fit_params=fit_params,
                            refit=True, random_state=42)
    
    
    #s_clf = xgb.XGBRegressor(max_depth=3,min_child_weight=1.0,subsample=0.6,colsample_bytree = 0.7,colsample_bylevel = 0.6,n_estimators = 20)
    
    rs_clf.fit(X_train, y_train)
    rs_clf.score(X_test, y_test)
    predict_y = rs_clf.predict(X_test)
    mutual_score = metrics.normalized_mutual_info_score(y_test, predict_y)
    roc_score = roc_auc_score(y_test, predict_y)
    return roc_score","# SVM
def svm_classifier(X_train,y_train, X_test, y_test):
    parameters = {'kernel': ['linear', 'rbf'], 'C': [0.01, 0.05, 0.1, 0.5, 1]}
    svc = SVC()
    clf = GridSearchCV(svc,parameters,cv=10)
    clf.fit(X_train, y_train)
    svm_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])
    return svm_score, clf","def count_words(filepath):
    with open(filepath, 'r') as file:
        string = file.read()
        string = string.replace("","", "" "")  # comm will be replace by space
        string_list = string.split("" "")
        return len(string_list)

print(count_words(""F:/words.txt""))",,Analysis->Objective:->Mutual_info_Score on target variable using non funding variable->Conclusion:,Hw3 Starter Notebook Submit->Step2 - ModelSet1 [35 points]->Data process we do,Problem26-50
282709,"canvas = toyplot.Canvas(width=600, height=150)
canvas.text(300, 100, ""foo <b>bar <i>baz <code>blah</code></i></b>"", style={""font-size"":""32px""});","canvas = toyplot.Canvas(width=600, height=150)
canvas.text(
    300,
    100,
    ""normal <b>bold</b> <i>italic</i> <strong>strong</strong> emphasis <small>small</small> <code>code</code>"",
    style={""font-size"":""24px""});","# All earthquake events in the last month; updates every 15 minutes
data = pd.read_csv(""http://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_month.csv"")",,Text->Rich Text,"Text
Text->Rich Text
Text->Rich Text
Text->Coordinate System Text",Quakes->Plotting Recent Earthquake Locations
215843,"%matplotlib inline
from preamble import *
plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures
# Comment out and restart notebook if you only want the last output of each cell.
InteractiveShell.ast_node_interactivity = ""all""","%matplotlib inline
from preamble import *
plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures
# Comment out and restart notebook if you only want the last output of each cell.
InteractiveShell.ast_node_interactivity = ""all""","import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0, 10, 100)
y = np.cos(x), np.cos(x + 1), np.cos(x + 2)
names = ['Signal 1', 'Signal 2', 'Signal 3']",,"A2-Iyer-Aklecha->Foundations of Data Mining: Assignment 2
Assignment 3->Foundations of Data Mining: Assignment 3
Assignment 1->Foundations of Data Mining: Assignment 1
Assignment 2 Stefan->Foundations of Data Mining: Assignment 2
Assignment 2 Part 1->Foundations of Data Mining: Assignment 2
A4-Himalini-Checkpoint->Foundations of Data Mining: Assignment 4
Assignment 2-Joost->Foundations of Data Mining: Assignment 2","A2-Iyer-Aklecha->Foundations of Data Mining: Assignment 2
Assignment 3->Foundations of Data Mining: Assignment 3
Assignment 1->Foundations of Data Mining: Assignment 1
Assignment 2 Stefan->Foundations of Data Mining: Assignment 2
Assignment 2 Part 1->Foundations of Data Mining: Assignment 2
A4-Himalini-Checkpoint->Foundations of Data Mining: Assignment 4
Assignment 2-Joost->Foundations of Data Mining: Assignment 2",2 Matplotlib->1\. Reproduce this figure
465467,"song_library = [
    (""Phantom Of The Opera"", ""Sarah Brightman""),
    (""Knocking On Heaven's Door"", ""Guns N' Roses""),
    (""Captain Nemo"", ""Sarah Brightman""),
    (""Patterns In The Ivy"", ""Opeth""),
    (""November Rain"", ""Guns N' Roses""),
    (""Beautiful"", ""Sarah Brightman""),
    (""Mal's Song"", ""Vixy and Tony"")
]

artists = set()
for song, artist in song_library:
    artists.add(artist)
print(artists)","plt.scatter(samples['x'], samples['y'], s=10)
plt.scatter(1, -2, marker='+', s=500, lw=5, c='white')
plt.gca().add_artist(plt.Circle((0, 0), 3, lw=4, ec='red', fc='none'))
plt.gca().set_aspect(1)","# Show overall metrics.
tfma.view.render_slicing_metrics(tfma_result_1)",,Data Structure->Data Structure,Mcmc->Data Analysis and Machine Learning Applications for Physicists->Markov Chain Monte Carlo->Examples,Chicago Taxi Tfma Local Playground->Visualization: Slicing Metrics
286903,"dt = tree.DecisionTreeClassifier(criterion='gini',min_samples_split=5, random_state=1024)
dt.fit(X_train, y_train)
#dt.fit(X, y)","from sklearn import tree
dt = tree.DecisionTreeClassifier(criterion='entropy')
dt = dt.fit(X_train, y_train)","status = batch_run_manager.get_latest_statuses()
status_counts = [[s, len(status[s])] for s in status]
print(tabulate.tabulate(status_counts, headers="""", tablefmt=""fancy_grid""))",,Decision Trees->Decision Trees->Exploration,"Session 4A Feature Eng->M/L Commando Course, Cambridge 2018->Feature extraction","Delta V Demo->This notebook demonstrates how to propagate an orbit back in time, then compute a state transition matrix on the forward propagation->Backwards propagation
Many Batch Run->This notebook shows how to run a whole lot of propagations"
393338,"# Perform evaluation on the test set
vectorizer = TfidfVectorizer(token_pattern='(?u)\\b\\w[a-zA-Z0-9_-]+\\b')
vectorizer.fit(get_features(data_cv))
features = vectorizer.transform(g.df['title'])
X, y = build_data(data_cv)
X_te, y_te = build_data(data_test)
score, clf = train_model_rf(X, y, X_te, y_te, best_depth)
print('Test accuracy:', score)","def fit_and_score(classifier, X_tr, Y_tr, X_te, Y_te):
    classifier.fit(X_tr, Y_tr)
    predictions = classifier.predict(X_te)
    return metrics.accuracy_score(Y_te, predictions)

print(""Naive Bayes w/count:   %0.3f"" % fit_and_score(MultinomialNB(), X_train_cv, Y_train, X_test_cv, Y_test))
print(""Naive Bayes w/tf-idf:   %0.3f"" % fit_and_score(MultinomialNB(), X_train_tfidf, Y_train, X_test_tfidf, Y_test))",t.head(8),,Final->Table of contents,Battle Of Tweet Classification Algorithms->Battle of the Tweet Classification Algorithms!->Time to Test,Section 1 - Zero Solution->Section 1 - Zero->What were the different towns that passengers embarked?
105649,"# redefined as dflogit before dummy variables
dflogit_1=flights_w_weather","flight_vars = [""Distance"", ""Month"", ""DayofMonth"", ""DayOfWeek"", ""Carrier"", ""Origin"", ""Dest"", ""CRSDepHr"", ""CRSArrHr"", 
          ""Month_Dest"", ""DayOfMonth_Dest"", ""DayOfWeek_Dest""]
hist_vars = [""Nflights_10"", ""DepDelayMedian_10"", ""DepDelayMax_10"", ""ArrDelayMedian_10"",
          ""ArrDelayMax_10"", ""TaxiOutMedian_10"", ""TaxiInMedian_10"", ""Ncan_10"", ""Ndiv_10"", ""Nflights_20"",
          ""DepDelayMedian_20"", ""DepDelayMax_20"", ""ArrDelayMedian_20"", ""ArrDelayMax_20"", ""TaxiOutMedian_20"",
          ""TaxiInMedian_20"", ""Ncan_20"", ""Ndiv_20"", ""Nflights_30"", ""DepDelayMedian_30"", ""DepDelayMax_30"", 
          ""ArrDelayMedian_30"", ""ArrDelayMax_30"", ""TaxiOutMedian_30"", ""TaxiInMedian_30"", ""Ncan_30"", 
          ""Ndiv_30"", ""TempFlight_30"", ""TempFlight_20"", ""TempFlight_10"", ""AllCanceled_30"", ""AllDiverted_30"", 
          ""AllCanceled_20"", ""AllDiverted_20"", ""AllCanDiv_20"", ""AllCanDiv_30"", ""AllCanceled_10"", 
          ""AllDiverted_10"", ""AllCanDiv_10""]
weather_vars = [""Temperature_Origin"", ""DewPoint_Origin"", ""Humidity_Origin"", ""WindSpeed_Origin"", 
          ""WindDirection_Origin"", ""Visibility_Origin"", ""Pressure_Origin"", ""Temperature_Dest"", ""DewPoint_Dest"",
          ""Humidity_Dest"", ""WindSpeed_Dest"", ""WindDirection_Dest"", ""Visibility_Dest"", ""Pressure_Dest"",
                ""Condition_Origin"", ""Condition_Dest""]",showUniques(train_test),,Capstone->EDA and Inferential Statistics->Data Prep,Extra Tree Data Source Importance->Flight data weather data->Converting string values to numerical values in all categorical columns,"1 Understand Data->Goals->In total we have 112,071 devices to predict"
274093,"pca = decomposition.PCA(n_components=2)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X = pca.fit_transform(X)","# TO BE COMPLETED

# for each feature remove the mean and scale to unit variance
X_scaled = StandardScaler().fit_transform(X)

# build a PCA instance  
pca = PCA()

# PCA transform 
X_pca = pca.fit_transform(X_scaled)",dfFull.hist(column='TOTCHG'),,"Dimensionality Reduction Solutions->Exercise: reconstruct the two primary components by hand.->To start with, a visualization of the first and second component has been done for you.","03 Cross Val->Model selection and assessment, cross validation->Data exploration through PCA",Hcup Imputation Rf Totchg->Check Outcome Variable Distribution
205270,global_data.head(),global_dfs_var_unc_data [start_label].head(1),"def set_fault(strike, dip, rake, depth):
    """"""
    Set the subfault parameters.
    Most are fixed for the examples below, 
    and only the strike, dip, and rake will be varied.
    """"""
    subfault = dtopotools.SubFault()
    subfault.strike = strike
    subfault.dip = dip
    subfault.rake = rake
    subfault.length = 100.e3
    subfault.width = 50.e3
    subfault.depth = depth
    subfault.slip = 1.
    subfault.longitude = 0.
    subfault.latitude = 0.
    subfault.coordinate_specification = ""top center""

    fault = dtopotools.Fault()
    fault.subfaults = [subfault]
    return fault, subfault

# Create a sample fault and print out some information about it...
fault, subfault = set_fault(0,0,0,5e3)
print ""This sample fault has %s meter of slip over a %s by %s km patch"" \
       % (subfault.slip,subfault.length/1e3,subfault.width/1e3)
print ""With shear modulus %4.1e Pa the seismic moment is %4.1e"" % (subfault.mu, subfault.Mo())
print ""   corresponding to an earthquake with moment magnitude %s"" % fault.Mw()
print ""The depth at the top edge of the fault plane is %s km"" % (subfault.depth/1e3)",,"Code->Exploring Weather Trends->Reading dataset
Explore Weather Trends->Explore Weather Trends-><a name=""moveplots"">Moving average data compare plots </a>",2->Load Prepared Data,Okada->The Okada model for computing seafloor deformation->Introduction->Import modules and define some utility functions used below...
322104,"#import os; os.chdir(""/notebooks/test_scripts"")

# System 
import os
import sys 
import shutil
from time import time

# Simple CNN model for CIFAR-10
import numpy as np

from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.constraints import maxnorm
from keras.optimizers import SGD
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.utils import np_utils
from keras.models import model_from_json

from keras import backend as K

# Datasets
from keras.datasets import cifar10
from keras.callbacks import ModelCheckpoint

# Images 
import matplotlib.pyplot as plt
from PIL import Image
from keras.utils.vis_utils import model_to_dot
from IPython.display import SVG

# Debugging 
import resource
import pprint
from keras.callbacks import TensorBoard
from keras_tqdm import TQDMNotebookCallback
from tensorflow.python import debug as tf_debug

# Stats
import pandas as pd","import numpy as np
import tensorflow as tf
import keras
from keras.datasets import mnist
from keras.models import Model
from keras.layers import Input, Dense, Dropout, Flatten, Activation
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
from matplotlib import pyplot as plt
from keras.models import model_from_json
from keras.utils import plot_model

from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot","mcalc.lcl(press, temp, dewpt)",,Cifar-10 Cnn->Setup: Load modules,Model Keras-Checkpoint,"Met Py->MetPy->Calculation functions
2015 Unidata Users Workshop->THREDDS?->Example"
126479,"data_transforms = transforms.Compose([
    transforms.RandomSizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])","print('\n[Phase 1] : Data Preparation')

##dataset
dataset = 'cifar100'

# Preparing the dataset
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(cf.mean[dataset], cf.std[dataset]),
]) # meanstd transformation","#We aggregated by Units in three of the pivot tables, so let's rename the items in the .columns attribute

store_agg_data.columns = ['avg_daily_sales','sum_promo_days','store_size','avg_sat_sales']
store_agg_data.head()",,Final Project->Image pre-processing and transformed to tensor,Lab11-Experiment1 4->Foundations Of AIML->Data Preparation,Hands On Learning->Python Exercises For All-GRAD Meetup->Pandas->Some Dataframe Features->Joining Data
218191,"# Set up and load modules nothing special here
%matplotlib inline
%load_ext autoreload
%autoreload 2
%load_ext autotime

import seaborn as sns
sns.set(color_codes=True)
import matplotlib.pyplot as plt
import numpy as np
plt.rcParams['savefig.dpi'] = 100

from IPython.display import display

from utils.readers import CsvReader
from clfpipeline import Clfpipeline
from classifiers.mlp import MLP
from classifiers.misc import merge_dicts","%matplotlib inline
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
import mpld3
import numpy as np
from datetime import timedelta
import os

sn.set_context('notebook')","equations = dict(hours=hours, lwage=lwage)
system_2sls = IV3SLS.from_formula(equations, data)
system_2sls_res = system_2sls.fit(method='ols', cov_type='unadjusted')
print(system_2sls_res)",,Report->Description of the repository structure,Explore Sensor Data 2,System Three-Stage-Ls->Three-stage Least Squares (3SLS)->Formulas
381089,"# Verify using pint's built in converter
f = 2 * ureg.pound * ureg.foot / ureg.second**2
f.to(ureg.force_pound)","# Verify using pint's built in converter
f = 2 * ureg.kilogram * ureg.meter / ureg.second**2
f.to(ureg.newton)",print(min(data['Approved'])),,02->2.4 Force and Weight->Test Yourself,02->2.4 Force and Weight->Test Yourself,"Cleaning Data - Solution->Cleaning Data - Solution->10. Fill The missing values of Approved Columns with 1997-9-19 date, then drop all rows with missing values, and then show .info() of the dataframe again, and oldest Approved Date"
226947,"from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating",ls_factor_returns = al.performance.factor_returns(factor_data),"numF = 10
coeffs31 = np.empty([numF, 3])
kf = sklearn.cross_validation.KFold(len(Y), n_folds=numF)
count = 0
for train, test in kf:
    trainSet = boston.ix[train]
    Y31 = np.array(trainSet['MEDV'])
    XX31 = standardize(np.array(trainSet[['RM', 'CRIM']]))
    a, b, i = multivariate_ols(XX31, Y31, 0.001, 100000, 0.00001)
    coeffs31[count][0] = a
    coeffs31[count][1] = b[0]
    coeffs31[count][2] = b[1]
    count += 1
print coeffs31",,"Collaborative Filtering->Collaborative Filtering with Spark
Fangyanc Hw5 Yelp Student->** Part 3: Collaborative filtering for recommendation **
Rec Sys->3. Usecase : Music recommender system->3.4 Training our statistical model->Question 7
[Lecture 3+4] Recommender Algorithm - Music Recommendations (1)-Checkpoint->3.4 Training our statistical model",Notebook->Alphalens Components->Information Coefficient,Ha Timothy-Ps4->3. Prediction
301672,"ev = np.argmax(mhws['intensity_max']) # Find largest event
print 'Maximum intensity:', mhws['intensity_max'][ev], 'deg. C'
print 'Average intensity:', mhws['intensity_mean'][ev], 'deg. C'
print 'Cumulative intensity:', mhws['intensity_cumulative'][ev], 'deg. C-days'
print 'Duration:', mhws['duration'][ev], 'days'
print 'Start date:', mhws['date_start'][ev].strftime(""%d %B %Y"")
print 'End date:', mhws['date_end'][ev].strftime(""%d %B %Y"")","ev = np.argmax(mhws['intensity_max']) # Find largest event
print 'Maximum intensity:', mhws['intensity_max'][ev], 'deg. C'
print 'Average intensity:', mhws['intensity_mean'][ev], 'deg. C'
print 'Cumulative intensity:', mhws['intensity_cumulative'][ev], 'deg. C-days'
print 'Duration:', mhws['duration'][ev], 'days'
print 'Start date:', mhws['date_start'][ev].strftime(""%d %B %Y"")
print 'End date:', mhws['date_end'][ev].strftime(""%d %B %Y"")","AUTOMATIC = ""Automatic""
MANUAL = ""Manual""

vehicles.loc[vehicles['Transmission'].str.startswith('A'),
             'Transmission Type'] = AUTOMATIC

vehicles.loc[vehicles['Transmission'].str.startswith('M'),
             'Transmission Type'] = MANUAL",,Example Synthetic-Checkpoint->Marine Heatwaves Definition->Marine Heatwave Detection,Example Synthetic-Checkpoint->Marine Heatwaves Definition->Marine Heatwave Detection,"Data Exploration->Data exploration->Aggregating to Higher-Level Categories
Data Exploratory Tool->Clustering"
341842,"# Here's a trick, try this out
print(favourite_drink[3:0:-1])","drink = 'beer'
price = 9.8
number = 1

print(""these %s's are good"" % (drink))
print('but %d costs $%.2f' % (number, price))","# d
(1 - p) ** 9 #* p",,"Day 1 - Unit 1->1. Your first steps with Python->2.3 Commenting
Data Visualization With Python - Cet Short Course - 14 - 15 March 2019->Case Study: Historical Daily Weather Records->Strings->Slicing",3-Functional-Programming->Learn Python for SEO,Chapter 17 - Exercises->Chapter 17 - Exercises->17.15
123193,"# we do some test on the loss function & grad
X_dev = X_train[:100]
y_dev = y_train[:100]

D = X_train.shape[1]  # 3073
num_classes = 10
# test the loss with W initialized very small
W = np.random.randn(D, num_classes) * 1.0e-5

loss,_ = svm_np(W, X_dev, y_dev, 1e-5)
print ('loss for weight close to zero: {:.4f}, we should expect loss is close to 9'.format(loss))","# we do some test on the loss function & grad
X_dev = X_train[:100]
y_dev = y_train[:100]

D = X_train.shape[1]  # 3073
num_classes = 10
# test the loss with W initialized very small
W = np.random.randn(D, num_classes) * 1.0e-5

loss,_ = svm_np(W, X_dev, y_dev, 1e-5)
print ('loss for weight close to zero: {:.4f}, we should expect loss is close to 9'.format(loss))","# Part 3 - Making predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)",,01->Linear classifier,01->Linear classifier,Ann->Original tutorial
110665,"train_set, test_set = sklearn.model_selection.train_test_split(df, test_size=.1, random_state=42)","X = normdata
y = df.rings.values
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.1, random_state=5)","# This is a demo screen.
print(""Here's how to print"")
a = 10",,1->GOAL: Predict median housing price per district->Build training and test set using **stratified sampling**.,Final Project->4. Predicting the Age of a New Data Point Using Nearest Cluster,Mission1
308210,"sns.factorplot(x=""Title"",y=""Survived"",data=dataset,kind=""bar"",size=6).set_ylabels(""Survival Probability"")","sns.factorplot(x=""Title"",y=""Survived"",data=dataset,kind=""bar"",size=6).set_ylabels(""Survival Probability"")","zip_income = dict(zip(zipcode, income))",,Titanic Challenge->3. Feature Analysis->5.1 Title from Name,Titanic Challenge->3. Feature Analysis->5.1 Title from Name,Final Cling->CS/STAT 287 Data Science Final Project->Business Aggregated Data
355991,"is_superkey_for(A, attributes, fds)",FD.params,"# Concat 2 series
s2 = pd.Series({'Y':0.01, 'N':0.03})
s3 = s.append(s2)
print(s3)",,"Sd202-Tp1->Keys and Superkeys
Sd202-Tp1->TP1 - Finding Keys using Functional Dependencies->How to discover keys in relations->Keys and Superkeys",Plot2,3-Advanced Data Structures-And-File-Parsing->Renaming and transforming a dataframe
224895,df.REVIEWS_COUNT.hist(bins=80),"df.review_count.hist(bins=np.arange(0, 40000, 400))
#df.review_count.hist(bins=100);",door1.status,,"Solution
Solution",Lab1-Pythonpandas->Visualizing,Python 3 Oop Part 1 Objects And Types->About this series
301225,"log_preds = learn.predict()
log_preds.shape",log_preds = learn.predict(),"ps.query('dress_similar', url=QUERY_URL)",,"Lesson1->00:24:12 - What does the model look like?
Breakfast->Predictions
Fast Ai-Kaggle-Starter-Kit->FastAI Kaggle Starter Kit->2. Data Prep->Looking at the initial validation scores
Dogsvs Cats J Pv2->Dogs vs Cats->Fine-tuning and differential learning rate annealing",Capstone->personal testing,Predictive Services->Query the model <a id='query'></a>
204551,"def get_page_source(url):
    with ur.urlopen(url) as page_source:
        return page_source.read()","# Example: List of events from nyc.com
import urllib.request as ur
url = 'http://www.nyc.com/events/?int4=1&from=10/15/2015&to=10/16/2015'
data = ur.urlopen(url).read().decode('utf-8')
print(data)",historical_avg = calculate_expanding_mean(df),,"Code-><u>Stock Market data</u>->Function to get the page-source of the company's url
Code->Stock Market data->Function to get the page-source of the company's url",Week 6 - Selenium-Checkpoint->Web Scraping part II,"04 Transform V2 Processed To Historical Averages->03 Transform V2 Processed into Historical Averages->Pull in V2 Processed DataFrame
04 Transform V2 Processed To Historical Averages-Checkpoint->03 Transform V2 Processed into Historical Averages->Pull in V2 Processed DataFrame"
1528,"#To access a row:
tempdf.loc[3]","#To access a row:
tempdf.loc[3]","iris=datasets.load_iris()
X=iris.data
y=iris.target

X=X[y !=0,:2] 
y = iris.target[y!=0]

svc = svm.SVC(kernel='linear', C=100).fit(X, y)
plot_estimator(svc,X,y)
#plt.scatter(svc.support_vectors_[:,0],svc.support_vectors_[:,1], c='r')",,Tutorial->Libraries->class methods->What is a `@staticmethod`?,Tutorial->Libraries->class methods->What is a `@staticmethod`?,Sv Ms And Kernels->Introduction to Support Vector Machines->And How would we split THIS dataset?->Review:
193349,"import matplotlib.pyplot as plt
%matplotlib inline
visualize_dataset(X_train, y_train, sign_names)","### Data exploration visualization code goes here.
### Feel free to use as many code cells as needed.

import matplotlib.pyplot as plt
# Visualizations will be shown in the notebook.
%matplotlib inline
import pandas as pd
sign_names = pd.read_csv('signnames.csv') 

i = 17330
image = X_train[i]
plt.imshow(image)
sign_name = sign_names['SignName'][y_train[i]]
print ('Name for this Sign is {}'.format(sign_name))","# Open primary input file
infile = open(barcodes_filename, 'r')
# Oops, the file is not there yet...",,Traffic Sign Classifier->Project: Building a Traffic Sign Recognition Classifier->Displaying random images from the dataset,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Include an exploratory visualization of the dataset,W4D5Part1->Week 4 Day 5 -- Selecting and Combining tools->First explorations
229287,"assert len(tokens) == len(token_to_id), ""dictionaries must have same size""

for i in range(n_tokens):
    assert token_to_id[tokens[i]] == i, ""token identifier must be it's position in tokens list""

print(""Seems alright!"")","assert len(tokens) == len(token_to_id), ""dictionaries must have same size""
for i in range(n_tokens):
    assert token_to_id[tokens[i]
                       ] == i, ""token identifier must be it's position in tokens list""

print(""Seems alright!"")",plot_hist(10),,Rnn-Task->Text processing->Cast everything from symbols into identifiers,Seminar Tf->Text processing->Cast everything from symbols into identifiers,Marshall Avery Hw4->Question 1->e.)
461797,"compare_images(images[1055],images[1056])",x.imag(),"def get_probability(one_score):
    '''
    This functions take in a single score - which could be any real number - 
    and squeezes it into the 0-1 range, respresenting the min and max of a prob
    '''
    s = one_score
    prob = round(sigmoid_function.subs({score:s}),4)
    return prob",,Duplicate Detection->DUPLICATE IMAGES,"Complex Viz Assignmnet-Solutions->Functions for absolute values, real and imaginary parts, and conjugates of complex numbers","Classifier - Week 1 - Extra->Effect on the sigmoid function
Classifier - Week 2 - Extra->Computing derivative and updating parameter->3. Comparison loop and vector solutions"
64406,"# collapsed gibbs sampler
theta_collapsed = np.array(theta_collapsed)


t1 = theta_collapsed[:,0]
t2 = theta_collapsed[:,1]
t3 = theta_collapsed[:,2]
t4 = theta_collapsed[:,3]
t5 = theta_collapsed[:,4]
t6 = theta_collapsed[:,5]
t7 = theta_collapsed[:,6]
t8 = theta_collapsed[:,7]
t9 = theta_collapsed[:,8]
t10 = theta_collapsed[:,9]

bins = np.linspace(0.03, 0.225, 70)

plt.hist(t1, bins, alpha=0.5, label='topic 1')
plt.hist(t2, bins, alpha=0.5, label='topic 2')
plt.hist(t3, bins, alpha=0.5, label='topic 3')
plt.hist(t4, bins, alpha=0.5, label='topic 4')
plt.hist(t5, bins, alpha=0.5, label='topic 5')
plt.hist(t6, bins, alpha=0.5, label='topic 6')
plt.hist(t7, bins, alpha=0.5, label='topic 7')
plt.hist(t8, bins, alpha=0.5, label='topic 8')
plt.hist(t9, bins, alpha=0.5, label='topic 9')
plt.hist(t10, bins, alpha=0.5, label='topic 10')

plt.legend(loc='upper right')
plt.show()
plt.savefig('figures/collapsed_distrib.png', bbox_inches='tight')","## define Chebyshev's up to from T1 to T8
T0 = lambda x: 1
T1 = lambda x: x
T2 = lambda x: 2*x**2 - 1
T3 = lambda x: 4*x**3 - 3*x
T4 = lambda x: 8*x**4 - 8*x**2 + 1
T5 = lambda x: 16*x**5 - 20*x**3 + 5*x
T6 = lambda x: 32*x**6 - 48*x**4 + 18*x**2 - 1
T7 = lambda x: 64*x**7 - 112*x**5 + 56*x**3 - 7*x
T8 = lambda x: 128*x**8 - 256*x**6 +160*x**4 - 32*x**2 + 1

## a plot of the first 6 Cheby Polys
plt.plot(T1(np.arange(-1,1,0.0001)),label='T1')
plt.plot(T2(np.arange(-1,1,0.0001)),label='T2')
plt.plot(T3(np.arange(-1,1,0.0001)),label='T3')
plt.plot(T4(np.arange(-1,1,0.0001)),label='T4')
plt.plot(T5(np.arange(-1,1,0.0001)),label='T5')
plt.plot(T6(np.arange(-1,1,0.0001)),label='T6')

plt.legend()","regex = re.compile(r'AAT\STAA')
if re.search(regex,'AATCTAA'):
    print('pattern found!')",,"Hw2->BGSE Text Mining Homework 2->Exercise 2->b) After the burn-in period, construct estimates of the predictive distribution of theta for each document across a number of draws from the samplers. Are the average values of these predictive distributions similar in the uncollapsed and collapsed samplers? How variable are these predictive distributions in the two algorithms across sample draws?",Is609 - Final Project Report->**Modeling with DSP Algorithms**->Part 2: Waveshaping Synthesis with Chebyshev Polynomials->Computation and Analysis in Python,"Lecture4-><span style=""color:blue"">Class exercise 4A</span>->The CSV format - a csv wrapper for files i/o->Being negative"
132206,"'''
from collections import Counter

# If a word appears only once, we ignore it completely (likely a typo)
# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller
def get_weight(count, eps=10000, min_count=2):
    if count < min_count:
        return 0
    else:
        return 1 / (count + eps)

eps = 5000 
words = ("" "".join(ques_train)).lower().split()
counts = Counter(words)
weights = {word: get_weight(count) for word, count in counts.items()}
'''

def tfidf_word_match_share(row):
    q1words = {}
    q2words = {}
    for word in str(row['question1']).lower().split():
        if word not in stops:
            q1words[word] = 1
    for word in str(row['question2']).lower().split():
        if word not in stops:
            q2words[word] = 1
    if len(q1words) == 0 or len(q2words) == 0:
        # The computer-generated chaff includes a few questions that are nothing but stopwords
        return 0
    
    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]
    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]
    
    R = np.sum(shared_weights) / np.sum(total_weights)
    return R","'''
from collections import Counter

# If a word appears only once, we ignore it completely (likely a typo)
# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller
def get_weight(count, eps=10000, min_count=2):
    if count < min_count:
        return 0
    else:
        return 1 / (count + eps)

eps = 5000 
words = ("" "".join(ques_train)).lower().split()
counts = Counter(words)
weights = {word: get_weight(count) for word, count in counts.items()}
'''

def tfidf_word_match_share(row):
    q1words = {}
    q2words = {}
    for word in str(row['question1']).lower().split():
        if word not in stops:
            q1words[word] = 1
    for word in str(row['question2']).lower().split():
        if word not in stops:
            q2words[word] = 1
    if len(q1words) == 0 or len(q2words) == 0:
        # The computer-generated chaff includes a few questions that are nothing but stopwords
        return 0
    
    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]
    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]
    
    R = np.sum(shared_weights) / np.sum(total_weights)
    return R","# Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView(""people"")

sqlDF = spark.sql(""SELECT * FROM people"")
sqlDF.show()",,Data Analysis,Data Analysis,Py Spark->Spark SQL->Running SQL Queries Programmatically
466227,"sentences = []  # Initialize an empty list of sentences

print ""Parsing sentences from training set""
for review in BPtrain[""review""]:
    sentences += review_to_sentences(review, tokenizer)

print ""Parsing sentences from unlabeled set""
for review in BP_unlabeled_train[""review""]:
    sentences += review_to_sentences(review, tokenizer)","# For label 
print(tf_train_label.shape ) 

# For label + unlabel
print(tf_train_label_new.shape)
print(tf_train_unlabel_new.shape)","plt.hist(np.log1p(x),bins=100)
plt.xlim([0,20])",,Word2Vec->Simple preprossing date (cleaning data),Ipython->data preprocessing->Vectorizing Training Data->label_dict = indexes of the possible label as per training data,Analysis Dist->Remove skewness using log1p
483770,"params = {
    'n_to_corrupt': param_map['n_to_corrupt'][2],
    'sigma': param_map['sigma'][0],
    'mu': param_map['mu'][0],
    'bad_chans': np.random.choice(128, param_map['n_to_corrupt'][2], replace=False)
}

bad_idx_30 = params['bad_chans']

D_30_corrupted= [corrupt(d, params) for d in D]
title = 'Discriminibility computed for various metrics, 30 Channels Corrupted.'
results_30 = auto_bench.metric_benchmark(D_30_corrupted, L, transforms, metrics, names, title)","RecordType = np.random.choice([""B""], record_count)
RecordType = corrupt_some_with(RecordType, 4, ["""",""X""])
df[""RecordType""] = RecordType.astype(str)
df[""RecordType""]","plot_val_with_title(most_by_correct(0, False), ""Most correct cats"")",,Metrics->Metric Methods->Behavior when corrupted,Data Generator->Mock Data Creator,Fastai Lesson1 V2->Review the code line by line
319127,"def curvature_hightlight(org_img, pt_img, Minv, ploty, left_fitx, right_fitx):

    warp_zero = np.zeros_like(pt_img).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))

    # Recast the x and y points into usable format for cv2.fillPoly()
    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])
    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])
    pts = np.hstack((pts_left, pts_right))
    # Draw the lane onto the warped blank image
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))

    # Warp the blank back to original image space using inverse perspective matrix (Minv)
    newwarp = cv2.warpPerspective(color_warp, Minv, (pt_img.shape[1], pt_img.shape[0])) 
    # Combine the result with the original image
    result = cv2.addWeighted(org_img, 1, newwarp, 0.3, 0)
    
    return result","def draw_lane(img_undistorted, img_warped, Minv, left_fitx, right_fitx, ploty):
    # Create an image to draw the lines on
    warp_zero = np.zeros_like(img_warped).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))

    if left_fitx is None:
        return cv2.warpPerspective(color_warp, Minv, (img_undistorted.shape[1], img_undistorted.shape[0])) 
    
    # Recast the x and y points into usable format for cv2.fillPoly()
    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])
    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])
    pts = np.hstack((pts_left, pts_right))

    # Draw the lane onto the warped blank image
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))

    # Warp the blank back to original image space using inverse perspective matrix (Minv)
    newwarp = cv2.warpPerspective(color_warp, Minv, (img_undistorted.shape[1], img_undistorted.shape[0])) 
    # Combine the result with the original image
    result = cv2.addWeighted(img_undistorted, 1, newwarp, 0.3, 0)
    return result","table = aberdeen.plot_heatmap(stream='T0095', year='2015', resample=True)",,Advanced-Lane-Lines-Checkpoint->Advanced-Lane-Lines->External Modules->3-lane region highlighting,Main->Apply lane detection to challenge video,Summary Code->What is in there ?
83451,"X = default[['balance', 'income', 'student_dummy']]
y = default['default']

qda = QuadraticDiscriminantAnalysis()

# Train qda model using default training data
default_qda = qda.fit(X,y)

# Get results for the training sample
default_predict = default_qda.predict(X)","X = default[['balance', 'income', 'student_dummy']]
y = default['default']

qda = QuadraticDiscriminantAnalysis()

# Train qda model using default training data
default_qda = qda.fit(X,y)

# Get results for the training sample
default_predict = default_qda.predict(X)","def flush(hand):
    return any([hand.count(suit) == 5 for suit in suits])",,4->Quadratic Discriminant Analysis,4->Quadratic Discriminant Analysis,Probability Introduction-Checkpoint->Creating Data->Examine the Cards within a Hand
454031,"#import the decision tree regressor from sklearn
from sklearn.tree import DecisionTreeRegressor
treereg = DecisionTreeRegressor()
treereg","# instantiate a DecisionTreeRegressor (with random_state=1)
from sklearn.tree import DecisionTreeRegressor
treereg = DecisionTreeRegressor(random_state=1)
treereg","import os,sys,cv2,timeit
from __future__ import print_function

from matplotlib import pyplot as plt
%matplotlib inline

# Bring in Xilinx ML-Suite Compiler, Quantizer, PyXDNN
from xfdnn.tools.compile.bin.xfdnn_compiler_caffe import CaffeFrontend as xfdnnCompiler
from xfdnn.tools.quantize.quantize import CaffeFrontend as xfdnnQuantizer
#import xfdnn.rt.xdnn as pyxfdnn
#import xfdnn.rt.xdnn_io as pyxfdnn_io
import xfdnn.rt.xdnn as pyxfdnn
import xfdnn.rt.xdnn_io as pyxfdnn_io

# Bring in darknet2caffe functions
from darknet2caffe import *

# Bring in Non-Max Suppression 
import nms

# Bring in utility for drawing boxes
from yolo_utils import generate_colors, draw_boxes

# Ignore some warnings from the quantizer
import warnings
warnings.simplefilter(""ignore"", UserWarning)

print(""Current working directory: %s"" % os.getcwd())
print(""Running on host: %s"" % os.uname()[1])
print(""Running w/ LD_LIBRARY_PATH: %s"" %  os.environ[""LD_LIBRARY_PATH""])
print(""Running w/ XILINX_OPENCL: %s"" %  os.environ[""XILINX_OPENCL""])
print(""Running w/ XCLBIN_PATH: %s"" %  os.environ[""XCLBIN_PATH""])
print(""Running w/ PYTHONPATH: %s"" %  os.environ[""PYTHONPATH""])
print(""Running w/ SDACCEL_INI_PATH: %s"" %  os.environ[""SDACCEL_INI_PATH""])
print(""Running w/ LIBXDNN_PATH: %s"" %  os.environ[""LIBXDNN_PATH""])

!whoami 

# Make sure there is no error in this cell
# The xfDNN runtime depends upon the above environment variables",,Support Vector Machine Regression & Decision Tree Regression - Bike Rentals->Build a Better model - Decision Tree Regression & K-fold Cross Validation,"17 Decision Trees-Checkpoint->Part 2: Classification trees->Building a regression tree in scikit-learn
Dmine-Circ-Ag-2017-Model1-Checkpoint->Machine Learning Analysis->Building a regression decision tree in scikit-learn
Dmine-Circ-Ag-Modeling-Part4-Checkpoint->Machine Learning Demonstration - CIRC DMINE AG->Building a regression decision tree in scikit-learn
15 Decision Trees-Checkpoint->Above are the regions created by a Regression Tree algorithm:->How does a computer build a regression tree?->Demo: Choosing the ideal cutpoint for a given feature",Ml-Suite-Developer-Lab->Xilinx ML Developer Lab
304153,"# So you can slice a column from the df and save it into its own variable that is a series
ser = df.ix[:,df.columns[-2]]
print(ser)","# So you can slice a column from the df and save it into its own variable that is a series
ser = df.ix[:,df.columns[-2]]
print(ser)","df_authCombined.drop(['name_x','name_y'],axis=1, inplace=True)",,Pandas Tutorial->A more in-depth look at pandas->Let's just make a mental note that a dataframe is a bunch of series together,Pandas Tutorial->A more in-depth look at pandas->Let's just make a mental note that a dataframe is a bunch of series together,02 Nips 2017 Data Download->Assess what needs to be done to merge 2017 with previous->Start with authors
294540,"# Multiple rows
brics.loc[[""RU"", ""IN"", ""CH""]]","merge(bricPop, gdpVsLife, on='Country name', how='left')",df_sent['nouns'] = nouns,,Pandas->Pandas->Indexing & Selecting Data->Row Access loc,"Week 3 Exercise Notebook->Exercise 4: Applying functions->Task->_So, here's how the joins work:_->Now we can try joining on this with gdpvsLife",Topic-Modelling->Linking body types to comments
365634,"# The .isnull() functions are used to find bad data
# The .any() function returns the columns that contain any True values

display(patients.isnull().head(5))
display(patients.isnull().any())

# Let's get a list of all of the columns with some missing data 
missing_cols=patients.columns[patients.isnull().any()]
print(missing_cols)

# We can see how many cells have missing data for each column
patients[missing_cols].isnull().sum()",patient_answers.head(),"# Create figure and axes and set figure size
f, ax = plt.subplots(figsize=(10,10))

# Create sorted average share prices per company
a = sp.groupby('company').mean()['Close'].sort_values(ascending=False)

# Barplot
sns.barplot(x=a, y=a.index)
ax.set_title('Average Share Price', fontsize=20)
ax.set_ylabel('Company', fontsize=15)",,"Amia Workshop->2.0 Read in the data->4.3 Remove rows with missing data
Nkbds Track1 Data Prep->4.3 Remove rows with missing data",Tremor Analysis->1. PreProcessing->1. Patient Questionnaire,Stock Data Prep-Checkpoint->Stock Data Preparation->Exploring
306742,"def getVaR(nFiles, 
          factor_columns, 
          factorMeans,
          factorCov,
          start,
          end,
          nYears = 5,
          nTrials = 10000):
    
    # get stocks history files and convert them into lists
    files = [join(stock_folder, f) for f in sorted(listdir(stock_folder)) if isfile(join(stock_folder, f))]
    files = files[:nFiles]
    rawStocks = [process_stock_file(f) for f in files]

    # select only stocks having nYears of history and values between start and end
    rawStocks = list(filter(lambda instrument: len(instrument) >= number_of_years * 260 and
                                                instrument[0][0] <= start and 
                                                instrument[-1][0] >= end,
                            rawStocks))
    
    # trim the time region from start to end and fill missing values
    stocks = list(map(lambda stock: \
                fillInHistory(
                    trimToRegion(stock, start, end), 
                start, end + timedelta(days=1)), 
            rawStocks))
    
    # get two-week returns
    stocksReturns = list(map(twoWeekReturns, stocks))

    # estimate weights
    weights = [estimateParams(ret, factor_columns) for ret in stocksReturns]

    # run Monte Carlo simulations
    parallelism = 12
    trial_indexes = list(range(0, parallelism))
    seedRDD = sc.parallelize(trial_indexes, parallelism)
    bFactorWeights = sc.broadcast(weights)
    trials = seedRDD.flatMap(lambda idx: \
                    simulateTrialReturns(
                        max(int(nTrials/parallelism), 1), 
                        factorMeans, factorCov
                    ))
    trials.cache()

    # get VaR and CVaR
    valueAtRisk = fivePercentVaR(trials)
    conditionalValueAtRisk = fivePercentCVaR(trials)

    # validate results
    varConfidenceInterval = bootstrappedConfidenceInterval(trials, fivePercentVaR, 100, 0.05)
    cvarConfidenceInterval = bootstrappedConfidenceInterval(trials, fivePercentCVaR, 100, .05)
    pValue, failPerc = kupiecTestPValue(stocksReturns, valueAtRisk, 0.05)
    
    return [valueAtRisk, conditionalValueAtRisk], [varConfidenceInterval, cvarConfidenceInterval], [pValue, failPerc]","def runEverything(num_stocks, factorReturns, factorMeans, factorCov, verbose=True):
    # select path of all stock data files in ""stock_folder""
    files_100 = [join(stock_folder, f) for f in listdir(stock_folder) if isfile(join(stock_folder, f))]
    files_100 = files_100[:num_stocks]

    # read each line in each file, convert it into the format: (date, value)
    rawStocks_100 = [process_stock_file(f) for f in files_100]

    # select only instruments which have more than 5 years of history
    number_of_years = 5
    rawStocks_100 = list(filter(lambda instrument: \
                                instrument[-1][0].year -instrument[0][0].year >= number_of_years, rawStocks_100))
    
    stocks_100 = list(map(lambda stock: fillInHistory(trimToRegion(stock, start, end), start, end), rawStocks_100))
    
    stocksReturns_100 = list(map(twoWeekReturns, stocks_100))
    
    # transpose factorsReturns
    factorMat = transpose(factorsReturns) 

    # featurize each row of factorMat
    factorFeatures = list(map(featurize,factorMat))

    # OLS require parameter to be a numpy array
    factor_columns = np.array(factorFeatures)

    # add a constant - the intercept term for each instrument i.
    factor_columns = sm.add_constant(factor_columns, prepend=True) 

    # estimate weights    
    weights_100 = [estimateParams(stockReturns, factor_columns) for stockReturns in stocksReturns_100] 
    
    parallelism = 12
    numTrials = 10000
    trial_indexes = list(range(0, parallelism))
    seedRDD = sc.parallelize(trial_indexes, parallelism)
    bFactorWeights_100 = sc.broadcast(weights_100)

    trials_100 = seedRDD.flatMap(lambda idx: \
                             simulateTrialReturns(max(int(numTrials/parallelism), 1), \
                                                  factorMeans, factorCov, bFactorWeights_100.value))
    trials_100.cache()

    valueAtRisk_100 = fivePercentVaR(trials_100)
    conditionalValueAtRisk_100 = fivePercentCVaR(trials_100)  
    varConfidenceInterval_100 = bootstrappedConfidenceInterval(trials_100, fivePercentVaR, 100, 0.05)
    cvarConfidenceInterval_100 = bootstrappedConfidenceInterval(trials_100, fivePercentCVaR, 100, .05)
    
    if verbose == True:
        print(""number of stocks selected:"", len(files_100))
        print(""number of stocks after filtering:"", len(stocks_100))
        print(""weights shape [> 100 stocks] :"", np.shape(weights_100))
        print(""Value at Risk(VaR) 5%:"", valueAtRisk_100)
        print(""Conditional Value at Risk(CVaR) 5%:"", conditionalValueAtRisk_100)
        print(""VaR confidence interval: "" , varConfidenceInterval_100)
        print(""CVaR confidence interval: "" , cvarConfidenceInterval_100)
        
        
    kupiecTest = kupiecTestPValue(stocksReturns_100, valueAtRisk_100, 0.05)
    pValue = kupiecTest[0]
    numFailures = kupiecTest[1]
    
    if verbose == True:
        print(""Kupiec test p-value: "" , pValue)
        
    return (num_stocks, valueAtRisk_100, conditionalValueAtRisk_100, numFailures, pValue)","new_record = prep.remove_baseline_wandering(raw_record)
wfdb.plot_wfdb(new_record)",,02 Monte Carlo Simulation->2. Illustrative example->2.4. Using the Monte Carlo Simulation method to improve our model->Question 9,[Lecture 7+8] Estimating Financial Risk Through Monte Carlo Simulation->Question 9,Preprocessing->Showcase
487709,"direc=pd.read_csv('/Users/alfonsodamelio/dumps/directors.csv',delimiter=';',encoding='latin-1')
direc=direc.set_index('id')
direc=direc.to_dict('index')

movies_direc=pd.read_csv('/Users/alfonsodamelio/dumps/movies_directors.csv',delimiter=';')
movies_direc=movies_direc.set_index('director_id')
movies_direc=movies_direc.to_dict('index')


for key in direc.keys():
    if key in movies_direc.keys():
        direc[key]['movies_id']=movies_direc[key]['movie_id']
        direc[key]['director_id']=key
              
director_genres=pd.read_csv('/Users/alfonsodamelio/dumps/directors_genres.csv',delimiter=';')
director_genres=director_genres.set_index('director_id')
director_genres=director_genres.to_dict('index')      


for key in direc.keys():
    if key in director_genres.keys():
        direc[key]['director_genre']=director_genres[key]['genre']
        direc[key]['director_id']=key
              

dataframe=pd.DataFrame.from_dict(direc,orient='index')
dataframe=dataframe.set_index('movies_id')
dataframe=dataframe.to_dict('index') 

movies=pd.read_csv('/Users/alfonsodamelio/dumps/movies.csv',delimiter=';')
movies=movies.set_index('id')
movies=movies.to_dict('index')


for key in dataframe.keys():
    if key in movies.keys():
        dataframe[key]['movie_name']=movies[key]['name']
        dataframe[key]['movie_rank']=movies[key]['rank']
        dataframe[key]['movie_year']=movies[key]['year']
        dataframe[key]['movie_id']=key

        
movies_genre=pd.read_csv('/Users/alfonsodamelio/dumps/movies_genres.csv',delimiter=';')
movies_genre=movies_genre.set_index('movie_id')
movies_genre=movies_genre.to_dict('index')


for key in dataframe.keys():
    if key in movies_genre.keys():
        dataframe[key]['movie_genre']=movies_genre[key]['genre']
        dataframe[key]['movie_id']=key


roles=pd.read_csv('/Users/alfonsodamelio/dumps/roles.csv',delimiter=';')
roles=roles.set_index('movie_id')
roles=roles.to_dict('index')


for key in dataframe.keys():
    if key in roles.keys():
        dataframe[key]['actor_role']=roles[key]['role']
        dataframe[key]['actor_id']=roles[key]['actor_id']
        dataframe[key]['movie_id']=key


new=pd.DataFrame.from_dict(dataframe,orient='index')
new=new.set_index('actor_id')
new=new.to_dict('index') 

actors=pd.read_csv('/Users/alfonsodamelio/dumps/actors.csv',delimiter=';')
actors=actors.set_index('id')
actors=actors.to_dict('index')

for key in new.keys():
    if key in actors.keys():
        new[key]['actor_name']=actors[key]['first_name']
        new[key]['actor_last_name']=actors[key]['last_name']
        new[key]['actor_gender']=actors[key]['gender']
        new[key]['actor_id']=key


IMDB=[i for i in new.values()]


#write json to work in local
with open('/Users/alfonsodamelio/Desktop/DATA SCIENCE/2°SEMESTRE/Data Management(Rosati)/HW3/IMDB.json', 'w') as fp:
    json.dump(IMDB, fp)

#read json
with open('/Users/alfonsodamelio/Desktop/DATA SCIENCE/2°SEMESTRE/Data Management(Rosati)/HW3/IMDB.json',encoding=""utf-8"") as json_data:
    imdb = json.load(json_data)
    
#read json of other dataset
with open('/Users/alfonsodamelio/Desktop/DATA SCIENCE/2°SEMESTRE/Data Management(Rosati)/HW3/movie.json',encoding=""utf-8"") as json_data:
    movie_json = json.load(json_data)
    
    
for i in movie_json:

    if len(i['Actors'].split())>=2:
        lista=i['Actors'].split()[0:2]
        i['actor_name']=lista[0]
        i['actor_last_name']="""".join((char for char in lista[1] if char not in string.punctuation))
        del i['Actors']
            
    else:
        pass
    
for i in movie_json:  
    
    if len(i['Director'].split())>=2:
        lista2=i['Director'].split()[0:2]
        i['director_name']=lista2[0]
        i['director_last_name']=lista2[1]
        del i['Director']
    else:
        pass
    
for i in movie_json:  
    
    if len(i['genre'].split(','))>=2:
        lista3=i['genre'].split(',')[0:2]
        i['movie_genre']=lista3[0]
        i[""actor_gender""]=""NaN""
        i[""actor_id""]=""NaN""
        i[""director_id""]=""NaN""
        i[""actor_role""]=""NaN""
        
        del i['genre']
    else:
        pass
       
def removekey(d, key):
    r = dict(d)
    del r[key]
    return r

nuovo_movies=[]
for i in movie_json:
    nuovo_movies.append(removekey(i, ""Description""))

nuovo_movies1=[]
for i in nuovo_movies:
    nuovo_movies1.append(removekey(i, ""Runtime (Minutes)""))
del nuovo_movies

nuovo_movies2=[]
for i in nuovo_movies1:
    nuovo_movies2.append(removekey(i, ""Rank""))
del nuovo_movies1

nuovo_movies3=[]
for i in nuovo_movies2:
    nuovo_movies3.append(removekey(i, ""Votes""))
del nuovo_movies2

nuovo_movies4=[]
for i in nuovo_movies3:
    nuovo_movies4.append(removekey(i, ""Revenue (Millions)""))
del nuovo_movies3

imdb2=[]
for i in nuovo_movies4:
    imdb2.append(removekey(i, ""Metascore""))
del nuovo_movies4","cols = movies.columns.values
cols = np.append(cols, list(genres))
movies_genr_expaned = pd.DataFrame(movies, columns=cols)
movies_genr_expaned = movies_genr_expaned.fillna(False)
for genr in genres:
    movies_genr_expaned[genr] = movies_genr_expaned['genres'].str.contains(genr)","xp = tf.placeholder(dtype=tf.int32)
sum_p = tf.add(xp, y)
sess.run(sum_p, {xp: 5})",,Mongodb->Change structure of data,Movie Data Analysis,Tf Intro To Tensorflow->Repeating calculations
323281,"# initialize variables (standardized)
y = np.ravel(np.vstack(standardize(bdata.target)))
x = np.vstack(standardize(bdata.data[:,5])) # rm
c = np.vstack(standardize(bdata.data[:,0])) # crim
xvalue_matrix = np.concatenate((x,c), axis = 1)


# create k indices
kf = KFold(len(xvalue_matrix), n_folds=10)

# initialize lists to hold alpha and betas
betalist = []
alphalist = []

# loop through each fold, run regression, store alpha and betas in lists
for train_ix, test_ix in kf:
    xvalue_matrix_train = xvalue_matrix[train_ix]
    y_train = y[train_ix]
    alpha, betas, count = multivariate_ols(xvalue_matrix_train, y_train, R=0.01, MaxIterations=100000)
    alphalist.append(alpha)
    betalist.append(betas)

# take means of lists to produce cross-validated estimates
alphahat = np.array(alphalist).mean()
betahats = np.array(betalist).mean(axis = 0)

print alphahat, betahats","xy_coordinates = read_Input(x_coordinates, y_coordinates)
x_coordinates = xy_coordinates['x']
y_coordinates = xy_coordinates['y']

alpha_list = construct_Alphalist(window_span, x_coordinates, y_coordinates, alpha_list)
beta_list = construct_Betalist(alpha_list, beta_list)

result = smoothing_Plot(2 * window_span, threshold, x_coordinates, y_coordinates, beta_list)
x_smooth1 = result['x_smooth1']
x_smooth2 = result['x_smooth2']
smooth_y_coordinates = result['smooth_y_coordinates']","np.rot90(my_3_8_array, k=-1)
# rotate 90 degrees clockwise",,Ps4->3. Prediction,Algo1 V3-Checkpoint->Main code,Array Rearrangnig Array Elements->Lynda.com->ROT functions
159512,"barplot = sns.barplot( x = ""Hipertension"", y = ""Scholarship"", data = parsed_df)
barplot.set(ylabel = ""Scholarship Received Proportion"",title = ""Scholarship Vs Hipertension"")","fig, ax = plt.subplots()
sn.barplot(data=hour_df[['month',
                        'total_count']],
          x = ""month"", y=""total_count"")
ax.set(title=""Month distribution of counts"")","crimes_weekday_hour = crimes.groupby(['day_of_week', 'hour']).size().reset_index().rename(columns={0:'count'})
crimes_weekday_hour.head()",,Investigate A Dataset->Project: Patient No Show Appointment Dataset->Scholarship Vs Hipertension,Bike Sharing Trends->Preprocessing->Distribution and Trends,Brano Temporal Analysis->Analysis->Crimes for every hour in a week
173940,"df1.loc[idx[:,'n3']]","df1.loc[idx, 'close'] = np.nan
df1","param = {'max_depth': 10, 'eta': 1, 'silent': 1, 'objective': 'reg:linear', 'num_feature': 1}
num_round = 2
train_dmatrix = xgb.DMatrix(data=x_train.reshape(-1,1), label=y_train.reshape(-1,1))
bst = xgb.train(param, train_dmatrix, num_round)",,04->Example for DataFrame->Now try slicing on the second dimension using `pd.IndexSlice`,02-Intro-To-Pandas-Part-2-Slides->DataFrame->Selecting and Assigning with DataFrames,Random Forests And Xgboost->Random Forest and XGBoost->Scikit-learn RandomForestRegressor->Train
462834,"schema_path = os.path.dirname('__file__') + '../data/twitter-swisscom/schema.txt'

# Load the schema
schema = pd.read_csv(schema_path, delim_whitespace=True, header=None)
schema","df = pd.read_csv(os.path.join('..', 'datasets', 'zillow-03-starter.csv'), index_col = 'ID')","# Histo + KDE
sns.distplot(tips.total_bill, hist=True, kde=True);",,Cleaning->Cleaning notebook->Importing the data,"Codealong-03-Descriptive-Statistics-For-Exploratory-Data-Analysis-Starter-Code-Checkpoint->DS-SF-25 | Codealong 03 | Descriptive Statistics for Exploratory Data Analysis
Codealong-03-Exploratory-Data-Analysis-Answer-Key->DS-SF-27 | Codealong 03 | Exploratory Data Analysis | Answer Key->Part B
Codealong-03-Answer-Key->SF-DAT-21 | Codealong 3",Seaborn Visualizations & Data->Visualize Data
279507,"from IPython.display import Image
Image(""/Users/surthi/gitrepos/ml-notes/images/confusion-matrix.jpg"")","Xm = -1/imag(Yex)
Xm","dropList = [""borough"",""ward"",""borough_code"",""latitude"",""longitude"",""yearBetween"",\
            ""YearMon"",""Year"",""Month"",""ward_code"",'avg_co2_emission_in_kt', 'avg_population']
londonPrice = londonPrice.drop(dropList,axis=1)

train = londonPrice[londonPrice[""year_month_new""]<201401]
test = londonPrice[londonPrice[""year_month_new""]>=201401]
testCols = train.columns[3:]",,6,Ch2-Problem 2-06->Excercises Electric Machinery Fundamentals->Problem 2-6->SOLUTION->(c),"Linear Regression->Global Imports->Filtering Data From 2009 (execute It only if working on subset of data)
Lasso Regression With Grid Search->Global Imports->Filtering Data From 2009 (execute It only if working on subset of data)"
373924,Image(filename='/Users/pinesol/nlp/hw1/two-layer-accuracy.png'),x.imag(),"gene_set_dropdown = widgets.Dropdown(
    options=gene_set_scores_df.columns,
    description='Gene Set:'
)

day_selector = widgets.SelectionRangeSlider(
    options=unique_days,
    continuous_update=False,
    index=(0,len(unique_days)-1),
    description='Days'
)

def update_gene_set_vis(name, days):
    gene_set_score_coords = gene_set_scores_df[(gene_set_scores_df[day_obs_name]>=days[0]) & (gene_set_scores_df[day_obs_name]<=days[1])]
    figure = plt.figure(figsize=(10, 10))
    plt.axis('off')
    plt.tight_layout()
    plt.title(name + ', days {}-{}'.format(days[0], days[1]))
    plt.scatter(coord_df['x'], coord_df['y'], c='#f0f0f0',
                   s=4, marker=',', edgecolors='none', alpha=0.8)
    plt.scatter(gene_set_score_coords['x'], gene_set_score_coords['y'], c=gene_set_score_coords[name],
                   s=4, marker=',', edgecolors='none')
    cb = plt.colorbar()
    cb.ax.set_title('Signature')
    figure2 = plt.figure(figsize=(10, 5))
    plt.title(name + ', days {}-{}'.format(days[0], days[1]))
    plt.hist(gene_set_score_coords[name])
    return figure, figure2

widgets.interact(update_gene_set_vis, name=gene_set_dropdown, days=day_selector)",,Homework 1->NLP Homework 1->Experiment 3: An additional convolution layer,"Complex Viz Assignmnet-Solutions->Functions for absolute values, real and imaginary parts, and conjugates of complex numbers",Fle-Cell Sets-Gene Sets->Gene signatures->Visualize gene set scores
489326,"traindataB=dataframeB[dataframeB.columns[:520]]

targetdataB=dataframeB[""class""]

clfB = GaussianNB()

predictvalueB= clfB.fit(traindata,targetdata).predict(traindataB)

print(accuracy_score(targetdataB,predictvalueB))","from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf = clf.fit(features_train, labels_train)   
accuracy = clf.score(features_train, labels_train)
print ""Accuracy Train:"", accuracy

pred = clf.predict(features_test)
accuracy = clf.score(features_test, labels_test)
print ""Accuracy Test:"", accuracy","k = f.test_conjugate_to(g)
print(k)
~k*f*k == g",,S1669411->1. Exploration of the dataset [40%]->========== Question 2.8 ==========,Final Project->Udacity: Intro to Machine Learning - Final Project->3 Algorhithm Selection->3.1 Naive Bayes,Thompson Example->An introduction to the `thompson` package
83365,"# Calculate predicted values.
fit_kmeans = KMeans(n_clusters=6, random_state=123).fit(X_pca)
pred_kmeans = fit_kmeans.predict(X_pca)

# Plot in 3D.
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = Axes3D(fig)
ax.scatter(X_pca[:, 0], X_pca[:, 1],  X_pca[:, 2], c=pred_kmeans)
plt.show()

# Run classification report.
from sklearn.metrics import classification_report
print(classification_report(Y_clus, pred_kmeans))","%timeit pass
# Calculate predicted values.
y_pred = KMeans(n_clusters=4, random_state=42).fit_predict(X_pca)

# Plot the solution.
plt.scatter(x=X_pca[:, 0], y=X_pca[:, 1], c=y_pred)
plt.show()","df_sign_hh = df.loc[(df['Number_households'] > 500) & (df['Apr'] > 0)]
df_sign_hh.head()",,4->B. Clusters->1. K-Means,Challenge-4-2-5->Kmeans->Four clusters,Water Usage->Cape Town water crisis->Subselecting significant suburbs
124258,"plot_by_age(non_survived_df,'nonsurvival')
plot_by_age(survived_df,'survival')
plt.show()","ag.unstack().plot.bar(figsize = (15, 4),stacked=True)
plt.xticks(rotation=-70)","def number_of_trips(filename):
    """"""
    This function reads in a file with trip data and reports the number of
    trips made by subscribers, customers, and total overall.
    """"""
    # Create dataframe
    df = ps.read_csv(filename)
    
    # initialize count variables
    n_subscribers = len(df[df['user_type']=='Subscriber'])
    n_customers = len(df[df['user_type']=='Customer'])
    
    # compute total number of rides
    n_total = n_subscribers + n_customers
    
    #compute proportion of trips made by subscribers
    p_subscribers = n_subscribers/n_total

    # return tallies as a tuple
    return(n_subscribers, n_customers, n_total, p_subscribers)",,Final Project,Baseball Game Tickets Demand Analysis And Preidiction->2. Weather,Bike Share Analysis
418715,"opt = widgets.RadioButtons(
    options=['Hide', 'Retrieve'],
    value='Hide',
    disabled=False,
)

opt","StaticInteract(makeSinglePlot,
               groupName=RadioWidget(['violence offenders','child sex offenders','general population']),
               participant=RangeWidget(0, 29, 1)
              )","# print ('Thermal Relic: {0}'.format(alphaTherm(100.,0.01))) # axth
# print ('V0: {0}'.format(v0func(100.)) )# v0
# print ('TreeCross: {0}'.format(sigmaVtree(100., 0.01, alphaTherm(100.,0.01))) )# sigmavTree
# print ('Sommerfeld: {0}'.format(sommerfeld(v0func(100),100,0.01, alphaTherm(100,0.01)))) #Ss
# print ('ThermAvgSommer: {0}'.format(thermAvgSommerfeld(100.,0.01, alphaTherm(100.,0.01)))) # Sav 
# print ('ThermAvgCross: {0}'.format(thermAvgCrossSectionV(100,0.01))) 
# print ('AnnihilationRate: {0}'.format(CAnnSomm(100., 0.01)))

# print ('test: {0}: '.format(test(v0func(100),100.,0.01,)))
# print ('thermAvgSommerfeld: {0}'.format(thermAvgSommerfeld(100,1,alphaTherm(100,1))))
# print ('CAnnSomm: {0}'.format(CAnnSomm(100.,0.01)))
# print ('CAnn: {0}'.format(CAnn(100.,0.01)))

# print (CAnn(100,0.1))

# print (CAnn(1,0))
# alpha_XTemp = alphaTherm(1000,1)
# print (sigmaVtree(1000, 1, alpha_XTemp))
# alpha_XT = alphaTherm(1000,0)
# print (sigmaVtree(1000,0,alpha_XT))

print ('Complete')",,Demo-Checkpoint->Sound-spy easy demo program->Hiding sound in image vs. retrieving sound from image,005 Fitting Functions->Plot fitted functions->plot curve parameters,[Debug]Dmcc V7->Annihilation Rate->Thermally Averaged Cross Section: $\langle \sigma_{ann}v \rangle$
303379,from machine_functions import *,"# imports from PySpark
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import \
    GBTRegressor as SparkML_GBTRegressor, \
    RandomForestRegressor as SparkML_RandomForestRegressor
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder","# When your index is datetimes, you have a Timeseries.

df = df.set_index('local_date_time_full')
df = df.sort_index()
df",,Presentation->Machine Learning->Timeline: K-Nearest Neighbors,Boston Housing Trees Random Forests Boosted Additive Models->_BONUS:_ Apache Spark solution->Import PySpark and initialize SparkContext & HiveContext,Slides->Group By!
466992,"url = 'http://www.ldeo.columbia.edu/~felixw/NCAeqDD/NCAeqDD.v201112.1.gz'
df = pd.read_csv(""./datasets/NCAeqDD.v201112.1"",  
                 delim_whitespace=True, skiprows=79, na_values=[-1],
                 names=[""YEAR"", ""MONTH"", ""DAY"", ""HOUR"", ""MINUTE"", ""SECOND"",
                        ""LAT"", ""LON"", ""DEP"", ""EH1"", ""EH2"", ""AZ"", ""EV"", ""MAG"", ""ID""])
df.head(10)","# read data into dataframe, including column headers
df = pd.read_csv(url, header=0)
# print out first entries
df.head()","def print_epoch_stats(epch_i, sess, last_features, last_labels):
    current_cost = sess.run(
        cost, 
        feed_dict={features: last_features, labels: last_labels})
    valid_accuracy = sess.run(
        accuracy, 
        feed_dict={features: valid_features, labels: valid_labels})
    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy'.format(
        epoch_i,
        current_cost,
        valid_accuracy))",,Plotting->Data Visualization in Python with [matplotlib](https://matplotlib.org/) and [pandas](https://pandas.pydata.org/),Unge Lovende->NRK Intervjuoppgave Data Scientist: Emiliano Guevara->Analysis->8. Exploring binge watching of Unge Lovende,Tensorflow Basics->Introduction to Tensorflow->Stochastic Gradient Descent->Training the Model on Epochs
260786,help(axs.read_arxiv),help( cust17.to_sql ) # figure out how to get rid of the index,"sequence = [(blobs, 'Singapore 1', png),]

fig, axes = plt.subplots(2, 1, figsize=(30, 30), sharex=True, sharey=True,
                         subplot_kw={'adjustable': 'box-forced'})
ax = axes.ravel()

for idx, (blobs, title, img) in enumerate(sequence):
    ax[idx].set_title(title)
    ax[idx].imshow(img, cmap='gray', interpolation='nearest')
    for blob in blobs:
        y, x, r = blob
        c = plt.Circle((x, y), r, color='blue', linewidth=7, fill=False)
        ax[idx].add_patch(c)
    ax[idx].set_axis_off()

plt.tight_layout()
plt.show()",,Example->Reading arXiv while Updating Your Personal Data,Dw3-Short->Accessing rows when there is a hierarchichal index,Ship Detection->Project export->Export as PNG
374274,"EBOLA_FOLDER = DATA_FOLDER + ""ebola/""
GUINEA_FOLDER = EBOLA_FOLDER + ""guinea_data/""
LIBERIA_FOLDER = EBOLA_FOLDER + ""liberia_data/""
SL_FOLDER = EBOLA_FOLDER + ""sl_data/""","ebola_df = pd.concat(
    [
        guinea_df,
        liberia_df,
        sl_df
    ],
    axis=0
)

# replace missing values by 0
ebola_df.fillna('0', inplace=True)","optimizer = torch.optim.Adam(
    get_trainable(model.parameters()),
    # model.fc.parameters(),
    lr=0.001,
    # momentum=0.9,
)",,Homework 1->1.1 Import Data,Homework 1->Table of Contents->Task 1. Compiling Ebola Data->Data reading,"03 Transfer Learning->The Optimizer
03 Transfer Learning->The Optimizer"
127408,"from sklearn.model_selection import GridSearchCV,cross_val_score
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score","from sklearn.model_selection import GridSearchCV,cross_val_score
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, precision_score, recall_score, accuracy_score","df['HP'].replace([99999, median])",,Final Project->Data Exploration,Final Project->Data Exploration,Manual Feature Selection And Engineering Tutorial->Feature selection and Logit Transformation
453723,"with plt.xkcd():
    plt.plot(x, x**2, px, py, ""ro"")

    bbox_props = dict(boxstyle=""rarrow,pad=0.3"", ec=""b"", lw=2, fc=""lightblue"")
    plt.text(px-0.2, py, ""Beautiful point"", bbox=bbox_props, ha=""right"")

    bbox_props = dict(boxstyle=""round4,pad=1,rounding_size=0.2"", ec=""black"", fc=""#EEEEFF"", lw=5)
    plt.text(0, 1.5, ""Square function\n$y = x^2$"", fontsize=20, color='black', ha=""center"", bbox=bbox_props)

    plt.show()","t=np.linspace(-1,1,101)
x=t**3
y=t**3-t
with plt.xkcd():
    plt.plot(x,y)
    plt.plot([1/np.sqrt(27),1/np.sqrt(27)],[-0.5,0.5],'r--')
    plt.plot([-1/np.sqrt(27),-1/np.sqrt(27)],[-0.5,0.5],'g--')
    plt.text(.25,-0.45,r'$x=1/\sqrt{27}$',color=""red"")
    plt.text(-.5,0.4,r'$x=1/\sqrt{27}$',color=""green"")

    plt.ylim([-0.5,0.5])",data.Platform.unique(),,"Basic->text
3 Matplotlib->Drawing text",2 Applications Of Differentiation,Ign->Preparing the Data
62182,"# Two 8x8 images with one channel
X_shape = [1, 6, 6, 1]
X_vals = np.random.uniform(size=X_shape)
X_vals","def rand_img(shape):
    return np.random.uniform(0, 1, shape)","# We observe that Keras is not able to work with these indexes.
# What it actually expects is a vector with an identical size to the output layer.
# Our framework provides functionality to do this with ease. What it basically does,
# given an expected vector dimension, it prepares zero vector with the specified dimensionality,
# and will set the neuron with a specific label index to one.

# For example:
# 1. Assume we have a label index: 3
# 2. Output dimensionality: 5
# With these parameters, we obtain the following vector in the DataFrame column: [0,0,0,1,0]

# First, we fetch the columns of interest.
dataset = dataset.select(""features_normalized"", ""label_index"")

# Number of classes (signal and background).
nb_classes = 2

# Construct a one-hot encoded vector using the provided index.
transformer = OneHotTransformer(output_dim=nb_classes, input_col=""label_index"", output_col=""label"")
dataset = transformer.transform(dataset)

# Only select the columns we need (less data shuffling) while training.
dataset = dataset.select(""features_normalized"", ""label_index"", ""label"")
dataset.cache()",,2->Working with Multiple Layers->Creating 2D images,"Neural Style Transfer With Tf And Keras->TensorFlow/Keras Implementation of ""A Neural Algorithm of Artistic Style""->Setup->Recreate style",Example 0 Data Preprocessing->Data Preprocessing->Dataset preprocessing->Label transformation
230948,"# Identifying Outliers
outliers = [380, 418, 405, 364, 367, 370, 365, 369, 371, 372, 368]
bos_outliers = bos.iloc[outliers].transpose()
outliers_df = pd.concat([bos_outliers, feature_means, feature_medians, feature_coef], axis=1)
outliers_df.columns = ['380', '418', '405', '364', '367', '370', '365', '369', '371', '372', '368', 'Feature Mean', 'Feature Median', 'Feature Coefficient']
print(outliers_df)","# Number of minimum price outliers
min_price_outlier_list = list()
for col in features['price_cols'][0:2]:
    min_price_outlier_list.append(find_tukey_outliers(df,col))
min_outlier_df = pd.concat(min_price_outlier_list,axis=0)
min_outlier_df['TM_id'].nunique()","# Tf Utils
def get_tensorflow_configuration(device=""0"", memory_fraction=1):
    """"""
    Function for selecting the GPU to use and the amount of memory the process is allowed to use
    :param device: which device should be used (str)
    :param memory_fraction: which proportion of memory must be allocated (float)
    :return: config to be passed to the session (tf object)
    """"""
    device = str(device)
    config = tf.ConfigProto()
    config.allow_soft_placement = True
    config.gpu_options.per_process_gpu_memory_fraction = memory_fraction
    config.gpu_options.visible_device_list = device
    return(config)


def start_tensorflow_session(device=""0"", memory_fraction=1):
    """"""
    Starts a tensorflow session taking care of what GPU device is going to be used and
    which is the fraction of memory that is going to be pre-allocated.
    :device: string with the device number (str)
    :memory_fraction: fraction of memory that is going to be pre-allocated in the specified
    device (float [0, 1])
    :return: configured tf.Session
    """"""
    return(tf.Session(config=get_tensorflow_configuration(device=device, memory_fraction=memory_fraction)))


def get_summary_writer(session, logs_path, project_id, version_id):
    """"""
    For Tensorboard reporting
    :param session: opened tensorflow session (tf.Session)
    :param logs_path: path where tensorboard is looking for logs (str)
    :param project_id: name of the project for reporting purposes (str)
    :param version_id: name of the version for reporting purposes (str)
    :return summary_writer: the tensorboard writer
    """"""
    path = os.path.join(logs_path,""{}_{}"".format(project_id, version_id)) 
    if os.path.exists(path):
        shutil.rmtree(path)
    summary_writer = tf.summary.FileWriter(path, graph_def=session.graph_def)
    return(summary_writer)",,Mini Project Linear Regression->EDA and Summary Statistics->Scatterplots,2->2.1.4 Outlier Detection->b. Count number of outliers,Script166->Tensorflow utilities
180969,"cols = ['tweet_id', 'tweet_status', 'full_text', 'timestamp', 'source', 'expanded_urls',
       'name', 'dog_stage', 'rating', 'retweet_count', 'favorite_count', 'in_reply_to_status_id',
        'in_reply_to_user_id']
cols",col[6][1],"fifa = pd.read_csv('https://assets.datacamp.com/production/course_735/datasets/fifa.csv')
fifa.head()",,"Wrangle Act->Assessing Data->Issues `Q-8`, `T-6`, and `T-7`",Scraping Exercise 1->Scraping MLB stats from ESPN go,Intro To Python For Data Science->NumPy->List Manipulation->Blend it all together
186104,"# Load pickled data
import pickle
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
# TODO: Fill this in based on where you saved the training and testing data

training_file = 'train.p'
validation_file= 'valid.p'
testing_file = 'test.p'

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(validation_file, mode='rb') as f:
    valid = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)
    
X_train, y_train = train['features'], train['labels']
X_valid, y_valid = valid['features'], valid['labels']
X_test, y_test = test['features'], test['labels']

X_train = np.concatenate((X_train,X_valid),axis=0)
y_train = np.concatenate((y_train,y_valid),axis=0)","# Load pickled data
import pickle

# TODO: Fill this in based on where you saved the training and testing data

training_file = 'dataset/train.p'
validation_file='dataset/valid.p'
testing_file = 'dataset/test.p'

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(validation_file, mode='rb') as f:
    valid = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)
    
X_train, y_train = train['features'], train['labels']
X_valid, y_valid = valid['features'], valid['labels']
X_test, y_test = test['features'], test['labels']","# Bayesian least squares.
# Choose the prior parameters.
theta0 = numpy.ones(3)      # theta_0 = (1,1,1) ... f(x) = 1 + x + x^2
Sigma0 = 1.0*numpy.identity(3)  # 3x3 identity matrix.
# Invert prior covariance matrix with Cholesky decomposition.
cholesky  = cho_factor(Sigma0)
Sigma0inv = cho_solve(cholesky, numpy.identity(3))
# Fit the weight as a function of the height.
# Step 1: Build the transpose of the design matrix.
N          = len(HeightMale)    # Number of training examples.
M          = 3                  # Number of fit parameter.
DesignT    = numpy.empty([M,N]) # D transposed is of format MxN.
DesignT[0] = 1.0
DesignT[1] = HeightMale
DesignT[2] = HeightMale**2
Design     = DesignT.T     # Get the design matrix from its transpose.
# Step 2: Compute least-squares solution.
DtD        = numpy.dot(DesignT,Design) + Sigma0inv
DtY        = numpy.dot(DesignT, WeightMale) + numpy.dot(Sigma0inv,theta0)
cholesky   = cho_factor(DtD)
thetaBayes = cho_solve(cholesky, DtY)
# Print least-squares solution.
print(thetaBayes)
print(thetaOLS)",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree,Traffic-Sign-Project-Solution-Final+Solution->Self-Driving Car Engineer Nanodegree->Load Data,Supervised Learning I->Supervised Learning I->Block I: Fundamentals->I.5 Ordinary Linear Least Squares Regression
367896,"seed(3141592)

S0 = 100
K = 92
r = 0.06
sigma = 0.23
T = 150 / 365

steps = 2000
t = np.linspace(0, T, steps)
delta_t = T / steps

plt.figure(figsize=(15,10), dpi=150)
for i in range(250):
    Wt  = normal(scale=sqrt(delta_t), size=steps).cumsum()
    St = S0 * exp(sigma * Wt + (r - sigma ** 2 / 2) * t)
    if St[-1] < K :
        plt.plot(t, St, linewidth=1, color=""gray"", alpha=0.8)
    else:
        plt.plot(t, St, linewidth=1, color=""gray"", alpha=0.1)

plt.xlim(0, T * 1.001)
plt.axhline(y=K, color=""teal"", linewidth=3)
plt.xticks(fontsize=20)
yticks = [K, 80, 100, 120, 140, 160]; yticks.sort()
plt.yticks(yticks, fontsize=20)


plt.savefig(""../images/european_call_montecarlo.pdf"")
plt.show()","# Lets create two time series of simulated stock prices
ipts = 120
dt = 1.0/12.0
r = 0.05
sigma = 0.30
y = np.random.standard_normal((ipts+1,2))
S = np.zeros((ipts+1,2))
x = np.zeros(ipts+1)
S[0,0]= 100
S[0,1]= 100
x[0] = 0
for t in range(1,ipts+1):
    S[t,0]= S[t-1,0]*np.exp((r - 0.5*sigma**2)*dt + sigma*np.sqrt(dt)*y[t,0])
    S[t,1]= S[t-1,1]*np.exp((r - 0.5*sigma**2)*dt + sigma*np.sqrt(dt)*y[t,1])
    x[t]= t*dt

plt.figure(figsize = figsize)
plt.plot(x,S[:,0], label = 'One')
plt.plot(x, S[:,1], label = 'Two');","names = [""Adam"", ""Bruno"", ""Carl"", ""David"", ""Elen""]
for name in names:
#   if name[0] in [""A"", ""E"", ""I"", ""O"", ""U""]:    #check with a list of the 5 vowels
    if name[0] in ""AEIOU"":   #check with a string that contains the 5 vowels
        print(""'"" + name + ""'"" + "" starts with a vowel."")",,Graphics->Montecarlo Pricing,Data Visualization With Matplotlib->Try this->Contrive Exampe in finance,Aml Hands On3->Python 101 (a crash course for students)->Recap on data structures->Tuples
103453,"df_emi.date=pd.to_datetime(df_emi.date) #change date object into datetime format
df_emi=df_emi[df_emi.date>='2010-01-01'] #choose recent dates only
df_emi.shape","pd.set_option('display.float_format', lambda x: '%.5f' % x)
df.Existing_EMI.describe()","lb1 = [0.,0.003,0.006,0.009,0.012,0.015]
lb2 = [0.,0.4,0.8,1.2,1.6]

n1,n2 = len(lb1),len(lb2)
fig, axarr = plt.subplots(n2,n1,figsize=(20,17))

for i,l1 in enumerate(lb1):
    for j,l2 in enumerate(lb2):
        show_image(smooth_image(l1,l2),axarr[j,i],str((l1,l2)))",,Project->IMPROVE CUSTOMER EXPERIENCE FOR EMIRATES AIRLINE TO INCREASE  PROFITS->Cleaning,Hackathon 3->Data-Cleaning->Write Out the train/test data->Existing_EMI,4 Denoising->-------------------------------------------------------------------------------->Show result for several values of the multipliers
253642,"train = pd.concat([
        train.select_dtypes([], ['object']),
        train.select_dtypes(['object']).apply(pd.Series.astype, dtype='category')
        ], axis=1).reindex_axis(train.columns, axis=1)

test = pd.concat([
        test.select_dtypes([], ['object']),
        test.select_dtypes(['object']).apply(pd.Series.astype, dtype='category')
        ], axis=1).reindex_axis(test.columns, axis=1)","features = df.columns
df_feature['% observations presented'] = df[df_dtype_object[features] != '?'].count()/101766*100
df_feature","order_book = exchange.fetch_order_book(symbol)
for ob in order_book:
    print('{} {}'.format(time(), ob))
    if ob in ['bids', 'asks']:
        for i in order_book[ob]:
            print('{} {}'.format(time(), i))
        continue
    print('{} {}'.format(time(), order_book[ob]))",,Untitled->Introduction to Boosting using LGBM (LB: 0.68357)->Converting object types to categorical,Final Project 3->1. Import raw dataset->1.3 Preliminary Analysis->1.4.3 Determine Missing Data point,Ccxt Tests->Order Books
366350,"plt.figure(figsize=(12,5))
with tf.Session() as sess:
    for i in range(n_iter):
        ind = random.sample(range(X.shape[0]), batch_size)
        batch_loss = sess.run(loss_op, feed_dict={X_ph: X, Y_ph: y, W_ph: w})
        loss[i] = batch_loss        
        w, c = sess.run([param_update_op, C_op], feed_dict={X_ph: X[ind, :], Y_ph: y[ind],  W_ph: w, C_ph: c})

visualize(X, y, w, loss, n_iter)
plt.clf()","n_iter = 1500
batch_size = 500

train_costs = np.zeros([n_iter / 50])
test_costs = np.zeros([n_iter / 50])

with tf.Session() as sess:
    sess.run(init)
    
    for i in range(n_iter):
        ind = np.random.choice(X_train.shape[0], batch_size)
        
        sess.run([optimizer, cost],
                feed_dict = {
                    X: X_train[ind,:],
                    y: y_train_1hot[ind]
                })
        
        if ((i + 1) % 50 == 0):
            train_cost = sess.run(cost,
                    feed_dict = {
                        X: X_train,
                        y: y_train_1hot
                    })

            train_costs[(i - 49) / 50] = train_cost

            test_cost = sess.run(cost,
                    feed_dict = {
                        X: X_test,
                        y: y_test_1hot
                    })

            test_costs[(i - 49) / 50] = test_cost
        
        if (i + 1) % 250 == 0:
            
            yhat_train = sess.run(yhat,
                           feed_dict = {X: X_train})
            
            yhat_test = sess.run(yhat,
                           feed_dict = {X: X_test})
            
            acc_train = accuracy(yhat_train, y_train)
            acc_test = accuracy(yhat_test, y_test)
            
            print(""Iteration"", i + 1,
                  ""Train cost"", train_cost,
                  ""Test cost"", test_cost,
                  ""Train accuracy"", acc_train,
                  ""Test accuracy"", acc_test)
        
    print("""")
    print(""Final test accuracy"", acc_test)","attack_counts = Counter(df_imputed.attacktype1_txt.fillna('Unknown'))
df_attacks = pd.DataFrame.from_dict(attack_counts, orient='index')
ax = df_attacks.plot(kind='barh', figsize=(10,6), legend=None, 
                     title=""U.S. Attack Types"", fontsize=13)
ax.title.set_size(18)",,Task 1->Training,My1St Nn->Peer-graded assignment: my1stNN->Logistic regression model->Train logistic regression model,Lab 1 - Exploring Table Data->Lab 1 - Exploring Table Data->Data Understanding->Attack by Targets
428373,"pre=teachers['Teacher Prefix'].value_counts().reset_index()
pre.columns=['Prefix','Count']
x=pre.Prefix
data=[]
for i in x:
    teacher_pre=teachers[teachers['Teacher Prefix']==i]
    year=teacher_pre['Years'].value_counts().reset_index().sort_values(by='index')
    year.columns=['Year','Count']
    trace = go.Scatter(
    x = year.Year,
    y = year.Count,
    name = i)
    data.append(trace)
layout = go.Layout(
    title='Over The Year Growth in First Project Posted Prefix-Wise',
    xaxis=dict(
        title='Year'
    ),
    yaxis=dict(
        title='Number Of Projects'
    ),
  
)

fig = go.Figure(data=data, layout=layout)
py.iplot(fig)","teacher_prefix = train[""teacher_prefix""].value_counts()

rejected = []
accepted = []
for val in teacher_prefix.index:
    accepted.append(np.sum(train[""project_is_approved""][train[""teacher_prefix""]==val] == 1))
    rejected.append(np.sum(train[""project_is_approved""][train[""teacher_prefix""]==val] == 0))    
trace1 = go.Bar(
    x = teacher_prefix.index,
    y = accepted,
    name='Accepted Proposals',
    marker=dict(
        color='blue',
        
)
)
trace2 = go.Bar(
    x = teacher_prefix.index,
    y = rejected, 
    name='Rejected Proposals',
    marker=dict(
        color='rgb(233, 32, 20)',
        
)
)

data = [trace1, trace2]
layout = go.Layout(
    title = ""Acceptance rate and project rejection rate for different teacher prefix"",
    barmode='stack',
    width = 1000
)

fig = go.Figure(data=data, layout=layout)
py.iplot(fig)","sns.pairplot(amazon_food, x_vars=featured_cols,y_vars='Length', kind='reg')",,Recommendation Engine->Projects,Donors Choose Eda->3. Looking at Data->4.20 Gender Analysis,Final Project Pat 1->Amazon Food Review with Sentiment Analysis->Dealing with Sarcasm->graphing the correlation between length and pos/neg probability
343199,"import tools

def myplay(data):
    """"""Apply fade in/out and play with 44.1 kHz.""""""
    data = tools.fade(data, 2000, 5000)
    sd.play(data, 44100)","def fade_img_left(img):
    M = blep.shape[0]
    N = blep.shape[1]
    D = blep.shape[2]
    matrix = fade_left(M, N)
    for i in range(0, D):
        img[:,:,i] *= matrix
    return (img)

blep = pp.imread(""blep.png"")
pp.imshow(fade_img_left(blep))
pp.axis(""off"")","def trial2(distances, maxiters, T, factor=1.0):
    start = random.randint(0, 29) #create initial permuation route.
    route = [start]
    parks = set(range(30))
    rest_parks = list(parks.difference(route))
    random.shuffle(rest_parks)
    route.extend(rest_parks)
    route.append(start)
    best = route
    dist_best = path_dist(distances, best)
    iter_dists = []
    for i in xrange(maxiters):
        old_dist = path_dist(distances, route)
        rand_indices = sorted(random.sample(xrange(30), 2)) #choose 2 parks randomly
        park1 = route[rand_indices[0]]
        park2 = route[rand_indices[1]]
        route_new = deepcopy(route)
        route_new[rand_indices[0]] = park2
        route_new[rand_indices[1]] = park1
        route_new[-1] = route_new[0]
        new_dist = path_dist(distances, route_new)
        delta_dist = new_dist - old_dist #find change in route distance
        if (delta_dist < 0) or (T > 0 and random.uniform(0.0, 1.0) < np.exp(-delta_dist / T)):
            route = route_new
        T *= factor
        route_dist = path_dist(distances, route)
        iter_dists.append(route_dist)
        if  route_dist < dist_best: #see if new route is better than the best route seen so far
            best = route
            dist_best = route_dist
    return best, dist_best, iter_dists",,"Intro->Introduction to Python et al., Working with Audio Signals->Listening to the Signal
00 Intro->Array, Vector, Matrix",Numpy Rush,Traveling Salesman
377835,float(np.sum(click_validate == 0))/len(validate1),"train_accuracy = {}
train_accuracy[0]   = np.sum(pred_l2_pen_0_train==sentiment_train)/float(len(sentiment_train))
train_accuracy[4]   = np.sum(pred_l2_pen_4_train==sentiment_train)/float(len(sentiment_train))
train_accuracy[10]  = np.sum(pred_l2_pen_10_train==sentiment_train)/float(len(sentiment_train))
train_accuracy[1e2] = np.sum(pred_l2_pen_1e2_train==sentiment_train)/float(len(sentiment_train))
train_accuracy[1e3] = np.sum(pred_l2_pen_1e3_train==sentiment_train)/float(len(sentiment_train))
train_accuracy[1e5] = np.sum(pred_l2_pen_1e5_train==sentiment_train)/float(len(sentiment_train))

validation_accuracy = {}
validation_accuracy[0]   = np.sum(pred_l2_pen_0_valid==sentiment_valid)/float(len(sentiment_valid))
validation_accuracy[4]   = np.sum(pred_l2_pen_4_valid==sentiment_valid)/float(len(sentiment_valid))
validation_accuracy[10]  = np.sum(pred_l2_pen_10_valid==sentiment_valid)/float(len(sentiment_valid))
validation_accuracy[1e2] = np.sum(pred_l2_pen_1e2_valid==sentiment_valid)/float(len(sentiment_valid))
validation_accuracy[1e3] = np.sum(pred_l2_pen_1e3_valid==sentiment_valid)/float(len(sentiment_valid))
validation_accuracy[1e5] = np.sum(pred_l2_pen_1e5_valid==sentiment_valid)/float(len(sentiment_valid))","p1=plot_implicit(Eq(2*x+6,y**2), title=""${ y^2=2x+6}$ and $ y=x-1$"", show=False, scale=2, line_color=""red"", aspect_ratio=(10.0,5.0))
p2=plot_implicit(Eq(x-1,y), show=False,  aspect_ratio=(10.0,5.0))
p1.append(p2[0])
p1.show()",,Summary->Random forest classifier,Week 2 Assign 2 Lin Reg L2 Reg->Measuring accuracy,Sec06->Step 1: Sketch the graph
337621,"#define structure for top_k
top_five_str = tf.nn.top_k(softmax_2, k=5, sorted=True, name=None)

#get top 5 for processed images
top_five = sess.run([top_five_str], feed_dict={x: processed_images})

for i in range(0,6):
    labels = [signs_dict[j] for j in top_five[0][1][i]]
    print(labels)

print(top_five)","#### Print top 5 soft max probabilities for keep right sign
sess.run(tf.nn.top_k(tf.nn.softmax(pred), k=5), feed_dict={x_unflattened: [image_normalized]})","a.update(['mango', 'grapes'])
a",,Traffic Signs Recognition->Question 7,Traffic Sign Classifier->Result -> The SECOND BEST softmax probability is what is the correct classification!->4.] Keep Right Sign,Pep Summer 2018 Py3->DAY 1->Modules->Set
331205,"print(len(y_data))
print(len(X_data_paths))

plt.figure(figsize=(10,5))
plt.hist(y_data, 200)
plt.ylabel('Frequency')
plt.xlabel('Steering angles')
plt.title('Histogram of steering angles')
plt.show()","#### feel exploration 
x_en = numpy.sort(profile.energy)
y_en =numpy.arange(1,len(x_en)+1)/len(x_en)

x_val = numpy.sort(profile.valence)
y_val =numpy.arange(1,len(x_val)+1)/len(x_val)

x_dan = numpy.sort(profile.danceability)
y_dan =numpy.arange(1,len(x_dan)+1)/len(x_dan)

_=plt.plot(x_en,y_en,marker='.',linestyle='none')
_=plt.plot(x_val,y_val,marker='.',linestyle='none')
_=plt.plot(x_dan,y_dan,marker='.',linestyle='none')
plt.ylabel('ECDF (% of data)')
plt.xlabel('Attribute Score')
plt.legend(('energy','valence','danceability'),loc='lower right')
print('Musical Feeling')
plt.show()

_ = plt.hist(x=profile.energy,histtype='step',color='Red',alpha=0.9)
_ = plt.hist(x=profile.valence,histtype='step',color='blue',alpha=0.9)
_ = plt.hist(x=profile.danceability,histtype='step',color='black',alpha=0.9)
plt.xlabel('Attribute Score')
plt.legend(('energy','valence','danceability'),loc='upper left')
plt.ylabel('Freq. of data')
plt.show()",fitter.reset(),,Model->Balance the data,Data Vis Springboard,Lipid
154898,"import h5glance
h5glance.install_ipython_h5py_display()","# Run First
import pandas as pd
import json
import matplotlib as mpl
import matplotlib.pyplot as plt
get_ipython().magic(u'matplotlib inline')
import seaborn as sns
import numpy as np

import os",y_train_shuffled[:9],,Demo->H5Glance demo->View all h5py objects,Plotting And Analysis Of Fear Study- Steve->Plotting of Fear Study- Steve->Description of Notebook,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Shuffle
468827,"toneC=(tone/2.+toneB/2.).astype(int16)
write('data/sound/mixtone.wav',amp2,toneC)","#####################################################
############Chinese Media Data#######################

china = tone.mask('CountryName','China')
us= tone.mask('CountryName','United States')
uk = tone.mask('CountryName','United Kingdom')","import time 
import tensorflow as tf
sess = tf.InteractiveSession()",,Fourier->Fourier Series->Guitar Sound,Main->Merging Dataset,A1->Digit Classification using TensorFlow->introduce the MNIST dataset
349676,"# import class for visualizing complete function approximation
from Function_Fit_Demo1 import Fit_Bases
%matplotlib inline
func_fit = Fit_Bases()
func_fit.load_target('sin_function.csv')
func_fit.browse_tree_fit()","# import class for visualizing complete function approximation
from Function_Fit_Demo1 import Fit_Bases
%matplotlib inline
func_fit = Fit_Bases()
func_fit.load_target('datasets/sin_function.csv')
func_fit.browse_tree_fit()",cml.calc_A(),,5->5.1: Going nonlinear with machine learning->3.  Decision Trees: recursively defined basis elements,5,Index->Precipitation analysis from commercial microwave links and the open source software pycomlink: A heavy rain event in South Germany->Processing of CML data->Coding example 4
48450,"default_RMSE = make_scorer(score_func=mean_squared_error)

param_grid = {""n_alphas"": np.arange(50, 200, 25),
              ""selection"": ['cyclic','random']}

lasso = LassoCV(alphas = (1e-3,1e-2,1e-1,1,1e1,1e2,1e3), max_iter=10000, cv=10)
regr = GridSearchCV(lasso, param_grid=param_grid, cv=10, scoring=default_RMSE,verbose=1, n_jobs=-1)

regr.fit(X_train, y_train)

regr.best_params_, np.sqrt(regr.best_score_)","default_RMSE = make_scorer(score_func=mean_squared_error)


param_grid = {'alpha': [10,1,.01,10e-2,10e-3,], 
              'hidden_layer_sizes': [(10, ), (10,10), (5, ), (5, 5)],
              'tol': [1e-2, 1e-3, 1e-4], 
              'epsilon': [1e-3, 1e-7, 1e-8]}

mlp = MLPRegressor()
regr = GridSearchCV(mlp, param_grid=param_grid, cv=10, scoring=default_RMSE,verbose=1, n_jobs=-1)

regr.fit(X_train_scaled, y_train)

regr.best_params_, np.sqrt(regr.best_score_)","feature_df.loc[times[0]].head()
X = feature_df.loc[times[0]].drop('y', axis=1).values
y = feature_df.loc[times[0]].y",,Ts2 Final->Seasonal ARIMA model,Ts2 Final,Sklearn Pd Sns->EEG dataset->Decoding->Organize into features and target
222988,"# Test undistortion on an image
#img = cv2.imread('test_images/straight_lines1.jpg')
img = mpimg.imread('test_images/straight_lines2.jpg') 
#img = mpimg.imread('test_images/test5.jpg') 
print(img.shape)
img_size = (img.shape[1], img.shape[0])

advImage = Line()
img_out = advImage.process_image(img)
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,15))
ax1.imshow(img)
ax2.imshow(img_out)","# Define camera and calibrate it
camera = Camera(nx=9, ny=6, show_images=False)
camera.calibrate(""./camera_calibration_images/calibration*.jpg"")

# Load image files
images = glob.glob(""./test_images/test*.jpg"")

for image in images:
    print('Processing image {}'.format(image))
    
    # Read image
    img = mpimg.imread(image)

    # Define lane detector
    laneDetector = LaneDetector(show_images=False)

    # Process the images
    combined_img = process_image(img)
    
    # Plot original and combined image
    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))
    ax1.imshow(img)
    ax2.imshow(combined_img)

plt.show()","import seaborn as sns
sns.set()

plt.figure()

sns_symptom_5 = sns.boxplot(x = 'symptom_5', y = 'amount', data = cost).set_title('Cost of Care and Symptom_5')
#sns_symptom_5.get_figure().savefig('symptom 5 and cost.png')",,Advanced Lane->Test on Videos,"Advanced-Lane-Line->** Images Pipeline **
Advanced-Lane-Line->** Video Pipeline **","Holmusk Data Science Task->4. Cost of Care and symptom_5, race, age, and resident status->Cost of Care and Symptom_5"
28492,"df.loc['Braund, Mr. Owen Harris', 'Survived'] = 100",df.name[:10],"# Order the lastLocID starting with the largest total delay.
top10['lastLocID_cat'] = pd.Categorical(
    top10['lastLocID'], 
    categories=[val for val in tdelay_stopID.loc[:9,'lastLocID']],
    ordered=True
)
top10 = top10.sort_values('lastLocID_cat').reset_index(drop=True).drop('lastLocID_cat', axis=1)
top10[:10]",,Notebook->7. Working with time series data->Systematic indexing with `loc` and `iloc`,Titanic V0->Cleaning Data->Missing Data: Age,Tri Met Vehicle Data Analysis->TriMet Vehicle Delays
261041,prova.check(my_pdb),"prova.viewer(my_pdb,'A',91,displayChains=False)","plt.subplots(figsize=(10,5))
col = np.where(pca_df['cluster4']==0,'k',
               np.where(pca_df['cluster4']==1,'b', np.where(pca_df['cluster4']==2,'g','r')))
plt.scatter(x,y, c=col, s=75)
plt.title('4 clusters')
plt.grid()
plt.show()",,Example->Contactome and Protonation assessment->3) Contactome methods->3.4 check,Example->Contactome and Protonation assessment->3) Contactome methods->3.2 Viewer,Mini Project Clustering
376617,iso=Isomap(n_components=2),"from sklearn.manifold import Isomap
iso = Isomap(n_neighbors= 6, n_components = 3)","science8 = pd.io.parsers.read_csv('../data/table_export/Science8_Subgroup_Results.csv')

print science8.shape
print science8.columns",,Digit Recognition->Unsupervised Learning:Dimensionality Reduction,Lecture7 Pipeline->Pipeline,Ny Teacher Data 0->NY Teacher Evaluation Data->Exploratory Work->Linear Regression
190697,"## Plot randomly selected examples in each class in training data
plotDataOri(X_train,y_train,signData)","## Plot randomly selected examples in each class in training data
plotDataOri(X_train,y_train,signData)","condM_mem_hits_byTask = df_old.groupby(['sbjId','task','respComp','trialType']).sbjACC.mean().reset_index()
condM_mem_hits_byTask.rename(columns={'sbjACC': 'Hits'}, inplace=True)
gpResult_hits_byTask = condM_mem_hits_byTask.groupby(['task','respComp','trialType']).Hits.mean()*100
print(gpResult_hits_byTask)
print('Subsequent Memory Task mean ACC(Hit rate) as a function of task')
g = sns.factorplot(x='respComp',y='Hits',col=""task"", hue=""trialType"",data = condM_mem_hits_byTask)
g.set(ylim=(0.4, 0.9))",,Traffic Sign Classifier->Training the pipeline,Traffic Sign Classifier->Training the pipeline,"Tsrc V2 Basic Analysis->3. Look at Subsequent Memory Data->3d. Hit rate (old item and rated as 'old', all correct trials during T1) as a function of  2 (task: Animacy, Size) x 2 (respComp: RC, RIC ) x 2 (Trial Type: switch/repeat)"
4631,WikiC.to_json(cleanfile),WikiC.to_json(cleanfile),"# take the data apart; numerical vars get scaled, then go to polynomial; state dummies pass through; 
# they get glued back together for the regression.
testPipe = Pipeline([
    ('features', 
     FeatureUnion([('num_vars', 
                    Pipeline([('extract', ColumnExtractor(columns=['num_days', 'size_kw'])) ,
                              ('scale', StandardScaler()),
                              ('poly', PolynomialFeatures())
                              ])),
                   ('cat_vars',
                    ColumnExtractor(columns=theStates))
                  ]) # end of FeatureUnion
    )]) # end of Pipeline",,Tutorial->Tutorial: Using omterms keyword extraction package->More detailed examples: the application interface options->Working with plain texts or already tokenized texts:,Tutorial->Tutorial: Using omterms keyword extraction package->More detailed examples: the application interface options->Working with plain texts or already tokenized texts:,Model 04->Build a pipeline that does the right thing->test ColumnExtractor
406232,"car_features = extract_features(car_images, hog_feat=True)
notcar_features = extract_features(noncar_images, hog_feat=True)

# Create an array stack of feature vectors
X = np.vstack((car_features, notcar_features)).astype(np.float64)
# Fit a per-column scaler
X_scaler = StandardScaler().fit(X)
# Apply the scaler to X
scaled_X = X_scaler.transform(X)

# Define the labels vector
y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))

# Split up data into randomized training and test sets
rand_state = np.random.randint(0, 100)
X_train, X_test, y_train, y_test = train_test_split(scaled_X, y,
                                                    test_size=0.2,
                                                    random_state=rand_state)
# Use a linear SVC
svc = LinearSVC()
# Check the training time for the SVC
t=time.time()
svc.fit(X_train, y_train)
t2 = time.time()
print(round(t2-t, 2), 'seconds to train SVC.')
print('Using:', orient, 'orientations', pix_per_cell,
    'pixels per cell and', cell_per_block, 'cells per block')
print('Feature vector length:', len(X_train[0]))","car_features = extract_features(car_images, hog_feat=True)
notcar_features = extract_features(noncar_images, hog_feat=True)

# Create an array stack of feature vectors
X = np.vstack((car_features, notcar_features)).astype(np.float64)
# Fit a per-column scaler
X_scaler = StandardScaler().fit(X)
# Apply the scaler to X
scaled_X = X_scaler.transform(X)

# Define the labels vector
y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))

# Split up data into randomized training and test sets
rand_state = np.random.randint(0, 100)
X_train, X_test, y_train, y_test = train_test_split(scaled_X, y,
                                                    test_size=0.2,
                                                    random_state=rand_state)
# Use a linear SVC
svc = LinearSVC()
# Check the training time for the SVC
t=time.time()
svc.fit(X_train, y_train)
t2 = time.time()
print(round(t2-t, 2), 'seconds to train SVC.')
print('Using:', orient, 'orientations', pix_per_cell,
    'pixels per cell and', cell_per_block, 'cells per block')
print('Feature vector length:', len(X_train[0]))","moby = text_cleaner(moby)
paradise = text_cleaner(paradise)",,Vehicle Detection->Udacity Self-Driving Car Engineer Nanodegree Program->Histogram of Oriented Gradients (HOG)->Train LinearSVC Classifier,Vehicle Detection->Udacity Self-Driving Car Engineer Nanodegree Program->Histogram of Oriented Gradients (HOG)->Train LinearSVC Classifier,Challenge Build Your Own Nlp Model->Natural language processing->tf-idf->Modeling Using tf-idf->Logistic Regression : Ridge
285919,"dataset = Dataset(X_train, y_train, X_test)
dataset","def splitter(dataset,y):
    X_train, X_test, y_train, y_test = train_test_split(dataset, y, test_size=0.3)
    return X_train,X_test,y_train,y_test","# smooth out by the week
df1_w = df1
df1_d.resample('1W').mean().plot(fontsize=20) 
plt.xlabel('login_time', fontsize=20)
plt.xticks(rotation= 30);",,Walkthrough->More on pipelines->Custom function,Nyc311->Exploring NYC 311 data->Train Test Split,Ultimate Part1->Part 1 EDA
42508,"#Undetermined
undetermined_race_counts ={}

# Undetermined gun deaths for each race
for index,elem in enumerate(intent):
    if elem == 'Undetermined':
        if race[index] in undetermined_race_counts:
                undetermined_race_counts[race[index]] += 1
        else:
            undetermined_race_counts[race[index]] = 1

#Undetermined gun deaths for each race            
undetermined_race_hundredk = {}
for key,elem in undetermined_race_counts.items():
    if key not in undetermined_race_hundredk:
        undetermined_race_hundredk[key] = (elem/mapping[key])*100000

undetermined_race_hundredk","#Accidental
accident_race_counts ={}

# Accidental gun deaths for each race
for index,elem in enumerate(intent):
    if elem == 'Accidental':
        if race[index] in accident_race_counts:
                accident_race_counts[race[index]] += 1
        else:
            accident_race_counts[race[index]] = 1

#Accidental gun deaths for each race            
accident_race_hundredk = {}
for key,elem in accident_race_counts.items():
    if key not in accident_race_hundredk:
        accident_race_hundredk[key] = (elem/mapping[key])*100000

accident_race_hundredk","roc_auc_score(titanic['Survived'], lm.predict(feature_set))",,Basics->Reading in files->Rates Of Gun Deaths Per Race: Filtered By Undetermined,Basics->Reading in files->Rates Of Gun Deaths Per Race: Filtered By Accidental,Lesson10->Visualizing and tuning a model
155873,"# Add 'year' column to names_1881 and names_1981
names_1881['year'] = 1881
names_1981['year'] = 1981

# Append names_1981 after names_1881 with ignore_index=True: combined_names
combined_names = names_1881.append(names_1981, ignore_index=True)

# Print shapes of names_1981, names_1881, and combined_names
print(names_1981.shape)
print(names_1881.shape)
print(combined_names.shape)

# Print all rows that contain the name 'Morgan'
print(combined_names.loc[combined_names.name == 'Morgan'])","# Add 'year' column to names_1881 and names_1981
names_1881['year'] = 1881
names_1981['year'] = 1981

# Append names_1981 after names_1881 with ignore_index=True: combined_names
combined_names = names_1881.append(names_1981, ignore_index=True)

# Print shapes of names_1981, names_1881, and combined_names
print(names_1981.shape)
print(names_1881.shape)
print(combined_names.shape)

# Print all rows that contain the name 'Morgan'
print(combined_names[combined_names.name == 'Morgan'])","pc.plot('IISERP_20151104_031152_59_tsliced.fit', 'IISERP_20151104_031152_59_tsliced.png', option=3, blevel=127, xtick= 1)",,10->2. Concatenating data->Appending & concatenating DataFrames->Resampling & concatenating DataFrames with inner join,2 Concatenating Data->Concatenating data->Appending & concatenating DataFrames,Slicing Along Time Axis->Slicing : Along time axis
432571,"for RF in [1.5,2.0,2.5,3.0,3.5]:
    PB = PeriFlakes.PeriBlock(1,10,RF*2.0/10.0)
    print RF,
    for l in PB.HAdj.view():
        print ((l.shape[-1]-1)/2)+1,
    print","aFloatVectorView=aFloatBlock.bind(0,2,10)
aFloatVectorView.ramp(1,1)
aFloatVectorView.mprint(f)","# Melt users: skinny
skinny = pd.melt(users, id_vars=['weekday', 'city'], value_vars=['visitors', 'signups'])

# Print skinny
print(skinny)",,Computation->Computational Efficiency of Peridynamics->Horizon size,Info Block And View Nb->Introduction->View,3-Rearranging-And-Reshaping-Data->Going from wide to long
236395,tf.set_random_seed(42),"# The graph-level seed
tf.set_random_seed(1)","plt.bar( left=years, height=nr_papers, width=1.0 )
plt.xlim(1950,2016)
plt.xlabel('year')
plt.ylabel('number of papers');",,"Rnn
01 Basic->Data Flow Graphs->Random constants
Ibm-Hr-Attrition -Ann","Code->2. Tensor->2.1. Constants, Sequences, and Random Values->Random Tensors",01 Inspecting->Dataset statistics->Papers per year
143131,"# How did you students do?
student_scores = df.groupby('user_id')['correct'].mean()","threshold = n_purch_per_user.mean() + 7*n_purch_per_user.std()
n_purch_per_user_f = n_purch_per_user[n_purch_per_user<threshold]
print(""Outliers represent a %.2f%% of the users"" % ((1.-len(n_purch_per_user_f)/float(len(df.userId.unique())))*100.))","df[['VERZ','MSFT']].iplot(filename='Tutorial Image',theme='white',colors=['pink','blue'],asImage=True,dimensions=(800,500))",,Eda->Knewton Machine Learning Challenge->Exploratory Data Analysis->1. Shape of data<a id='ds'></a>,Description Exploration->Exploration of the dataset->Explore purchases per user and per product->Descriptive statistics,"Cufflinks Tutorial - Plotly->Cufflinks->Output as Image
Cufflinks Basic Tutorial->Output as Image"
492612,"# We create a 3 x 3 rank 2 ndarray that contains integers from 1 to 9
X = np.array([[1,2,3],[4,5,6],[7,8,9]])

# We print X
print()
print('X = \n', X)
print()

# Let's access some elements in X
print('This is (0,0) Element in X:', X[0,0])
print('This is (0,1) Element in X:', X[0,1])
print('This is (2,2) Element in X:', X[2,2])","print('Before: ', x.shape)
X = x[:, np.newaxis]
print('After: ', X.shape)",data.iloc[3],,Numpy,Linear Regression Vs K Neighbors Regressor->Regression Analysis,Singapore Create-Nus-Yale-Nus Data Science Meetup
482879,"def get_month_year(df):
    df['month'] = df.date.apply(lambda x: x.split('-')[1])
    df['year'] = df.date.apply(lambda x: x.split('-')[0])
    
    return df

get_month_year(pd_train_filtered);","def dfDummies_concat(dfx): 
    ''' Create and concatenate named non-default dummy variables for identified columns'''
    
    holidayDummies = pd.get_dummies(dfx['IsHoliday_x'])
    holidayDummies.columns = ['IsHolidayF','IsHolidayT']
    dfx.drop('IsHoliday_y', 1, inplace=True)
    dfx.drop('IsHoliday_x', 1, inplace=True)
    
    dfx['Type'] = 'Type_' + dfx['Type'].astype(str)
    typeDummies = pd.get_dummies(dfx['Type'])
    typeDummies.columns = ['TypeA','TypeB', 'TypeC']
    dfx.drop('Type', 1, inplace=True)
    
    dfx['Store'] = 'Store_' + dfx['Store'].astype(str)
    storeDummies = pd.get_dummies(dfx['Store'])
    dfx.drop('Store', 1, inplace=True)
    
    dfx['Dept'] = 'dept_' + dfx['Dept'].astype(str)
    deptDummies = pd.get_dummies(dfx['Dept'])
    dfx.drop('Dept', 1, inplace=True)

    # There HAS to be a faster way than this lambda f(x)
    dateSplit = dfx['Date'].apply(lambda x: pd.Series(x.split('-')))
    dateSplit.columns  = ['year','month','day']
    dateSplit['year']  = 'year_' + dateSplit['year'].astype(str)
    dateSplit['month'] = 'month_' + dateSplit['month'].astype(str)
    dateSplit['day']   = 'day_' + dateSplit['day'].astype(str)
    
    dfx.drop('Date',1,inplace=True)
    

    yearDummies  = pd.get_dummies(dateSplit['year'])
    monthDummies = pd.get_dummies(dateSplit['month'])
    dayDummies   = pd.get_dummies(dateSplit['day'])

  
    df_concat = pd.concat( [dfx, holidayDummies, typeDummies, 
                            storeDummies, deptDummies, yearDummies,
                            monthDummies, dayDummies],
                          join='outer', axis=1, ignore_index=False)

    return df_concat",dask.__version__,,Script457->Merge test - here is the code->3. Quick look and modification on the data->**- Reformat the date**,"Walmart Challenge-Study->Encoding
Kaggle 2014 Walmart Challenge->Encoding",Reproduceit-Reddit-Dask
245902,"names = {i: name for name, i in index_map.items()}","names = {i: name for name, i in index_map.items()}","def cleandata(qa_array, wv_array, ref_array):
    """"""
    Clean a series of SP spot observations using a QA array.
    
    Parameters
    ----------
    qa_array : array
               (m,n) the Quality Assurance array
    wv_array : array
               (n, ) a vector of wavelengths
    ref_array : array
                (m,n) an array of reflectance values
                
    Returns
    --------
    masked_wv : array
                (n,) a vector of wavelengths with 'bad' values masked
    
    masked_ref : array
                 (m, n) a relfectance array with 'bad' values masked
    """"""
    masked_wv = wv_array[qa_array[0] < 2000]
    mask_size = len(masked_wv)
    masked_ref = np.empty((ref_array.shape[0], mask_size), dtype=np.float64)
    
    for i, v in enumerate(ref_array):
        masked_ref[i] = v[qa_array[0] < 2000]
    
    return masked_wv, masked_ref
    
masked_wv, masked_ref = cleandata(qa_array, wv_array, ref_array)",,7->7. PageRank with Eigen Decompositions->DBpedia Dataset->Algorithm->Why it works,7->7. PageRank with Eigen Decompositions->DBpedia Dataset->Algorithm->Why it works,06 Spectral Plotting->Spectral Plotting
152960,case2.frames[5].head(),"#extra credit a)
data_5=pd.read_csv(""https://serv.cusp.nyu.edu/~lw1474/ADS_Data/Session02/Housing_Pluto_Session2.csv"")
#data_5.columns=[""ZipCode"", ""x"",""y""]
data_5 =pd.DataFrame(data_5)
data_5 = data_5[data_5.LotArea != 0]  #removing rows containing 0 in coloumn x
#data_5 = data_5[data_5.ZipCode != 'nan']
data_5 = data_5[data_5.AssessTot != 0]  ##removing rows containing 0 in coloumn 
data_5.dropna()  #removing rows containing NaN value
data_5.head()","#Actual gradient descent minimizing routine
def gradientDescent(X, theta_start = np.zeros(2)):
    """"""
    theta_start is an n- dimensional vector of initial theta guess
    X is matrix with n- columns and m- rows
    """"""
    theta = theta_start
    j_history = [] #Used to plot cost as function of iteration
    theta_history = [] #Used to visualize the minimization path later on
    for meaninglessvariable in range(iterations):
        tmptheta = theta
        # append for plotting
        j_history.append(computeCost(theta,X,y))
        theta_history.append(list(theta[:,0]))
        #Simultaneously updating theta values
        for j in range(len(tmptheta)):
            tmptheta[j] = theta[j] - (alpha/m)*np.sum((h(X,theta) - y)*np.array(X[:,j]).reshape(m,1))
        theta = tmptheta
    return theta, theta_history, j_history",,Demo->Tutorial: Intermediate->Mixing Geometries,"Session 02 Homework Ps3369->(a). As we see, this data set is not clean.  Drop all the rows which contain ""NaN"" and 0 terms.->*Additional extra-credit question (Using the housing price data set from question (4)) (40 pt to be added to the cumulative homework score up to the maximum of 100%(for whole semester))->(a) How many different zip codes are in the data? report the number of observations for each zipcode.",Lec05-Multivariate-Regression->Gradient Descent Function
34803,"def print_classification_scores(y_test, y_pred):
    print 'Accuracy score  ', accuracy_score(y_test, y_pred).round(4)
    print 'Precision score ', precision_score(y_test, y_pred).round(4)
    print 'Recall score    ', recall_score(y_test, y_pred).round(4)
    print 'F1 score        ', f1_score(y_test, y_pred).round(4)
    print 'ROC AuC score   ', roc_auc_score(y_test, y_pred).round(4)
    print ''
    print 'Classification report'
    print classification_report(y_test, y_pred)
    print ''
    print 'Confusion matrix'
    predicted_cols = ['Predicted ' + str(c) for c in y.unique()]
    actual_rows = ['Actual ' + str(c) for c in y.unique()]
    conf = confusion_matrix(y_test, y_pred)
    print pd.DataFrame(conf, index = actual_rows, columns = predicted_cols)","clf_init = None
X_train, X_test, y_train, y_test = train_test_split(train_set[feature_names],train_set.churn, test_size=0.50,random_state=123456)
gnb = GaussianNB()
y_train = y_train.astype('int')
y_test = y_test.astype('int')
gnb.fit(X_train, y_train)
churn_col = ['Alive','Churned']
predicted = gnb.predict(X_test)
accuracy = accuracy_score(y_test, predicted)
print(""Gaussian Naive Bayes Classifier "")
print(""Accuracy:   "" + str(accuracy))
cm = pd.DataFrame(confusion_matrix(y_test, predicted), columns= churn_col, index=churn_col)
sns.heatmap(cm, annot=True)
print(""Note \n0: Alive and 1:Churned"")
print(metrics.classification_report(y_test, predicted))

# calculating AUC   
bayes_auc = roc_auc_score(y_test ,predicted,average='macro', sample_weight=None)
print(""AUC = "", bayes_auc)

# storing matrix
bayes_accuracy = metrics.accuracy_score(y_test, predicted)  
bayes_confus_matrix = metrics.confusion_matrix(y_test, predicted)  
bayes_classification_report = metrics.classification_report(y_test, predicted)  
bayes_precision = metrics.precision_score(y_test, predicted, pos_label=1)  
bayes_recall = metrics.recall_score(y_test, predicted, pos_label=1)  
bayes_f1 = metrics.f1_score(y_test, predicted, pos_label=1)","print(""Cells column A that are in same row as"", c, ""are"", tab.excel_ref(""A"").same_row(c))
print(""Cells column 7 that are in same column as"", c, ""are"", tab.excel_ref(""7"").same_col(c))",,Using Machine Learning To Predict Investment Fund Outperformance->Linear Regression in StatsModels,Final Churn->Churn Modeling and Analysis->Gaussian Naive Bayes Classifier,Tutorial Reference->Cell bag selection->cellbag1.waffle(cellbag2)
413050,"from __future__ import absolute_import, print_function, unicode_literals","# ignore me, needed to make demos work on my computer
from __future__ import division, print_function, absolute_import",genres_tuple[3],,Dasa->Discourse-Aware Sentiment Analysis->Code,7 Algorithms Recursion,"2->Access the element, with respect to index 3:"
88148,loss_fn = MSE,"# loss function
loss_fn = torch.nn.MSELoss(size_average=False)","playoffs = dfs.query('Game > 30000')
playoffs = playoffs[['Player', 'Team', 'Game', 'Season', 'TOION(60s)', 'CFON', 'CAON']]
playoffs['Round'] = (playoffs['Game'] - 30000) // 100
playoffs.head()",,Linear Regression->Metric,Simple Pytorch Nn Example->Create computational graph,Top Playoffs Cf
8574,"import copy
from io import BytesIO

from owslib.ows import ExceptionReport
from pandas import DataFrame, read_csv


def collector2table(collector, col='sea_water_temperature (C)'):
    c = copy.copy(collector)
    c.features = None
    try:
        response = c.raw(responseFormat=""text/csv"")
    except ExceptionReport:
        response = c.filter(end=c.start_time).raw(responseFormat=""text/csv"")

    df = read_csv(BytesIO(response.encode('utf-8')),
                  parse_dates=True)
    g = df.groupby('station_id')
    df = dict()
    for station in g.groups.keys():
        df.update({station: g.get_group(station).iloc[0]})
    df = DataFrame.from_dict(df).T
    
    names = []
    for sta in df.index:
        names.extend([offering.description for offering in c.server.offerings if sta == offering.name])
    df['name'] = names
    
    observations = []
    for k, row in df.iterrows():
        station_id = row['station_id'].split(':')[-1]
        c.features = [station_id]
        response = c.raw(responseFormat=""text/csv"")
        kw = dict(parse_dates=True, index_col='date_time')
        data = read_csv(BytesIO(response.encode('utf-8')), **kw).reset_index()
        data = data.drop_duplicates(subset='date_time').set_index('date_time')
        
        series = data[col]
        series._metadata = [dict(name=row['name'],
                                 station=row['station_id'],
                                 sensor=row['sensor_id'],
                                 lon=row['longitude (degree)'],
                                 lat=row['latitude (degree)'],
                                 depth=row['depth (m)'],)]

        observations.append(series)
    return observations","#edes-iPython def readByte(self, ID, Pos):
def readByte(ID, Pos):
    # Returns a byte from servo defined by ID in pos position
    return __visionlib.dxlReadByte( ctypes.c_int(ID), ctypes.c_int(Pos))","import numpy as np
kmeans = KMeans(n_clusters=4, random_state=0).fit(x_cols)
labels = kmeans.labels_
unique_elements, counts_elements = np.unique(labels, return_counts=True)
unique_elements, counts_elements",,Ndbc Alt->Can we get data near the [Coastal Endurance](http://oceanobservatories.org/array/coastal-endurance/) array?->What do we have as `SOS` and `OPeNDAP`?,Servo->readByte,Mini Project Clustering Dk->K-Means Clustering->Choosing K: The Elbow Sum-of-Squares Method
207352,"df_oversea_nationality['percentage_citizen'] = df_oversea_nationality.british_estimate / (df_oversea_nationality.british_estimate + df_oversea_nationality.cob_estimate + df_oversea_nationality.other_estimate)

f20 = figure()
a20 = f20.gca()
f20.set_figwidth(20)
f20.set_figheight(12)
p20 = sns.barplot(x = 'country',
              y = 'percentage_citizen', 
              data = df_oversea_nationality.sort_values('percentage_citizen', ascending=False)) 
# plot by descending order of percentage citizen

a20.set_xticks(x)
a20.set_xticklabels(df_oversea_nationality.sort_values('percentage_citizen', ascending=False).country, fontsize = 12) 
# order by percentage descending

for tick in a20.get_xticklabels():
    tick.set_rotation(90) 
a20.set_xlabel('Country',fontsize=20)
a20.set_ylabel('Percentage of British Citizens',fontsize=20)
a20.set_title('Figure 21.Percentage of British Citizens by coutry of birth',fontsize=20)
plt.savefig('21.png')
show()","# Energy spectrum, focusing on 2.6 MeV peak
cuts_fullspectrum = C_wide

fig = plt.figure(1)
fig.set_figheight(4.0)
fig.set_figwidth(6.0)

y, x, _ = plt.hist(A_Ec[cuts_fullspectrum]*CAL_FACTOR, 1000, range=[300,2900])
plt.xlabel('E (keV)')
plt.ylabel('Counts/bin')
#plt.yscale('log')

plt.savefig(""fig/CSTL_espectrum_full.pdf"", bbox_inches='tight')","DOS_all = [tbt.DOS(), tbt.ADOS(0), tbt.ADOS(1)]
plt.plot(tbt.E, DOS_all[0], label='G');
plt.plot(tbt.E, DOS_all[1], label=r'$A_L$');
plt.plot(tbt.E, DOS_all[2], label=r'$A_R$');
plt.ylim([0, None]); plt.ylabel('Total DOS [1/eV]'); plt.xlabel('Energy [eV]'); plt.legend();",,Exploratory Data Analysis->Task B->A4. Visualising the Relationships between Unemployment and Job Vacancies->2.2 Fit a linear regression model to the Victoria job vacancy data,Analysis Th Nov2017->Energy spectrum (Cs 662 keV peak)->Energy spectrum,Run->Exercises->Density of states
167388,"#creating a data frame to hold per 1000 residents features by city, while still including the population of the city
crime_per_cap = pd.DataFrame()
crime_per_cap['City'] = crime['City']
crime_per_cap['Population'] = crime['Population']

for col in crime.drop(['City', 'Population'], axis=1):
    crime_per_cap[col] = crime[col]/crime['Population']*1000

crime_per_cap.head()","crime_temp_per_day_DF=pd.DataFrame({'Temperature':T, 'Incidents':C})
crime_temp_per_day_DF.head()",test_preprocessed = loaded_model.vectorizer_obj.transform(test_preprocessed),,3->3.3.4 Challenge:  Advanced Regression - Crime Data->Exploring the Data,"Crime Dc-Checkpoint->DC crime analysis->**Objective 2**. Find the total number of registered offenses that took place in 2011, 2012 and 2013 and see if there is a trend on this.",Day30 Assignment2 Haptikapi->2.6. `Haptik` Class: Modeling (the dataset)->Dry run to verify whether our pre-processing & prediction workflow works:
463306,"# for finding the p-value
count = 0
temp = 0
# for gammagt and output variable
for i in range(50):
    # take 100 samples from dataset
    np.random.shuffle(indices)
    sample_indices = indices[:100]
    gammagt_sample = gammagt_col[sample_indices]
    output_sample = selector_col[sample_indices]
    pcc = np.corrcoef(gammagt_sample, output_sample)[0][1]
    # check if pcc is negatively correlated or not
    if pcc<=0:
        count = count + 1

p_value = count/50
p_value","# numpy.corrcoef returns Pearson product-moment correlation coefficients
print(np.corrcoef(sgot_col, selector_col)[0][1])
print(np.corrcoef(gammagt_col, selector_col)[0][1])","# ANSI escape sequences are a standard for in-band signaling to control the cursor location, color, and other options on video text terminals.
t= ""\t\u001b[0;35mgoogle.com\u001b[0m \u001b[0;36m216.58.218.206\u001b[0m""

r= re.compile(r'\x1b[^m]*m')
print(r.sub('', t))",,Assignment 4->__ Hypothesis testing:__->Null Hypothesis:,Assignment 4->Understanding the Data->features :,Regexp Notes->remove the ANSI escape sequences from a string
492495,"matrix = np.random.random((5, 5))
print(matrix)
min = np.min(matrix)
max = np.max(matrix)
matrix = (matrix - min)/(max - min)
print(matrix)","Z = np.random.random((5,5))
maxZ, minZ = Z.max(), Z.min()
Z = (Z - maxZ) / (maxZ -minZ)
print(Z)","def show_image(array):
    """"""Transform the array so that io.imshow
    function can use it as an input. 
    """"""
    img = Image.new('RGB',(512,512))
    arr = np.empty((262144,), dtype=object)
    arr[:]=[tuple([int(pixels) for pixels in line])
            for line in array.reshape(262144, 3)]
    img.putdata(arr)
    return np.asarray(img)",,Numpy,100 Num Py Exercises->Exercise 20,Pca For Filtering->Image Processing
164523,"bins = [dta.temp.min(), 32, 55, 80, dta.temp.max()]
bins","bin_values = np.arange(start=temp_obs.min(), stop=temp_obs.max(), step=.05)","# Function to draw polygon to visualize perspective transform
def draw_polygon(img, pts):
    if len(img.shape) == 2:
        img = np.dstack((img, img, img)).astype('uint8')*255
        
    line_image = np.copy(img)
    for i in range(len(pts)):
        x1, y1 = pts[i-1]
        x2, y2 = pts[i]
        cv2.line(line_image,(x1,y1),(x2,y2),(0,0,255),10)
    
    return line_image",,3->Putting it all together->Scatterplots->Ticks and Tick Labels,Normal Body Temp Eda-Checkpoint->Comparing Z-statistic and t-statistic,P4 Advanced Lane Lines-Final->Step 4. Perspective Transform
133493,"filters = ""inning == 2 and over == 20""","raw.filter(1, 20)","df2.drop(2893, inplace=True)",,Data Analysis->Which non-striker has been part of maximum number of runouts?,"Mne Notebook 1 Intro->Read in raw data; raw objects
Mne Showcase->Plot GFP
Mne Notebook 1 Preproc",Analyze Ab Test Results Notebook->Comparison to built in t-test
313976,"# Create two pandas.DataFrames

control = pandas.read_table(control_filename, index_col=0)
knockdown = pandas.read_table(knockdown_filename, index_col=0)","# Create a charttable object
tabl= ct.HTMLTable()

# add a formated columns using properties text, bar (size) and cell color 
tabl.addcolumn(name= ""A"", 
               text = df.A.values , 
               bar=[50,80,30], 
               colour=['red','green','blue'], # colour in word or hex
               overwrite=True)

tabl.addcolumn(name= ""B"", 
               text = ct.Leyend_cat(['c','z']).apply_colour(df.B.values), 
               bar=None, 
               colour = ct.Leyend_cat(['c','z']).apply_colour(df.B.values),
               overwrite=True)

tabl.addcolumn(name= ""C"", 
               text = df.C.values , 
               bar=None, 
               colour=['#FF0000','#800000','#FFFF00'], 
               overwrite=True)

# display the table
tabl.display()","import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import plotly.plotly as py
import plotly.figure_factory as ff
from plotly.offline import init_notebook_mode, iplot
import colorlover as cl

init_notebook_mode(connected=True)",,Example Session 2->Example 2: Differential expression scatterplots->Scatterplots,Charttable->Charttables demo->The basic,Mapping Match Pct->Exploring Match Percent Across the US
458935,df1=df[df['title'].notnull()],"df['cos_titl_quer'] = cos_titl_quer
df['cos_desc_quer'] = cos_desc_quer
df['eucl_quer_titl'] = eucl_quer_titl
df['eucl_quer_desc'] = eucl_quer_desc
df['jacc_quer_titl'] = jacc_quer_titl
df['jacc_quer_desc'] = jacc_quer_desc
df['jaro_quer_titl'] = jaro_quer_titl
df['jaro_quer_desc'] = jaro_quer_desc
df['leven_quer_titl'] = leven_quer_titl
df['leven_quer_desc'] = leven_quer_desc
df['fuzzr_quer_titl'] = fuzzr_quer_titl
df['fuzzr_quer_desc'] = fuzzr_quer_desc
df['fuzzt_quer_titl'] = fuzzt_quer_titl
df['fuzzt_quer_desc'] = fuzzt_quer_desc
df['title_noun_count'] = title_noun_count
df['desc_noun_count'] = desc_noun_count
df['query_noun_count'] = query_noun_count
df['title_verb_count'] = title_verb_count
df['desc_verb_count'] = desc_verb_count
df['query_verb_count'] = query_verb_count
df['title_length'] = title_length
df['query_length'] = query_length
df['word_in_title'] = word_in_title
df['word_in_description'] = word_in_description

df['fitment_score'] = data['fitment_score']
df['TrainingData'] = data['TrainingData']","# Cross validation test harness 

# Make this into a function so we can reuse it later

def eval_models(X, y):
    seed = 543
    # prepare models
    models = []
    models.append(('LR', LogisticRegression(class_weight='balanced')))
    models.append(('LDA', LinearDiscriminantAnalysis()))
    models.append(('KNN', KNeighborsClassifier()))
    models.append(('CART', DecisionTreeClassifier()))
    models.append(('NB', GaussianNB()))
    #models.append(('SVM', SVC()))
    models.append(('RF', RandomForestClassifier(n_estimators=100, class_weight='balanced')))
    # evaluate each model in turn
    results = []
    names = []
    #scoring = 'accuracy'
    scoring = 'roc_auc' # others include: 'accuracy', 'f1', 'roc_auc', 
                        # or found here: http://scikit-learn.org/stable/modules/model_evaluation.html
    for name, model in models:
        kfold = model_selection.KFold(n_splits=10, random_state=seed)
        cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)
        results.append(cv_results)
        names.append(name)
        msg = ""%s: %f (%f)"" % (name, cv_results.mean(), cv_results.std())
        print(msg)
        
    return results, names

# Execute the function
results, names = eval_models(X_train,y_train)  #TODO:  X,y",,Data Visualization,Model V4->Create final dataframe for training.,Amia Workshop->2.0 Read in the data->7.4 Testing many models
47353,"import numpy as np
Population_mean = np.mean(TotalAttributes)
Population_std = np.std(TotalAttributes)

print ""Population mean is"", Population_mean, ""\n"",""Population_std is"",Population_std","import numpy as np
pd = genPokedex(numer_pd)
total = [poke[3] for poke in numer_pd][1:]

pkmean = np.mean(total)
pksd = np.std(total)
print pkmean, pksd","# Plot number of male and female
ax = sns.countplot(x=""Sex"", data=titanic_data_cleaned)
ax.set_title('Number of males and females')",,"Project-1-V2-Starter-Code->Creating a simple ""hard-coded"" pokemon list->This could be done in Numpy easily such as this",Project-1-V2-Starter-Code->Package imports,Project 2->Titanic Data Analysis->Fix Missing Values
135868,from sklearn.model_selection import ParameterGrid,grid.HFacC.shape,"# coefficients: alpha, beta, detla_1, delta_2, gamma_1, gamma_2
results.params",,"Hw4->Neural Networks for MNIST dataset->Question 3 : 3 Layer model
Hw4 Applied Ml->Homework4 - Applied Machine Learning->Question 1: Propensity score matching->1. A naive analysis->Naive grid search for `n_estimators` and `max_depth`.
Cppn->Parameters Grid Search
Parameter Estimation->Lecture 18: Parameter Estimation->Part 4: Cross-validation",Bump Analysis Basics->Flow Across Bump - Model Basics->Plot grid parameters,Report->Cointegration test and stationary test on the spread
133544,"from sklearn.ensemble import GradientBoostingRegressor

stopwatch.start()
rf_test = Grf_test = GradientBoostingRegressor(max_depth=2, n_estimators=1000, max_features = 100,  learning_rate = 0.05, random_state=1234)
cv_scores = cross_val_score(rf_test, features, target, cv = 5, n_jobs = -1)
print('CV Score is: '+ str(np.mean(cv_scores)))

stopwatch.print_elapsed_time()","from sklearn.ensemble import AdaBoostRegressor
start_time = time.time()
ADA = AdaBoostRegressor().fit(X_train, y_train)
scores = cross_val_score(ADA, X_train, y_train, cv=10, verbose=1)
#print scores
print round(np.mean(scores),3), '= avg_r2'
print 'Runtime =', round(time.time() - start_time, 2), 'seconds'","data = read_data(""data.txt"")
grid = build_map(data, [-3.3, 3.3], 0.01)
plt.plot([x[0] for x in data if x[2]==1], [x[1] for x in data if x[2]==1], 'r.')
plt.plot([x[0] for x in data if x[2]==0], [x[1] for x in data if x[2]==0], '.')
plt.show()",,Data Analysis->Data preparation->Gradient Boosting Regressor,Kaggle Housing->Kaggle || Housing Prices: Advanced Regression Techniques->Step 3: Clean Dataset->5.3: Ensemble Methods->5.3.2: AdaBoost,Exercise 02->Exercise 2
164205,"iris.keys()                                                # what column names are present?
iris['Sepal.length'] = [4.6, 6.3, 7.2, 5.1, 5.7]           # add a column
if 'Petal.width' in iris: del iris['Petal.width']          # delete a column (if present)
iris['P/S'] = iris['Petal.length'] / iris['Sepal.length']  # vectorized whole-column operation
iris","#Make IRI outlier DF - none for PD or FS; after SHL checked skew/kurtosis, winsorized the HC outlier for 
#IRI_PT and IRI_EC

IRI_outs = raw[['ID', 'Group', 'IRI_EC_total', 'IRI_EC_total_outliers','IRI_PD_total','IRI_PD_total_outliers',
               'IRI_PT_total','IRI_PT_total_outliers','IRI_FS_total','IRI_FS_total_outliers']].copy()","from wordcloud import WordCloud
wordcloud = WordCloud(width=1440, height=1080).generate("" "".join(train.comment_text.astype(str)))
plt.figure(figsize=(20, 15))
plt.imshow(wordcloud)
plt.axis('off')",,"3->3.4 Tabulations and indexed arrays<span id=""table""></span>->(d) Clever joins<span id=""wrangling""></span>",Chr Beh New->Modeling->IRI Outliers,Toxic Comment Classification->Toxic Comment Classification Challenge->Wordclouds for clean dataset
223350,"params={
    ""cg"":[50,120,160,255], # color and gradient, when
    ""p"":[200,1200,250], #perspective transformation
}","param = ""Jobs""","# Extracting patient scan data
def folder_explorer(folder):
    patient_info = {}
    for d in os.listdir(folder):
        patient_info[d] = int(len(os.listdir(folder + d)))
    return patient_info

# Sample Data
patient_scans_sample = folder_explorer(sample_data)
df_patient_scans_sample = pd.DataFrame(list(patient_scans_sample.items()), 
                             columns=[""id"", ""scans-per-patient""])
patient_scans_sample_describe = pd.DataFrame.describe(df_patient_scans_sample)

# Train Data
patient_scans_train = folder_explorer(train_data)
df_patient_scans_train = pd.DataFrame(list(patient_scans_train.items()), 
                             columns=[""id"", ""scans-per-patient""])
patient_scans_train_describe = pd.DataFrame.describe(df_patient_scans_train)


print(""Descriptive statistics for sample data:"")
print(patient_scans_sample_describe)

print(""\n"")

print(""Descriptive statistics for train data:"")
print(patient_scans_train_describe)",,P4->Advanced Lane Line Finding->Stage 5: Warp back onto the original image,Parse Banchmark->Parse Banchmark->Single_Read,Data Exploration->Data Exploration
379850,"def pca_rf(method, interval=(20,100,10)):
    train, test, train_labels, test_labels = read_features(method)
    rf = RandomForestClassifier(n_jobs=-1, random_state=seed)
    pca = PCA()
    pipe = Pipeline(steps=[('pca', pca), ('rf', rf)])
    pca.fit(train)
    plt.figure(1, figsize=(10,5))
    plt.clf()
    plt.axes([.2, .2, .7, .7])
    plt.plot(pca.explained_variance_[:interval[1]], linewidth=2)
    plt.axis('tight')
    plt.xlabel('n_components')
    plt.ylabel('explained_variance_')
    # Prediction
    n_components = range(*interval)

    estimator = GridSearchCV(pipe,
                             dict(pca__n_components=n_components),
                             scoring='accuracy', n_jobs=-1)
    estimator.fit(train, train_labels)

    plt.axvline(estimator.best_estimator_.named_steps['pca'].n_components,
                linestyle=':', label='n_components chosen')
    plt.legend(prop=dict(size=12))
    plt.title(method)
    plt.show()","rf = RandomForestClassifier(n_estimators=100, class_weight='balanced')

max_depth = [1, 3, 5, 10, 20, 30]
tuned_parameters = [{'max_depth': max_depth}]

clf = GridSearchCV(rf, tuned_parameters, cv=10)
clf.fit(features, labels);

plt.plot(max_depth, clf.cv_results_['mean_test_score'], 'b>-')
plt.plot(max_depth, clf.cv_results_['mean_train_score'], 'go-');
plt.title('Max Depth vs Accuracy'); plt.legend( ('Test', 'Train'), loc='upper left')
plt.xlabel('Maximum depth'); plt.ylabel('Mean accuracy');","data = sparkSession.read.format(""csv"") \
  .option(""inferSchema"", ""true"").option(""header"", ""true"") \
  .load(""hdfs://127.0.0.1:39000/datasets/census/census.csv"")

data.head()",,02->Classification using traditional learning methods,Applied Ml->Applied ML->2. Machine Learning->2.1 Random Forest->Tuning agains overfitting-><ul> <li> Max depth <ul\>,"Deploy Spark Ml Census Decision Tree->Generate Spark ML Decision Tree->Step 0: Load Libraries, Data, and SparkSession"
153739,"doc = nlp(""王小明在北京的清华大学读书"")",doc = nlp(u'This is a sentence.'),"fig, ax = plt.subplots(1, 1, figsize=(5, 80))

sns.barplot(x=df_train['Murder and nonnegligent manslaughter'], y=df_train['MSA'], color = 'c')",,Demo->parse doc,6->Break down a document,Eda->Intro - Loading and checking completeness->Begining EDA - Basic Correlations->Murder rate->How does murder rate vary by MSA?
54022,"u = np.array([1, 6])
u","u = np.arange(-1,0.5,0.05)",print word_count.shape,,2->Expression of the dot product with norms->Example 4.,Value Of Adaptation,Amazon Baby Products->Build the word count vector for each review
83305,"# You have to use nltk.download() to get the PunktSentenceTokenizer

from nltk.tokenize import PunktSentenceTokenizer

sent_detector = PunktSentenceTokenizer()
sent_detector.sentences_from_text(easy_text)","from nltk.tokenize import PunktSentenceTokenizer
sent_detector = PunktSentenceTokenizer()
sent_detector.sentences_from_text(easy_text)","nb_days_month = [30, 31, 31, 30, 31, 30, 9]",,4->![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Feature Extraction from Text (Natural Language Processing)->STUDENT PRE-WORK,"W7D1-Nlp-Lecture->![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Feature Extraction from Text + NLP->Let's do some segmentation with NLTK
W7D1-Nlp-Lecture->No.->Introducing NLTK","Homework 1->5. dataframe for survivors by age, class and sex->The median of age is 28"
145870,"# broadcasting example:
A=np.array([[1,2,3,4],
          [5,6,7,8],
          [4,3,2,1],
          [8,7,6,5]])
A.sum(axis=0) # top to bottom sum","np.sum(np_array, axis=1)","for item in dir(ecmwf_wcs.contents['temp2m']):
    if ""_"" not in item:
        print(item)",,Week2->Week 2->Vectorization,Numpy-Solution->7.b ravel by column,01 Owslib Example->OWSlib library - Using WCS from Python-><i>Retrieve a list of available properties of a coverage</i>
273172,"from sklearn.cross_validation import train_test_split

selected = np.random.randint(y_all.shape[0],size=10000)
X = features[selected,:]
y = y_all[selected]

X, X_test, y, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42)","rand_state = np.random.randint(0, 100)
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                  test_size=0.2,
                                                  random_state=rand_state)","B = np.arange(4).reshape(2, 2)
B * B",,Learning->Learning->Learning the Manner of death (7 bins),Submission Code->Vehicle Detection Project->Classifier->Split the data into train and test sets,Intro Num Py->Intro to numerical computing with NumPy->Basics of Numpy arrays->Arrays versus lists
445507,"apple = pd.read_csv(data)

apple[:5]","appl_df = pd.read_csv('aapl.csv')
print(appl_df.shape)
appl_df.head()","# Stacked Lasso
stack_lasso = LassoCV(cv=5)
stack_lasso.fit(predictions_train, ytrain)

print('Stacked Lasso Regression estimator\'s score on the training set is:',
     stack_lasso.score(predictions_train, ytrain))

print('Stacked Lasso Regression estimator\'s score on the test set is:',
     stack_lasso.score(predictions_test, ytest))",,Exercises->Apple Stock->Step 3. Assign it to a variable apple,Data Manipulation-Checkpoint->Data Frame I/O from CSV file,Models->Models->Stacking->Stacked Lasso
293879,"df.fillna(0,inplace=True)
print(df['WindSpeedMPH'].mean())","mean1 = df.WindSpeed.mean()
print(mean1)
# Replace all the missing values in the windspeedcolumn with the mean
df['WindSpeed'] = df.WindSpeed.fillna(mean1)",y_test.value_counts(normalize=True).sort_index(),,Pandas->Pandas is a python module that makes data science easy and effective.->Shift + Tab in jupter notebook to see function documentation,Random Forest Classifier,"Wk8 Hw->Question 11->Question 9
Week8 Decision Trees Hw"
288864,"number = 1
total = 1
count = 1

while count < 21:

    total = total * number
    number += 2
    count += 1
    
print(total)","number = db_train['Age'].count()
total_values = len(db_train['Age'])

print(number/total_values)",print(mat[:2]),,"Lesson 1 - Numeric Types, Loops, Flow Control, Strings->Lesson 1 - Numeric types, loops, flow control, strings->`while` loops->Challenge: calculate the product of the first 20 odd numbers",Titanic Logistic Regression->Optimizing precision with LogRegression - 1 attempt,Linear Algebra Review->Linear Algebra / NumPy Review->NumPy Warmup
156139,"x = numpy.arange(-2, 1, 0.005)
y = numpy.arange(-1, 1, 0.005)
X, Y = numpy.meshgrid(x, y)","x = linspace(-10, 10, 100)
y = linspace(-10, 10, 100)

X, Y = meshgrid(x, y)","dt = np.dtype([('value', np.int), ('status', np.bool)])
np.array([(0, True), (1,False)], dtype=dt)",,10->Using `numba` with `ipyparallel`->Mandelbrot `ipyparallel` & `numba`,Complex Plots,Numpy Advanced->Structured arrays
71937,"from keras.preprocessing import image                  
from tqdm import tqdm

def path_to_tensor(img_path):
    #print(img_path)
    # loads RGB image as PIL.Image.Image type
    type(image)
    img = image.load_img(img_path, target_size=(224, 224))
    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)
    x = image.img_to_array(img)
    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor
    return np.expand_dims(x, axis=0)

def paths_to_tensor(img_paths):
    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]
    return np.vstack(list_of_tensors)","## Repeated for posterity

from keras.preprocessing import image                  
from tqdm import tqdm

def path_to_tensor(img_path):
    img = image.load_img(img_path, target_size=(224, 224)) # loads RGB image as PIL.Image.Image type
    x = image.img_to_array(img) # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)
    return np.expand_dims(x, axis=0) # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor (The 1 is the sample!)

def paths_to_tensor(img_paths):
    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)] # Batch 'this up, use count of img_path loaded from img_paths
    return np.vstack(list_of_tensors)","training_data = titles[titles['class'].isin([0, 1, 2])]
training_data = training_data[['class', 'title']].drop_duplicates()
training_data.head()",,Dog App->Pre-process the Data,Doggo Ai->doggoAI->Assess the Dog Detector,Pandas Playground->Selecting best book title from multiple candidates->Manually classified data
104376,"df.pivot_table(index=['country_name'], values=['status_num']).sort_values(by=['status_num'], ascending=False)","for i in fee_tabl.colnames: print(i, '->', fee_tabl.col(i)[0])","explore_variable(train, ""Neighborhood"", ""SalePrice"")",,Capstone,Check Chain->DIOMIRA->FEE table:,"Housing Sale Price Prediction (Kaggle)->Further Tweaking of Best Model
Housing Sale Price Prediction (Kaggle) Old"
445665,"x = Symbol('x')
y = Symbol('y')
Fxy = 2*x*y + x*y**2
Derivative(Fxy, x).doit()","def f(x):
    return -2*(x**3)

def f_deriv(x):
    return -6*(x**2)

print(f(2))
print(f_deriv(2))","train = pd.read_csv('train.csv')
test =pd.read_csv('test.csv')",,Exercises->Functions,"02A Numpy,Mathematics->Session 2a: Numpy and mathematics review->Derivatives",Titanic For Kaggle (Simple Solution)->the simplest and  easiest way to solve the titanic problem as a beginner in the data science (with a great accuracy)->reading the training and the testing  from their location
458994,"# reading the dataset using pandas as table
df = pd.read_table('/home/aman/tmp/opensignal.txt',index_col=0)","df = pd.read_csv('College_Data', index_col =0)","# Check item type first
print(chipo['item_price'].dtype)

# Now convert to flaot
print(chipo.item_price.head())

# Remove the dolloar sign 
chipo['item_price'] = chipo.item_price.str.replace(""$"","""")",,Data Visualization,K Means Clustering Project->Get the Data,"Getting And Knowing Your Data Exo2-><span style = ""color : brown""> PANDAS HWK 1 </span>->10. Convert the item_price into a float
Getting And Knowing Your Data Exo2-><span style = ""color : brown""> PANDAS HWK 1 </span>->14. How many different items are sold?"
437301,pred1 = pred.tolist(),"pcasR_pred = pca_pred_R_pred['pcaFeatures'].tolist()
pcasD_pred = pca_pred_D_pred['pcaFeatures'].tolist()","sampler_8.run_mcmc(pos, 5000)",,Classification->Classification->Project 2: SVM,Cluster->K-means Clustering->Capturing a wider political spectrum->Prepare Plot for K-Means,Hw6->Problem 2 - UV Luminosity Functions->3.2
482681,"superhero_ranks = {}
superhero_ranks[""Aquaman""] = 1
superhero_ranks[""Superman""] = 2

planet_numbers = {""mercury"": 1, ""venus"": 2, ""earth"": 3, ""mars"": 4}

new_dict = dict()","# String global variable
planet = 'Mercury'

# Function to change the current planet
def planet_change(new_planet):
    planet = new_planet # planet is a local variable
   
# Global scope
print(""Planet = "", planet)
planet_change('Mars')
print(""Planet = "", planet)","for x,y in enumerate(a):
    print(f""{x}:{y}"")",,Dictionaries->Dictionaries->Creating and Populating a Dictionary,3-4->Section 4.2: Variable Scope->Global Variables->Assigning a value to a global variable in a local scope without `global`,Python Quick->Python QUICK->Lists & Sets
258810,"prediction_space = np.linspace(min(X_rooms),
                               max(X_rooms)).reshape(-1,1)","x = np.linspac(0, 10, 20)","## TODO : NVL vs fillna ( Bfill, ffill)",,Regression->Fitting a regression model,Py Exploratory Comp 7 Sol->Notebook->`invalid syntax`,Rd Pandas Vs Sql->Missing Data->Filling Missing Data
266162,"data = ThreesAndEights(""data/mnist21x21_3789.pklz"")
print(""Number of examples in training set -"")
print(len(data.X_train))

# Number of examples in the test set
print(""Number of examples in test set -"")
print(len(data.X_valid))

# Number of pixels in each image.
print(""Number of pixels in each image -"")
print(data.X_train.shape[-1])","mylist = 'zero one two three four five six seven eight nine ten'.split()
print('My list:', mylist)
print('First:', mylist[0])
print('Last:', mylist[-1])
print('Next to last:', mylist[-2])",order_id_2 = con.get_order_ids()[0],,Boosting->Boosting Homework->Overview,Hard Way 32-34 Loops And Lists->Accessing List Elements,08 Oco Orders->OCO Orders->Creating an OCO Order
477447,"def t_score(eps,v,sigma2):
    return ((v+1)/v)*(eps)/(sigma2 + np.power(eps,2)/v)","def mean_squared_error(X):
    return np.power(X, 2).mean(axis=None)",r0.branches,,Py Data Time Series Talk->Model 2: Score-Driven Models->Amazon Beta from Kalman Filter/Smoother,Artificial Neural Network Example->Background,Branching->`Branching Reader`
176706,"test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=300,
             batch_size=128, n_hidden=500, n_hiddenLayers=3,
             verbose=False, smaller_set=True)","test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0000, n_epochs=50,
             batch_size=128, n_hidden=500, n_hiddenLayers=3,
             verbose=False, smaller_set=False)","G2 = np.zeros((N2, N2))

for j in range(N2):
    if j == 0:
        G2[j, j] = 1
    elif j == 1:
        G2[j, j-1] = 1
        G2[j, j] = 1
    else:
        G2[j, j-2] = 1
        G2[j, j-1] = 1
        G2[j, j] = 1",,Hw3->ECBM E6040 Homework 3 - Programming Problem->Problem A: Regularization Methods for Neural Network,Hw3->ECBM E6040 Homework 3 - Programming Problem->Problem a: Regularization Methods for Neural Network->Test with the original dataset,03Git Hw->Question 3.3.
64749,PM25_2017.head(),"states_pm25 = gpd.sjoin(states, geo_pm25, how='left', op='contains')
states_pm25.head(1)","from collections import Counter
def document_adder(text,title):
    c = conn.execute(""SELECT MAX(docno) FROM doc_names"")
    doc_number = c.fetchone()[0] + 1
    dtm_data=[] 
    words = tokenizer(article)
    words = filter(lambda x:x in tfidf.vocabulary_,words)
    wordcounts = Counter(words)
    number_words = len(words) * 1.0
    for word in wordcounts:
        word_number = vocabulary.ix[word][""row""]
        idf = vocabulary.ix[word][""idf""]
        tf = wordcounts[word] 
        dtm_data.append([doc_number,word_number,tf*idf])
    df = pd.DataFrame(dtm_data,columns=[""docno"",""word_number"",""tfidf""])
    df.to_sql(""frequency"",if_exists = 'append', index=False, con = conn)
    conn.execute(""INSERT INTO doc_names (docno, title) VALUES (%s, '%s')"" %(doc_number,title))",,Hw2->[ERG-190C] Homework 2: Pandas EPA Air Quality->We can then organize this data and read it better by putting it in a table! We will go over this in the next section.,Homework-14-Powerplants-Group Three Daskalopoulou->1. Do states with coal power plants have dirtier air?,Searchengine->Databases and Pandas->Add Documents
223375,"def draw_lines(img, binary_warped, Minv, left_fitx, right_fitx, ploty, radius, distance): 
    h,w = binary_warped.shape
    # new_img = np.copy(img)
    warp_zero = np.zeros_like(binary_warped).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))
    
    # Recast the x and y points into usable format for cv2.fillPoly()
    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])
    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])
    pts = np.hstack((pts_left, pts_right))

    # Draw the lane onto the warped blank image
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))
    cv2.polylines(color_warp, np.int32([pts_left]), isClosed=False, color=(255,0,255), thickness=15)
    cv2.polylines(color_warp, np.int32([pts_right]), isClosed=False, color=(0,255,255), thickness=15)

    # Warp the blank back to original image space using inverse perspective matrix (Minv)
    warp = cv2.warpPerspective(color_warp, Minv, (w, h), flags=cv2.INTER_LINEAR) 
    # Combine the result with the original image
    result = cv2.addWeighted(img, 1, warp, 0.5, 0)
    
    font = cv2.FONT_HERSHEY_SIMPLEX
    cv2.putText(result,'Curve Radius: %.1f m'%(radius),(50,50), font, 1,(255,255,100),2,cv2.LINE_AA)
    curve = 'left'
    if distance > 0:
        curve = 'right'
    distance = abs(distance)
    cv2.putText(result,'Vehicle is %.1fm '%(distance) + curve + ' from the Lane Center',(50,100), font, 1,(255,255,100),2,cv2.LINE_AA)

    
    return result","def draw_lane(image,img):
    """"""
        Drawing detected lane area onto original picture
        
        input:
            image: original image
            
            img: binary warped image
            
    """"""
    h = img.shape[0] # image height
    ploty = np.linspace(0,h-1,h)
    # Create an image to draw the lines on
    warp_zero = np.zeros_like(img).astype(np.uint8)
    color_warp = np.dstack((warp_zero, warp_zero, warp_zero)) # create three channels
    
    # 
    left_fitx = left_lane.draw_fit[0]*ploty**2 + left_lane.draw_fit[1]*ploty+left_lane.draw_fit[2]
    right_fitx = right_lane.draw_fit[0]*ploty**2 + right_lane.draw_fit[1]*ploty + right_lane.draw_fit[2]
     
    # Recast the x and y points into usable format for cv2.fillPoly()
    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])
    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))]) # flipud put each column upside down
    pts = np.hstack((pts_left, pts_right))
    
    # Draw the lane onto the warped blank image
    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))
    cv2.polylines(color_warp, np.int32([pts_left]), isClosed=False, color=(255,0,255), thickness=25)
    cv2.polylines(color_warp, np.int32([pts_right]), isClosed=False, color=(0,255,255), thickness=25)
    
    # Warp the blank back to original image space using inverse perspective matrix (Minv)
    newwarp = cv2.warpPerspective(color_warp, InvM, (image.shape[1], image.shape[0])) 
    # Combine the result with the original image
    result = cv2.addWeighted(image, 1, newwarp, 0.3, 0)
    #plt.figure()
    #plt.imshow(result)
    return result","#Cycle through all methods and all prediction columns
for col in spendingCol:
    y = resp[col]
    for m in methods:
        name,acc = testModelAcc(m(),X,y)
        fiveClass[name].append(acc - .2)
        
#Put the results in a data frame
fiveClass = pd.DataFrame(fiveClass,index=spendingCol)
fiveClass",,P4->Build a Lane Finding Pipeline->Draw the detected lane back to the Original Image,Advanced Lane Findings->Notebook for Project - Traffic Lane Findings->Draw Line,Predict Pref->Predicting Preferences->What can we predict?
184197,"only_dog_clean.breed.value_counts()[:5,]","# test for dog
dog_breed('dogImages/valid/001.Affenpinscher/Affenpinscher_00041.jpg')","import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline

plt.style.use('seaborn-paper')
plt.rc('text', usetex=False)",,Wrangle Act->Data Analysis and Visualization->Most Popular Breed !,Dog App->(IMPLEMENTATION) Predict Dog Breed with the Model,"Figure 4->Chapter 4: Machine Learning state of the art
Figure 3->Chapter 4: Machine Learning state of the art
Figure 3->Chapter 4: Machine Learning state of the art
Figure 3->Chapter 4: Machine Learning state of the art"
302573,"def time_to_int(time):
    return int(time.replace(':', '')[:4])

gtfs['stop_times'] = gtfs['stop_times'].assign(
    arrival_time_int=gtfs['stop_times']['arrival_time'].map(time_to_int),
    departure_time_int=gtfs['stop_times']['departure_time'].map(time_to_int)
)

gtfs['stop_times'].head()","# Create time column

def getTime(created):
    time = (created.split()[-1])
    time = time.split(sep=':')
    time = tuple(time)
    return time

def getMonth(created):
    month = (created.split()[0])
    month = month.split(sep = '-')
    month = int(month[1])
    return month

def getDay(created):
    day = (created.split()[0])
    day = day.split(sep = '-')
    day = int(day[-1])
    return day

created_series = calldata.iloc[:,0]
month_series = created_series.apply(getMonth)
day_series = created_series.apply(getDay)
time_series = created_series.apply(getTime)

calldata = calldata.assign(Month=month_series.values)
calldata = calldata.assign(Day=day_series.values)
calldata = calldata.assign(Time=time_series.values)

#calldata.head()","dt = DecisionTreeClassifier(labelCol=""delayed"", featuresCol=""features"", maxDepth=12, maxBins=9000, impurity=""gini"")
print(""DecisionTreeClassifier parameters:\n"" + dt.explainParams() + ""\n"")",,Presentation->Transform the departure and arrival times in values we can use,"Data Science Final->Create Columns for Month, Day, and Time",Tutorial-Ml-Final->This notebook is part of Hadoop and Spark training delivered by IT-DB group->choose the machine learning algorithm
77608,digits = datasets.load_digits(),"# Load not-MNIST data and instantiate the classifier
notmnist_dataset, notmnist_label = load_notmnist(scaled=True)
classifier = DigitNotDigit()","my_dict3 = dict(arch1='cnn', arch2='rnn', arch3='recommendation system')
print(my_dict3)
print(my_dict3['arch3'])",,"Main->Load the digits dataset: digits
Index->SVC Cross Validtion Scores vs C-value->The data
02->Visualizing Data from an External Dataset
Introduction To Machine Learning->Creating a Model->We Don't Have Too Many Samples
Python Svm Classifier Example->Python SVM Classifier Example->SVN Example using
Tutorial1
Datasets->Scikit-learn datasets->Handwritten Digits
One Vs Rest->Experiments with the one vs rest  multiclass classification scheme
One Vs Rest
K-Nn Classifier With Scikit-Learn (Supervised Learning)->K-NN classifier with scikit-learn, KerasClassifier->Loading of the digits dataset
Ml->Introducting to machine learning->1. Data collection and dimensionality reduction
Machine Learning Workshop->Handwritten digits
Machine Learning Ml Recognizing Hand-Written Digits
07-Prac7-><span style=""color:#0b486b"">SIT 112 - Data Science Concepts</span>-><span style=""color:#0b486b"">Note</span>
19 01 Tensor->Intro to Neural Networks through SKFlow->Digit Recognition
Part 4 Machine Learning->Machine Learning->Recognising digits
Digit Recognisation->Digit Recognisation using Logistic Regression Algorithm
Digit Recognition Svm->Recognizing hand-written digits
07 Learning
Building, Tuning And Deploying Models->Classification Example->Load dataset
Digits Sci-Kit Learn Example Personal Attempt->My attempt at the whole digits example
Digits Recognition
Mnist Practice->Load Data
Workshop1->Workshop 1 - Classification of Handwritten Digits from MNIST Dataset->The objective of the workshop is to learn a simple machine learning workflow in order to solve a classic machine learning problem: classifying handwritten digits from 0-9->Load the MNIST digits dataset
Boltzmann Feature Extraction->Restricted Boltzmann Machine features for digit classification->Load data
Benchmark Classifier->Load the digits datasets
Digit Recognization Logistic Regression->Digit Recognization->Load Data
Environment Test->Testing the environment->scikit-learn
Experiment-Notebook->Author: Gael Varoquaux - License: BSD 3 clause->The digits dataset
Clustering Digits->Clustering using Digits dataset->Loading digits dataset",Experiment Not Mnist->Using trained model with 5 epochs,"Python Dictionaries->**python Container: Dictionaries** (_key, value_)->* Iterating through my_dict ... LOOP"
147784,"countries_df = pd.read_csv('./countries.csv')
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')
df_new.head(8)","countries_df = pd.read_csv('./countries.csv')
countries_df.head()
df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')","audio_err, _ = librosa.load(test_files[7], sr=samplerate)",,Analyze Ab Test Results Notebook,Analyze Ab Test Results Notebook->Analyze A/B Test Results,04-Sound Classifier
325826,journalist_followed_gender_summary(follower_to_journalist_followed_df[follower_to_journalist_followed_df.gender == 'M']),journalist_mention_gender_summary(journalists_mention_df),"import os
import pandas as pd
import numpy as np
from torch import optim
from field_analysis.model.dataset import dataperiod as dp
from field_analysis.model.nets.cnn import DroneYieldMeanCNN
import field_analysis.settings.model as model_settings
%matplotlib inline

db_32 = 'field_analysis_10m_32px.db'
db_64 = 'field_analysis_20m_64px.db'
db_128 = 'field_analysis_40m_128px.db'
dbs = [db_32, db_64, db_128]
depth_models_dir = os.path.join(model_settings.MODELS_DIR,'depth')
results_dir = os.path.join(os.getcwd(),'results')
os.makedirs(depth_models_dir,exist_ok=True)
os.makedirs(results_dir,exist_ok=True)

def test_depth(dataloader, bands):
    depths = list(range(4, 14, 2))
    multi_index = pd.MultiIndex.from_product([[32,64,128],depths])
    losses = pd.DataFrame(index=['test','train'],columns=multi_index)
    for i, db in enumerate(dbs):
        dataset = dataloader(db_name=db)
        dataset_name = dataset.__class__.__name__
        source_bands = 1  # NDVI
        if 'RGB' in dataset_name:
            source_bands = 3
        for depth in depths:
            source_dim = 32*(2**i)
            cnn = DroneYieldMeanCNN(
                source_bands=bands,
                source_dim=source_dim,
                cnn_layers=depth,
                fc_layers=2,
                optimizer=optim.Adadelta)
            cnn.model_path = os.path.join(depth_models_dir,cnn.model_filename)
            print(cnn.model_path)
            losses_dict= cnn.train(
                epochs=50,
                training_data=dataset,
                k_cv_folds=3,
                suppress_output=True)
            test_errors = cnn.produce_test_errors(dataloader(db,test=True))
            print(f""Test MAE: {test_errors['mean_error']}"")
            print()
            best_train_loss = np.array(losses_dict['training_losses_mean_std'])[:, 0].min()
            losses.loc['test',(source_dim,depth)] = test_errors['mean_error']
            losses.loc['train',(source_dim,depth)] = best_train_loss
    return losses",,Gender Dynamics->Gender dynamics->Prepare the tweeter data->Load user info,"Gender Dynamics->Gender dynamics->Tweeter analysis->Beltway journalists followed by male beltway journalists->Of journalists mentioning other journalists, how many are male / female?",Ml Iii->2. Optimal Depth
3790,"async def asynchronous_requests ():
    tasks = []
    
    # obtain the session to make HTTP requests
    async with aiohttp.ClientSession() as session:
        
        # accumulate a bunch of Future objects, one for each `fetch_async` call
        for i in range(num_requests):
            task = asyncio.ensure_future(fetch_async(session, i))
            tasks.append(task)
    
        # yield execution (i.e don't return) until all of the gathered Future objects have resolved
        return await asyncio.gather(*tasks)","async def asynchronous_requests ():
    tasks = []
    
    # obtain the session to make HTTP requests
    async with aiohttp.ClientSession() as session:
        
        # accumulate a bunch of Future objects, one for each `fetch_async` call
        for i in range(num_requests):
            task = asyncio.ensure_future(fetch_async(session, i))
            tasks.append(task)
    
        # yield execution (i.e don't return) until all of the gathered Future objects have resolved
        return await asyncio.gather(*tasks)",ed_2012.head(),,Tutorial->Introduction->Step 1.2 - Asynchronous HTTP,Tutorial->Introduction->Step 1.2 - Asynchronous HTTP,Choropleth Maps Exercise->Choropleth Maps Exercise->USA Choropleth
188156,"plt.rcParams[""figure.figsize""] = (15.0, 20.0)

# get the counts for each key, assuming the values are numerical
y_pos,signCounts = np.unique(y_train, return_counts = True)

#map label numbers to values in CSV
#BarH plots count of each sign
plt.barh(y_pos, signCounts, align='center', alpha=0.4)
plt.yticks(y_pos, named_y_train)
plt.xlabel('Counts per image-ID')
plt.title('Number of each sign classification')

plt.show()","height = cast_count
bars = cast_count.index
y_pos = np.arange(len(bars))
plt.figure(figsize=(10,10))

plt.title('Cast or Actor mostly occur in top 500 profitable Movie',fontsize=18)
plt.xlabel('Number of Movies', fontsize = 20)
plt.ylabel('Cast', fontsize = 20)
# Create horizontal bars
plt.barh(y_pos, height,edgecolor='yellow',color = (1.0,0.1,0.5,0.6))
 
# Create names on the y-axis
plt.yticks(y_pos, bars)
 
# Show graphic
plt.show()","%%writefile data/model.sif
Model parent_of ViewModel_1
Model parent_of ViewModel_2
Model parent_of ViewModel_3
ViewModel_1 parent_of Presentation_A
ViewModel_1 parent_of Presentation_B
ViewModel_2 parent_of Presentation_C
ViewModel_3 parent_of Presentation_D
ViewModel_3 parent_of Presentation_E
ViewModel_3 parent_of Presentation_F",,Traffic Sign Classifier,Investigate-A-Tmdb-Dataset->Project: Investigate a TMDb Dataset->Now removind duplicated row,Lesson 1 Introduction To Cy Rest->Exercise 1: _Guess_ URLs->Introduction to Cytoscape Data Model
256660,"for student in students:
    student_data = (""%-20s %-20s %-10f %-10f %-5s"" % 
          (student[""name""], 
           student[""email""], 
           student[""midterm""], 
           student[""final""], 
           student[""grade""]))
    print(student_data)","for student in students:
    student_data = (""%-20s %-20s %-10f %-10f %-5s"" % 
          (student[""name""], 
           student[""email""], 
           student[""midterm""], 
           student[""final""], 
           student[""grade""]))
    print(student_data)","print ""Specific capacitance: %f uf/cm2"" % soma.cm",,9->Mangle Data Like a Pro->Old Style With %,9->Mangle Data Like a Pro->Old Style With %,Egng1220 Sp2018 Hw3 V1->NEURON: Ball-and-stick electrical model of a neuron->7. Adding active ion channels
352603,"from keras import backend as K
from keras.models import Model
from keras.layers import (BatchNormalization, Dense, Input, 
    TimeDistributed, Bidirectional, LSTM, Dropout)","from keras.layers import Embedding
from keras.layers import Dense, Input
from keras.layers import Embedding, Dropout, LSTM, Bidirectional
from keras.models import Model, load_model

from keras import backend as K
from keras.engine.topology import Layer, InputSpec",np.linalg.inv(np.diag(D)),,Vui Notebook->Final Model Iterations,Question Type Identifier->0. Importing libraries,2->2.9 The Moore-Penrose Pseudoinverse
242432,"def individual_rmsd(ind):
    """"""Return the mean rmsd scores of the poses in the individual training set.
    
    :param ind: individual
    :returns: mean rmsd of training set defined by the individual""""""
    
    total_rmsd = 0
    for i, bit in enumerate(ind):
        if bit:
            total_rmsd += float(true_actives[i][0])  # Get the RMSD value.
    return total_rmsd / sum(ind)  # Sum being the count of poses included.","def individual_rmsd_frequencies(ind):
    """"""Plot the rmsd distributions of an individual.
    
    :param ind: individual
    :returns: None, just plots and prints statistics regarding an individual.""""""
    
    rmsds = []
    ligands = []
    
    ## Build data lists.
    for i, bit in enumerate(ind):
        if bit:
            rmsds.append(float(true_actives[i][0]))  # Get the RMSD value.
            ligands.append(true_actives[i][1])  # Ligand value.

    ## Catergorize rmsds into 3 bins.
    for i, rmsd in enumerate(rmsds):
        if rmsd < 2.0:
            rmsds[i] = 1
        elif rmsd < 4.0:
            rmsds[i] = 0
        else:
            rmsds[i] = -1
    
    ## Plots!
    plt.hist(rmsds)
    plt.show()
    
    ## Also print popular ligands.
    ligand_counts = Counter(ligands).items()
    print ""Ligands w/ at least 3 poses""
    for tuple_ in ligand_counts:
        if tuple_[1] >= 3:
            print tuple_[0]","sm.qqplot(model.resid, line='s')
pass",,03->Individual Analysis->Individual RMSD,03->Individual Analysis->RMSD Frequencies,"Smart-Cities-Final-Project->Logistic Regression->Notes on the model above
Smart-Cities-Final-Project->PurchasePower and QualityLife->Linear Regression: PurchasePower->Notes on the model above"
258074,"fig, ax = plt.subplots(figsize=(10, 8))
ax.plot(alphas, mspes, label=""MSPE"")
ax.axvline(x=alphas[mspes.argmin()] + 0.01,
           c='r', linestyle='dashed',
           label=""Min. MSPE"")
ax.set_xscale('log')
plt.legend(loc=""upper right"")
ax.text(alphas[mspes.argmin()] + 0.05,
        1300000,
        r""$MSPE_{{min}} = {:.0f}$"".format(mspes.min()),
        fontsize=13)
plt.title(""MSPE vs. Alpha (Ridge Regression)"")
plt.xlabel(""Alpha"")
plt.ylabel(""MSPE"")
plt.show()","t_theo = [tdata_min[0], tdata_min[0]/2, tdata_min[0]/4, tdata_min[0]/8, tdata_min[0]/16]
fig = plt.figure()
ax = fig.add_subplot(111)
ax.set_xscale(""log"")
ax.set_yscale(""log"")
ax.set_xlim(1e-0, 1e2)
ax.set_title(""Timing Plot for Computing Minimum Using Multiple CPUs"")
ax.set_xlabel('Processors')
ax.set_ylabel('Time(sec)')
l1,= ax.plot(p, tdata_min, ""o-"", color='r')
l2,= ax.plot(p, t_theo, ""o-"", color='b')
ax.legend([l1, l2],[""Practical"", ""Theoractical""])
fig.show","df = pd.read_csv('SEG_Saleshistory_Stores.csv')
df.columns = ['date', 'store', 'item', 'sales']
df.date = pd.to_datetime(df.date)
df.item = df.item.astype('category')
df.store = df.store.astype('category')
df.dtypes",,Regression->Further Exploration,Hw1-2->Computational Statistics->Computing Minimum with Multiple CPUs->Processors vs Time Taken (Minimum),Store Model 2->Import and format data
182848,"#  get rid of the unnecessasry columns in dog_copy
dog_copy = dog_copy[[""tweet_id"", ""dog"", ""dog_prob""]]",dog = Dog(-5),"from datetime import datetime

class Comment(object):
    def __init__(self, email, content, created=None):
        self.email = email
        self.content = content
        self.created = created or datetime.now()

comment = Comment(email='leila@example.com', content='foo bar')",,"Wrangle Act->Project ""Wrangle and Analyze Data""->Clean Data->TIDINESS->finding best result for dog breed in data of the neural network",Property Python,2-Serialization->Declaring Serializers
107176,"x = np.arange(-5.0, 5.0, 0.1)

## Please feel free to change the parameters
beta0=0
beta1=1
beta2=0   # this represents quadratic term
beta3=0  # this represents cubic term


X =  beta0 + beta1*x + beta2*(x**2)+ beta3*(x**3)

# linking function.
y = 1/(1+np.exp(-X))

plt.plot(x,y) 
plt.ylabel('Dependent Variable')
plt.xlabel('Indepdendent Variable')
plt.show()","## Please feel free to change the parameters
x = x_data
beta0=250    # represents the intercept on yaxis
beta1=-10.0   # represents the slope of the line
beta2= 0   # this represents quadratic term
beta3=-0.0  # this represents cubic term


y =  beta0 + beta1*x + beta2*(x**2)+ beta3*(x**3)

plt.plot(x_data,y_data,'ro')
plt.plot(x_data,y)","sigmas = corr_tbl['meas_errs'][good_sig_iis].data
mass = stlr_mass[good_sig_iis]
sfrs = corr_sfrs[good_sig_iis]

z_sp = zspec0[good_sig_iis]
badz_iis = np.array([x for x in range(len(z_sp)) if z_sp[x] < 0 or z_sp[x] > 9])
filt_lambda_list = {'NB704':7045.0, 'NB711':7126.0, 'NB816':8152.0, 'NB921':9193.0, 'NB973':9749.0}
ffs = filts[good_sig_iis]
for ff in filt_lambda_list.keys():
    badf_match = np.where(ffs[badz_iis] == ff)[0]
    z_sp[badz_iis[badf_match]] = (filt_lambda_list[ff]/6562.8) - 1
    
mz_data = np.vstack([mass, z_sp]).T",,1,1,Mc Fit Errors
204151,"filled_data=resampled_data.interpolate()
filled_data.isnull().sum().sum()","np.fill_diagonal(Q, 0)
for i in range(len(Q)):
    Q[i]=Q[i]/sum(Q[i])","# Import Packages

from math import sqrt
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt

# Getting more than one output Line
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = ""all""",,Code,Ha 2,Recommender-System-Python->Recommender System
165492,"region = img[250:350, 400:220]
img[100:200, 250:70] = region","def remove_region(img, region: Region, inplace=True):
    img = np.asarray(img)
    if not inplace:
        img = img.copy()
    img[region.y0:region.y1,region.x0:region.x1,3] = 0
    return img","cities = ['new york', 'sydney', 'canberra', 'santa marta']

capitalized_cities = [city.title() for city in cities]
print(capitalized_cities)",,3->Image Operations->Image ROI,Sprite Extraction,Ai-Python-Chap01-Lesson3->List Comprehensions
415344,"# Make this more readable...

bye_spain = ""buenas noches""
english_greeting = ""hello""
english_goodbye = ""sod off, lad""
greeting_japanese = ""konichiwa""
spanish_greeting = ""hola""
hello_in_french = ""bonjour""
japanese_bye = ""sayonara""","# Put all 4 stop-word lists into one list
STOP_WORDS = ENGLISH_STOP_WORDS + FRENCH_STOP_WORDS + GERMAN_STOP_WORDS + SPANISH_STOP_WORDS",fw1,,13->Homework Assignment #2,"10 Good Model->Make a Good Model for the mood of the song->Models (LinReg, Lasso and RF) - TF-IDF->Make list with more stopwords (bring in other languages)",Hofstadter-Buterfly
309510,"pl.figure(figsize=(16,8))
for i in np.arange(nfl):
    pl.plot(wav_corr[i,:],flux_norm[i,:],lw=0.5,alpha=0.5)
pl.xlim(550,551)
pl.ylim(0,1.1);","pl.plot(bin_edges[:-1]+dp/2,hist,'bo',x,gauss,'r')
pl.ylim(0,3.5)
pl.xlim(9.5, 10.6)","import seaborn as sns
sns.set(style=""ticks"", palette=""muted"", color_codes=True)
ax = sns.boxplot(x='quality', y='pH', data=wine)",,Hd127334 1->Preprocessing,"Pressure Calibration Example->Calibration and uncertainty analysis - virtual experiment->we expect to see the Gaussian, if our pressure measurements contain random errors",Data Analysis Exercises-Solutions->2-1) Import the dataset wines.csv from your data folder and retrieve the first 5 rows:
169546,sess.run(tf.transpose(a=a)),"sess.run(tf.matmul(a=a, b=b))","# Declare the figure object.
fig = plt.figure()

# Declare an axis object within / on the figure object.
# See the below link for a discussion of matplotlib's subplot.
# https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html
ax = Axes3D(fig)

# Add the points to the scatter plot.
ax.scatter(x_vals, y_vals, z_vals, c=c_vals, s=s_vals)

xyz_vals = zip(x_vals, y_vals, z_vals)

# Iterate through the edges, and draw line segments for each set.
for edge in tqdm_notebook(test_graph.edges()):
    
    # Assign the node edge index names.
    node_a = edge[0]
    node_b = edge[1]
    
    # Assign the x, y, z variables.
    xa = test_graph.node('x')[node_a]
    ya = test_graph.node('y')[node_a]
    za = test_graph.node('z')[node_a]

    xb = test_graph.node('x')[node_b]
    yb = test_graph.node('y')[node_b]
    zb = test_graph.node('z')[node_b]   

    # Get the xyz coordinates for each node, and draw a segment.
    ax.plot([xa, xb], [ya, yb], [za, zb], color='b', alpha=0.3)

# Show the plot.
plt.show()",,3->Working with Matrices->Transpose,3->Working with Matrices->Matrix multiplication,Networkx Exploration->Adding Edges->Viewing the Edges
69162,"### TODO: Define your architecture.
Resnet50_model = Sequential()
## global average pooling layer
Resnet50_model.add(GlobalAveragePooling2D(input_shape=train_Resnet50.shape[1:]))
## fully connected layer
Resnet50_model.add(Dense(133, activation='softmax'))

Resnet50_model.summary()","### TODO: Define your architecture.
Resnet50_model = Sequential()
#Resnet50_model.add(GlobalMaxPooling2D(input_shape=train_Resnet50.shape[1:]))
Resnet50_model.add(GlobalAveragePooling2D(input_shape=train_Resnet50.shape[1:]))
Resnet50_model.add(Dense(133, activation='softmax'))

Resnet50_model.summary()","# Read the original dataset to pandas and take a peek at the first five rows
file = ""World Religion Dataset - National Religion Dataset.XLS""
bigdata = pd.read_excel(file,thousands=',')
bigdata.head(5)",,Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Model Architecture,Dog App->(IMPLEMENTATION) Model Architecture,"Final->Appendix I->1.2 Religion in the world->The World Religion Database
Final->Prediction time!"
21187,"plot_distribution(train, var='Fare', target='Survived')","train.plot('ID', 'Age', kind='scatter')","%%time
blend.fit(200, e_rel=1e-3)
print(""Fit for {0} iterations"".format(blend.it))",,Titanic->Titantic: Machine Learning from Disaster->2. Deeper Analysis->2.2 Fare,Predicting Population Income Group->Outlier Treatment,Point Source->Point Source Tutorial->Initialize the sources
198833,"# A more elegant approach
fig, axes = plt.subplots(nrows=2, ncols=5, figsize = (14,4.5))
for i, ax in enumerate(axes.flatten()):
    ax.set_title(""x to the {:d}"".format(i+1))
    ax.plot(x, x**(i+1))
    
fig.tight_layout();","fdpi = 300
fig = plt.figure(figsize=(1200/fdpi, 1200/fdpi), dpi=fdpi)
ax = fig.add_subplot(111)
ax.set_xlim(0, 1.0)
ax.set_ylim(-1.2, 1.2)
ax.plot(x, y)
ax.text(0.0,0.0,""(0,0)"",transform=fig.transFigure,fontsize=12,fontweight='bold',
       ha='center',va='center')
ax.text(1.0,1.0,""(1,1)"",transform=fig.transFigure,fontsize=12,fontweight='bold',
       ha='center',va='center')
plt.tight_layout()","# Batch Size
batch_size = 512
# Number of Epochs
num_epochs = 700
# RNN Layers
rnn_layer_size = 2
# RNN Size
rnn_size = 256
# Embedding Dimension Size
# Using 300 as it is commonly used in Google's news word vectors and the GloVe vectors
embed_dim = 300
# Sequence Length
seq_length = 10
# Learning Rate
learning_rate = 0.001
# Dropout
dropout = 0.6

# Show stats for every n number of batches
show_every_n_batches = 100
# Save progress for every n number of epochs
save_every_n_epochs = 100

run_id = '0007'

# Define saving directories
save_dir = './checkpoints/save'
logs_dir = './logs/'",,"Matplotlib->Common customization: colors, labels, ticks, tickmarks, limits and annotations->Object Oriented Interface->Example: Creating a figure showing the shape of the power functions.",Transformations Tutorial,Mlnd-Vidyabagish->Vidyabagish->Vidyabagish Neural Network Training
348989,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_rel, ttest_ind, kruskal, wilcoxon, mannwhitneyu","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_rel, ttest_ind, kruskal, wilcoxon, mannwhitneyu",df_reduced.head(2),,5->What statistical test to use?,5->What statistical test to use?,Analytic Report Project->Assignment 2: Analytic Report project->Data Preparation
265504,combine[0].pipe(lambda x:x[x.Fare.isnull()]),"n = 100
k = 5
p = 0.01

binomial_proba = combin(n, k) * p**k * (1 - p)**(n - k)
binomial_proba","#Split the dataset into 5 datasets for a 5-fold cross-validation
split_full_df = np.split(full_df, 5, axis=0)
crossval_test1 = split_full_df[0]
crossval_test2 = split_full_df[1]
crossval_test3 = split_full_df[2]
crossval_test4 = split_full_df[3]
crossval_test5 = split_full_df[4]
crossval_train1 = pd.concat([split_full_df[1],split_full_df[2],split_full_df[3],split_full_df[4]], axis=0, join='outer', join_axes=None, ignore_index=False,
          keys=None, levels=None, names=None, verify_integrity=False,
          copy=True)
crossval_train2 = pd.concat([split_full_df[0],split_full_df[2],split_full_df[3],split_full_df[4]], axis=0, join='outer', join_axes=None, ignore_index=False,
          keys=None, levels=None, names=None, verify_integrity=False,
          copy=True)
crossval_train3 = pd.concat([split_full_df[1],split_full_df[0],split_full_df[3],split_full_df[4]], axis=0, join='outer', join_axes=None, ignore_index=False,
          keys=None, levels=None, names=None, verify_integrity=False,
          copy=True)
crossval_train4 = pd.concat([split_full_df[1],split_full_df[2],split_full_df[0],split_full_df[4]], axis=0, join='outer', join_axes=None, ignore_index=False,
          keys=None, levels=None, names=None, verify_integrity=False,
          copy=True)
crossval_train5 = pd.concat([split_full_df[1],split_full_df[2],split_full_df[3],split_full_df[0]], axis=0, join='outer', join_axes=None, ignore_index=False,
          keys=None, levels=None, names=None, verify_integrity=False,
          copy=True)",,Titanic Notebook->Model->Load data->Interaction of Pclass and Age,Pmf 1->Probability Mass Functions->Poisson Distribution,"17205389 Comp47350 Homework2->(4). Improving Predictive Models.->Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset)."
319590,"p = X_r.dot(Theta_r.T)
my_predictions = p[:,943] + Ymean","def predict(X, theta, binary=True):
    p = sigmoid(theta.dot(X))
    if binary:
        return 1 if p > 0.5 else 0
    else:
        return p
    
test = np.array([1, 45, 85])
predict(test, theta, binary=False), (predict(test, theta))","from sklearn.metrics import roc_curve
fpr, tpr, th = roc_curve(y_test == ypred, score)",,Recommender Systems->Recommendation for you,Logistic Regression->Logistic Regression->Initialize Parameters->Predicting new values,Roc Example->ROC->ROC with scikit-learn
48359,"import random, timeit

def direct_pins(N, L, sigma):
    configuration = []
    while len(configuration) < N:
        configuration.append(random.uniform(sigma, L - sigma))
        for k in range(len(configuration) - 1): 
            if abs(configuration[-1] - configuration[k]) < 2.0 * sigma:
                configuration = []
                break
    return configuration

def direct_pins_improved(N,L,sigma):
    while True:
        configuration = [random.uniform(sigma, L - sigma) for k in range(N)]
        configuration.sort()
        min_dist = min(configuration[k + 1] - configuration[k] for k in range(N - 1)) 
        if min_dist > 2.0 * sigma:
            return configuration

# Larger N and L, so the faster algorithm is more prominent
# Timing a random algorithm is problematic
# The improved algorithm is better, when there is large N
# and low density so only few rejections and thus repetitions will be produced
N = 400
L = 10000.0
sigma = 0.1
%timeit direct_pins(N,L,sigma)
%timeit direct_pins_improved(N,L,sigma)","def direct_disks_box(N, sigma):
    overlap = True
    while overlap == True:
        L = [(random.uniform(sigma, 1.0 - sigma), random.uniform(sigma, 1.0 - sigma))]
        for k in range(1, N):
            a = (random.uniform(sigma, 1.0 - sigma), random.uniform(sigma, 1.0 - sigma))
            min_dist_sq = min(((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2) for b in L) 
            if min_dist_sq < 4.0 * sigma ** 2: 
                overlap = True
                break
            else:
                overlap = False
                L.append(a)
    return L

N = 4
sigma = 0.1197
n_runs = 1000000
histo_data = []
for run in range(n_runs):
    pos = direct_disks_box(N, sigma)
    for k in range(N): histo_data.append(pos[k][0])",tuple_1,,"Week3->Entropic interactions, phase transitions",Hw2,Python-Epiphanies-3-More-Namespace-Operations->3 More Namespace Operations->3.4 Augmented Assignment Statements
462462,"S1.decision_path.filepath, S1.decision_path.filename, S1.decision_path.value, S1.decision_path.name",'pol' not in s1,"data1 = pd.read_excel('F:\Machine Learning\Projects\Scalend\sample_data_test.xlsx',sep='\t')
data = pd.DataFrame(data1)
data.columns = ['cid_no','channel_id','transaction_id','transaction_datetime','transaction_amount','product','service_category','transaction_location','transaction_type','customer_type','sex','date_of_birth','educational_status','income','minor','marital_status','dependents']
data",,"Pysumma Demo Example 1 Fig 7->pySUMMA Example 1 (Testcase Figure 7, SUMMA 2nd Paper(2015))->(3) running summa model.->(2-2) read and understand FileManager text file","Lecture 3->Loading and saving files->Substrings (1)
Lecture 3->Containers - key based->Web APIs (2)","Scalend - Bank Data Analysis, Clustering And Classification->Scalend Bank Data Analysis->This data includes 17 columns out of which 7 features have more have 70% of missing data. I have used several methods to predict the missing data and have replaced it with the predicted classes."
46416,"## Use this and additional cells to answer Question 5. ##

plt.hist(collect_trip_times(filenames['in_file']), range = (0,75))
labels = ['5','10','15','20','25','30','35','40','45','50','55,''60','65','70','75']
plt.title('Distribution of Trip Durations')
plt.xlabel('Duration (m)')
plt.xticks(range(0,80,5))
plt.show()","## Use this and additional cells to collect all of the trip times as a list ##
## and then use pyplot functions to generate a histogram of trip times.     ##

def collect_trip_times(filename):
    """"""
    This function reads in a file with trip data and collects
    the duration of each trip into an array
    """"""
    with open(filename, 'r') as f_in:
        # set up csv reader object
        reader = csv.DictReader(f_in)
        
        # initialize trip variables
        trip_times = []

        # count total trips and convert duration to numeric
        for row in reader:
            trip_times.append(float(row['duration']))
            
        return trip_times
        


city_info = {'Washington': {'in_file': './data/Washington-2016-Summary.csv'}}

for city, filenames in city_info.items():
    collect_trip_times(filenames['in_file'])
    
plt.hist(collect_trip_times(filenames['in_file']))
plt.title('Distribution of Trip Durations')
plt.xlabel('Duration (m)')
plt.show()",stokes.fn_bodyforce = buoyancyFn,,Bike Share Analysis,Bike Share Analysis,051-Rayleigh Benard Convection-Pt2->Functions
291965,df['a'],df['returns'].iloc[0:4],"merged['bedroomcnt'].loc[merged['bedroomcnt']>7] = 7 # Set maximum number of bedrooms to seven
plt.figure(figsize=(12,8))
sns.violinplot(x='bedroomcnt', y='logerror', data=merged)
plt.xlabel('Bedroom count', fontsize=12)
plt.ylabel('Log Error', fontsize=12)
plt.show()",,"Pandas
Insights In Python->Insights in Python->Good coding practices <a name=""good-code""></a>->Debugging",Time Series Modelling With Python Master->1.0 Introduction to Time Series (Chapter 1)->Scale Mixture of Normal Distributions,Predicting Home Prices For Zillow->Predicting Zillow Prices->Step 8: Build and test multiple models->Correlation Analysis
214338,_ = df.hist(column='temperature'),"df.hist(column='Age', by='Sex')",cx.view(mnist.targets[0]),,Sliderule Dsi Inferential Statistics Exercise 1->What is the True Normal Human Body Temperature?->Q1. Is the distribution of body temperatures normal?,3 4 Visualisation Pandas->Recap: Filling null values->Feature Cabin: null values,Mnist->The MNIST Dataset
149370,"def nbc_find_features(word, pos):
    return {'first_letter': word[0], 'pos': pos, 'word':word}

def nbc_train_classifier(trainset, testset):
    train_feats = [(nbc_find_features(word, pos_tag), ne_tag) for (word, pos_tag, ne_tag) in trainset]
    test_feats = [(nbc_find_features(word, pos_tag), ne_tag) for (word, pos_tag, ne_tag) in testset]
    classifier = nltk.NaiveBayesClassifier.train(train_feats)
    return train_feats, test_feats, classifier

nbc_train_featuresets, nbc_test_featuresets, nbc_classifier = nbc_train_classifier(nbc_train, nbc_test)","def nbc_evaluate(classifier, testset, test_featureset):   
    num_correct = 0
    pred_list = []
    ground_truths = []
    for i in range(len(testset)):
        prediction = classifier.classify(nbc_find_features(testset[i][0], testset[i][1]))
        pred_list.append(prediction)
        ground_truths.append(testset[i][2])
        if prediction == testset[i][2]:
            num_correct = num_correct + 1
    
    acc = (nltk.classify.accuracy(classifier, test_featureset))*100
    print('Predicted', num_correct, 'out of', len(testset), 'giving an accuracy of {}%'.format(round(acc, 2)))
    print('\n')
    print(classification_report(ground_truths, pred_list))
    
nbc_evaluate(nbc_classifier, nbc_test, nbc_test_featuresets)","def convert_place(x):
    x['Place'] = x['Place'] == ""1""    # Converts any value in the column that is = 1 (First Place) into the boolean ""True"", anything else (including strings or other numbers) to ""False""
    x['Place'] = x['Place'].astype(int)",,Ner->Training,Ner->Evaluation,Lifting Project->3. Data Cleaning & Featurization.->5. Machine Learning Algorithms->a. Decision Tree
35396,"x_dframe = dframe.drop([""Gender"", ""Purchased"", ""User ID""], axis=1)
x_dframe.head()","x_data_frame = dframe[dframe.columns.tolist()[:-1]]
x_data_frame.head()","import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import plotly
import plotly.plotly as py
import plotly.offline as pyoff
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot
from IPython.display import display, HTML",,Logistic Regression,Multivariate Linear Regression->Multivariate Linear Regression->Step 2: Reading CSV dataset to pandas Dataframe,Analysis->Importing libraries and settings
331356,"plt.figure()
plt.errorbar(data_x,data_y,xerr=data_xerr,yerr=data_yerr,fmt='*')
plt.xlabel(""r (kpc)"")
plt.ylabel('V (km/s)')
plt.title(""The measured rotational speed of the interstellar medium as a fucntion of the galactocentric radius (reference 1)"")
plt.show()","x = [2,3,4,5,6,7,8,9,10]
y = avg_means[1,:]  #4 iterations
yerr = avg_stds[1,:] # 4 iterations

#plt.figure()
plt.errorbar(x, y, yerr, fmt='o')
plt.xlabel('Number of Training Points Per Gaussian Cluster')
plt.ylabel('Mean Accuracy +/- Standard Deviation')
plt.title(""Accuracy as a function of the number of training points per cluster given 4 iterations"")
plt.show()","ct.loadMatrixRandom('pixelMatrix_combined.dbm', samples=100)",,Model->Model->Loading the and visualizing the data,Ml Perceptron Learning Alg And Least Squares->CompSci 273A - HW 1->Problem 1 - Perceptron Learning->Part C:,Circle Learn Fast->Comparsion of Fit and CNN
290344,"sym.limit(1 / x, x, sym.oo)","expr = sym.Limit((sym.cos(x) - 1)/x, x, 0)
expr","all_columns = ['dayofweek', 'temp', 'humidity', 'year', 'month', 'hour', 'weather']
one_hot_columns = ['dayofweek', 'year', 'month', 'hour']
X = train[all_columns]
X = pd.get_dummies(X, columns = one_hot_columns)

y_reg = train['registered']
y_cas = train['casual']",,Sym Py->Sympy: Symbolic Mathematics in Python->Calculus->Integration,Sym Py->Sympy: Symbolic Mathematics in Python->Basic Operations,Bike Sharing->Prepare Data for ML
401455,"from IPython.display import HTML, Image, display
HTML('''<script>
code_show=false; 
function code_toggle() {
 if (code_show){
 $('div.input').hide();
 } else {
 $('div.input').show();
 }
 code_show = !code_show
} 
$( document ).ready(code_toggle);
</script>
<form action=""javascript:code_toggle()""><input type=""submit"" value=""Show/Hide Code.""></form>''')",imag_unit = (0+1j),"fig, axes = plt.subplots(2,1, figsize = (16,12))
plt.subplots_adjust(bottom = .1)
# plot katz centrality
axes[0].set_title(""Hub/Authority Centrality"", size = 16)
axes[0].bar([x + .2 for x in range(0,num_nodes)], [x[1] for x in sorted_authority_score], color = 'indianred', width = .4)
axes[0].bar([x + .6 for x in range(0,num_nodes)], [dict_hub[x[0]] for x in sorted_authority_score], color = 'brown', width = .4)
axes[0].set_ylabel(""Hub/Authority Centrality Values"", size = 16)
axes[0].set_xticks([x + .8 for x in range(0,num_nodes)])
_ = axes[0].set_xticklabels([x[0] for x in sorted_authority_score], rotation = 90)
axes[0].legend([""authority score"", ""hub score""])
# compare with other measures
fade = .3
node_ordering = sorted_degree_centrality #sorted_pagerank
axes[1].plot(range(0,num_nodes), [dict_authority[x[0]] for x in node_ordering], color = 'indianred')
axes[1].plot(range(0,num_nodes), [dict_hub[x[0]] for x in node_ordering], color = 'brown')
axes[1].plot(range(0,num_nodes), [dict_pagerank[x[0]] for x in node_ordering], color = 'purple', alpha = fade)
axes[1].plot(range(0,num_nodes), [dict_katz_centrality[x[0]] for x in node_ordering], color = 'limegreen', alpha = fade)
axes[1].plot(range(0,num_nodes), [dict_eigenvector_centrality[x[0]] for x in node_ordering], color = 'royalblue', alpha = fade)
axes[1].plot(range(0,num_nodes), [scaled_degree[x[0]] for x in node_ordering], color = 'navajowhite', alpha = fade)
axes[1].plot(range(0,num_nodes), [dict_authority[x[0]] for x in node_ordering], color = 'indianred', marker = 'o')
axes[1].plot(range(0,num_nodes), [dict_hub[x[0]] for x in node_ordering], color = 'brown', marker = 'o')
axes[1].plot(range(0,num_nodes), [dict_pagerank[x[0]] for x in node_ordering], color = 'purple', marker = 'o', alpha = fade)
axes[1].plot(range(0,num_nodes), [dict_katz_centrality[x[0]] for x in node_ordering], color = 'limegreen', marker = 'o', alpha = fade)
axes[1].plot(range(0,num_nodes), [dict_eigenvector_centrality[x[0]] for x in node_ordering], color = 'royalblue', marker = 'o', alpha = fade)
axes[1].plot(range(0,num_nodes), [scaled_degree[x[0]] for x in node_ordering], color = 'navajowhite', marker = 'o', alpha = fade)
axes[1].set_ylabel(""Hub/Authority vs PageRank vs Katz vs Eigenvector vs Degree"", size = 12)
axes[1].set_xticks([x for x in range(0,num_nodes)])
axes[1].grid()
axes[1].set_xlim(-0.5,38.5)
axes[1].set_ylim(0,1.05)
axes[1].legend([""authority score"", ""hub score"", 
                ""pagerank centrality"", ""katz centrality"", ""eigenvector centrality"", ""degree centrality""])
_ = axes[1].set_xticklabels([x[0] for x in node_ordering], rotation = 90)",,I Python Notebook Demo->IPython Notebook Demo,Numbers And Variables->Complex,Centrality-Metrics->Centrality in networks->Hubs and Authorities
367613,"for nassemble in [ 1 , 5, 10, 100, 200 ]:
    batch_xs, batch_ys = mnist.train.next_batch(nassemble)
    m = sess.run(Hess,feed_dict={tf_x: batch_xs, tr_y: batch_ys})
    plt.figure()
    plt.spy(m)
    plt.show()
    #print ""It's rank is "", np.linalg.matrix_rank(m)","for nassemble in [ 1 , 5, 10, 100, 200 ]:
    batch_xs, batch_ys = mnist.train.next_batch(nassemble)
    m = sess.run(Hess,feed_dict={tf_x: batch_xs, tr_y: batch_ys})
    plt.figure()
    plt.spy(m)
    plt.show()
    #print ""It's rank is "", np.linalg.matrix_rank(m)","word_sequences = map(lambda s: clean_text(s).split(), training_data[""text""].values)
sequence_lengths = map(len, word_sequences)

print(f""Average text length in training dataset: {np.mean(list(sequence_lengths))} words"")",,Hessian Mnist->MNIST with Newton's Method->The Model,Hessian Mnist->MNIST with Newton's Method->The Model,Notebook->Naive Bayes for Text Classification->Pre-processing
353605,"predictions = sess.run(output_layer, feed_dict={X_: X_test})
print(100*np.sum(np.argmax(predictions,1)==np.argmax(y_test,1))/y_test.shape[0])","print('test avg loss = {0:.2f}'.format(sess.run(avg_loss, feed_dict={x:mnist.test.images, y:mnist.test.labels})))

y_test_pred = sess.run(fc_2, feed_dict={x:mnist.test.images, y:mnist.test.labels})
ncorrect = np.sum(np.argmax(y_test_pred, axis=1) == np.argmax(mnist.test.labels, axis=1))
print('test accuracy = {0:.2f}%'.format(100*(float(ncorrect) / float(mnist.test.labels.shape[0]))))","print(4 < 5)
print(4 < 4)
print(4 < 3)",,Tensor Flow Multi-Classification Neural Network->Testing,Matthew Epland-02C Cnn Assignment->TensorFlow Assignment: Convolutional Neural Network (CNN)->Evaluate model,Comparison Operators->Comparison Operators->lt / lte
17672,"import math
print(math.sqrt(4))  # square root
print(math.log(4))  # natural log 
print(math.exp(4))  # exponentiate (e^x)
print(math.factorial(4))  # factorial","import math
x = 4

print('exp    = ', math.exp(x))    # exponent of x (e**x)
print('log    = ',math.log(x))     # natural logarithm (base=e) of x
print('log2   = ',math.log(x,2))   # logarithm of x with base 2
print('log10  = ',math.log10(x))   # logarithm of x with base 10, equivalent to math.log(x,10)

print('sqrt   = ',math.sqrt(x))    # square root

print('cos    = ',math.cos(x))     # cosine of x (x is in radians)
print('sin    = ',math.sin(x))     # sine
print('tan    = ',math.tan(x))     # tangent
print('arccos = ',math.acos(.5))   # arc cosine (in radians)
print('arcsin = ',math.asin(.5))   # arc sine
print('arctan = ',math.atan(.5))   # arc tangent
# arc-trigonometric functions only accept values in [-1,1]

print('deg    = ',math.degrees(x)) # converts x from radians to degrees
print('rad    = ',math.radians(x)) # converts x from degrees to radians

print('e      = ',math.e)          # mathematical constant e = 2.718281...
print('pi     = ',math.pi)         # mathematical constant pi = 3.141592...","from nipype.caching import Memory
mem = Memory(base_dir='.')",,Intro To Python->Introduction to Python->Mathematical Operations,02 Basic Numerical Operations->Numerical Operations in Python->Python's mathematical functions,Advanced Interfaces Caching->Interface caching->A big picture view: using the [`Memory`](http://nipype.readthedocs.io/en/latest/api/generated/nipype.caching.memory.html#memory) object
222401,"already_rated, predictions = recommend_books(preds, 20, books, ratings, 20)","from sklearn.metrics import roc_curve, auc
y_pred = forest.predict(X_test)
false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)
roc_auc = auc(false_positive_rate, true_positive_rate)
roc_auc",train_corpus = list(read_corpus(train_data.comment_text)),,Machine Learning->Machine Learning->We will try to recommend 20 books for user_id 20:,Heart Disease Prediction->Heart disease prediction->Calculating roc_auc,Doc2Vec1->Kaggle competition: doc2vec approach
3263,"baseline_model_results_daily = eemeter.fit_caltrack_usage_per_day_model(
    design_matrix_daily,
)","reporting_design_matrix_billing = eemeter.create_caltrack_billing_design_matrix(
    reporting_meter_data_billing, temperature_data_billing,
)","plt.figure()
for x, filename in enumerate(os.listdir(""test_images/"")):
    img = cv2.imread('test_images/' + filename)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)
    
    plt.subplot(2, 3, x + 1)
    plt.imshow(img)
    plt.axis('off')",,Tutorial->Importing the eemeter library->Running the CalTRACK Billing/Daily Methods,Tutorial->Importing the eemeter library->Annual weather-normalized modeled savings,Traffic Sign Classifier->Test the Model on New Images:
370858,k[k.size-1] # yay,K,"net = caffe.Net('deploy_test_bn_net.prototxt', caffe.TEST)
net.blobs['data'].data[...] = np.ones(shape=(1,1,3,3))",,Week4->Creating a Probability Density Function (PDF),"Scalar Field Cokriging Minimal Example->Minimal working example for cokringing of a scalar field for geological modeling->Adding universal kriging terms->Set up K matrix
Dahlin Controller
14 Bicycle->Introduction",Caffe Pb2 Gen Layers->Load model and bind data
171153,"df_x = df_x.drop(['California', 'State'], axis =1)
df_x[:5]","y = df.iloc[:, 0:1]
X = df.iloc[:,:]
X.drop('price', axis = 1, inplace = True)",?? px.hdf_utils.getDataSet,,Multiple Linear Regression->Multiple Linear Regression->Variables,Ml Exam Question1->Preprocessing->Defining 'X' and 'y',Scan 74 C Kpfm-Checkpoint->Computes the Low Rank Representation of the PE Loops->Display averaged cKPFM data
234014,"influence = m.get_influence()
#c is the distance and p is p-value
(c, p) = influence.cooks_distance
_ = plt.stem(np.arange(len(c)), c, markerfmt="","")
_ = plt.title('Cooks distance plot for all observations')
_ = plt.xlabel('Observation number')
_ = plt.ylabel('Cooks distance')","influence = m.get_influence()
(c, p) = influence.cooks_distance
plt.subplots(figsize=(15,5))
plt.scatter(np.linspace(1,len(c), len(c)),c, s=5)
plt.ylim([0,0.05])
plt.ylabel(""Cook's Distance"")
plt.xlabel('Index')
plt.title(""Cook's Distance"")
plt.show()","a = tf.constant(1.0)
b = tf.constant(2)

try:
    c = a + b
except TypeError:
    print(""Whoops"")
    
with tf.Session() as sess:
    sess.run(c)",,Mini Project Linear Regression->Answer to Part 4 Checkup Exercise->Answer Continued:,Mini Project Linear Regression->your turn->Part 4: Comparing Models->A Q-Q plot,Basics->Variables and Placeholders->A note on datatypes
52426,"# If we want to do some real linear algebra, we can skip elementwise 
# syntax by declaring an array as a numpy matrix :
B = np.matrix(A)

print(B * B, ""\n"") # now MATRIX multiplication
print(A * A) # still ELEMENTWISE multiplication","a = [5,6,5] #Capacities of depots
b = [7,9] # Demands of warehouses
c = np.matrix(([1,2],[4,5],[6,3])) #Cost matrix
print('Cost Matrix:')
print(c)",good_reads.items(),,2->The Scientific Python Stack->Numpy->Why do we need this `np.` prefix everywhere ?,"Basic Lp->Transportation Problem in Pulp->Create model and define parameters
Basic Lp->Transportation Problem in Pulp->Solve and view result","Chapter 1 - Getting Started-Checkpoint->Chapter 1: Getting started->Conditions->if, elif and else"
170385,"#relative paths
import os, sys, inspect

currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parentdir  = os.path.dirname(currentdir)

rootdir = parentdir # is it?
dwhdir = os.path.join(rootdir, 'dwh')

channelChangesFilename = os.path.join(dwhdir, 'channel_changes.csv')

print('In :', channelChangesFilename)

import pandas as pd

df = pd.read_csv(channelChangesFilename)
print (df.head())","#relative paths
import os, sys, inspect

currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parentdir  = os.path.dirname(currentdir)

rootdir = parentdir # is it?
dwhdir = os.path.join(rootdir, 'dwh')
activityFilename = os.path.join(dwhdir, 'activity.log')

print(activityFilename)","fig, ax = plt.subplots(2, sharex=True, figsize=(10, 8))
ax[0].plot(x, y, 'b')
ax[0].plot(x, s, 'm--')

ax[1].plot(x, d, 'g');",,3->Let's Try it out,3->Cheat Sheet,02 Nn->Algorithmic Trading->Neural Network for Classification->Sigmoid Function
404262,"# find the distance betIen each cluster centroid
def compute_distances(X, centroids):
    """"""
    compute distances for each centroid. output will be a matrix dimensions (num data points, num centroids)
    
    """"""
    distances = np.zeros((len(X), len(centroids))) # build empty (10, 3) numpy array
    for idx, centroid in enumerate(centroids):
        distances[:, idx] = euclidean_distance(X, centroid)
    return distances","flights_by_airline_month = flights.groupby(['AIRLINE', 'MONTH'])
summary_distanc_taxi_in = flights_by_airline_month.aggregate(
        {'DISTANCE': [np.min, np.max], 
         'TAXI_IN': np.mean})","dos=D.DoS(fermi_level=6.5)
for rVal in [9.0,10.0]: #C6H6:
    nalpha = C6H6[rVal]['eigenproblems'].keys()
    nalpha.sort()
    nvirt = nalpha[-1]   
    eng = HaeV*np.sqrt(C6H6[rVal]['eigenproblems'][nvirt]['eigenvalues'][C6H6_ind_at[rVal]])
    dos.append(energies=np.array([eng]),label='_rVal_'+str(rVal))
dos.plot(sigma=0.001*HaeV)",,K-Means->Cluster similar movies with k-means->Compute distances betIen X and each centroid,17 Python Pandas Group By->Activity:,Lr Analysis->LR Analysis->Analysis of $N_2$
428357,cosine_sim[0],sim.fieldbag_file,"# Only keep rows in which the body weight is less than 200 kg.
mammals = mammals.loc[mammals.loc[:,'body']< 200,:]
mammals.shape",,"Recommendation Engine->Donations->Content Based Recommender->Cosine Similarity
Movies Recsys->Recommendation Systems for movies->Content Based Recommendation model->1. Description Based Recommendation->Cosine Similarity
Movie-Recommender-Systems->Movies Recommender System->Collaborative Filtering->Movie Description Based Recommender->Cosine Similarity",Using Jcmpython - The Mie2D-Project->Preparations->Running a single simulation,Bias-And-Variance->Go back to Step 1 and try adding new variables and transformations.
436475,"plt.plot(loss_slow_array)
plt.ylabel('Seconds')
plt.xlabel('Loss')
plt.show()","# A useful debugging strategy is to plot the loss as a function of
# iteration number:
plt.plot(loss_hist)
plt.xlabel('Iteration number')
plt.ylabel('Loss value')
plt.show()",df_nonull.describe(),,Linear Regression-Checkpoint->Running Naive LP SVRG->Plotting Error,Svm->Stochastic Gradient Descent,Project2-Starter-Sami->Question 7. What do this plots show?
252115,"long_flights1 = flights.filter(""distance > 1000"")

# Filter flights with a boolean column
long_flights2 = flights.filter(flights.distance > 1000)

print(long_flights1.show())
print(long_flights2.show())","# Filter flights with a SQL string
long_flights1 = flights.filter('distance > 1000')

# Filter flights with a boolean column
long_flights2 = flights.filter(flights.distance > 1000)

# Examine the data to check they're equal
print(long_flights1.show())
print(long_flights2.show())","fig, ax = plt.subplots(nrows=2, ncols=1)
df_noa.groupby(""Mission Date"").count()[""Mission ID""].plot(ax=ax[0])
ax[0].set_title(""Allied Attacks"", size=14)
df_noa.groupby(""Mission Date"").count()[""Mission ID""].cumsum().plot(ax=ax[1])
ax[1].set_title(""Cumulated Allied Attacks"", size=14)
plt.tight_layout()",,Datacamp-Spark->DataCamp Spark->Manipulating Data->Filtering Data,"Introduction-To-Py Spark->Introduction to PySpark->Filtering Data
Intro To Py Spark->2. Manipulating data->2.1 Creating columns",03E - Exploratory Data Analysis Ww2 - Eda-Number Of Attacks->Research questions
278775,"def parse_csv_row(csv_row):
    
    columns = tf.decode_csv(csv_row, record_defaults=HEADER_DEFAULTS)
    features = dict(zip(HEADER, columns))
    
    for column in UNUSED_FEATURE_NAMES:
        features.pop(column)
    
    target = features.pop(TARGET_NAME)

    return features, target

def process_features(features):

    capital_indicator = features['capital_gain'] > features['capital_loss']
    features['capital_indicator'] = tf.cast(capital_indicator, dtype=tf.int32)
    
    return features","def parse_csv_row(csv_row):
    
    columns = tf.decode_csv(csv_row, record_defaults=HEADER_DEFAULTS)
    features = dict(zip(HEADER, columns))
    
    for column in UNUSED_FEATURE_NAMES:
        features.pop(column)
    
    target = features.pop(TARGET_NAME)

    return features, target


def process_features(features):
    
    features['CRIM'] = tf.log(features['CRIM']+0.01)
    features['B'] = tf.clip_by_value(features['B'], clip_value_min=300, clip_value_max=500)
    
    return features","result=get_bestmlp(X_train.reshape(-1, 1), X_test.reshape(-1, 1), y_train, y_test,kf,[256,""relu"",""lbfgs""])
result",,06->Define Data Input Function->a. Parsing and preprocessing logic,"08->Define Data Input Functions->a. Parsing and preprocessing logic
09 - Tf Regression Example - Housing Price Estimation + Keras->Housing Price Estimation + Keras->Define Data Input Functions->a. Parsing and preprocessing logic",Capstone Analytics->Benchmark model->Use the national vector->Polynomial regressor
246923,"from sklearn.neighbors import NearestNeighbors

nn = NearestNeighbors(n_neighbors=len(X)).fit(X)
distances, indices = nn.kneighbors(X)","# Now create an instance of the classifier
NN = NearestNeighbor()

# Assign the training data
NN.train(Xtr_rows, Ytr)","from scipy.optimize import fsolve
reached_max = fsolve(fbt2-100000, x0=800)/(7*24)
print(""100,000 hits/hour expected at week %f"" % reached_max[0])",,7->Find epsilon using k-distance,Css231 1 Knn,Numpy Scipy 20180220->Stepping bakc to go forward - another look at  our data->Answering our initial question
192118,"### Preprocess the data here. It is required to normalize the data. Other preprocessing steps could include 
### converting to grayscale, etc.
### Feel free to use as many code cells as needed.


#Lecun's paper describes better accuracy with grayscaled data, so our experiments will be with both colored and grayscaled
#although initial experiments are with colored images.



#Experiments with data:
#1. colored
#Colored data
#color='True'
#2. grayscale
color=False

#Exp with colored pictures and pictures not normalized


#norm='False'
#3. non normalized

norm=True
#4. normalized


print(""Set color to: {} and Nomralized to: {}"".format(color,norm))","fig, ax = plt.subplots()
ax.set_title('Botched and Unbotched Reweighting of Gimmicked Gamma')
ax.set_xlim(-0.006, 0.006)
offset = 0.00006
label = 'botched (norm={0:.2f})'.format(botched_gimmick.norm())
botched_gimmick.vlines(ax, offset, label, sns.color_palette()[0])
label = 'unbotched (norm={0:.2f})'.format(gimmick.norm())
gimmick.vlines(ax, -offset, label, sns.color_palette()[1])
ax.legend();","# Display Figure
from IPython.display import Image
Image(filename='/Users/paha3326/main/Teaching/ASTR3720/hayne/notes/ideal-gas-law.jpeg', width=400)",,Traffic Sign Classifier->Experiments,How Basener And Sanford Got Their Results->How Basener and Sanford Got Their Results->Appendix: The botched process addressed in Section 5.4,Lecture00 Gases->Gases: Fundamentals->Number vs. number density vs. mass density
186857,"# pre-processing helper methods.

import cv2
from skimage import exposure

def grayscale(X_data):
    return np.mean(X_data, axis=3)

def normalize(X_data):
    X_n = np.divide(np.subtract(X_data,128),128)
    return X_n

def histEqualize(X_data):
#     return cv2.equalizeHist(X_data)
    return exposure.equalize_adapthist(X_data)

def reshape(X_data):
    X_data = np.reshape(X_data, [-1, X_data.shape[1], X_data.shape[2], 1])
    return X_data","def feature_normalize(X):
    X_norm = X
    mu = np.zeros((1, X.shape[1]))
    sigma = np.zeros((1, X.shape[1]))
    mu = np.mean(X, axis=0)
    sigma = np.std(X, axis=0)
    X_norm = np.divide(np.subtract(X, mu), sigma)

    return X_norm, mu, sigma","# Get a layer reference from the DEM image service item
# Display an exported image of the DEM layer with the bounding box set to Ventura County.
lyr_dem.extent = ventura_county['extent']
lyr_dem",,"Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Pre-process the Data Set (normalization, grayscale, etc.)",Ex1->ML_Ng Homework 1,Calculating Cost Surfaces Using Weighted Overlay Analysis->Access the data for analysis->DEM layer->Set the extent to match the area of interest
339560,"#Loading final cleaned job dataset
jobs = pd.read_csv('jobs_cleaned.csv')
jobs.rename(columns={'Unnamed: 0':'jobId'},inplace=True)","ethosposts = pd.read_csv(get_job_filepath(ethospagejob, 'posts'),
                         parse_dates=['created_time', 'updated_time'], skipfooter=1, engine=""python"")","print(lasso_reg.intercept_, lasso_reg.coef_)
plot_learning_curve(lasso_reg, X_poly, y)",,Technical Report->Job Recommendation for Stack Overflow users->Data Munging and Cleaning->2. Job Description Data,Fb Scraper->8. Collect output,Linear-Regression->Regularized Linear Models->Lasso Regression
154086,"seg_image_files = glob(data_dir + '/select_slid_window/*.png')
fig, axs = plt.subplots(2,3,figsize = (10,10))
idx = 0
for row in axs:
    for ax in row:
        ax.imshow(io.imread(seg_image_files[idx]))
        ax.axis('off')
        idx += 1","# Create a list of all images
root_path = os.path.expanduser('~/ml_basics/images/')
all_images = glob.glob(root_path + '/*.jpg')
# To avoid memory errors we will only use a subset of the images
all_images = random.sample(all_images, 500)

# Plot a few images
i = 0
fig = plt.figure(figsize=(10, 10))
for img_path in all_images[:4]:
    img_arr = io.imread(img_path)
    i += 1
    ax = fig.add_subplot(2, 2, i)
    ax.imshow(img_arr)
    ax.set_title(f""Image example {i}"")","q_3.hint()
q_3.solution()",,Demo->Demo Segementation via Sliding Window Method,Image Preprocessing->How to preprocess an image dataset for machine learning,Partial Dependence Plots->Question 3
427592,cat_data=convert_categorical(transform_data),"def convert_categorical(column,data):
#     for each in columns:
    column_cat = pd.Categorical(data[column])
    codes = column_cat.codes
    return codes
    
income['education'] = convert_categorical('education',income)
income['marital_status'] = convert_categorical('marital_status',income)
income['occupation'] = convert_categorical('occupation',income)
income['relationship'] = convert_categorical('relationship',income)
income['race'] = convert_categorical('race',income)
income['sex'] = convert_categorical('sex',income)
income['native_country'] = convert_categorical('native_country',income)
income['high_income'] = convert_categorical('high_income',income)","# importing packages
import numpy
import random
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# setting ageNetWorthData
def ageNetWorthData():

    random.seed(42)
    numpy.random.seed(42)

    ages = []
    for ii in range(100):
        ages.append( random.randint(20,65))
    net_worths = [ii * 6.25 + numpy.random.normal(scale=40.) for ii in ages]

    ages       = numpy.reshape( numpy.array(ages), (len(ages), 1))
    net_worths = numpy.reshape( numpy.array(net_worths), (len(net_worths), 1))

    from sklearn.cross_validation import train_test_split
    ages_train, ages_test, net_worths_train, net_worths_test = train_test_split(ages, net_worths)

    return ages_train, ages_test, net_worths_train, net_worths_test

# using ageNetWorthData
ages_train, ages_test, net_worths_train, net_worths_test = ageNetWorthData()",,House Sales,"Applying Decision Trees->Applying Decision Trees
Introduction To Random Forests->Introduction to Random Forests",Regression Basic-Checkpoint->1. Information Generation
387858,"#range of neighborhood size to consider
range_eps=[0.05, 0.1, 0.15, 0.25, 0.5, 0.75, 1]

#best score holder for k clusters
best_cluster_score=0

#variable to store the best value for eps
best_k=0

#loop over n_size to find the best 
for n_eps in range_eps:
    
    # Initialize the clusterer with eps value, 
    clusterer = DBSCAN(eps=n_eps)
    
    #save fitted and predicted labels 
    cluster_labels = clusterer.fit_predict(pca_features)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed clusters
    
    #calculate number of clusters by counter on key values
    clusters=len(Counter(cluster_labels).keys())
    
    #calculate silhouette score per sample only if more than one label/cluster
    if clusters > 2:
        # Create a subplot with 1 row and 2 columns
        fig, (ax1) = plt.subplots(1, 1)
        fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
      

        silhouette_avg = silhouette_score(pca_features, cluster_labels)
        print(""For n_eps ="", n_eps,
          ""The average silhouette_score is :"", silhouette_avg)
    
    #capture the best value for the silhouette score and cluster number
        if silhouette_avg > best_cluster_score:
            best_cluster_score = silhouette_avg
            best_k = n_eps
   
    # Compute the silhouette scores for each sample
        sample_silhouette_values = silhouette_samples(pca_features, cluster_labels)
        y_lower = 10
    #calculate number of clusters
        clusters=len(Counter(cluster_labels).keys())
    
        for i in range(-1,clusters-1):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
            ith_cluster_silhouette_values = \
                sample_silhouette_values[cluster_labels == i]

            ith_cluster_silhouette_values.sort()

        #make the size for each cluster on the graph
            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i

        #add color to each cluster
            color = cm.spectral(float(i) / clusters)
            ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
            y_lower = y_upper + 10  # 10 for the 0 samples

    #add labels and title
        ax1.set_title(""The silhouette plot for the various clusters."")
        ax1.set_xlabel(""The silhouette coefficient values"")
        ax1.set_ylabel(""Cluster label"")

    # The vertical line for average silhouette score of all the values
        ax1.axvline(x=silhouette_avg, color=""red"", linestyle=""--"")

        plt.show()
    
    #if only one label/cluster print value of eps and we did not score
    else:
        print('neighborhood distance is to far to cluster appropriately for {} only {} clusters'.format(n_eps,clusters))
    
#print the best cluster overall along with score
print(""The best number of clusters is {} with a silhouette_avg of {}"".format(best_k,best_cluster_score))","y_lower = 10
nn_cluster = number_cluster  # use cluster number defined before

fig, ax1 = plt.subplots(1,1)
fig.set_size_inches(5,5)


for i in range(0,nn_cluster):
    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels[row_selections] == i]
    ith_cluster_silhouette_values.sort()
    
    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i
    color = cm.spectral(float(i)/nn_cluster)
    ax1.fill_betweenx(np.arange(y_lower,y_upper), 0,
                     ith_cluster_silhouette_values,
                     facecolor = color, edgecolor = color, alpha = 0.7)
    ax1.text(-0.05, y_lower + 0.5*size_cluster_i, str(i))
    y_lower = y_upper + 10
    ax1.set_yticks([])
    print size_cluster_i","# Sets up import path, if you're running from the source distrib
import os, sys
sys.path.append(os.path.join(os.getcwd(), '../..'))",,Mini Project Clustering->Other Clustering Algorithms->Clustering Algorithms in Scikit-learn,Stage1 - Collecting Training Set-Rev1->Step 5: Clustering analysis->5-2 silhouette coefficient,Ipython-Notebook
245375,df_lt90,df['Capital_Net']=df.Capital_Gain-df.Capital_Loss,"for test_case in cases:
    print(""\nTest case: "", test_case['test'])
    
    # pgmpy doesn't allow you to include evidence vars in the set of query variables:
    evidence_vars = test_case['evidence'].keys()
    variables = set(mixed_cpd_model.nodes()) - set(evidence_vars)
    [test_case['expected'].pop(var, None) for var in evidence_vars]
    
    query = infer_mixed_cpd_model.query(variables=variables, evidence=test_case['evidence'])
    results = get_probability_of_True(query)

    print(""Marginal probability of True: "", results)
    
    compare_expected_to_observed(test_case['expected'], results)",,Author Gender Prediction->Extracting author names,"Project->Building,Training and Predicting Using Various Model->Univariate Analysis->Race Countplot","Compare Pgmpy Belief Propagation->II. Custom CPD model->3. Run inference using pgmpy's implementation of the belief propagation algorithm->Run inference for each test case, then compare to expected results"
12987,"f, ax = plt.subplots(figsize=(8, 8))
choices[choices['hitting_average'] < .45].hitting_average.hist(bins=30)
ax.set(xlabel=""Player Hitting Average"")
plt.show()","f, ax = plt.subplots(figsize=(8, 8))
choices[choices['hitting_average'] < .45].hitting_average.hist(bins=30)
ax.set(xlabel=""Player Hitting Average"")
plt.show()","def clean_data(data):
    for column in ['income', 'lifeExpectancy', 'population']:
        data = data.drop(data[data[column].apply(len) <= 4].index)
    return data

def extrap_interp(data):
    data = np.array(data)
    x_range = np.arange(1800, 2009, 1.)
    y_range = np.interp(x_range, data[:, 0], data[:, 1])
    return y_range

def extrap_data(data):
    for column in ['income', 'lifeExpectancy', 'population']:
        data[column] = data[column].apply(extrap_interp)
    return data",,"Analysis->Beat the Streak - Analysis->How does the past performance of hitters and starting pitchers effect the probability?
Analysis->Beat the Streak - Analysis->Does the batters batting hand or the pitchers throwing hand matter?","Analysis->Beat the Streak - Analysis->How does the past performance of hitters and starting pitchers effect the probability?
Analysis->Beat the Streak - Analysis->Does the batters batting hand or the pitchers throwing hand matter?","Wealth Of Nations->Cleaning and Formatting JSON Data
Wealth Of Nations->Clean data"
207184,"## Assumption: null values represent 0 months
## mths_since_last_delinq: The number of months since the borrower was last delinquent on a payment

loan_df['mths_since_last_delinq'].fillna(0,inplace=True)","## Assumption: null values represent 0 months
## mths_since_last_delinq: The number of months since the borrower was last delinquent on a payment

loan_df['mths_since_last_delinq'].fillna(0,inplace=True)","L = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]

L[:5]",,Exploratory Data Analysis->Table of Content:->Quick Overview of the Dataset->2) Removing/Replacing null values,Exploratory Data Analysis->Table of Content:->Quick Overview of the Dataset->2) Removing/Replacing null values,Mynotes
358572,str(2.36575),"print(str(1).zfill(3)) # convert the number 1 to a string and prepend zeros as needed to get three characters
print(str(150).zfill(3)) # convert the number 150 to a string and prepend zeros as needed to get three characters","# Select the subset of data between 1945 and 1950
discoveries_subset_1 = discoveries['1945':'1950']

# Plot the time series in your DataFrame as a blue area chart
ax = discoveries_subset_1.plot(color='blue', fontsize=15);",,Lecture 1,Sentiment Analysis->Sentiment Analysis->Sentiment Analysis of Shakespeare's Sonnets,Visualizing-Time-Series-Data-In-Python->Visualizing Time Series Data in Python->Summary Statistics and Diagnostics->Subset time series data
31919,"from PIL import Image
Image.open(path + '/test/' + batches.filenames[0])",x.imag(),q0.loadVcovar('search_data/V_'+SEARCH+'.dat'),,"Notebook->Generate Predictions
Vgg16 Initial Submission","Complex Viz Assignmnet-Solutions->Functions for absolute values, real and imaginary parts, and conjugates of complex numbers","Test->Find significance of rectangles <a class=""anchor"" id=""significance""></a>"
345821,"# loading reviews and meta data
path = './data/reviews_Home_and_Kitchen.json.gz'
meta_path = './data/meta_Home_and_Kitchen.json.gz'","#data locations
data_5min_path = ""../data/External/station_5min/2015/d11/""
meta_path = ""../data/External/meta/2015/d11/""","q = np.array([0.228,0.619,0.153])
print(""\nq:="",q)",,Process->Loading data,Wiggles Magnitude Weekend Weekday->Wiggle Magnitudes (weekend vs. weekday)->Results,"Crossentropy->You can interpret the above ""true"" distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C."
186692,"### Calculate the accuracy for these 5 new images. 
### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images.
from collections import Counter
test_match = np.argmax(probabilities,1) == test_labels
coun = Counter(test_match)
test_accuracy = coun[True]/len(test_match)
print(""{}%"".format(test_accuracy*100))","### Calculate the accuracy for these 5 new images. 
### For example, if the model predicted 1 out of 5 signs correctly, it's 20% accurate on these new images.
from collections import Counter
test_match = np.argmax(probabilities,1) == test_labels
coun = Counter(test_match)
test_accuracy = coun[True]/len(test_match)
print(""{}%"".format(test_accuracy*100))","#setup
from __future__ import print_function
import math
from IPython import display
from matplotlib import cm
from matplotlib import gridspec
from matplotlib import pyplot as plt
import numpy as np
import pandas as pd
from sklearn import metrics
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.ERROR)
pd.options.display.max_rows = 10
pd.options.display.float_format = '{:.1f}'.format",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Analyze Performance,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Analyze Performance,Untitled->Tensoflow tutorial
249460,"# Display the README.md file supplied with the course.
from IPython.display import display, Markdown

with open('README.md', 'r') as fh:
    content = fh.read()

display(Markdown(content))","with open('data/speech.wav', 'rb') as fh:
    audio_content = fh.read()
    
Audio(audio_content)","model.input_shape[1:3], model.output_shape",,Readme->Readme.md from Course,Speechtotext->Transcribing audio using the Cloud Speech to Text API.,02 Generate Bottlenecks->Use pre-trained network to compute bottlenecks->Generate Train / Val / Test bottlenecks
450563,"dim = 10

examples = X[:, 1:][np.random.randint(M, size=dim * dim)]","import numpy as np

two_dim_arr = np.random.randint(0, high=20, size=(16, 16))
two_dim_arr",Rest = Deck - Hole,,Ex3->1 Multi-class classification->1.4 One-vs-all classification->Train all classifiers.,Whirlwind Tour->Whirwind Tour->Numerical Computing with NumPy,Poker->Case Study: Computation of Probabilities in Poker
39737,"# mean normalise and standardise our image data features
scaler = StandardScaler()
X_norm = scaler.fit_transform(X)

# find covariance matrix and eigenvalues, eigenvectors
X_covariance = covariance(X_norm)

# the columns eigvecs[:, i] correspond to the eigenvector for eigenvalue[i]
eigvalues, eigvecs = eigenvectors(X_covariance)

print(eigvalues.shape)
print(eigvecs.shape)

print(eigvalues[:5])","from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_training)
X_training_scaled = scaler.transform(X_training)
print(X_training_scaled[0:2])","readfile = open(""data//wiki_materials_rows_out.txt"", 'r')
tempfile = open(""data/link_joined.txt"", 'w')
t0 = time.time()
i=0
for line in readfile:
    line = join_links(line)
    tempfile.write(line)
    
t = time.time()
print(""Time taken = "" + str(t-t0) + ""s."")
tempfile.close()",,"Coursera Exercise 7 Part 2 - Pca->Principle Component Analysis->Choosing the number of principle components, k->Preprocess, construct covariance matrix and obtain eigenvalues, eigenvectors",Kocikcy 1->Kocicky 1,Wiki-Pre-Processing And Topic Modeling->Broader classification->Get categories for each title from dbpedia->Analyzing another pre-processed dump
82995,"def tax(total, rate):        # define tax, taking total and rate
    rate = 1 + rate          # we add the rate to 1 for easy multiplying
    result = total * rate    # the result is the price times the 1+rate
    return result            # return the results 

test_total = 1.00            # try it on a dollar to check it easily
test_rate = 0.06             # we'll use a 6% rate for whatever reason

print(tax(test_total, test_rate))    # print out the results","def tax(total, rate = 0.05):        # define a rate in the definition 
    rate = 1 + rate 
    result = total * rate
    return result    


default_tax = tax(1.00)             # call the function with no rate specifed 
print(default_tax)                  # viola! it ""knows"" to use 5%

specified_tax = tax(1.00, 0.07)     # we can still specify if we want to though
print(specified_tax)                # see that the specific one works, too",!python plot_script.py,,4->A Function in Context,4->Default Arguments,01 Getting Started->Imports
128968,"fig = plt.figure()

ax = fig.add_subplot(111, projection=ccrs.Stereographic())
ax.coastlines()
ax.set_extent([-30, 50, 50, 85], crs=ccrs.PlateCarree())

ax.plot(cloudsat_lons, cloudsat_lats, linewidth=2, transform=ccrs.PlateCarree());","fig = plt.figure(figsize=(10,10) )

ax = fig.add_subplot(1, 1, 1, 
                     projection=ccrs.PlateCarree(), )

water_extent = (152.5, 153.5, -27, -26)
ax.set_extent(water_extent)

URL = 'http://services.ga.gov.au/site_1/services/Shoreline_WM/MapServer/WmsServer?'
layer= 'Shoreline'
ax.add_wms(URL, [layer], alpha=1)

gl = ax.gridlines(draw_labels=True)

plt.show()","connections = Series(
    df['distance'].values.ravel(),
    list(map(np.ravel, np.meshgrid(*[df.index]*2))))",,Quickstart->Example of reading and plotting CloudSat radar reflectivity data->Creating simple plots,Coastline->Coastline Datset Comparison->Qld Government Coastline Data,2018-08-25-Quantifying-Distances-Between-Historical-Notebooks->Using statistics to historically compare notebooks and cells
326755,"from sklearn.preprocessing import StandardScaler #on the cell, which represents the pixels, since they vary between 0 and 255.
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))","#Scaling features
from sklearn.preprocessing import StandardScaler
import numpy as np
std_scale = StandardScaler()
std_scale.fit(X_train)
X_train_sc = std_scale.transform(X_train)
X_test_sc = std_scale.transform(X_test)","feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]
train_data = train_df[feats].copy()
train_target = train_df['TARGET'].copy()
cv = StratifiedKFold(n_splits = 5, shuffle = True)",,Mnist->Binary Classifier->Measuring the classifier perfomance,01-Description,Homework 5->Classification III->EDA
338867,"from sklearn.metrics import confusion_matrix
rfc = RandomForestClassifier(n_estimators=75, max_depth=7,
     min_samples_split=59, random_state=0)

rfc.fit(X_train, y_train)

yPred = rfc.predict(X_test)

confusion_matrix(y_test , yPred, labels=[0, 1, 2])
rfc.score(X_test, y_test)","from sklearn.ensemble import RandomForestClassifier
from sklearn import cross_validation
from sklearn.metrics import confusion_matrix
rfc = RandomForestClassifier(n_estimators=100)
rfc=rfc.fit(X_train, y_train)
Y_pred= rfc.predict(X_test)
print('Confusion Matrix of : ',confusion_matrix(y_test, Y_pred), ' and its Accuracy: ', accuracy_score(y_test, Y_pred))","from pypif_sdk.readview import ReadView
rv = ReadView(pif)

print(rv.keys())",,Abalone->5 Parameter tuning,Model Creation File (1),Working With Pi Fs->Working with PIFs->`pypif` package->High[er] level API: ReadView
488242,test_matrix * test_matrix.getI(),"def get_predictions(x, t, x_test, k = 0):
    X = get_design_matrix(x, k)
    X_test = get_design_matrix(x_test, k)
    w_hat = get_w_hat(X, t)
    return X_test.dot(w_hat)","print(sns.scatterplot(x = ""SalePrice"", y = ""GrLivArea"", data = data_raw))

print(sns.scatterplot(x = ""SalePrice"", y = ""GrLivArea"", data = data_raw.loc[(data_raw.loc[:,""GrLivArea""] > 4000), :]))",,Cpds 02-Checkpoint->Workshop 2: Matrix and Data Frame->Matrix and Array->Difference Between Array and Matrix->Transpose and Inverse,Bayesian-Linear-Regression->2.3. Uncertainty in predictions,House Prices Data Exploration->Data Exploration for Ames Housing Dataset->Response Variable - SalePrice
150590,"import sys
import pandas as pd
import matplotlib.pyplot as plt
import scipy as sp
import numpy as np
import xml.etree.ElementTree as ET
from glob import glob
import datetime
from scipy.stats.stats import pearsonr

%pylab inline","import pandas as pd
import numpy as np
import xml.etree.ElementTree as ET
import matplotlib.pyplot as plt

%matplotlib inline","# To see all the objects of the modules you have imported.

dir(my_list)",,Batch->Process new data from liulin,2-4-1->Unit 2 - Intro to Data Science Toolkit,Manipulating Data-Checkpoint->1. Modular Programming->Importing Modules
252827,"i = 1
while i <= 10:
    print(i)
    i += 1
else:
    print('Loop ended, i =', i)","i = 0

while i < 10:
    i = i + 1
    print(i)",dt.tests['5x500e'].sp.info,,Untitled->practica 41,Python1B->Python Logic->Repetition,Spatial Mapper->Lets test it
333789,"# get correlation matrix and visualize it in figure
cmat = hour_data.corr()

f, ax = plt.subplots(figsize=(10, 5))
sns.heatmap(cmat, square=True)
f.tight_layout()","# show correlation again
_ , axsn2 = plt.subplots(figsize=(8,8))
sn.heatmap(df_cpy.corr()*100, ax=axsn2, square=False, fmt='.0f', annot=True, cbar=False)
# rotation is recognized for matplotlib version 2.1.X and above
axsn2.tick_params(axis = 'x',rotation = 90.0)
axsn2.tick_params(axis = 'y',rotation = 0.0)
plt.tight_layout()","# the fiducial BOSS DR12 cosmology
cosmo = cosmology.Cosmology(h=0.676).match(Omega0_m=0.31)

# add Cartesian position column
data['Position'] = transform.SkyToCartesian(data['RA'], data['DEC'], data['Z'], cosmo=cosmo)
randoms['Position'] = transform.SkyToCartesian(randoms['RA'], randoms['DEC'], randoms['Z'], cosmo=cosmo)",,Bike Rentals->Bike Sharing Analysis and Modeling->B. Building a Predictive Model->2) Feature correlation,02 Feature Extraction->Strategy->Correlation->Save our new dataframe,Boss-Dr12-Data->The Multipoles of the BOSS DR12 Dataset->Adding the Cartesian Coordinates
201736,"df = riasec.pivot(index = 'onetsoc_code', columns = 'element_name', values = 'RIASEC')
df.reset_index(inplace = True)
df['riasec'] = df.apply(lambda x: createRIASEC(x), axis = 1)
df = df.merge(jobTitles, left_on = 'onetsoc_code', right_on = 'onetsoc_code', how = 'left')
df.head()","df = riasec.pivot(index = 'onetsoc_code', columns = 'element_name', values = 'RIASEC')
df.reset_index(inplace = True)
df['riasec'] = df.apply(lambda x: createRIASEC(x), axis = 1)
df = df.merge(jobTitles, left_on = 'onetsoc_code', right_on = 'onetsoc_code', how = 'left')
df.head()","%matplotlib inline
state_stub2_ratio = clean_irs.pivot_table(index=['State_x'], 
                                    columns=['agi_stub'], 
                                    values='paid_ratio', #aggfunc='sum'
                                    )
new_columns = ['\$1 under $25,000','\$25,000 under $50,000','\$50,000 under $75,000','\$75,000 under $100,000','\$100,000 under $200,000','\$200,000 or more']
state_stub_ratio.columns = new_columns
stub_category_ratio = state_stub2_ratio.mean().sort_values(ascending=False)
us_scr = stub_category_ratio.plot(kind=""bar"",figsize=(20,6),title = 'distribution of returns categorized by Income range for top 10 states')
us_scr.set_ylabel('percentage')
us_scr.set_xlabel('Income range')",,Data Wrangling->Create RIASEC groups,Data Wrangling->Create RIASEC groups,Intuit Data Observation&Clean->Intuit data challenge->Split data into train and test dataset
260803,"topic_word = model.topic_word_
n = 5
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]
    print('*Topic {}\n- {}'.format(i, ', '.join(topic_words)))","# Print topic words
topic_word = model.topic_word_  # model.components_ also works
n_top_words = 8
for i, topic_dist in enumerate(topic_word):
    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]
    print('Topic {}: {}'.format(i, ' '.join(topic_words)))","t, v, a = np.loadtxt('data.txt', skiprows=1, unpack=True)
t",,Example->Code Example,Nlp Tools And Methods->Topic Modeling->lda,Basic Data Processing With Num Py And Matplotlib->Lecture 3 - Basic Data Processing with NumPy and Matplotlib->Data Input and Output via NumPy
313675,"#Fitting

fitrangeR = np.linspace(0.1,1.1,2)
poptR,pcovR = curve_fit(linfit,timeR,linearR, sigma=linearR_std)

#Results
print(""The fit determined to proportionality for the R-filter to: ("" + str(round(poptR[0]*10**(-3),2)) + ' +/- ' + str(round(np.sqrt(pcovR[0,0])*10**(-3),2)) + "") 10e3 1/s"" )","#Fitting

fitrange = np.linspace(0.0035,0.0057,100)
popt,pcov= curve_fit(dark_current,xval[:70],yval[:70], sigma=yval_err[:70])

#Calculating the band gap using our fitting results
Eg = popt[1]*2*kb/1.602e-19
Eg_err = np.sqrt(np.diag(pcov))[1]*2*kb/1.602e-19

print('The band gap of silicon has been determined to:')
print('Eg = (' + str(round(Eg,3)) + ' +/- ' + str(round(Eg_err,3)) + ') eV')",clean_df_null_removed.groupby('Pclass')['Survived'].mean(),,Telescope Data->F30 Stellar CCD Photometry in modern astronomy->Evaluation of the data measured with the 70cm KING telescope at MPIA->4.4 Linearity and dynamical range of the CCD->a) R-filter ($ \ \lambda$ = 658 +/- 69 nm),Telescope Data->F30 Stellar CCD Photometry in modern astronomy->Evaluation of the data measured with the 70cm KING telescope at MPIA->4.2 Determination of the band gap of silicon,P2-Complete 2
326514,"def mp2d(x):
    return tf.layers.max_pooling2d(
        inputs=x,
        pool_size=2,
        strides=2)","from keras import models
from keras.layers import Flatten, Dropout, MaxPooling2D,Activation
from keras.layers import Dense, InputLayer, Convolution2D
import tensorflow as tf

def Ngram_CNN(n_classes):
    model = models.Sequential()

    # input layer
    model.add(InputLayer(input_shape=(32,100,1)))

    # first convolutional layer
    model.add(Convolution2D(64, 5, 5, activation='relu', subsample=(1, 1),border_mode='same'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))


    # second convolutional layer
    model.add(Convolution2D(128, 5, 5, activation='relu',subsample=(1, 1),border_mode='same'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))


    # third convolutional layer
    model.add(Convolution2D(256, 3, 3, activation='relu',subsample=(1, 1),border_mode='same'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))


    # fourth convolutional layer
    model.add(Convolution2D(512, 3, 3, activation='relu',subsample=(1, 1),border_mode='same'))


    # fifth convolutional layer
    model.add(Convolution2D(512, 3, 3, activation='relu', subsample=(1, 1),border_mode='same')) 


    # first fully connected layer
    model.add(Flatten())
    model.add(Dropout(0.5))
    model.add(Dense(4096, activation='relu')) 


    # second fully connected layer
    model.add(Dropout(0.5))
    model.add(Dense(4096))

    # N_gram classes layer
    model.add(Dropout(0.5))
    model.add(Dense(n_classes))
    model.add(Activation('sigmoid'))
    
    model.compile(loss ='binary_crossentropy', optimizer='adam',metrics=['accuracy'])
    return model",Structure.from_file(hist.filepath) == hist.structures[-1],,Mnist,Ngram Training->Conv-net model,Hist->The HIST.nc file
488237,"test_matrix = np.matrix('[1,2;3,4]')
test_matrix",np.shape(test_matrix),"filenames = os.listdir(base_dir)
aFiles = [filename for filename in filenames if letter in filename]

data = numpy.empty([0, 2])
for file in aFiles:
    tmp_data = numpy.loadtxt(base_dir + ""/"" + file)
    data = numpy.concatenate((data, tmp_data))

#numpy.savetxt(letter + output_suffix, data)

plt.plot(data[::,0], -data[::,1], '.')
plt.savefig(""Report/img/AmergePlot.pdf"")
plt.show()",,Cpds 02-Checkpoint->Workshop 2: Matrix and Data Frame->Matrix and Array->Difference Between Array and Matrix->Transpose and Inverse,Classification Week 6->Exploring precision and recall,Fpdm Lab02
490170,"def list_max(a_list) :
    m = a_list[0]
    for element in a_list[1:] :
        if element > m :
            m = element
    return m
print(list_max([1,5,23,-3]))","def list_max(a_list) :
    m = a_list[0]
    for element in a_list[1:] :
        if element > m :
            m = element
    return m
print(list_max([1,5,23,-3]))","predictions = np.array([+1 if score > 0 else -1 for score in scores])
Result_Q5   = sum(predictions == 1)
print('Q5: Number of pos predictions: %s' % Result_Q5)",,Python Intro->Recitation 1: An introduction to Python->Variables and basic Python types,Python Intro->Recitation 1: An introduction to Python->Variables and basic Python types,Module-3-Linear-Classifier-Learning-Assignment-Blank->Predicting sentiments
297607,"def extrapolate_lines(img, lines, roi, m_thresh, color=[255, 0, 0], thickness=2):
    '''
    This is the improved 'draw_lines' function that extrapolates the lane, 
    based on provided hough_lines, slope threshold, and ROI.
    '''
    left_lane_points = []
    right_lane_points = []
    
    for line in lines:
        # Get line endpoints and calculate slope.
        x0, y0, x1, y1 = line[0]
        m = -(y1-y0) / (x1-x0)
        
        # Reject lines with slope not representative of a lane line.
        if abs(m) < m_thresh:
            continue
            
        # Classify line endpoints as part of left lane or right lane.
        if m > 0:
            left_lane_points.append([x0, y0])
            left_lane_points.append([x1, y1])
        else:
            right_lane_points.append([x0, y0])
            right_lane_points.append([x1, y1])

    # Use np.polyfit to find the line of best fit for the LEFT lane.
    if (len(left_lane_points)) > 0:
        left_lane_points = np.array(left_lane_points)
        m, b = np.polyfit(left_lane_points[:,0], left_lane_points[:,1], 1)
        bot_y = int(roi[0][0,1])
        bot_x = int((bot_y - b) / m)
        top_y = int(roi[0][1,1])
        top_x = int((top_y - b) / m)
        cv2.line(img, (bot_x,bot_y), (top_x,top_y), color, thickness)
                 
    # Use np.polyfit to find the line of best fit for the RIGHT lane.
    if len(right_lane_points) > 0:
        right_lane_points = np.array(right_lane_points)
        m, b = np.polyfit(right_lane_points[:,0], right_lane_points[:,1], 1)
        bot_y = int(roi[0][3,1])
        bot_x = int((bot_y - b) / m)
        top_y = int(roi[0][2,1])
        top_x = int((top_y - b) / m)
        cv2.line(img, (bot_x,bot_y), (top_x,top_y), color, thickness)

    return [left_lane_points, right_lane_points]","def Draw_Lines(road_image, Hough_Lines, ROI_vertices):
    """"""
    Draws left and right lane lines
    This function modifies the given image and draws lines on it.

    The line drawing function “Draw_Lines()” is implemented to connect the line segments for each lane-line (left or right) 
    to produce one solid line that tracks the actual lane-line in the images. The line segments are produced by Hough transform 
    and the “Draw_Lines()” function is connecting them. 
    The function is achieving that through the following steps:
    1. Classification (Left or Right): Classifying all the Hough line-segments as to belong to the left or right lines. 
       This is done based on the slope of the line segment. If the slope is positive and lies between 0.4 => 1.0 then this 
       segment belongs to the left line class, and if it is negative and -0.4 => -1.0 then it belongs to the right line class.
    2. For all the classified Left & Right line-segments, the lengths and intercepts (intersections with the x-axis) are 
       calculated and stored along with their slopes.
    3. A weighted average slope is calculated for each class (Left and Right) taking the length of each line segment as its 
       weight.
    4. A weighted average intercept is calculated for each class (Left and Right) taking the length of each line segment as 
       its weight. 
    5. Now the Left and Right lines can be drawn, however to reduce the jitter the information of previous frames are 
       incorporated to produce a 1st order filter.
    6. The FIR equation considered in this implementation is given as Y(k) = a0*X(k) +a1*X(k-1)+a2*X(k-2) .... an*X(k-n).
    7. The resultant lines slopes and intercepts and using the information of “the region of interest” (ROI) calculated 
       in step 5 of the pipeline, are used to draw the Left line in blue and the right line in red.
       :type ROI_vertices: object

    """"""
    # Global variable to store the information of previous frame
    # global prev_lines, is_video_file
    Line_thickness = 20                            # desired lane line thickness
    lan_lines_color=[[255, 0, 0], [0, 0, 255]]     # desired lane line color : RED & BlUE

    # Go through each line segment and classify them as right and left lines based on their slode
    # Store slope, intercept for each line segment
    # Determine_Right_or_Left_lines(Hough_lines)
    Lane_Left_Lines =  []
    Lane_Right_Lines = []
    Lane_Left_Lines_Slopes =  []
    Lane_Right_Lines_Slopes = []

    for each_line in Hough_Lines:
        for x1, y1, x2, y2 in each_line:
            if (x2 != x1):
                Slope_of_each_line = (y2 - y1) / (x2 - x1)
                if Slope_of_each_line > 0:
                    Lane_Left_Lines.append((x1, y1, x2, y2))
                    Lane_Left_Lines_Slopes.append(Slope_of_each_line)
                elif Slope_of_each_line < 0:
                    Lane_Right_Lines.append((x1, y1, x2, y2))
                    Lane_Right_Lines_Slopes.append(Slope_of_each_line)
    
    # Special handling of the challenge problem
    if video_file_name == ""challenge.mp4"":
        Left_Lane_Slope_Lower_Limit  = 0.35
        Left_Lane_Slope_Upper_Limit  = 1.2
        Right_Lane_Slope_Lower_Limit = -0.35
        Right_Lane_Slope_Upper_Limit = -1.2
    else:
        Left_Lane_Slope_Lower_Limit  = 0.4
        Left_Lane_Slope_Upper_Limit  = 1.0
        Right_Lane_Slope_Lower_Limit = -0.4
        Right_Lane_Slope_Upper_Limit = -1.0
    
    # Removing undesired lines with improper slopes (noise)
    i = 0
    Lane_Left_Lines_X = []
    Lane_Left_Lines_Y = []
    for x1, y1, x2, y2 in Lane_Left_Lines:
        if ( Left_Lane_Slope_Lower_Limit  < Lane_Left_Lines_Slopes[i] < Left_Lane_Slope_Upper_Limit):
            Lane_Left_Lines_X.append(x1)
            Lane_Left_Lines_Y.append(y1)
            Lane_Left_Lines_X.append(x2)
            Lane_Left_Lines_Y.append(y2)
            i = i + 1
            
    # Removing undesired lines with improper slopes (noise)
    i = 0
    Lane_Right_Lines_X = []
    Lane_Right_Lines_Y = []
    #print(""Lane_Right_Lines:"", Lane_Right_Lines)
    for x1, y1, x2, y2 in Lane_Right_Lines:
        if ( Right_Lane_Slope_Lower_Limit  > Lane_Right_Lines_Slopes[i] > Right_Lane_Slope_Upper_Limit):
            Lane_Right_Lines_X.append(x1)
            Lane_Right_Lines_Y.append(y1)
            Lane_Right_Lines_X.append(x2)
            Lane_Right_Lines_Y.append(y2)
            i = i + 1
    
    # Using line fitting technique to construct Lane Lines
    if Lane_Left_Lines_X != [] and Lane_Left_Lines_Y != []:
        Solid_Left_Lane_Line = np.polyfit(Lane_Left_Lines_X, Lane_Left_Lines_Y, 1)
    else:  Solid_Left_Lane_Line = [0,0]
    
    if Lane_Right_Lines_X != [] and Lane_Right_Lines_Y != []:
        Solid_Right_Lane_Line = np.polyfit(Lane_Right_Lines_X, Lane_Right_Lines_Y, 1)
    else:  Solid_Right_Lane_Line = [0,0]
    
    # implementing four ""shift registers"" (data structures)
    # Used to biuld the FIR filter
    Slope_Left_Lane_Stack.append(Solid_Left_Lane_Line[0])
    Intercept_Left_Lane_Stack.append(Solid_Left_Lane_Line[1])
    Slope_Right_Lane_Stack.append(Solid_Right_Lane_Line[0])
    Intercept_Right_Lane_Stack.append(Solid_Right_Lane_Line[1])
    
    # Averaging the information in the previous N frames (N=Filter Order)
    if np.shape(Slope_Left_Lane_Stack)[0] > (FIR_Filter_Order - 1):
        Av_Slope_Left_Lane  = np.mean(Slope_Left_Lane_Stack)
    else: Av_Slope_Left_Lane = Solid_Left_Lane_Line[0]
    
    if np.shape(Slope_Right_Lane_Stack)[0] > (FIR_Filter_Order - 1):
        Av_Slope_Right_Lane = np.mean(Slope_Right_Lane_Stack)
    else: Av_Slope_Right_Lane = Solid_Right_Lane_Line[0]
    
    if np.shape(Intercept_Left_Lane_Stack)[0] > (FIR_Filter_Order - 1):
        Av_Intercept_Left_Lane = np.mean(Intercept_Left_Lane_Stack)
    else: Av_Intercept_Left_Lane = Solid_Left_Lane_Line[1]
    
    if np.shape(Intercept_Right_Lane_Stack)[0] > (FIR_Filter_Order - 1):
        Av_Intercept_Right_Lane = np.mean(Intercept_Right_Lane_Stack)
    else: Av_Intercept_Right_Lane = Solid_Right_Lane_Line[1]
 
    # Implement the FIR Filter or order ""Filter Order""
#     if np.shape(Slope_Left_Lane_Stack)[0] > (FIR_Filter_Order - 1):
#         Av_Slope_Left_Lane  = np.dot(Slope_Left_Lane_Stack, FIR_Coeffs)
#     else: Av_Slope_Left_Lane = Solid_Left_Lane_Line[0]
    
#     if np.shape(Slope_Right_Lane_Stack)[0] > (FIR_Filter_Order - 1):
#         Av_Slope_Right_Lane = np.dot(Slope_Right_Lane_Stack, FIR_Coeffs)
#     else: Av_Slope_Right_Lane = Solid_Right_Lane_Line[0]
    
#     if np.shape(Intercept_Left_Lane_Stack)[0] > (FIR_Filter_Order - 1):
#         Av_Intercept_Left_Lane = np.dot(Intercept_Left_Lane_Stack, FIR_Coeffs)
#     else: Av_Intercept_Left_Lane = Solid_Left_Lane_Line[1]
    
#     if np.shape(Intercept_Right_Lane_Stack)[0] > (FIR_Filter_Order - 1):
#         Av_Intercept_Right_Lane = np.dot(Intercept_Right_Lane_Stack, FIR_Coeffs)
#     else: Av_Intercept_Right_Lane = Solid_Right_Lane_Line[1]

    # Biulding the X axis data points for both Left & Right Lines
    XX_left  = np.linspace(min(ROI_vertices[:,0]), max(ROI_vertices[:,0]), 30)
    XX_right = np.linspace(min(ROI_vertices[:,0]), max(ROI_vertices[:,0]), 30)

    # Biulding the Y axis data points for both Left & Right Lines (from line equations)
    YY_left = Av_Slope_Left_Lane * XX_left + Av_Intercept_Left_Lane
    YY_right = Av_Slope_Right_Lane * XX_right + Av_Intercept_Right_Lane

    # Apply the Region of Interest concept (only get the data points in this region)
    ROI_YY_left = []
    ROI_XX_left = []
    for i in range(0, np.shape(YY_left)[0]):
        if YY_left[i] > ROI_vertices[1,1]:
            ROI_YY_left.append(YY_left[i])
            ROI_XX_left.append(XX_left[i])

    ROI_YY_right = []
    ROI_XX_right = []
    for i in range(0, np.shape(YY_right)[0]):
        if YY_right[i] > ROI_vertices[2, 1]:
            ROI_YY_right.append(YY_right[i])
            ROI_XX_right.append(XX_right[i])

    # Initialization     
    global Previous_ROI_left_x1 
    global Previous_ROI_left_x2 
    global Previous_ROI_left_y1 
    global Previous_ROI_left_y2 
        
    global Previous_ROI_right_x1 
    global Previous_ROI_right_x2 
    global Previous_ROI_right_y1 
    global Previous_ROI_right_y2   
    
    # Protection against NULL data points
    if ROI_XX_left == [] or ROI_YY_left == []:
        ROI_left_x1 = Previous_ROI_left_x1
        ROI_left_x2 = Previous_ROI_left_x2
        ROI_left_y1 = Previous_ROI_left_y1
        ROI_left_y2 = Previous_ROI_left_y2
    else:
        ROI_left_x1 = int(max(ROI_XX_left))
        ROI_left_x2 = int(min(ROI_XX_left))
        ROI_left_y1 = int(max(ROI_YY_left))
        ROI_left_y2 = int(min(ROI_YY_left))
        
    if ROI_XX_right == [] or ROI_YY_right == []:
        ROI_right_x1 = Previous_ROI_right_x1
        ROI_right_x2 = Previous_ROI_right_x2
        ROI_right_y1 = Previous_ROI_right_y1
        ROI_right_y2 = Previous_ROI_right_y2
    else:
        ROI_right_x1 = int(min(ROI_XX_right))
        ROI_right_x2 = int(max(ROI_XX_right))
        ROI_right_y1 = int(max(ROI_YY_right))
        ROI_right_y2 = int(min(ROI_YY_right))
    
    # Assigning the previuos data to the current one for the next iteration
    Previous_ROI_left_x1 = ROI_left_x1
    Previous_ROI_left_x2 = ROI_left_x2
    Previous_ROI_left_y1 = ROI_left_y1
    Previous_ROI_left_y2 = ROI_left_y2
        
    Previous_ROI_right_x1 = ROI_right_x1
    Previous_ROI_right_x2 = ROI_right_x2
    Previous_ROI_right_y1 = ROI_right_y1
    Previous_ROI_right_y2 = ROI_right_y2
        
#     # ""plt.plot"" can be used as well instead of cv2.line
#     
#     plt.plot(ROI_XX_left, ROI_YY_left, color='b', linestyle='solid', linewidth=Line_thickness, alpha=0.7)
#     plt.plot(ROI_XX_right, ROI_YY_right, color='r', linestyle='solid', linewidth=Line_thickness, alpha=0.7)
    
    # Draw the left and right lane lines 
    cv2.line(road_image, (ROI_left_x1, ROI_left_y1), (ROI_left_x2, ROI_left_y2), lan_lines_color[0], Line_thickness)
    cv2.line(road_image, (ROI_right_x1, ROI_right_y1), (ROI_right_x2, ROI_right_y2), lan_lines_color[1], Line_thickness)

    return 0","def p(x):
    return normPDF(x, mu, sig2)

x = np.linspace(-3.0,5.0,1000)
px = [p(i) for i in x]
f = figure(title=""Real distribution"", x_axis_label='x', y_axis_label='p(x)',
           plot_width=300, plot_height=300)
f.line(x, px, line_width=2, color=""firebrick"", legend=""Real dist."")
show(f)",,P1->Project: **Finding Lane Lines on the Road**->Improved 'draw_lines' function - extrapolates lane lines,Lane+Detection+P1+Ver+2->Supporting Functions,Sampling Unknown Variance And Mean->2. Unknown mean ($\mu$) and unknown variance ($\sigma^2$)
387333,"# Create numpy matrix with only the columns representing the offers
x_cols = df_pivot[df_pivot.columns[:]]
x_cols.head()","# apply KMeans
x_col=df_pivot
cluster = KMeans(n_clusters=5)
label = cluster.fit_predict(x_col) # no need to train
df_pivot['label'] = label
print('SS is %.3f.' % round(cluster.inertia_,4))
df_pivot.head()","a = 3
print(a)
if a == a:
    print('Yes, a = a. This is Reflexive.')
else:
    print('No, a does not equal a.')",,Mini Project Clustering->K-Means Clustering->Choosing K: The Elbow Sum-of-Squares Method,Mini Project Clustering->K-Means Clustering->Choosing K: The **Elbow** Sum-of-Squares Method,Algebra 002-1 Formulas Juypter->1. Reflexive a = a
45286,"# Dropping missing records
df_clean.dropna(axis = 0, inplace = True)","# remove last set of null values.
df_cleanData = df_cleanData.dropna(subset = ['favorite_count'])
df_cleanData = df_cleanData.dropna(subset = ['dogConfsMax'])","# Mutation data
samples_in_mutation_data = list(set(tcga_id_map.loc[tcga_id_map[""SNVs_id""].isin(set(tcga_mutation_data[""Tumor_Sample_Barcode""]))
                                          ,""sample_id""]))
print(len(samples_in_mutation_data))


# CNV data
samples_in_cnv_data = list(set(tcga_id_map.loc[tcga_id_map[""CNV_id""].isin(list(tcga_cnv_data)[2:])
                                          ,""sample_id""]))
print(len(samples_in_cnv_data))

# Fusion data
samples_in_fusion_data = list(set([x for x in samples_in_fusion_data if x in list(tcga_id_map[""sample_id""])]))
print(len(samples_in_fusion_data))
# use sample IDs from expression data as a background list (PRADA is working on RNA-Seq data)
samples_in_expression_data = list(set([str(i)[:15] for i in tcga_id_map[""mRNA_id""]]))
print(len(samples_in_expression_data))
samples_in_fusion_data = list(set(samples_in_fusion_data + samples_in_expression_data))
print(len(samples_in_fusion_data))


# Clinical data
samples_in_clinical_data = list(set(tcga_id_map.loc[tcga_id_map[""Clinical_id""].isin(set(tcga_clinical_data[""bcr_patient_barcode""]))
                                          ,""sample_id""]))
print(len(samples_in_clinical_data))


# All sample IDs
set_of_tcga_samples = set(tcga_id_map[""sample_id""])
set_of_tcga_samples = [x for x in set_of_tcga_samples if x == x] #excludes 'nan' from list
print(len(set_of_tcga_samples))",,Airline Delay->Airline On-Time Arrivals->Data Handling,Wrangle Act->Data Wrangling Act WeRateDogs->Table of Contents->Query JSON Data: Twitter API,Define Subtype Labels-Checkpoint->Read in data->Samples in the different data types
148669,"color = data['color']
set(color)","plt.figure(figsize=(15, 6))
sns.set_color_codes(""muted"")
sns.barplot(x=""embarked"", y=""survived"", data=training_data,
            label=""survived"", color=""b"")","plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
Bobsleigh.Event.value_counts().plot(kind='bar')
plt.ylabel('Medal Count')
plt.title('Events for Bobsleigh in the Winter Olympics')

plt.subplot(1, 2, 2)
Bobsleigh.Year.value_counts().plot(kind='bar')
plt.ylabel('Medal Count')
plt.title('Medal distribution for Bobsleigh per Winter Olympics')

plt.show()",,Movies->Feature engineering:->color,Titanic - Initial Workings->Titanic - Initial Data Exploration and model fitting->Exploratory Data Analysis->Use pandasql to join ages table to training data and impute the age where missing.,Prep Course Capstone->PREP COURSE CAPSTONE->African and Carribean Countries in the Summer Olympics->Ask and answer analytic questions.
362604,"Val_pred_LR=LR.predict(Val_X_scaled)
Train_pred_LR=LR.predict(Train_X_scaled)
get_basic_metrics(Val_Y,Val_pred_LR)","my_func.classifier_summary(y_val, lr_quad_basic.predict(X_val_quad));","img = train_dataset[0][0]
#making img into a 1d tensor
reshaped_img = img.view(-1, inputs)     #-1 infers the correct size given the other dimensions (inputs)
print(reshaped_img.size())
print(reshaped_img.dim())",,Meetup Final->Train Test (Validation) Split with Stratification->Lets Try out Logistic Regression with Cross Validation,3 1 Logistic Regression->Feature Selection->Base Model,Logistic Regression Mnist->Logistic regression and MNIST dataset->Reshaping an element of train_dataset
312224,"from sklearn.preprocessing import Imputer

imputer = Imputer(strategy=""median"")
housing_num = housing.drop(""ocean_proximity"", axis=1)
imputer.fit(housing_num)","drop_columns= ['Formation', 'Well Name', 'Depth','Facies']
depth = training_data['Depth']
well = data['Well Name'].values
training_data = feat_imput(training_data,drop_columns)","model_config = {
    'inputs': dict(images={'shape': (28, 28, 1)},
                   labels={'classes': (10), 
                           'transform': 'ohe', 
                           'name': 'targets'}),
    'loss': 'softmax_cross_entropy',
    'decay': ('exp', {'learning_rate': 0.001,
                      'decay_steps': 150,
                      'decay_rate': 0.96}),
    'optimizer': 'RMSProp',
    'input_block/inputs': 'images',
    'head/units': 10,
    'output': dict(ops=['labels', 'proba', 'accuracy'])
}",,Housing->Prepare the Data->Data cleaning,Facies Classification Es->Feature-imputation,Cnn->Convolutuinal Neural Networks With Dataset
423474,ag.isnull().sum(),"NoManC100S = totSimsAllNoMan[(totSimsAllNoMan.Time == 100) & (totSimsAllNoMan.Sim == 'nomanC')]
NoManC100S = NoManC100S.groupby('rep').sum()
NoManC100S.AGBw.describe()","visits_df = pd.DataFrame()
visits_df['year'] = visits.apply(lambda dt: int(dt.strftime(""%Y"")))
visits_df['month'] = pd.Categorical(visits.apply(lambda dt: dt.strftime(""%B"")), calendar.month_name[1:])
visits_df['day'] = visits.apply(lambda dt: int(dt.strftime(""%d"")))
visits_df['hour'] = visits.apply(lambda dt: int(dt.strftime(""%H"")))
visits_df['min'] = visits.apply(lambda dt: int(dt.strftime(""%M"")))
visits_df['weekday'] = pd.Categorical(visits.apply(lambda dt: dt.strftime(""%A"")),calendar.day_name)

print(visits_df.head())
print('-'*15)
print(visits_df.dtypes)",,Assignment2,Osceola Output->Dig into the AGC dataframe and get out some specifics for the MS,Gym Visits->Scrape Downloaded Data->*visits_df* is a df representation for ease of grouping
321064,"fig = plt.figure(figsize=[20, 20])
i = 1
for br in data[~data.AgeBracket.isnull()].AgeBracket.unique().sort_values():
    fig.add_subplot(4, 4, i)
    plt.title(br)
    i += 1
    sns.countplot(data[data.AgeBracket == br].Title)","# Create a figure instance, and the subplots
fig = plt.figure(figsize=(14,14))
ax1 = fig.add_subplot(2,2,1)
ax2 = fig.add_subplot(2,2,2)
ax3 = fig.add_subplot(2,2,3)

sns.set_palette(""husl"",2)
sns.countplot(y=""job"", hue=""y"", data=data,ax=ax1,saturation=1).set_title('job category distribution')
sns.countplot(y=""education"", hue=""y"", data=data,ax=ax2,saturation=1).set_title('education level distribution')
sns.countplot(x=""marital"", hue=""y"", data=data,ax=ax3,saturation=1).set_title('marital status distribution')","M, N = A.shape # rows x cols",,Exploration->Feature Exploration,"Predict Successof Bank Telemarketing->Machine Learning : Model Building->`job` , `education` and `marital` variables",Sci Py-Linalg-Module-Essentials->A SciPy Module for Linear Algebra: scipy.linalg->Basics->Singular value decompositions
451585,"gender_clean = {
    ""female"":""Female"",
    ""male"":""Male"",
    ""Male"":""Male"",
    ""male-ish"":""Male"",
    ""maile"":""Male"",
    ""trans-female"":""Female"",
    ""cis female"":""Female"",
    ""f"":""Female"",
    ""m"":""Male"",
    ""M"":""Male"",
    ""something kinda male?"":""Male"",
    ""cis male"":""Male"",
    ""woman"":""Female"",
    ""mal"":""Male"",
    ""male (cis)"":""Male"",
    ""queer/she/they"":""Female"",
    ""non-binary"":""Unspecified"",
    ""femake"":""Female"",
    ""make"":""Male"",
    ""nah"":""Unspecified"",
    ""all"":""Unspecified"",
    ""enby"":""Unspecified"",
    ""fluid"":""Unspecified"",
    ""genderqueer"":""Unspecified"",
    ""androgyne"":""Unspecified"",
    ""agender"":""Unspecified"",
    ""cis-female/femme"":""Female"",
    ""guy (-ish) ^_^"":""Male"",
    ""male leaning androgynous"":""Male"",
    ""man"":""Male"",
    ""male "":""Male"",
    ""trans woman"":""Female"",
    ""msle"":""Male"",
    ""neuter"":""Unspecified"",
    ""female (trans)"":""Female"",
    ""queer"":""Unspecified"",
    ""female (cis)"":""Female"",
    ""mail"":""Male"",
    ""a little about you"":""Unspecified"",
    ""malr"":""Male"",
    ""p"":""Unspecified"",
    ""femail"":""Female"",
    ""cis man"":""Male"",
    ""ostensibly male, unsure what that really means"":""Male"",
    ""female "":""Female"",
    ""Female"":""Female"",
    ""Male-ish"":""Male""
}

data.Gender = data.Gender.str.lower()
data.Gender = data.Gender.apply(lambda x: gender_clean[x])","df['female'] = df.Gender.apply(lambda x: 1 if x.lower()[0] =='f'  else 0 )
#Many respondents would put F or f or female for gender. This will classify all these responses as females.","# Explore the missing data:
missing = pd.read_csv(""../data/table/missing.csv"")
missing",,Mental Health In Tech- Eda->Mental Health Survey in the Tech Industry->Let us take a brief look at the data to get an idea -,Draft 2 Capstone->Introduction,Report-Checkpoint->A mini Machine Learning Project exploring feature and model selection->1. About the data->Missing data
154656,"# predict 
# 1/9/15
x_data = [length+1, length+2, length++6, length+10]
y_data = [slope*x + intercept for x in x_data]
print(y_data)","# predict 
# 1/9/15
x_data = [length+1, length+2, length++6, length+10]
y_data = [slope*x + intercept for x in x_data]
print(y_data)","cmath.isclose(RHS, 0)",,Demo->A1. Investigating the Population Data,Demo->A1. Investigating the Population Data,13 - Complex Numbers->Complex Numbers->Euler's Identity and the **isclose()** function
49509,"plt.hist(x='alcohol',data=df,bins=30)
plt.show()","plt.hist(x = 'atemp' , data = df , bins =15,color='#4d79ff' ) 
plt.show()","weather_data['weekday'] = get_weekday(weather_data, 'date')
weather_data['weekday']",,White Wine Eda->Exploratory Data analysis on White Wine Dataset->Alcohol Distribution Plot,Bike Renting Eda Report->Multivariate Analysis->season-humidity-total count plot,Subway-Weather->Lesson 4 - Problem Set 2: Wrangling Subway Data->Quiz 3: Mean Temp on Weekends->Create day of week column
390204,"customer1 = Customer(name = ""Partner 1 Name"", email=""partner1@meet.mit.edu"", address = ""3 Lincoln St."")
password=""my_super_secret_password""
customer1.hash_password(password)
session.add(customer1)
shoppingCart = ShoppingCart(customer=customer1)
session.add(shoppingCart)
session.commit()

customer2 = Customer(name = ""Partner 2 Name"", email=""partner2@meet.mit.edu"", address = ""3 Lincoln St."")
password=""another_super_secret_password""
customer1.hash_password(password)
session.add(customer2)
shoppingCart = ShoppingCart(customer=customer2)
session.add(shoppingCart)
session.commit()","session.add(c)
session.commit()",Image(filename='Appendix_B.png'),,Lab4->Create two New Customers,"Disease Tagging Tutorial 2->Disease Tagging Tutorial->Writing a basic `CandidateExtractor`->Saving the extracted candidates
Intro Tutorial 2->Intro. to Snorkel: Extracting Spouse Relations from the News->Running the `CandidateExtractor`->Saving the extracted candidates
1 Candidate Generation->1: Candidate Extraction->Running the `CandidateExtractor`->Saving the extracted candidates",Ex7->Training Exercise 7.2
255476,"import io
import os

# Imports the Google Cloud client library
from google.cloud import vision
from google.cloud.vision import types


# Instantiates a client
client = vision.ImageAnnotatorClient()


# Loads the image into memory
with io.open(file_name, 'rb') as image_file:
    content = image_file.read()

image = types.Image(content=content)

# Performs label detection on the image file
response = client.label_detection(image=image)
labels = response.label_annotations

print('Labels:')
for label in labels:
    print(label.description)","def transcribe_file(speech_file):
    """"""Transcribe the given audio file.""""""
    from google.cloud import speech
    from google.cloud.speech import enums
    from google.cloud.speech import types
    client = speech.SpeechClient()

    with io.open(speech_file, 'rb') as audio_file:
        content = audio_file.read()

    audio = types.RecognitionAudio(content=content)
    config = types.RecognitionConfig(
        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=44100,
        language_code='en-US')

    response = client.recognize(config, audio)
    # Each result is for a consecutive portion of the audio. Iterate through
    # them to get the transcripts for the entire audio file.
    for result in response.results:
        # The first alternative is the most likely one for this portion.
        print('Transcript: {}'.format(result.alternatives[0].transcript))
        
    return response.results","ratings['avg_rating'] = df.groupby('title')['rating'].mean() # Pandas will match the indicies.
ratings.sort_values(by='avg_rating', ascending=False)[::50]",,Untitled->Test vision api's->Test Google Vision API,Hw 3->1) Monty: The Python Siri->Answer begins here,26 Basic Recommender Systems->Basic Recommender Systems->Poking Around
324351,"create_pairplot(df, ['Wage scaled', 'Stadium capacity scaled'], 'Big six', 3.5, 'reg', 'Wages vs stadium size')","# Setup a MLP / Neural Network
layers = creat_layer(num_fully_connected =[200, 200], 
                     act_func_str = ""relu"")

nn_deeper = NeuralNetwork(layers)
# Train neural network
t0 = time.time()
nn_deeper.train(X_train, y_train, X_valid, y_valid, learning_rate= 0.5, 
         max_epochs=25, batch_size=500, y_one_hot=True, do_print = False)
 
print('Duration: {:.1f}s'.format(time.time()-t0))
plt_nn_valid_error(nn_deeper)
print_nn_test_error(nn_deeper)","for student in exam:
    print(student)",,Exploration Of Total Dataset->Wages,Exercise 1->Figure out a reasonable Network that achieves good performance,Dictionaries->Dictionary->Literal syntax
99161,"from sklearn.ensemble import RandomForestRegressor

X = StandardScaler().fit_transform(cars_data.drop(['normalized-losses'], axis=1))
pca = decomposition.PCA(n_components = 26)
XVV = pca.fit_transform(X)

y = cars_data['normalized-losses'].astype('float64')

plt.scatter(list(range(0,26)), pca.explained_variance_ratio_.cumsum()*100)
plt.xlabel('Number of Principal Components')
plt.ylabel('% of Variance Explained')
plt.title(""PCA Variance Explained"")
plt.show()","X_std = StandardScaler().fit_transform(X)
pca = PCA(n_components=30)
X_pca = pd.DataFrame(pca.fit_transform(X_std))

print(
    'The percentage of total variance in the dataset explained by each component from Sklearn PCA.\n',
    pca.explained_variance_ratio_.cumsum()
)","spy = spy.sort_values(by='Date')
spy.set_index('Date',inplace=True)
spy['Close'].plot(figsize=(16, 12))",,Project->Principal Component Analysis Tutorial->Performing PCA,"4->Let's Try the Multilayer perceptron Classifier on this dataset
3->About the data",Slides
325445,"import numpy as np

tensor = np.random.rand(10, 10)
A = np.random.rand(10, 10)
B = np.random.rand(10, 10)

# Filtering down
print(""All elements in the tensor less than 0.1"", tensor[tensor < 0.1])

print()

# Transforming
print(""Multiplying two tensors"", A * B)

print()

# summing a row
print(""Sum all the elements of a tensor row"", tensor[0].sum())

print()

# summing a column
print(""Sum all the elements of a tensor column"", tensor[:, 0].sum())","_X = np.random.rand(2,3,4)
X = tf.convert_to_tensor(_X)","fig = aplpy.FITSFigure('data/src_bin4_500-4000_gapsfilled_flux.img')
fig.show_colorscale(vmin=2e-9, vmax=6e-7, cmap='viridis', smooth=1, stretch='log')",,Data Frames->An introduction to the DataFrame Data Structure->An Introduction to Advanced Slicing,Math Part2,Python Tutorial - Part 2->Plotting a FITS Image with APLpy
304519,df.groupby('NOC')['Medal'].value_counts().head(10),"#replace values after 1992 with the sums and drop the winter months
drop_yrs = [1994, 1998, 2002, 2006, 2010, 2014]
rep_yrs = [1996, 2000, 2004, 2008, 2012, 2016]

#grab USA medals alone and add winter and summer after 1994
USA_grouped = df.loc[df.NOC == 'USA'].groupby('Year')['Medal'].count()
usa_split = USA_grouped[USA_grouped.index >= 1994].reset_index()
usa_split_winter = usa_split[usa_split.index % 2 == 0]
usa_split_summer = usa_split[usa_split.index % 2 != 0]
usa_split_winter = usa_split_winter['Medal'].tolist()
usa_split_summer = usa_split_summer['Medal'].tolist()
usa_win_sum_list = [x+y for x,y in zip(usa_split_winter, usa_split_summer)]

#drop winter and add the combined counts
USA_grouped = USA_grouped.drop(drop_yrs)
USA_grouped.loc[rep_yrs] = usa_win_sum_list

#grab RUS medals alone
RUS_grouped = df.loc[(df.NOC == 'RUS') | (df.NOC == 'URS')].groupby('Year')['Medal'].count()
rus_split = RUS_grouped[RUS_grouped.index >= 1994].reset_index()
rus_split_winter = rus_split[rus_split.index % 2 == 0]
rus_split_summer = rus_split[rus_split.index % 2 != 0]
rus_split_winter = rus_split_winter['Medal'].tolist()
rus_split_summer = rus_split_summer['Medal'].tolist()
rus_win_sum_list = [x+y for x,y in zip(rus_split_winter, rus_split_summer)]

#drop winter and add the combined counts
RUS_grouped = RUS_grouped.drop(drop_yrs)
RUS_grouped.loc[rep_yrs] = rus_win_sum_list

#check to make sure RUS was RUS the whole time and nothing else
rus_check = df['NOC'].value_counts()
RUS = df.loc[df.NOC == 'RUS'].groupby('Year').count()
anomaly = df[(df['Year']==1924) & (df['NOC']=='RUS')]
print(anomaly)
URS = df.loc[df.NOC == 'URS'].groupby('Year').count()

#grab the cold war years alone (1947 - 1991)
USA_cold = USA_grouped.loc[1947:1991]
RUS_cold = RUS_grouped.loc[1947:1991]

#plot USA and RUS medals over time
fig, (ax, ax1) = plt.subplots(1, 2, figsize=(15, 5))
USA_grouped.plot(x='Year', kind='line', ax=ax, marker='o')
RUS_grouped.plot(x='Year', kind='line', color='black', title='Russia and USA Medals over years', ax=ax, marker='o')
URS.Medal.plot(kind='line', color='red', ax=ax, marker='o')

ax.set_ylabel('Medal Count')
leg = ax.legend([""USA"", 'Russia', ""Soviet Union""])

USA_cold.plot(x='Year', kind='line', ax=ax1, marker='o')
RUS_cold.plot(x='Year', kind='line', ax=ax1, title='Cold War Medals', marker='o', color='red')

plt.ylabel('Medal Count')
plt.legend([""USA"", 'Russia'])

plt.show()","img = mpimg.imread(""test_images\\test5.jpg"")

dst = cv2.undistort(img, mtx, dist, None, mtx)

Minv = np.linalg.inv(M)

# Create an image to draw the lines on
warp_zero = np.zeros_like(warped_bin).astype(np.uint8)
color_warp = np.dstack((warp_zero, warp_zero, warp_zero))

# Recast the x and y points into usable format for cv2.fillPoly()
pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])
pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])
pts = np.hstack((pts_left, pts_right))

# Draw the lane onto the warped blank image
cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))

# Warp the blank back to original image space using inverse perspective matrix (Minv)
newwarp = cv2.warpPerspective(color_warp, Minv, (image.shape[1], image.shape[0])) 
# Combine the result with the original image
result = cv2.addWeighted(dst, 1, newwarp, 0.3, 0)

font = cv2.FONT_HERSHEY_DUPLEX

cv2.putText(result,""Radius of Curvature is {:.5}m"".format(curve_rad),(50,75), font, 2,(255,255,255),2)
cv2.putText(result,""Car is {:.3}m left of center"".format(left_of_center),(50,150), font, 2,(255,255,255),2)

plt.imshow(result)

resultc = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
cv2.imwrite(""output_images\\Full Pipeline Example.png"", resultc)",,Pandas Tutorial->Acessing Values,Olympic Capstone Project Final Report (1)->Who won the most medals at the Olympics?,"Advanced Lane Finder->Pipeline: Draw Found Lines, Curvature and Offset"
131619,"%matplotlib inline

import matplotlib
import matplotlib.pyplot as plt

import re

import pandas as pd
import numpy as np

from sklearn.preprocessing import MaxAbsScaler
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.decomposition import SparsePCA
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss

from xgboost.sklearn import XGBClassifier","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

%matplotlib inline

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

from sklearn.linear_model import LinearRegression
from sklearn.svm import LinearSVR
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import xgboost as xgb",direcs.nlargest(3),,2016-Telstra-Model->1. Initial setup and loading of data,"Predictive Modeling->Table of contents->Loading libraries <a name=""libraries""></a>",Challenge Set 3 Solomon->Challenge 5
437344,"#plot var importance
title = ""Variable Importance of Tuned ExtraTrees for Status Response""
savefig = ""results/variable_importance_status_extra_trees.png""
var_imp_plot(best_ext, train_preds, title, savefig)","model = LogisticRegressionCV()
model.fit(train_X, train_y)

plot_model_var_imp(model, train_X, train_y)","# A table containing the possible rolls of a 20-sided die.
possible_rolls_table = Table().with_columns(
    ""Roll"",
    ...
)
# A table containing the results of a single sample from possible_rolls_table.
roll_results_table = ...
# The roll in the table above.
roll_result = ...
# The modified roll.
modified_result = ...

# The next line just prints out your results in a nice way
# once you're done.  You can delete it if you want.
print(""On a modified roll of {:d}, Alice's action {}."".format(modified_result, ""succeeded"" if modified_result > 15 else ""failed""))",,Classification->Exploratory Analysis #6: Explore Categorical Variables to Classify Company Status->Extremeley Randomized Trees Classifier,Titanic Data Analysis->2. Data Understanding->4. Predict survival->4.2 Logistic Regression,Lab13->Lab 13->Dungeons and Dragons and Sampling
398685,"visualize = True
image = mpimg.imread('test_images/test1.jpg')
#i1 = process_image(image)
window_img, heat, small_window_img = process_image(image)
f, ax = plt.subplots(1, 3, figsize=(15,10))
ax[0].imshow(small_window_img)
ax[1].imshow(heat)
ax[2].imshow(window_img)","test_images = glob.glob('test_images/test*.jpg')
n = len(test_images)
plt.figure(figsize=(12,n*3))
grid = gs.GridSpec(n,2)

for idx, img_file in enumerate(test_images):
    #create lanes, analyse each image independently
    ll = Line(sample_img.shape[:2])
    rl = Line(sample_img.shape[:2])
    # read in image
    img = mpimg.imread(img_file)
    # plot image
    ax = plt.subplot(grid[idx*2])
    ax.imshow(process_image(img))
    # plot transformed image
    ax = plt.subplot(grid[(idx*2)+1])
    # process image
    ax.imshow(process_image(img, debug=True))","import nbloader
from sklearn.utils import shuffle
import tensorflow as tf
import tensorflow.contrib.slim as slim
from demo_process import *
print (""Packages loaded."")",,Pipeline->More details about the pipeline->Multiple detections & false positives,P4-Video->Video,Main->More tuning
43100,"#Read the csv file into a string
f = open(""US_births_1994-2003_CDC_NCHS.csv"", 'r')
f = f.read()

#Split the string on the newline character ('\n')
f = f.split('\n')

f[:10]","# Load the lines of the text into a list of strings
f = open('/root/data/artist_data.txt', 'r')
txt = f.read().split('\n')
f.close
txt[0:5]","unemployment = unemployment[[""Cantone"", ""Settembre 2017"", ""Settembre 2017.3""]]
unemployment.dropna(inplace=True)
unemployment.drop(27, axis=0, inplace=True)
unemployment[""Settembre 2017""] = unemployment[""Settembre 2017""].apply(pd.to_numeric)
unemployment.columns = [""Cantone"", ""Unemployed"", ""People searching for a job""]
unemployment.head()",,Basics->Introduction to the Dataset,Music->Music recommendations with PySpark->7. Use the Machine Learning Library,Interactive Viz-Checkpoint->Bonus 4->Question 3: showing unemployment rates for swiss and foreigns grouped in age categories
254961,kalman_dataset.to_csv(result_dataset_path+'/'+str(millis)+'_dataset_rest.csv'),"window_index = 0
milliseconds_per_instance = 100

ws = window_sizes[window_index]
print('Creating features with window size %d'%ws)
selected_predictor_cols = [c for c in dataset.columns if not 'label' in c]
print('doing mean numerical abstraction')
dataset_ws = NumAbs.abstract_numerical(dataset, selected_predictor_cols, ws, 'mean')
print(""doing std numerical abstraction"")
dataset_ws = NumAbs.abstract_numerical(dataset_ws, selected_predictor_cols, ws, 'std')
print('Number of features generated:%d'%(len(dataset_ws.columns)-features_before))
# print('now doing categorical abstraction')
# dataset_ws = CatAbs.abstract_categorical(dataset_ws, ['label'], ['like'], 0.03, ws, 2)
dataset_ws.to_csv(os.path.join(result_dataset_path,
                               str(milliseconds_per_instance)+'_dataset_ws_'+str(ws)+'.csv'))","# Load FabLab list
url = ""https://api.fablabs.io/v0/labs.json""
fablab_list = requests.get(url).json()

# Print a beautified version of the FabLab list
# print json.dumps(fablab_list, sort_keys=True, indent=4)

features = {}
features[""type""] = ""FeatureCollection""
features_list = []

for k,i in enumerate(fablab_list[""labs""]):
    # print ""Name:"", i[""name""]
    # print ""Latitude:"", i[""latitude""]
    # print ""Logitude:"", i[""longitude""]
    # print ""Link:"",i[""url""]	
    # print
    feature_dict = {}
    coord = []
    coord.append(i[""longitude""])
    coord.append(i[""latitude""])
    feature_dict[""type""] = ""Feature""
    feature_dict[""properties""] = {}
    feature_dict[""geometry""] = {}
    feature_dict[""properties""][""name""] = i[""name""]
    feature_dict[""properties""][""link""] = i[""url""]
    feature_dict[""geometry""][""type""] = ""Point""
    feature_dict[""geometry""][""coordinates""] = coord
    if i[""latitude""] and i[""longitude""] != ""null"": 
        features_list.append(feature_dict)

features[""features""] = features_list

# print ""Our GEOjson:""
# print json.dumps(features, sort_keys=True, indent=4)

with open('data.json', 'w') as outfile:
    json.dump(features, outfile)",,Untitled->SECTION 2 Data Filtering Cell (U can continue from here)->Section 2.3 Apply PCA to get principal component values.,Untitled->Section 3 : Chapter4 -  Feature Engineering,Fab Lab-Mapping--Python->Mapping the world of FabLabs->Load the data from fablabs.io API
94043,"def find_initial(name):
    names = name.split(' ')
    first_name = names[0]
    last_name = names[1]
    return first_name[0] + last_name[0]","# Split a full name into the first and last names
def split_name(name):
    names = name.split("" "")
    first_name = names[0]
    last_name = names[-1]
    # pack the variables into a tuple, then return the tuple
    return (first_name, last_name)

# Ask user for input
name = input(""Enter your full name: "")

# Unpack the returned tuples into first, last variables
# looks like the function returns 2 variables
first, last = split_name(name)

# Unpacked variables can be used separately
print(""First name: {:s}"".format(first))
print(""Last name: {:s}"".format(last))","len_count +=1
display(ai_button_len)

if len_count>=2:
    print(""Length of sequence is:"")
    print(len(""ACCAGGTT""))",,"Index->Map in Python->Finding what's common
Index->Functions with Arguments Lab->Finding what's common
Index->Finding what's common","3-3->Section 3.3: Tuples->Creating Tuples from User Input->Returning function values
3-3->Section 3.3: Tuples->Changing Tuple Elements->Merging tuples",Bioinformatics Intro
435017,"# Quick look
Male.head(5)",male.head(),"samples = [(X_train, y_train), 
           (X_test, y_test)]",,Project 2->Age Histograms->2. Spouse and Survival->2.1 A list of spouces,Data+And+Design+With+Python+-+Project+A->Distplot,Mlp Classification Regression-4->MLP
130545,"debug_size = 5000
test_size = 150000
min_doc_count = 10

subset_sizes = elasticsearch_base.get_els_subset_size(_es, ""unfiltered_stream"", ""hs_keyword_matches"")
doc_count = subset_sizes[""positive_count""] + subset_sizes[""negative_count""]

unfiltered_stream_pos_subset = elasticsearch_base.aggregate(_es, ""unfiltered_stream"", ""tokens.keyword"", False, positive_hs_filter, size=test_size, min_doc_count=min_doc_count)
unfiltered_stream_neg_subset = elasticsearch_base.aggregate(_es, ""unfiltered_stream"", ""tokens.keyword"", False, negative_hs_filter, size=test_size, min_doc_count=min_doc_count)

unfiltered_stream_pos_hs_freqs, unfiltered_stream_pos_vocab_freqs, unfiltered_stream_pos_hs_idfs, unfiltered_stream_pos_vocab_idfs = model_helpers.get_els_word_weights(unfiltered_stream_pos_subset[0], doc_count, hs_keywords)
_, unfiltered_stream_neg_vocab_freqs, _, unfiltered_stream_neg_vocab_idfs = model_helpers.get_els_word_weights(unfiltered_stream_neg_subset[0], doc_count, hs_keywords)

print(""Unfiltered tweet count: {0}\n"".format(doc_count))
print(""Clean: {0} | Has HS Keyword: {1}"".format(subset_sizes[""negative_count""], subset_sizes[""positive_count""]))

file_ops.write_json_file(""unfiltered_stream_pos_vocab_freqs"", settings.CW_SEARCH_PATH, unfiltered_stream_pos_vocab_freqs)
file_ops.write_json_file(""unfiltered_stream_neg_vocab_freqs"", settings.CW_SEARCH_PATH, unfiltered_stream_neg_vocab_freqs)

del unfiltered_stream_pos_subset
del unfiltered_stream_neg_subset

try:
    pprint(unfiltered_stream_pos_hs_freqs['faggot'])
except Exception as e:
    print(e)","debug_size = 5000
test_size = 200000
min_doc_count = 5

subset_sizes = elasticsearch_base.get_els_subset_size(_es, ""core_tweets"", ""hs_keyword_matches"")
doc_count = subset_sizes[""positive_count""] + subset_sizes[""negative_count""]

core_tweets_pos_subset = elasticsearch_base.aggregate(_es, ""core_tweets"", ""tokens.keyword"", False, positive_hs_filter, size=test_size, min_doc_count=min_doc_count)
core_tweets_neg_subset = elasticsearch_base.aggregate(_es, ""core_tweets"", ""tokens.keyword"", False, negative_hs_filter, size=test_size, min_doc_count=min_doc_count)

core_tweets_pos_hs_freqs, core_tweets_pos_vocab_freqs, core_tweets_pos_hs_idfs, core_tweets_pos_vocab_idfs = model_helpers.get_els_word_weights(core_tweets_pos_subset[0], doc_count, hs_keywords)
_, core_tweets_neg_vocab_freqs, _, core_tweets_neg_vocab_idfs = model_helpers.get_els_word_weights(core_tweets_neg_subset[0], doc_count, hs_keywords)

file_ops.write_json_file(""core_tweets_pos_vocab_freqs"", settings.CW_SEARCH_PATH, core_tweets_pos_vocab_freqs)
file_ops.write_json_file(""core_tweets_neg_vocab_freqs"", settings.CW_SEARCH_PATH, core_tweets_neg_vocab_freqs)

del core_tweets_neg_subset
del core_tweets_pos_subset

print(""Core tweet count: {0}\n"".format(doc_count))
print(""Clean: {0} | Has HS Keyword: {1}"".format(subset_sizes[""negative_count""], subset_sizes[""positive_count""]))","e = ERDDAP(
    server='https://data.ioos.us/gliders/erddap'
)",,4 Codeword Selection->Dependency2vec code word identification->Initialize params and objects->Examine Codeword Search Results->unfiltered_stream,4 Codeword Selection->Dependency2vec code word identification->Initialize params and objects->Examine Codeword Search Results->core_tweets,"Glider Data Via Erddap->Part 2 - Searching for Datasets
Example 06 - Glider Data Via Erddap->Part 2 - Searching for Datasets"
460838,"X, y_true = make_blobs(n_samples=300, centers=4,
                       cluster_std=0.60, random_state=0)
print (""Shape of data: "" + str(X.shape) + ""\n"")
print (""Example data: "")
X.view()","X = make_blob_dataset(300)
plt.scatter(X[:, 0], X[:, 1], color='b');","individual_returns['IGHS'] = np.NaN
for zc in individual_returns.index:
    if str(zc) in ca_income_change1.index:
        individual_returns.loc[zc,'IGHS'] = ca_income_change1.loc[str(zc),'IGHS']
        
individual_returns = individual_returns.dropna()",,Cluster Analysis->Read sample dataset from the library,Project Kalboussi Eya And Bresson Lea->I. Set up,Eda Growth->1.3 Analysis of Real Estate Value->1.3.3 IGHS and ZHVI Growth Correlations
98391,"import matplotlib.pyplot as plt
import google.datalab.bigquery as bq
train_df = bq.Query(""SELECT image_url, label FROM demos.coastline_train"").execute().result().to_dataframe()
train_df['label'].value_counts().plot(kind='bar', title='Training dataset');","#Complete the #FILL IN# gaps

# Create the training DF for the binary problem 3 vs. 5 
## Select digits 3 and 5
trainDFBinaryAux= scaledData.filter(scaledData.label.isin([3.0, 5.0]))
## Convert labels to 0 and 1
trainDFBinary = trainDFBinaryAux.select(changeLabels(trainDFBinaryAux.label).alias(""label""), trainDFBinaryAux.features)
trainDFBinary.show(5)
print 'Number of traning samples ' + str(trainDFBinary.count())

# Create the test DF for the binary problem 3 vs. 5 
## Select digits 3 and 5
testDFBinaryAux = scaledDataTest.filter(scaledDataTest.label.isin([3.0, 5.0]))
## Convert labels to 0 and 1
testDFBinary = testDFBinaryAux.select(changeLabels(testDFBinaryAux.label).alias(""label""), testDFBinaryAux.features)
testDFBinary.show(5)
print 'Number of test samples ' + str(testDFBinary.count())","print('The following is the list of prime numbers from o to 45')
print(primeNumbers(45))",,Coastline,Ml Lib Classification->**Classification tools in MLLib**->**2. Solving classification problems**,Assignment01 Spring2019 Vincent Roy-Checkpoint->Question 3->Answer 3
100615,"total = 0
count = len(normalized_budgets)
for value in normalized_budgets:
    total += int(value)
mean = total/count
print(""Mean= "" + str(mean))
median = (sorted(normalized_budgets))[len(normalized_budgets) // 2]
print(""Median= "" + str(median))


plt.hist(normalized_budgets, 20)
plt.title(""Distribution of Log10 Movie Budgets"")
plt.xlabel(""Budgets Log10"")
plt.show()

for value in all_boxoffice:
    total += int(value)
mean = total/count
print(""Mean= "" + str(mean))
median = (sorted(all_boxoffice))[len(all_boxoffice) // 2]
print(""Median= "" + str(median))


plt.hist(all_boxoffice, 20)
plt.title(""Distribution of Box Office Revenue (not normalized)"")
plt.xlabel(""Box Office Revenue"")
plt.show()","plt.hist(chain_converged)

chain_sorted = np.sort(chain_converged)
print(""Maximum a posteriori value:"", np.median(chain_sorted))
print(""1-sigma confidence level:"", 
      chain_sorted[int(0.16*len(chain_sorted))],
      chain_sorted[int(0.84*len(chain_sorted))])


plt.show()","img = Image.new(""RGB"", (1150,100), ""#FFFFFF"")
draw = ImageDraw.Draw(img)
scale = 255.
for i in range(1150):
    scale = scale - 255/1150
    draw.line((i,0,i,100), fill=(int(scale), int(scale), int(255/6*3)))",,Project->IMDB Reviews Text Analysis Regression->Distributions of Data,Distance Example->Bayesian Statistical Inference and Markov Chain Monte Carlo->Our results,Generate Gray Gradient Plot
216156,"import os, sys
sys.path.append(os.pardir)","import sympy as sy
sy.init_printing()",Image(filename='../Images/git/pull2.png'),,"Examples->PySDD: SDD Python Wrapper
Examples",03-Tof Real Problem->A Real Problem->Exercise 2->Soln,Git Tutorial->How to use Git for IC
154485,"#number of samples
K = 100","K = [(0,), (1,), (2,), (0,1), (0,2), (1, 2), (0, 1, 2)]","from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size = 0.2)

train_x = train[df_mean]
train_y = train.diagnosis

test_x = test[df_mean]
test_y = test.diagnosis",,"Demo->Demo of thesis ""Outdoor localization with Wi-Fi""->Positioning Algorithm",Morse Complex->Morse Complex,Uw Breast Cancer Dataset Analysis->Making Train and Test sets
397593,"movies_reviews_data['2grams features'] = gl.text_analytics.count_ngrams(movies_reviews_data['review'],2)
train_set, test_set = movies_reviews_data.random_split(0.8, seed=5)
model_2 = gl.classifier.create(train_set, target='sentiment', features=['1grams features','2grams features'], verbose=False)
result2 = model_2.evaluate(test_set)
print ""Accuracy        : "", result2[""accuracy""]
print ""Confusion Matrix: \n"", result2[""confusion_matrix""]","movies_reviews_data['1grams features'] = gl.text_analytics.count_ngrams(movies_reviews_data ['review'],1)
train_set, test_set = movies_reviews_data.random_split(0.8, seed=5)
model_1 = gl.classifier.create(train_set, target='sentiment', features=['1grams features'],verbose=False)
result1 = model_1.evaluate(test_set)
print ""Accuracy        : "", result1[""accuracy""]
print ""Confusion Matrix: \n"", result1[""confusion_matrix""]","from sklearn.tree import export_graphviz
with open(""bechdel.dot"", 'wb') as f:
    f = export_graphviz(treeclf, out_file=f, feature_names=feature_cols)",,Graphlab For Python->Bag of Words,Graphlab For Python->Bag of Words,Project Paper->I: Preliminary Charts
227717,"b[0,:3]",b[1:3],"sns.set_context('notebook', font_scale=1.45, rc={""lines.linewidth"": 3, ""figure.figsize"" : (7, 4.5)})",,Num Py->10. Finding elements satisfying conditions,"Boolean Operations And Indexing - Tutorial-Checkpoint->Boolean operations (AND, OR, and NOT)
Kenji Doya - Python Intro->Matplotlib->Lists
Boolean And If Statement - Tutorial->Boolean Operations and Indexing->Extracting Information From Data (Indexing)",Statistics Examples->Does paying attention to children help them to succeed?->Standard Wages Regression
155187,"grade = 46 #change for testing

# YOUR CODE HERE #",grade('tests/q1.py'),"df=df[['gender','SeniorCitizen','Partner','Dependents','tenure','PhoneService','InternetService','Contract','PaperlessBilling','PaymentMethod','MonthlyCharges','Churn']]
df.head()",,Boolean Logic And Control Statements-Checkpoint->Projects->Credit (advanced)->Advanced: `xor`,Example-Notebook->Using okgrade,"Final Project 04 Revised->Final Project 4->Random Forest Models
Fin Proj4-Alfredorojas->Step 2: Reading the data set"
158609,"df_rev['release_month'] = df_rev['release_date'].map(lambda x: x.month)
df_mean = df_rev.groupby(df_rev['release_month']).mean()

df_mean['revenue_adj'].plot(kind = 'line', legend = True,
                            figsize=(7.5,3.75), title = 'Average Adjusted Revenue Grouped by Release Month');
ax = df_mean['budget_adj'].plot(kind = 'line', legend = True);
ax.set_ylabel('Avg Revenue/Budget ($)');","_s = rev_agg_date['rev_len_diff'].rolling(60).mean()
ax = _s.plot()
ax.set_xlabel('Date')
ax.set_ylabel('Median size of revision')","atoms.species(""H"")",,Investigate A Dataset->Project: Movie Revenues Over Time and Relations to Properties->Initial Data Cleaning,Lab 1 - Revision Histories->Lab 1 - Revision Histories->Import modules and setup environment,Selecting Atoms
455603,"model_filename = './models/model1.h5'

if isfile(model_filename):
    print(""Weight file found, loading weights."")
    model_weights_file = model_filename
    model.load_weights(model_weights_file)
else:
    print(""No weight file found."")","ws3 = ws.WeightSource(iTrainedOn = ""MonteCristo"", iLang=""fr"",iSavePath = None)
model = ws3.load_model()","for english, spanish in e2s.items():
    s2e[spanish] = english",,Manuscrits->Train the model,Demo Notebook->Load existing model,Practice Exercises - Dictionaries
343728,"from msmbuilder.decomposition import tICA
tica_model = tICA(lag_time=1, n_components=4)
# fit and transform can be done in seperate steps:
tica_model = diheds.fit_with(tica_model)
tica_trajs = diheds.transform_with(tica_model, 'ticas/', fmt='dir-npy')

print(diheds[0].shape)
print(tica_trajs[0].shape)","tica_model = tICA(lag_time=10, n_components=2, kinetic_mapping=True)
tica_trajs = tica_model.fit_transform(scaled_data)

ax, side_ax = msme.plot_trace(tica_trajs[0][:, 0], window=10,
                              label='tIC1', xlabel='Timestep')
_ = msme.plot_trace(tica_trajs[0][:, 1], window=10, label='tIC2',
                    xlabel='Timestep', color='rawdenim', ax=ax,
                    side_ax=side_ax)",race_sex_raw,,Intro->Modeling dynamics of FS Peptide->Intermediate kinetic model: tICA,"Fs-Peptide-Example->Perform Dimensionality Reduction
Fs-Peptide-Example->Perform Dimensionality Reduction",Crime And Prisons Part3->Imprisonment by Race->Data Source
64097,"def test_orthogonal_vectors():
    assert not orthogonal_vectors([[1,2,3],[-2,1,-4],[-3,4,1]])
    assert orthogonal_vectors([[1,2,4],[-1,-1,7],[3,-1,1]])
    assert orthogonal_vectors([[0,25,-1],[0,4,10],[0,5,-3]])","def coo_spmv (n, R, C, V, x):
    """"""
    Returns y = A*x, where A has 'n' rows and is stored in
    COO format by the array triples, (R, C, V).
    """"""
    assert n > 0
    assert type (x) is list
    assert type (R) is list
    assert type (C) is list
    assert type (V) is list
    assert len (R) == len (C) == len (V)
    
    y = cse6040.dense_vector (n)
    
    # @YOUSE: Fill in this implementation
    pass
        
    return y","# HIDDEN
from sklearn.feature_extraction import DictVectorizer

items = walmart[['MarkDown']].to_dict(orient='records')
encoder = DictVectorizer(sparse=False)
pd.DataFrame(
    data=encoder.fit_transform(items),
    columns=encoder.feature_names_
)",,Hw2->Orthogonal Vectors,"19--Sparse-Matrix->CSE 6040, Fall 2015 [19]: Sparse matrix storage->Alternative formats: COO and CSR formats",Feature One Hot->The One-Hot Encoding
291555,"list(map(f, [2,3,4]))","F_temps = map(fahrenheit, temp)
F_temps = list(F_temps)","import numpy as np
import matplotlib
import matplotlib.pyplot as plt

%matplotlib notebook",,Standard Library->(And a quick reminder),Map->map()->Store permanantly in a list,"Comp Plot9
Comp Plot1 1"
41408,cars = cars.reset_index(drop=True),"df_car = df.groupby('type')['price'].mean()

df_car = pd.DataFrame(df_car).reset_index()
df_car","data_byyear = pd.DataFrame()
data_byyear['Terrorists'] = data.groupby(['Year'])['Terrorists'].sum()
data_byyear['Innocents'] = data.groupby(['Year'])['Innocents'].sum()
data_byyear['Injured'] = data.groupby(['Year'])['Injured'].sum()
data_byyear['No of strikes'] = data.groupby(['Year'])['No of Strike'].sum()
data_byyear['Accuracy'] = data.groupby(['Year'])['Accuracy'].mean()
data_byyear['Total Died'] = data.groupby(['Year'])['Total Died'].sum()
data_byyear['Avg Innocents Killed'] = data.groupby(['Year'])['Innocents'].mean()",,Basics->Predicting car prices->Multivariate model,1->Decision Trees Lab->1.1 Manually build the decision tree,Pakistan Drone Attacks->1. Cleaning Data->3.2 Making the Time-Series Stationary->1.5 Creating Year-wise dataset
275591,dfpnb.isnull().sum(),dfpnb.columns,"lists = [[a_word.upper(), a_word[::-1].upper(), len(a_word)] for a_word in some_words]
lists",,Capstone Iii->Capstone Project: Network and Host Log Analysis->Table of Contents->2. Describe the Data,Capstone Iii->Capstone Project: Network and Host Log Analysis->Table of Contents->5. Make the column names more descriptive,Md-1-Slices For If-Checkpoint
488128,"movie_names = rating_crosstab.columns
movies_list = list(movie_names)
star_wars = movies_list.index('Star Wars (1977)')
star_wars","my_log_pred = pd.DataFrame()
my_log_pred[""pred""] = [""No"" if x < .5 else ""Yes"" for x in m1.predict()]
my_log_pred[""actual""] = [""No"" if x == 0 else ""Yes"" for x in war[""start""]]
conf_log = pd.crosstab(my_log_pred[""pred""], my_log_pred[""actual""])
conf_log","# create transformer
transformer = caffe.io.Transformer({'data': imagenet_net.blobs['data'].data.shape})
transformer.set_transpose('data', (2,0,1))  # move image channels to outermost dimension
transformer.set_mean('data', mu)            # subtract the dataset-mean value in each channel (mu is BGR)
transformer.set_raw_scale('data', 255)      # rescale from [0, 1] to [0, 255]
transformer.set_channel_swap('data', (2,1,0))  # swap channels from RGB to BGR",,Model-Based Collaborative Filtering->Isolating Star Wars From the Correlation Matrix,Lab4->Lab4 - Predicting Binary Outcomes->Question 3,Tutorial Caffe Bids->6. Test CellNet->Instantiate a new CellNet and initialize it with our pre-trained weights
45463,"def print_first_point(filename):
    """"""
    This function prints and returns the first data point (second row) from
    a csv file that includes a header row.
    """"""
    # print city name for reference
    city = filename.split('-')[0].split('/')[-1]
    print('\nCity: {}'.format(city))
    
    with open(filename, 'r') as f_in:
        ## TODO: Use the csv library to set up a DictReader object. ##
        ## see https://docs.python.org/3/library/csv.html           ##
        trip_reader = csv.DictReader(f_in)
        
        
        ## TODO: Use a function on the DictReader object to read the     ##
        ## first trip from the data file and store it in a variable.     ##
        ## see https://docs.python.org/3/library/csv.html#reader-objects ##
        first_trip = next(trip_reader)
        pprint(first_trip)
        
        ## TODO: Use the pprint library to print the first trip. ##
        ## see https://docs.python.org/3/library/pprint.html     ##
        
    # output city name and first trip for later testing
    return (city, first_trip)

# list of files for each city
data_files = ['./data/NYC-CitiBike-2016.csv',
              './data/Chicago-Divvy-2016.csv',
              './data/Washington-CapitalBikeshare-2016.csv',]

# print the first trip from each file, store in dictionary
example_trips = {}
for data_file in data_files:
    city, first_trip = print_first_point(data_file)
    example_trips[city] = first_trip
    
pprint(example_trips)","def print_first_point(filename):
    """"""
    This function prints and returns the first data point (second row) from
    a csv file that includes a header row.
    """"""
    # print city name for reference
    city = filename.split('-')[0].split('/')[-1]
    print('\nCity: {}'.format(city))
    
    with open(filename, 'r') as f_in:
        ## TODO: Use the csv library to set up a DictReader object. ##
        ## see https://docs.python.org/3/library/csv.html           ##
        trip_reader = csv.DictReader(f_in)
        
        ## TODO: Use a function on the DictReader object to read the     ##
        ## first trip from the data file and store it in a variable.     ##
        ## see https://docs.python.org/3/library/csv.html#reader-objects ##
        first_trip =next(trip_reader)
        
        ## TODO: Use the pprint library to print the first trip. ##
        ## see https://docs.python.org/3/library/pprint.html     ##
        pprint(first_trip)
        
    # output city name and first trip for later testing
    return (city, first_trip)

# list of files for each city
data_files = ['./data/NYC-CitiBike-2016.csv',
              './data/Chicago-Divvy-2016.csv',
              './data/Washington-CapitalBikeshare-2016.csv',]

# print the first trip from each file, store in dictionary
example_trips = {}
for data_file in data_files:
    city, first_trip = print_first_point(data_file)
    example_trips[city] = first_trip","print ""total volume:"", A_max * min_thickness(visc_oil)",,Bike Share Analysis,Analyzing Bike Share System->resources:,Chris Spreading->Compute with some real numbers:
183613,"# Code
len(twitter_c[twitter_c['expanded_urls'].isnull()]['expanded_urls'])
## There are 59 null records","# Code
len(twitter_c[twitter_c['expanded_urls'].isnull()]['expanded_urls'])
## There are 59 null records",female_mean = female.mean(),,Wrangle Act->Tidiness Issues->DEFINE->Define,Wrangle Act->Tidiness Issues->DEFINE->Define,"Sliderule Dsi Inferential Statistics Exercise 1
Inferential Statistics Exercise 1 Akshata->What is the true normal human body temperature?"
134990,annual_returns.head(),"DailyReturn = Daily.drop(Daily.columns[[0,2,3]], axis=1)
MonthlyReturn = Monthly.drop(Daily.columns[[0,2,3]], axis=1)
AnnualReturn = Annual.drop(Daily.columns[[0,2,3]], axis =1)
DailyReturn.head(5)",set2.issuperset(set1),,Modern Portfolio Theory->Modern Portfolio Theory: Creating The Most Efficient Portfolio,Google->Analysis & Interpretation->Stock Return,"03->Data Structures->Sets->Built in List Functions
03->Sets
03-Data Structures->Data Structures->Sets->Copying a list"
216178,Source(fabcd.dot()),"for t in range(Ny):
    v[t] = y[t] - Z.dot(a[t])
    F[t] = Z.dot(P[t]).dot(Z.T) + H
    Finv = np.linalg.inv(F[t])
    aa[t] = a[t] + P[t].dot(Z.T).dot(Finv).dot(v[t])
    PP[t] = P[t] - P[t].dot(Z.T).dot(Finv).dot(Z).dot(P[t])
    K[t] = T.dot(P[t]).dot(Z.T).dot(Finv)
    L[t] = T - K[t].dot(Z)
    if t + 1 < Ny:
        a[t + 1] = T.dot(a[t]) + K[t].dot(v[t])
        P[t + 1] = T.dot(P[t]).dot(L[t].T) + R.dot(Q).dot(R.T)",list(string.ascii_lowercase)[0],,"Examples->PySDD: SDD Python Wrapper->Vtree->Example: Load from CNF file
Examples->SDD Manager",Kalman Integration Tests->Let's make a posterior->Prior,Script For Cleaning And Analyzing 1111 Test Data-Fall 2018 Exam2 - Copy
130305,"z = np.linspace(0,100,50)
z",z * np.sqrt(2 / 1000),"L = len(x)

f1 = int(19500*L/samplerate)
f2 = int(20500*L/samplerate)

f = range(len(x))
for k in range(len(x)):
    f[k] = 1.0e-3*f[k]*samplerate/len(x)",,09 Numpy Ii Scipy->Numpy II->Review,Variance-Reduction->Overfitting and Bias,20180110-S Montparnasse Beacon->Analysing ultrasound beacons in the wild->Let's plot the signal->FFT analysis
431925,"assert is_features_normal, 'You skipped the step to normalize the features'
assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'

# Get randomized datasets for training and validation
train_features, valid_features, train_labels, valid_labels = train_test_split(
    train_features,
    train_labels,
    test_size=0.05,
    random_state=832289)

print('Training features and labels randomized and split.')","import os
import csv
import cv2
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer
from collections import Counter
# Allow image embeding in notebook
%matplotlib inline

print('All modules imported.')

# Set flags for feature engineering.  This will prevent you from skipping an important step.
is_features_normal = False
is_labels_encod = False","def load_emotion (folder):
    image_files = os.listdir(folder)
    numb_image_files = len(image_files)
    numb_processed_image = 0
    dataset = np.ndarray(shape = (numb_image_files, IMAGE_SIZE, IMAGE_SIZE, RGB),
                         dtype = np.float32)
    for image_file in glob.glob(folder + '/*.png'):
        try:
            image_data = ndimage.imread(image_file).astype(float)
            # Skip any image that all pixels sharing one value
            if (np.all(image_data == image_data[0])):
                continue
            dataset[numb_processed_image, :, :] = image_data
            numb_processed_image += 1
        except IOError as e:
            print ('Could not read: ' + image_file)
    # Rescale dataset to actually succesfully loaded image
    dataset = dataset[0:numb_processed_image, :, :, :]
    return dataset

def make_pickle (label_names):
    pickle_files = []
    for label in label_names:
        dataset = load_emotion(label)
        emotion_name = label_to_emotion[int(label)]
        plot_images(dataset[:25], emotion_name); 
        pickle_name = label + '.pickle'
        with open (pickle_name, 'wb') as f:
            pickle_files.append(pickle_name)
            pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)
    return pickle_files
            
label_names = [str(i) for i in range(NUMB_EMOTIONS)]
pickle_files = make_pickle(label_names)",,"Lab
Lab
Lab
Lab->An example of one-hot encoding
Intro To Tensorflow
Intro To Tensorflow
Intro To Tensorflow
Intro To Tensorflow
Intro To Tensorflow
Dnn Lab
Dnn Lab
Mnist Tensorflow->Groom the Data
Not Mnist Tensor Flow
Dropout Not Mnist->Build a Neural Network with Dropout->1. Pre-process Data
Edge Model->Simple AI model for Embedded AI & Edge computing [+info](https://blog.crespum.eu/2018/05/28/spaceup-bcn-18-edge-computing/)->Data pre-processing",Traffic Signs Recognition->Self-Driving Car Engineer Nanodegree,Instruction->PREPROCESSING FACIAL EMOTION DATASET->PREPROCESSING
320508,"diff_in_rates = Table().with_column('Diff', difference_in_rates).hist(bins = np.arange(-1, 1.1, 0.05))","uniform_samples = Table(['Increases minus decreases'])
for i in np.arange(5000):
    sample = np.diff(uniform.sample_from_distribution('Chance', num_changes).column(2))
    uniform_samples.append(sample)

    
uniform_samples.hist(0, bins=np.arange(-200, 201, 25))",5 * A,,Exploration->Impact of Introduction of Ethereum on the Price of Bitcoin,Project3->4. Further evidence->The rest of the states,"06-Scientific-Libraries->Chapter 06: An overview of scientific libaries->Numpy - http://www.numpy.org/
02-Linear-Algebra-With-Numpy->Linear algebra with Numpy"
426735,"# visualize the distribution of missing value counts to find these columns
azdias_summary.hist(column='NaN Occupancy Rate', bins = 20)","# Plot different bins using different styles
axis = hist[hist.frequencies > 5].plot(label=""High"", alpha=0.5)
hist[1:-1][hist[1:-1].frequencies <= 5].plot(ax=axis, color=""green"", label=""Low"", alpha=0.5)
hist[[0, -1]].plot(ax=axis, color=""red"", label=""Edge cases"", alpha=0.5)
hist.plot(histtype=""scatter"", ax=axis, s=hist.frequencies / 10, label=""Scatter"")
# axis.legend();     # Does not work - why?","import warnings
warnings.simplefilter('ignore')

import numpy as np
import matplotlib.pyplot as plt
import seaborn

import paramnormal

clean_bkgd = {'axes.facecolor':'none', 'figure.facecolor':'none'}
seaborn.set(style='ticks', rc=clean_bkgd)",,Arvato Project Workbook Final->Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services->Part 0: Get to Know the Data->Part 0.1: Load the data set->We may decide to drop columns with higest NaN occupancy rate,Tutorial->Physt tutorial->Plotting,Fitting->Fitting distributions to data with `paramnormal`.
370117,"# A set with non-unique elements?
my_set = {""Hello"", ""Hello"", 9.0, 9.0}","my_set=set([1,3,5,6])","params = {
     'base_score': ave_y,
     'booster': 'gbtree',
     'colsample_bytree': 0.9,
     'eta': 0.01,
     'eval_metric': 'auc',
     'max_depth': 12,
     'nthread': 4,
     'objective': 'binary:logistic',
     'reg_alpha': 0.001,
     'reg_lambda': 0.01,
     'seed': 12345,
     'silent': 0,
     'subsample': 0.1}

watchlist = [(rsontrain_dm, 'train'), (rsonvalid_dm, 'eval')]

rson_model = xgb.train(params, 
                       rsontrain_dm, 
                       400,
                       early_stopping_rounds=50,
                       evals=watchlist, 
                       verbose_eval=True)",,Sets->Initializing a Set,Ng Ps Generic Python->Data Structures->Methods,Binary-Classification-One-Function-Noise-Valid->Local Explanations (variable importance) at Several Percentiles of Model Predictions->Explanation at three deciles->Utility function that displays the LIME result at the choosen decile
16730,"# imports jasons, pandas as pd, and json_normalize
import json
import pandas as pd
from pandas.io.json import json_normalize","for index, row in json_to_panda.iterrows():
    if(row['name']== ''):
        print(row['name'])
        for index,row1 in new_json_data.iterrows():
            if(row1['code']==row['code']):
                row['name'] = row1['name']
print(json_to_panda.name.value_counts().head(10))","import sys
print(sys.version)
print(sys.modules['six'])",,Sliderule Dsi Json Exercise->JSON exercise,Json Exercise->Task 3,01 Notebook Basics->Tutorial 1: Basics about Jupyter Notebooks
264899,"plot_ratio(QS_ratio_region, 'international/total', 'international/total by Region')","plot_ratio(THE_ratio_region, 'faculty/students', 'faculty/students by Region')","test=pd.concat([test_numeric,test_dummy],axis=1)
test.shape",,Homework2->Question 4->Region->Best international ratio,Homework2->Question 1->University->Best faculty/student ratio,Housing Final->Machine Learning Project->Modeling->IV. Impute Missing Values
317114,"if 'united state':
    print(True)
else:
    print(False)","# get that ratio of filter_range/all_houses
total_filtered = len(filtered_sales)
total_sales = len(sales['sqft_living'])
print(""Float: {}"").format(float(total_filtered) / total_sales)
# let's be rational
from fractions import Fraction
print(""Rational: {}"").format(Fraction(total_filtered, total_sales))","last_letters = defaultdict(list)
words = nltk.corpus.words.words('en')

for word in words:
    key = word[-2:]
    last_letters[key].append(word)
    
last_letters['ly']",,Midterm,Foundations Week2 Assignment->Filtering Data,Nltk Book Tutorial->Chapter 05: Learning to Classify Text
192055,"### Train your model here.
### Calculate and report the accuracy on the training and validation set.
### Once a final model architecture is selected, 
### the accuracy on the test set should be calculated and reported as well.
### Feel free to use as many code cells as needed.
x = tf.placeholder(tf.float32, (None, 32, 32, 1))
y = tf.placeholder(tf.int32, (None))
one_hot_y = tf.one_hot(y, 43)

rate = 0.005

logits = LeNet(x)
cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)
loss_operation = tf.reduce_mean(cross_entropy)
optimizer = tf.train.AdamOptimizer(learning_rate = rate)
training_operation = optimizer.minimize(loss_operation)","logits = LeNet(x)
cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = one_hot_y)
loss_operation = tf.reduce_mean(cross_entropy)
optimizer = tf.train.AdamOptimizer(rate)
training_operation = optimizer.minimize(loss_operation)","#plotting probability density function vs histogram for heart_rate
a = df.heart_rate.plot(kind = 'hist', color = 'black')
b = df.heart_rate.plot(kind = 'density', color = 'red', secondary_y = True)

a.set_ylabel('Frequency',color='black')
b.set_ylabel('Density',color='red')
plt.title('probability density function vs histogram for heart_rate:')
print('\n normality test results of heart_rate distribution: \n statistic= %f, p-value = %f' %st.normaltest(df.heart_rate))

plt.show()

#plotting probability density function vs histogram for temperatures of male and female samples
plt.subplot(121)
a=df.heart_rate[df.gender == 'M'].plot(kind = 'hist', color = 'blue')
b=df.heart_rate[df.gender == 'M'].plot(kind = 'density', color = 'red', secondary_y = True)
a.set_ylabel('Frequency',color='blue')
b.set_ylabel('Density',color='red')
plt.title('heart_rate distribution for ""Male"" sample:', color = 'blue')

plt.subplot(122)
a=df.heart_rate[df.gender == 'F'].plot(kind = 'hist', color = 'green')
b=df.heart_rate[df.gender == 'F'].plot(kind = 'density', color = 'red', secondary_y = True)
a.set_ylabel('Frequency',color='green')
b.set_ylabel('Density',color='red')
plt.title('heart_rate distribution for ""Female"" sample:',color = 'green')


plt.subplots_adjust(wspace= 0.75, hspace = 1)",,Traffic Sign Classifier->Training Pipeline,Traffic Sign Classifier Aws->Training Pipeline,Sliderule Dsi Inferential Statistics Exercise 1-Mahesh Yerra->What is the True Normal Human Body Temperature?
470061,"from sklearn.preprocessing import StandardScaler

sc_x = StandardScaler()
sc_y = StandardScaler()
X_std = sc_x.fit_transform(X)
y_std = sc_y.fit_transform(y)","from sklearn.preprocessing import StandardScaler

sc_x = StandardScaler()
sc_y = StandardScaler()
X_std = sc_x.fit_transform(X)
y_std = sc_y.fit_transform(y[:, np.newaxis]).flatten()
# StandardScaler expects 2D-array. np.newaxis adds second dimension, flatten() removes it again",bd_melt.head(2),,Ch10->Chapter 10: Predicting Continuous Target Variables with Regression Analysis->Single variable regression adapting Adeline,"Chapter 10, Part 1->Regression parameter estimation with Gradient Descent->Simple regression",P2-Solution
183522,"#Download programmatically tweet image predictions'image_predictions'

folder_name = 'image_predictions'
if not os.path.exists(folder_name):
    os.makedirs(folder_name)
    
    
url = 'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv'
response = requests.get(url)
    
with open(os.path.join(folder_name,
                       url.split('/')[-1]), mode = 'wb') as file:
    file.write(response.content)","with open(os.path.join(folder_name,
                      url.split('/')[-1]), mode='wb')as file:
    file.write(response.content)","import numpy as np
import pandas as pd
import scipy as scipy
from scipy.stats import ttest_ind
import matplotlib.pyplot as plt
import seaborn as sns
import random
%matplotlib inline
#df = pd.read_csv(""C:\\Users\\Peter\\Desktop\\Prep Captstone\\StudentAlcohol\\student-por.csv"", low_memory = False) 
df = pd.read_csv(""C:\\Users\\Peter\\Desktop\\Prep Captstone\\StudentAlcohol\\student-mat.csv"", low_memory = False)",,Wrangle Act->Data Wrangling of 'WeRateDogs' by Ashish->Gather Data:,Wrangle Act->Investigate the identity of the dog with the highest 'rating_numerator',Student+Alcohol+Consumption8
243559,"plt.plot(np.cumsum(sklearn_pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
plt.show()","plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');
plt.show()","import numpy as np
import matplotlib.pyplot as plt
import sklearn.ensemble as en

import pickle

%matplotlib inline",,P5->Principal Component Analysis,Santander Value Prediction Challenge->Santander Value Prediction Challenge->4.Feature Selection->4.3 PCA transformation,Example
144927,"# Useful modules, make sure you can import them before proceeding further
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import pandas as pd
import json
import string
import os
from sklearn import dummy
import seaborn as sbn
from pandas.io.json import json_normalize
from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.preprocessing import Normalizer
from sklearn import preprocessing, neighbors
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from nltk.stem.snowball import EnglishStemmer
from nltk.corpus import stopwords
import warnings
warnings.filterwarnings(""ignore"")

# Random seed to make results reducible
SEED = 1","# Useful modules, make sure you can import them before proceeding further
import matplotlib.pyplot as plt
%matplotlib inline
import numpy as np
import pandas as pd
import json
import string
import os
from sklearn import dummy
import seaborn as sbn
from pandas.io.json import json_normalize
from sklearn.model_selection import GridSearchCV
from sklearn import svm
from sklearn.preprocessing import Normalizer
from sklearn import preprocessing, neighbors
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from nltk.stem.snowball import EnglishStemmer
from nltk.corpus import stopwords
import warnings
warnings.filterwarnings(""ignore"")

# Random seed to make results reducible
SEED = 1","check_unique = df.groupby(""title"").item_id.nunique()
check_unique[check_unique > 1]",,Script->Project Baudet Baligand,Script->Project Baudet Baligand,22-Recommender-Systems-Model-Based-Collaborative-Filtering->Recommender Systems->explicit feedback model/ implicit feedback model
111131,"tup1 = (1, 2, 3)
print(hash(tup1)) # hashable","tup2 = (1, 2, [3, 4])
print(hash(tup2)) # not hashable, contains list!","stop = stopwords.words('english') # Common words
porter = PorterStemmer() # Getting root of words
char3=stop[:17] # Getting 1st and 2nd person pronouns
stop=stop[17:116]+stop[118:] 

def tokenizer(text):
    text = re.sub('<[^>]*>', '', text)
    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text.lower())
    text = re.sub('[\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')
    text = [w for w in text.split() if w not in stop]
    tokenized = [porter.stem(w) for w in text]
    return text",,"1->Difference in id, hash, and \__eq\__ and ==->Relative immutability of tuples","1->Difference in id, hash, and \__eq\__ and ==->Relative immutability of tuples",Neural Networks->Preprocessing Text Data
405459,"eigen_values, eigen_vectors = np.linalg.eig(cov_mat)
print('Eigen vectors shape:',eigen_vectors.shape)
print('Eigen vectors:\n%s\n' % eigen_vectors)
print('Eigen values shape:',eigen_values.shape)
print('Eigen values:\n%s ...' % eigen_values[:5])","# calculate and print the eigenvalues and eigenvectors
evals, evecs = np.linalg.eig(cov_mat)
print('Eigenvalues:\n', evals)
print('\nEigenvectors:\n', evecs)","simu_1.write(outdir='./data_ebl/', use_sherpa=True)
simu_2.write(outdir='./data_ebl/', use_sherpa=True)",,Pca->Principal Component Analysis - PCA - Dimensionality Reduction->PCA Step by Step->2. Obtain Eigenvector and Eigenvalues->Obtain Eigenvectors and Eigenvalues from Covariance Matrix,Pca-Iris->Principal Component Analysis using the Iris dataset->1. Data preparation->Visualization of the data,Tutorial Ebl->Extragalactic use case: constrain an EBL scale factor->Gammapy simulations->Simulations
401827,buttons.read(),"nl_input = widgets.Textarea(value='conflict causes displacement')
read_nl = widgets.Button(description='Read')
read_nl.on_click(on_nl)
display(nl_input, read_nl)","# get the two numbers to add from the command prompt
intNum1 = 5
intNum2 = 4

# establish two tensors, one for each input number
num1 = tf.Variable(intNum1)
num2 = tf.Variable(intNum2)

# establish graph
sum = tf.add(num1, num2)

# note that this shows information about sum, but does NOT evaluate anything yet
print(""sum = "" + str(sum))",,"Pynqtutorial Gpio Mmio->PYNQ tutorial: using GPIO and MMIO->MMIO class->1. Reading the push buttons
Pynq Tutorial Gpio Example->Pynq GPIO tutorial using MMIO->Read from pushbuttons
2 Axi Gpio->Using AXI GPIO with PYNQ->AxiGPIO class->1. Controlling the switches and push-buttons",Paper Reading->Interactive reading and assembly->Extend the model with natural language,2->TensorFlow 101 (Really Awesome Intro Into TensorFlow)->Add Two Numbers via TensorFlow
323540,"def cofiCostFunc(params, Y, R, num_users, num_movies, num_features, lambda_):
    #COFICOSTFUNC Collaborative filtering cost function
    #   [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...
    #   num_features, lambda) returns the cost and gradient for the
    #   collaborative filtering problem.
    #

    # Ensure we have a 1 row matrix
    params = params.reshape(1, -1)
    # Unfold the U and W matrices from params
    X = params[0, :num_movies*num_features].reshape((num_movies, num_features));
    Theta = params[0, num_movies*num_features:].reshape((num_users, num_features));
    
    # You need to return the following values correctly
    J = 0;
    X_grad = np.zeros(X.shape);
    Theta_grad = np.zeros(Theta.shape);

    # ====================== YOUR CODE HERE ======================
    # Instructions: Compute the cost function and gradient for collaborative
    #               filtering. Concretely, you should first implement the cost
    #               function (without regularization) and make sure it is
    #               matches our costs. After that, you should implement the 
    #               gradient and use the checkCostFunction routine to check
    #               that the gradient is correct. Finally, you should implement
    #               regularization.
    #
    # Notes: X - num_movies  x num_features matrix of movie features
    #        Theta - num_users  x num_features matrix of user features
    #        Y - num_movies x num_users matrix of user ratings of movies
    #        R - num_movies x num_users matrix, where R(i, j) = 1 if the 
    #            i-th movie was rated by the j-th user
    #
    # You should set the following variables correctly:
    #
    #        X_grad - num_movies x num_features matrix, containing the 
    #                 partial derivatives w.r.t. to each element of X
    #        Theta_grad - num_users x num_features matrix, containing the 
    #                     partial derivatives w.r.t. to each element of Theta
    #

    J = (1/2) * np.sum(np.multiply(np.power(np.dot(X, Theta.T) - Y, 2), R));
    J_Reg = (lambda_ / 2) * np.sum(np.power(Theta, 2)) + (lambda_ / 2) * np.sum(np.power(X,2));
    J = J + J_Reg
    
    X_grad = np.dot(np.multiply(R, np.dot(X, Theta.T) - Y), Theta) + lambda_ * X;
    Theta_grad = np.dot(np.multiply(R, (np.dot(X, Theta.T) - Y)).T, X) + lambda_ * Theta;
    # =============================================================

    grad = np.hstack([X_grad.flatten(), Theta_grad.flatten()]);

    return [J, grad]","def cofiGradientFunction(params, Y, R, num_users, num_movies, num_features, lambdaval):
    # Compute the Gradients X_grad, Theta_grad
    # Loop over movies to compute X_grad for each movie
    # Unfold the X and Theta matrices from params
    X = params[0:num_movies*num_features].reshape(num_movies, num_features)
    Theta = params[num_movies*num_features:].reshape(num_users, num_features)
    # Initialize Gradients
    X_grad = np.zeros(X.shape)
    Theta_grad = np.zeros(Theta.shape)
    # Loop over movies and compute X_grad for each movie
    for i in range(0,num_movies):
        Z = np.dot(((np.dot(X[i,:],Theta.T) - Y[i,:])*R[i,:]),Theta) + lambdaval * X[i,:]
        X_grad[i,:] = Z
    # End of loop
    # Loop over users to compute Theta_grad for each user
    for j in range(0,num_users):
        Z = np.dot(((np.dot(Theta[j,:],X.T) - Y[:,j].T)*R[:,j]),X) + lambdaval * Theta[j,:]
        Theta_grad[j,:] = Z
    # End of loop
    grad = np.concatenate((X_grad, Theta_grad)).flatten()    
    return np.asarray(grad)","clf = svm.SVC(kernel='rbf')
clf.fit(X_train_bal, y_train_bal)",,Ex8->Collaborative Filtering->============ Part 2: Collaborative Filtering Cost Function ===========,Recommender->Recommender System->Part 1: Loading the movie rating dataset,Ensemble Analysis->3. Bring out the Models->3.4 SVM->Exploring Success
240535,result_tempfilt = temporal_filter.run(),"nr = InitNornir(config_file=""handling_connections/config.yaml"")
rtr = nr.filter(name=""rtr00"")
r = rtr.run(
    task=napalm_get,
    getters=[""facts""]
)
print_result(r)","samples = fit.extract(permuted=True)  # return a dictionary of arrays
y_hat = np.mean(samples[""alpha""].T + np.dot(X_test, samples[""beta""].T), axis=1)",,Lecture Live->Motion Correction->Temporal filtering of FMRI Data,Handling Connections->How to handle connections to devices,05 - Regression Models - Part 2-Solutions->Week 5 - Regression models->STAN: Train on a slightly larger subset of the data
248427,"#cars = shuffle(cars, random_state=0)
rand_state = np.random.randint(0, 100)

train_valid_cars, test_cars = train_test_split(cars, test_size=0.1, random_state=rand_state) 
train_cars, valid_cars = train_test_split(train_valid_cars, test_size=0.2, random_state=rand_state) 

noncars = shuffle(noncars, random_state=0)
train_valid_noncars, test_noncars = train_test_split(noncars, test_size=0.1) 
train_noncars, valid_noncars = train_test_split(train_valid_noncars, test_size=0.2) 

print('Number of samples in cars training set: ', len(train_cars))
print('Number of samples in notcars training set: ', len(train_noncars))

print('Number of samples in cars validation set: ', len(valid_cars))
print('Number of samples in notcars validation set: ', len(valid_noncars))

print('Number of samples in cars test set: ',len(test_cars))
print('Number of samples in notcars test set: ',len(test_noncars))","X = np.vstack((features_car, features_noncar)).astype(np.float64)
y = np.hstack((np.ones(len(features_car)), np.zeros(len(features_noncar))))
# Split data into random training and test sets
rand_state = np.random.randint(0, 99)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=rand_state)
print('Hog properties:')
print('orient = ', orient)
print('pix_per_cell = ', pix_per_cell)
print('cell_per_block = ', cell_per_block)
print('Lenght of feature vector:', len(X_train[0]))",departure_time = 1493283600,,"Readme->1. Data Exploration->* Split dataset into 70% training, 20% validation and 10% testing set*",Vehicle Detection 1->Vehicle Detection and Tracking Project,Transport+Problems+Akademichesky->Define departure time as timestamp - choose 9am
312335,sam.run(),sam.run(),"%matplotlib inline

import math
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
import numpy as np
import scipy as sp
import scipy.io
import random
import sklearn.preprocessing
import csv
from numpy import linalg as LA
import scipy.stats.mstats as mstats
from sklearn.feature_extraction import DictVectorizer
import pickle
import time

import pdb",,Basic Tutorial->Running SAM,Basic Tutorial->Running SAM,Generator
254141,df[df['Count_3-6_months_late'].isnull()].head(),"groupby_month = df.groupby('Month').count()
groupby_month.head()","## LOF - Local Outlier Filter
from sklearn.neighbors import LocalOutlierFactor

X = transFeaturesScaled.values

clf = LocalOutlierFactor(n_neighbors=5)
isOutlier = clf.fit_predict(X)",,Untitled->EDA ( Exploratory Data Analysis) Section->EDA: Data Cleaning,911,Driver Safety Estimation - Pandas->Driver safety estimation->Modeling->Anomaly detection:
459828,"# evaluate the average diameter of each asteroid
df['Diameter_m'] = df[['Min_Diameter_m', 'Max_Diameter_m']].mean(axis=1)
# construct a sub-dataframe only with diameter, distance and velocity data
df2=df[['Diameter_m','Distance_lunar','RelVelocity_KMperS']]

# evaluate mean, variance, CoV and entropy of diameter
kdeDiam = sm.nonparametric.KDEUnivariate(df2['Diameter_m'])
kdeDiam.fit()
entDiam=kdeDiam.entropy
meanDiam = df2['Diameter_m'].mean()
varDiam = df2['Diameter_m'].var()
covDiam = float(varDiam)**0.5/meanDiam

# evaluate mean, variance, CoV and entropy of distance
kdeDist = sm.nonparametric.KDEUnivariate(df2['Distance_lunar'])
kdeDist.fit()
entDist = kdeDist.entropy
meanDist = df2['Distance_lunar'].mean()
varDist = df2['Distance_lunar'].var()
covDist = float(varDist)**0.5/meanDist

# evaluate mean, variance, CoV and entropy of relative velocity
kdeVel = sm.nonparametric.KDEUnivariate(df2['RelVelocity_KMperS'])
kdeVel.fit()
entVel = kdeVel.entropy
meanVel = df2['RelVelocity_KMperS'].mean()
varVel = df2['RelVelocity_KMperS'].var()
covVel = float(varVel)**0.5/meanVel

# create a table in a text file containing statistics of the data
tableData = [[meanDiam,varDiam,covDiam,entDiam],[meanDist,varDist,covDist,entDist],[meanVel,varVel,covVel,entVel]]
tableHead = [ ""Mean"", ""Variance"", ""Coefficient of Variation"", ""Entropy""]
tableInx = [ ""Diameter (m)"", ""Distance (lunar)"", ""Relative Velocity (km/s)"" ]
tbl = sm.iolib.table.SimpleTable(tableData, tableHead, tableInx, title=""Data Statistics"")
with open('StatData.txt','w') as fhand:
    fhand.write( tbl.as_text() )","df2_f_m.plot(kind='bar', title='ETH - Favorites per month') #Barplot","from sklearn.datasets import make_gaussian_quantiles
X1, y1 = make_gaussian_quantiles(cov=2., n_samples=200, n_features=2, n_classes=2, random_state=1)
X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=1)
X = np.concatenate((X1, X2))
y = np.concatenate((y1, - y2 + 1))

# And plot it!
plt.figure(figsize=(8,8))
plot_colors = ""br""
class_names = ""AB""
for i, n, c in zip(range(2), class_names, plot_colors):
        idx = np.where(y == i)
        plt.scatter(X[idx, 0], X[idx, 1], c=c, s=40, cmap=plt.cm.Paired,label=""Class %s"" % n)
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.legend(loc='upper right')
plt.xlabel('x')
plt.ylabel('y')
plt.show()",,Nasa-Near-Earth-Objects->NASA Near Earth Objects Data Analysis->Data Analysis:,Final Exam->Differences between EPFL and ETHZ->5.2 Full-ETH,4 Model Ensembles Solutions->Solutions: Ensemble Modeling
217238,ClassMetrics_results.plot_predicted_distribution(),"def analyze_lgbm_result():
    print('Plot metrics during training...')
    ax = lgb.plot_metric(evals_result, metric='rmse')
    plt.title(""Light GBM Metrics"")
    plt.show()

    print('Plot feature importances...')
    fig, ax = plt.subplots(figsize=(10, 14))
    lgb.plot_importance(clf, max_num_features=80, ax=ax)
    plt.title(""Light GBM Feature Importance"")
    plt.show()

#     print('Plot 84th tree...')  # one tree use categorical feature to split
#     ax = lgb.plot_tree(clf, tree_index=83, figsize=(20, 8), show_info=['split_gain'])
#     plt.show()

#     print('Plot 84th tree with graphviz...')
# #     graph = lgb.create_tree_digraph(clf, tree_index=83, name='Tree84')
# #     graph.render(view=True)
    
analyze_lgbm_result()","#Define scraping parameters
pageSize = 100 #Number of results to ask for per page
craft = 'loom-knitting' #Get the loom knitting patterns only
waitTime = 5 #Number of seconds to wait between requests to avoid spooking the API

#100 results per page, start with page 1 of the results and sort the results by the date created*
searchVals = {'page_size': str(pageSize), 'page': '1', 'craft': craft, 'sort': 'created'} #searchVals is the list of parameters to send to the API

#Make an initial request to the API to figure out how many pages of results there will be for our desired pageSize
response = rq.get('https://api.ravelry.com/patterns/search.json?', params = searchVals, auth = (user,pswd))
print(response) #Print the response code so we can check whether the query was successful
printPatternNames(response) #Print the names of the patterns found in this first call, so that we can check that we are getting the patterns we expect
numPages = response.json()['paginator']['last_page'] #Number of pages of results

#Display some info about # of requests that will be made and # of results per page
print('Making ',numPages,' requests: ',numPages,' pages of data with ',pageSize,' results per page.')

#Display how long this code should take to run
print('This code should take approximately ',waitTime*numPages/60,' minutes to complete.')

#Perform the scraping to get a list of unique pattern IDs
IDs = scrapePatternIDs(numPages,searchVals,user,pswd,waitTime)

#(*my original thought for sorting by date created was that maybe it won't mess things up as much if new patterns get added
#during the scraping, since it will return the newest patterns first and maybe we'll be able to catch all the patterns even
#if they get shifted by a couple, as long as the new patterns don't push the results over the final page; as opposed to if the
#results were sorted by popularity then the changes would be spread out over the pages but I guess sorting by date added doesn't
#really help anything. IDK, I don't think it really matters for my purposes since I don't really need ALL patterns, just most.)",,"Examples->Instantiate ClassMetrics Object from dataset and predictions->Plot Actual Class Distributions, Predicted Class Distributions, or both together",Lgbm Final-0->LightGBM->Analyzing results,Get Rav Pattern I Ds->Loom knitting patterns
45547,"def number_of_trips(filename):
    """"""
    This function reads in a file with trip data and reports the number of
    trips made by subscribers, customers, and total overall.
    """"""
    with open(filename, 'r') as f_in:
        # set up csv reader object
        reader = csv.DictReader(f_in)
        
        # initialize count variables
        subscribers = 0
        customers = 0
        
        # tally up ride types
        for row in reader:
            if row['user_type'] == 'Subscriber' or row['user_type']=='Registered':
                subscribers += 1
            else:
                customers += 1
        
        # compute total number of rides
        total = subscribers + customers
        
        # return tallies as a tuple
        return(total, subscribers, customers)","def number_of_trips(filename):
    """"""
    This function reads in a file with trip data and reports the number of
    trips made by subscribers, customers, and total overall.
    """"""
    with open(filename, 'r') as f_in:
        # set up csv reader object
        reader = csv.DictReader(f_in)
        
        # initialize count variables
        n_subscribers = 0
        n_customers = 0
        
        # tally up ride types
        for row in reader:
            if row['user_type'] == 'Subscriber':
                n_subscribers += 1
            else:
                n_customers += 1
        
        # compute total number of rides
        n_total = n_subscribers + n_customers
       # print('subcriber={},customer={},')
        # return tallies as a tuple
        return(n_subscribers, n_customers, n_total)","# dropping outliers

trimssid_df = trimssid_df[trimssid_df.OutlierTT != True]
trimssid_df.sort_values(['TravelTime'], ascending=False, inplace=True)
trimssid_df = trimssid_df.drop(['OutlierTT'], axis=1)
trimssid_df.reset_index(inplace=True)
trimssid_df = trimssid_df.drop('index', axis=1)
trimssid_df",,Bike Share Analysis,Bike Share Analysis,"Rscv Modelling Of Ssid 30213022 Iter=100 Iqr2->First set number of cores on machine to use and SSID number to analyse, then run all"
42564,"import furl

search_url = furl.furl(OSF_APP_URL)
search_url.args['size'] = 3
search_url.args['sort'] = 'providerUpdatedDateTime'
recent_results = requests.get(search_url.url).json()

print('The request URL is {}'.format(search_url.url))
print('----------')
for result in recent_results['results']:
    print(
        '{} -- from {} -- updated at {}'.format(
            result['title'].encode('utf-8'),
            result['shareProperties']['source'],
            result['providerUpdatedDateTime']
        )
    )","search_url.args['q'] = 'shareProperties.source:mit'
recent_results = requests.get(search_url.url).json()

print('The request URL is {}'.format(search_url.url))
print('---------')
for result in recent_results['results']:
    print(
        '{} -- from {} -- updated on {}'.format(
            result['title'].encode('utf-8'),
            result['shareProperties']['source'],
            datetime.strftime(datetime.strptime(result['providerUpdatedDateTime'], ""%Y-%m-%dT%H:%M:%S+00:00""), '%B %d %Y')
        )
    )","def heapsort(x):
    h = Heap(*x)
    print(f'heap: {h}')
    for idx in range(len(x)):
        x[idx] = h.downheap()
        print(f'idx: {idx} heap: {h}')",,Basics->Calling the SHARE API->Simple Queries,Share Api Webinar->Calling the SHARE API->Setup->Narrowing Results by Source,Sorting Fun->Sorting Fun->Heapsort
138517,"df.dropna(subset=['price'], inplace=True)
df.info()","# After discussing the structure of the data and any problems that need to be
#   cleaned, perform those cleaning steps in the second part of this section.
df.dropna(subset=['imdb_id'], inplace=True)  
df.info()","import pandas as pd

df = pd.read_csv('TrainingData.csv', index_col=0)

df.head()",,Eda->EDA,Investigate A Dataset(3)->Project: Investigate a Dataset - TMDb Dataset->Data Cleaning (Replace this with more specific notes!),School Budgets->Machine Learning Case Study: School Budgets->Exploring the data
98228,"# search for a particular term in the documentation
np.lookfor('broadcast')",np.lookfor('weighted average'),data.Category.value_counts(),,Generic Python Learning->numpy concepts->Text to Numeric - CountVectorizer->Why numpy?,14-Numpy-Intro->![](http://www.numpy.org/_static/numpy_logo.png) Introduction to NumPy->Looking for help?,Assignment 02 Salary Prediction Solutions->Assignment 2: Linear Regression->Checking your answers
336508,"# Ploting the registration via
f,ax=plt.subplots(1,2,figsize=(18,8))
train['registered_via'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])
ax[0].set_title('Number Of User by registration via')
ax[0].set_ylabel('Count')
ax[0].set_xlabel('Registered via')
sns.countplot('registered_via',hue='is_churn',data=train,ax=ax[1])
ax[1].set_title('registered via: is_churn VS is no churn')
plt.show()","# Ploting the city
f,ax=plt.subplots(1,2,figsize=(18,8))
train['city'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'],ax=ax[0])
ax[0].set_title('Number Of User by City')
ax[0].set_ylabel('Count')
ax[0].set_xlabel('City')
sns.countplot('city',hue='is_churn',data=train,ax=ax[1])
ax[1].set_title('City: is_churn VS is no churn')
plt.show()","neg_xs = np.linspace(-0.01, -0.09, 20)
pos_xs = np.linspace(0.01, 0.09, 20)

pos_ys = map(g, pos_xs)
neg_ys = map(g, neg_xs)

plt.scatter(pos_xs, pos_ys, alpha=0.5)
plt.scatter(neg_xs, neg_ys, alpha=0.5)

plt.xlabel('x')
plt.ylabel('y');",,K Kbox-Les-Tanches->IV) Predictive Modeling->I.3) Preparing data for the Exploration Data Analysis->Registration Via,K Kbox-Les-Tanches->IV) Predictive Modeling->I.3) Preparing data for the Exploration Data Analysis->City,Python And Mathematics->Python And Mathematics
153592,"banana = LiquidData('banana-mc')

for mcType in [""AvA_hinge"", ""OvA_hinge"", ""AvA_ls"", ""OvA_hinge""]:
    print(""\n======"", mcType, ""======"")
    model = mcSVM(banana.train, mcType=mcType)
    result, err = model.test(banana.test)
    
    print(""global err:"", err[0,0])
    print(""task errs:"", err[1:,0])
    
    print(result[:3,])","covtype = LiquidData('covtype.5000')
model = mcSVM(covtype, display=1, useCells=True)
result, err = model.lastResult
err[0,0]","from sklearn.linear_model import LinearRegression
linear = LinearRegression()",,Demo->liquidSVM for Python->Learning Scenarios->Multiclass classification,Demo->liquidSVM for Python->liquidSVM in one Minute->Cells,Introduction To Machine Learning->Demo 13: Plotting Numeric vs Numeric Data with Scatter plot
194322,"x = tf.placeholder(tf.float32, (None, 32, 32, 1))
y = tf.placeholder(tf.int32, (None))
keep_prob = tf.placeholder(tf.float32) 
one_hot_y = tf.one_hot(y, n_classes)","# Colored: (None, 32, 32, 3)
# Gray: (None, 32, 32, 1)
x = tf.placeholder(tf.float32, (None, 32, 32, 1))
y = tf.placeholder(tf.int32, (None))
keep_prob = tf.placeholder(tf.float32)
one_hot_y = tf.one_hot(y, n_classes)","def SOR(A,b,omega,maxiter=1000000):
    x = np.ones(len(b))
    error=1
    iteration=1
    diagonal = np.diagonal(A) #storing diagonals of A 
    while error > 10**(-8):
        x_old = x.copy() #initializing x of the iteration that just ended to use both in the calculation of x[i] and error
        for i in range(len(x)):
            #constructing variables before after for each row
            after = np.dot(A[i,:i],x[:i])
            before = np.dot(A[i,i:len(x)], x_old[i:len(x)])
            #calculating x row by row
            x[i] = x[i] + ((omega / diagonal[i])*(b[i] -  after -before))
        
        error = np.linalg.norm(x-x_old)/np.linalg.norm(x)#relative error
        
        iteration+=1
    return x,iteration",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Place Holders for input images and labels,Traffic Sign Classifier->Features and Labels,Linear System Solving->SUCCESSIVE OVER RELAXATION
230680,"g_boost_model= g_boost.fit(X,y)","boost_model.fit(X_train, y_train)","def metric(old, new):
    n = old.shape[0]
    m = old.shape[1]
    return np.linalg.norm(old - new) / (n * m)",,Starter Code Project 3 - Mm->Natural Langauge (Pre) Processing->Repeating with job titles->Gradient Boost,Boosting Tree Credit Card Fraud Recall 90 Precision 1 Accuracy->Logistic regression classifier - Undersampled data,Spherical Spline Interpolation->Real Data Analysis->Analyze toy simulation data->7. Write code to quantify the results
245614,"def bucketize(point, bucket_size):
    """"""floor the point to the next lower multiple of bucket_size""""""
    return bucket_size*math.floor(point / bucket_size)

def make_histogram(points, bucket_size):
    """"""buckets the points and counts how many in each bucket""""""
    return Counter(bucketize(point, bucket_size) for point in points)

def plot_histogram(points, bucket_size, title=""""):
    histogram = make_histogram(points, bucket_size)
    plt.bar(histogram.keys(), histogram.values(), width=bucket_size)
    plt.title(title)
    plt.show()","def make_chart_histogram(plt):
    grades = [83,95,91,87,70,0,85,82,100,67,73,77,0]
    decile = lambda grade: grade // 10 * 10 
    histogram = Counter(decile(grade) for grade in grades)

    plt.bar([x - 4 for x in histogram.keys()], # shift each bar to the left by 4
            histogram.values(),                # give each bar its correct height
            8)                                 # give each bar a width of 8
    plt.axis([-5, 105, 0, 5])                  # x-axis from -5 to 105,
                                               # y-axis from 0 to 5
    plt.xticks([10 * i for i in range(11)])    # x-axis labels at 0, 10, ..., 100
    plt.xlabel(""Decile"")
    plt.ylabel(""# of Students"")
    plt.title(""Distribution of Exam 1 Grades"")
    plt.show()

make_chart_histogram(plt)","unique, counts = np.unique(train_labels, return_counts=True)
train_counts = {}
ind = 0
for val in unique:
    train_counts[val] = counts[ind]
    ind += 1
    
unique, counts = np.unique(valid_labels, return_counts=True)
valid_counts = {}
ind = 0
for val in unique:
    valid_counts[val] = counts[ind]
    ind += 1
    
unique, counts = np.unique(test_labels, return_counts=True)
test_counts = {}
ind = 0
for val in unique:
    test_counts[val] = counts[ind]
    ind += 1

print('train counts: \n', train_counts, '\n\nvalid counts: \n', valid_counts, '\n\ntest counts: \n', test_counts)",,7,Ch03 - Visualizing Data->2) Bar Charts,Notmnist
430148,"f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)
ax1.imshow(im1, cmap='gray')
ax1.axis('off')
ax2.imshow(im4, cmap='gray')
ax2.axis('off')
ax3.imshow(im7, cmap='gray')
ax3.axis('off')
ax4.imshow(im8, cmap='gray')
ax4.axis('off')
plt.show()","cmap = 'plasma'
extent = [0, minicolumns * hypercolumns, T_recalling, 0]

fig = plt.figure(figsize=(16, 12))

ax1 = fig.add_subplot(221)
im1 = ax1.imshow(history['o'], aspect='auto', interpolation='None', cmap=cmap, vmax=1, vmin=0, extent=extent)
ax1.set_title('Unit activation')

ax2 = fig.add_subplot(222)
im2 = ax2.imshow(history['z_pre'], aspect='auto', interpolation='None', cmap=cmap, vmax=1, vmin=0, extent=extent)
ax2.set_title('Traces of activity (z)')

ax3 = fig.add_subplot(223)
im3 = ax3.imshow(history['a'], aspect='auto', interpolation='None', cmap=cmap, vmax=1, vmin=0, extent=extent)
ax3.set_title('Adaptation')

ax4 = fig.add_subplot(224)
im4 = ax4.imshow(history['p_pre'], aspect='auto', interpolation='None', cmap=cmap, vmax=1, vmin=0, extent=extent)
ax4.set_title('Probability')

fig.subplots_adjust(right=0.8)
cbar_ax = fig.add_axes([0.85, 0.12, 0.05, 0.79])
fig.colorbar(im1, cax=cbar_ax)","df_test = testing[tcols].dropna()
X_test = df_test.loc[:,cols]
y_test = df_test.loc[:,['Survived']]
y_test = [val[0] for val in y_test.values]
score_log_test = clf_log.score(X_test,y_test)  # clf_log has been trained by training data
print(score_log_test)",,Assignment9-Checkpoint->Autoencoders->Part B,2016-11-09(Connections Among Non-Trained Patterns)->Connections among non-trained patterns->Network recalling->Without reinitializing the values,Titanic-Checkpoint->Missing values->Model validation
143227,"import statsmodels.formula.api as smf
lm = smf.ols(formula='admit ~ prestige', data=df_raw_update).fit()
lm.summary()","# A:

# stattical package
import statsmodels.formula.api as smf
import statsmodels.api as sm
#create fitted model
lm = smf.ols(formula='Y ~ X', data = df_).fit()
print(lm.summary())","df.drop('Pop_Anacondas Less Than 50', axis = 1, inplace = True)
df",,Eda->Question 13. Explore the association between grad school admissions rates and prestige of  undergraduate schools.,"[Lab] Linear Regression-Review-Lab-Starter->Using statsmodels, fit an OLS regression to your data and print our the summary",Advanced Pandas->3.2 Try Doing These When You Can->Pandorable->3.2.3 Lambdas
227295,"dbootstrap.reset_index(inplace=True)

dbootstrap.shape","dbootstrap.reset_index(inplace=True)

dbootstrap.shape","fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(10,10),sharex='col', sharey='row')
ax1.scatter(test_full[test_full.Cluster == 0]['intvlEntry'], test_full[test_full.Cluster == 0]['pred2'], color = 'b', alpha = 0.4)
ax1.plot([0,30000],[0,30000], color = 'b')
ax2.scatter(test_full[test_full.Cluster == 1]['intvlEntry'], test_full[test_full.Cluster == 1]['pred2'], color = 'r',marker='^', alpha=.4)
ax2.plot([0,30000],[0,30000], color = 'r')
ax3.scatter(test_full[test_full.Cluster == 2]['intvlEntry'], test_full[test_full.Cluster == 2]['pred2'], color = 'g',marker='d', alpha=.4)
ax3.plot([0,30000],[0,30000], color = 'g')
ax4.scatter(test_full[test_full.Cluster == 3]['intvlEntry'], test_full[test_full.Cluster == 3]['pred2'], color = 'k', alpha=.4)
ax4.plot([0,30000],[0,30000], color = 'k')",,Nasrudin House Price Prediction->I tried->Isolation Forest->Categorical relationships,Nasrudin House Price Prediction->I tried->Isolation Forest->Categorical relationships,Analysis Demo->Heartbeat of the New York City Subway->Motivation
386057,"fig, ax = plt.subplots()
ax.hist(df_['Number of homicides by firearm'] / df_['pop'] * 1000)
ax.set_xlabel('homicides by firearm per 1000 people');
pl.show()
print(""Federica's histogram of homicides by firearm"")","fig = pl.figure(figsize=(10,5))
ax = fig.add_subplot(111)
ax.hist(myDmean, color='lightcoral', alpha=0.5)
ax.set_title(""Distribution of means of 1000 distributions with 200 samples each"", fontsize=20)
ax.set_xlabel(""$\mu$"", fontsize=15)
ax.set_ylabel(""Frequency"", fontsize=15)","def solve(l, loc=0, step=0):
    if loc not in range(len(l)):
        return step
    else:
        l[loc] += 1
        return solve(l, loc + l[loc] - 1, step+1)",,Assignment 3->exploration->plot the average number of homicide by fire arms per person,Assignment1 Uc288->$z = \frac{\mu_{pop} - \mu_{sample}}{\frac{\sigma}{\sqrt{N}}}$,Day5->Problem - Part One
242377,"# Read catalog of Crossref DOIs
path = os.path.join('data', 'doi.tsv.xz')
doi_df = (
    pandas.read_table(path, parse_dates=['issued'])
    .dropna(subset=['issued'])
)","filename=os.path.join('data','CrowdstormingDataJuly1st.csv') 
df = pd.read_csv(filename)
df = df.dropna(subset=['rater1', 'rater2'])",# Not used,,03->Sci-Hub Request Logs access rates->Read DOI and Sci-Hub Request Data,Ex1-Preprocess,00Raw Datasets Observation->2. Secondhand Song Dataset (SHS)->2.1 SHS - Not Used
331438,"print (""Full Train Dataset----------->"") 
print (test_stationarity(train['Vehicles']))
print (""Train Dataset for Junction 1----------->"")
print (test_stationarity(train[train['Junction']==1]['Vehicles']))
print (""Train Dataset for Junction 2----------->"") 
print (test_stationarity(train[train['Junction']==2]['Vehicles']))
print (""Train Dataset for Junction 3----------->"") 
print (test_stationarity(train[train['Junction']==3]['Vehicles']))
print (""Train Dataset for Junction 4----------->"") 
print (test_stationarity(train[train['Junction']==4]['Vehicles']))","print('dimension of train:', train.shape)
print('dimension of test:', test.shape)","fig, ax = plt.subplots(1,1)
plot_beta(100,100,ax)
handles, labels = ax.get_legend_handles_labels()
ax.legend(handles, labels);",,Model->Visulatisation,"Model6->Load data for test
Model1->load data for test
Model6 (3)->Load data for test",Bayesian Inference->Introduction to Bayesian Inference: A Coin Flipping Example->Example: coin flip->Prediction
198886,"#  recall the data
np.array([x,y])","x, y = np.mgrid[0:4, 0:4]
x","import matplotlib as mlt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline",,"Matplotlib->Object Oriented API Method->subplots()
Matplotlib->Control of Axes appearance",01-Num Py-Intro->NumPy -  Multidimensional Arrays->Linear algebra->Element-wise Operations,Simulation Problem
191908,"### Storing Varibales for Checkpoint1

if not os.path.isfile(checkpoint1_file) or checkpoint1_force:
    print('Caching variables in pickle file...')
    try:
        with open(checkpoint1_file, 'wb') as pfile:
            pickle.dump(checkpoint1_var, pfile, pickle.HIGHEST_PROTOCOL)
    except Exception as e:
        print('Unable to save data to', checkpoint1_file, ':', e)
        raise
    print('Data cached in pickle file.')
else:
    print('Pickle file already created. Set checkpoint1_force to force new file.')","pickle_file = 'newtrain.p'
if not os.path.isfile(pickle_file):
    print('Saving data to pickle file...')
    try:
        with open('newtrain.p', 'wb') as pfile:
            pickle.dump(
                {
                    'newfeatures': X_train,
                    'newlabels': y_train,
                },
                pfile, pickle.HIGHEST_PROTOCOL)
    except Exception as e:
        print('Unable to save data to', pickle_file, ':', e)
        raise

print('Data cached in pickle file.')","action = 'stop' # The current action is 'stop'
light = 'green' # The light is 'green'


if light == 'green':
    action = 'drive' # The if statement changes the action to 'drive'",,Traffic Sign Classifier->Question 8,P2 Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Affine Transoformation - Data Augmentation->Save the augmented training data to disk,Python 1->Python: conditional execution (`if`) and repetition structures (loops)->Required Reading
344990,"%matplotlib inline
import numpy as np
import random
import matplotlib.pyplot as plt

nStep = 5000 # 5000
nParticles = 400 # 400

particleXdata = [] # array of arrays of x-positions
particleYdata = [] # array of arrays of y-positions

## e.g. particleXdata[i,t] is the x-position of the ith molecule at time step t

for particle in range(nParticles):
    xs = []
    ys = []
    x = 0
    y = 0
    for n in range(nStep):
        xs.append(x)
        ys.append(y)
        x += random.randrange(-1,2,2)
        y += random.randrange(-1,2,2)
        
    particleXdata.append(xs)
    particleYdata.append(ys)

## Convert these into numpy arrays so that matplotlib can use them
xdat = np.array(particleXdata)
ydat = np.array(particleYdata)","%matplotlib inline
import numpy as np
import random
import matplotlib.pyplot as plt

nStep = 5000 # 5000
nParticles = 400 # 400

particleXdata = [] # array of arrays of x-positions
particleYdata = [] # array of arrays of y-positions

## e.g. particleXdata[i,t] is the x-position of the ith molecule at time step t

for particle in range(nParticles):
    xs = []
    ys = []
    x = 0
    y = 0
    for n in range(nStep):
        xs.append(x)
        ys.append(y)
        x += random.randrange(-1,2,2)
        y += random.randrange(-1,2,2)
        
    particleXdata.append(xs)
    particleYdata.append(ys)

## Convert these into numpy arrays so that matplotlib can use them
xdat = np.array(particleXdata)
ydat = np.array(particleYdata)","from sklearn.linear_model import LogisticRegression
lr_model_f = LogisticRegression(class_weight = ""balanced"", C = best_c_value, random_state = 42)
lr_model_f.fit(x_train, y_train.ravel())
lr_predict = lr_model_f.predict(x_test)

print(""Accuracy score: {0:0.4f}"".format(metrics.accuracy_score(y_test, lr_predict)))
print("""")
from sklearn import metrics
print(""{0}"".format(metrics.confusion_matrix(y_test, lr_predict)))
print("""")
print(metrics.classification_report(y_test, lr_predict, labels=[1, 0]))
print(metrics.recall_score(y_test, lr_predict))",,Diffusion->Diffusion->Problem 2: Diffusion with time,Diffusion->Diffusion->Problem 2: Diffusion with time,Pima-Prediction->Training with Random Forest->Logestic regression with Class_weight = 'balanced'
15912,"sess.run(init)
sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]})","# x and y are placeholder and we promise to provide value later, Now this was the time when we provide the values.

with tf.Session() as sess:
    sess.run(init)
    print(sess.run(loss, {x:[1,2,3,4], y:[0,-1,-2,-3]}))","print(raw_data.approvalfy.max())
print(raw_data.approvalfy.min())
print(raw_data.approvalfy.value_counts())",,Getting Started->1. Create computational graph->1.5 Variables->1.5.1 Loss function,A Very First Introduction With Tensorflow->Build the computational Graph and Run it->The mathematical formula behind loss function:,"Dsi Json Exercise->Working with JSON exercise->imports for Python, Pandas->* In order to understand the data a little better, I will determine the time range the data covers and try to identify and time gaps"
221465,"# See where the missing values are
titanic_df.isnull().sum()","yvad_predict = titan.sigmoid(np.dot(Xvadq, w[0]))
yvad_predict[yvad_predict > 0.5] = 1
yvad_predict[yvad_predict <= 0.5] = 0
(yvad_predict == yvad).sum()*1.0/yvad.size","A = np.random.random((3,3))",,Titanic Analysis->Titanic Dataset Analysis->Check out the data,Summary,Lab4 100514352->Create random 3 x 3 matrix
235923,min(s),s.max() - s.min(),"# finding the lag providing the best result
best_result = np.corrcoef(alldata[0]['Rebates_Payable'],alldata[0]['Sales'])[1,0]
lagging = 0

for dataset in alldata:
    print(str(lagging) + 'months lag: correlation coefficient = ' + 
          str(round(np.corrcoef(dataset['Rebates_Payable'],dataset['Sales'])[1,0],4)))
    lagging = lagging + 1
    
    if best_result < np.corrcoef(dataset['Rebates_Payable'],dataset['Sales'])[1,0]:
        best_result = np.corrcoef(dataset['Rebates_Payable'],dataset['Sales'])[1,0]
        bestlagging = lagging - 1

data = alldata[bestlagging]

print('Hirest correlation of ' + str(round(best_result,4)) + ' returns dataset with '+ str(bestlagging) + ' months lagging')",,Solutions-Checkpoint->Tips for debugging `apply`->min,Chapter 04 - Displaying And Summarizing Quantitative Data->Spread - Range,Balance Sheet Items Forecast->Balance Sheet Items Forecast
458615,"cur = conn.cursor()   # Create the cursor

# Run the SQL code and use the for loop to
#     iterate through the resulting rows
for row in cur.execute('SELECT * FROM customers'):        
    print(row)","conn = sqlite3.connect(sqlite_db)    

with conn:
    
    conn.row_factory = sqlite3.Row
       
    cur = conn.cursor() 
    cur.execute(""SELECT * FROM Titanic LIMIT 5"")

    rows = cur.fetchall()

    for row in rows:
        print(""%s %s %s"" % (row[""Name""], row[""PClass""], row[""Age""]))","def importance_wrapper(fits, ctype, model_feats, title, condition, tall_plot=False, imp_cutoff=.01, imp_subset=10):

    # replace ""happs"" with ""happy"" (eg. LabMT_happs -> LabMT_happy)
    model_feats = [re.sub('happs','happy',x) for x in model_feats]

    # Plot the feature importances of the forest
    fimpdf = pd.DataFrame(fits[ctype]['clf'].feature_importances_, index=model_feats, columns=['importance'])
    
    if tall_plot:
        fsize = (5,11)
    else:
        fsize = (3,4)
    plt.figure() 
    fimpdf = fimpdf.sort_values('importance', ascending=False).ix[fimpdf.importance > imp_cutoff,:]
    
    feat_names = fimpdf.index
    fimpdf.reset_index(drop=True)
    ax = fimpdf.ix[0:imp_subset,'importance'].plot(kind='barh', figsize=fsize, fontsize=14)
    plt.gca().invert_yaxis()
    major_formatter = FuncFormatter(drop_leading_zero_formatter)
    ax.xaxis.set_major_formatter(major_formatter)
    plt.xticks(fontsize=10)
    plt.title(""Top {} predictors ({})"".format(condition.upper(),title), fontsize=16)
    plt.show()",,Sql->Retrieving data from a SQL database,2-Sql-Sqlite3 Python->SQLite3 in Python->sqlite3 Module Basics,Eda-Twitter->Subset master actions
413109,ndf.describe().transpose(),re = ndf.corr().iloc[0],"QUERY = '''
SELECT Invoice.BillingCity, COUNT(*)
FROM Invoice,InvoiceLine,Track,Genre
WHERE Invoice.InvoiceId = InvoiceLine.InvoiceId
AND InvoiceLine.TrackId = Track.TrackId
AND Track.GenreId = Genre.GenreId
AND Invoice.BillingCountry = 'France'
AND Genre.Name = 'Alternative & Punk'
GROUP BY Invoice.BillingCity
ORDER BY COUNT(*) DESC

'''


'''
---Visual Guide---

Before Query...

#################       #################       #############      #############
#    Invoice    #       #  InvoiceLine  #       #   Track   #      #   Genre   #
#################       #################       #############      #############
|  InvoiceId    | --->  |  InvoiceId    |       |  GenreId  | ---> |  GenreId  |
+---------------+       +---------------+       +-----------+      +-----------+
|  BillingCity| |       |  TrackId      |  ---> |  TrackId  |      |  Name     |  
+---------------+       +---------------+       +-----------+      +-----------+
| BillingCountry|
+---------------+

After Query..

###############################
#        InvoiceGenre         #
###############################
|  BillingCity  |  NumTracks  |
+---------------+-------------+

'''",,Explore Enron Data->Exploratory data analysis,A2->Correlation way to approach,01 - Problem Set2->Q7:
457206,"sol = scipy.integrate.solve_ivp(dhdt, tspan, [h0])
smoothsol = scipy.integrate.solve_ivp(dhdt, tspan, [h0], t_eval=tsmooth)","%%timeit
sol = scipy.integrate.solve_ivp(dhdt, tspan, [h0], max_step=0.1)","print (model)
# print (model.layers)
print (model.input)
print (model.output)",,Equation Solving Tools->Equation solving tools->Exact solution using sympy,Equation Solving Tools->Equation solving tools->Nonlinear equations->Numeric solution,Keras-Openface-Test->Test of nn4.small2.v1.h5 in Keras-OpenFace
385434,"# Normalize a pixel vector and return
# an image. This code is from:
# https://github.com/joelgrus/shirts/blob/master/visuals.py#L55
def image_from_component_values(component):
    """"""takes one of the principal components and turns it into an image""""""
    hi = max(component)
    lo = min(component)
    n = int(len(component) / 3)
    divisor = hi - lo
    if divisor == 0:
        divisor = 1
    
    def rescale(x):
        return int(255 * (x - lo) / divisor)
    
    d = [(rescale(component[3 * i]),
          rescale(component[3 * i + 1]),
          rescale(component[3 * i + 2])) for i in range(n)]
    im = Image.new('RGB',IMAGE_SIZE)
    im.putdata(d)
    return im","def solve_33(image, letter_size=100, columns=8):
    """"""Solve puzzle 33""""""
    letters = []
    imdata = list(image.getdata())
    while len(imdata):
        brightest = max(imdata)  # brightest color
        root = int(len(imdata) ** 0.5)   
        if root ** 2 == len(imdata): # is it a ""fair and square""
            letter = Image.new('L', (root, root))
            # enhance brightest pixels
            enhanced = [255 if p == brightest else p for p in imdata]
            letter.putdata(enhanced)
            topleft = (root // 2 - letter_size // 2)  # top left corner of cropped letter
            cropsize = (topleft, topleft, topleft + letter_size, topleft + letter_size)
            letters.append(letter.crop(cropsize))
        # If you are blinded by the light ...
        # remove its power, with its might
        imdata = [p for p in imdata if p != brightest]  # remove brightest pixels
    return stitch_images(letters, columns)

solution = solve_33(image=beer2, letter_size=60, columns=11)
add_answer('gremlins.html', 33)
solution","# (A) Fit and evaluate a linear regression model
lr = LinearRegression()
lr.fit(X1_tr, y_tr)

y_pred_lr = lr.predict(X1_te)
mae_lr = MAE(y_te, y_pred_lr)

# (B) Same with Huber loss
huber = HuberRegressor(epsilon=1.35)
huber.fit(X1_tr, y_tr)
y_pred_hbr = huber.predict(X1_te)
mae_hbr = MAE(y_te, y_pred_hbr)

print('MAE LR: {:2F}'.format(mae_lr))
print('MAE Huber: {:2F}'.format(mae_hbr))",,Assignment 3->PCA->Analyze the Results,Python Challenge->Puzzle 33: 33 bottles of beer,Exercise - Bike Sharing Linear Regression->Bike sharing linear regression->Exercise - Fit a linear regression model
218863,"#postId='10154056181289641'
#fb= postReactions(postId,1,'HM2')
filename='facebookHM2.sqlite'
getAllGraphsForPost(filename,50)","#postId= '10154210843679641'
#fb= postReactions(postId,1,'CNN_OS3')
filename='facebookCNN_OS3.sqlite'
getAllGraphsForPost(filename,50)",r.content[:300],,Report->Regressors->Theme : Homosexuality,Report->Analysis on Facebook data,Marketcapscrape->Scraping coinmarketcap for ICO prices->Scraping a Website with Requests and Beautiful Soup <a id='scrape'></a>
416053,"hidden_linout = linear(X, hidden_layer_weights, hidden_layer_biases)
hidden_output = activation(hidden_linout)

print('hidden output')
print(hidden_output)","X = tf.placeholder(tf.float32 , [None,timesteps,num_input])
y = tf.placeholder(tf.float32, [None,num_output])
weight = {
	'weigh' : tf.Variable(tf.random_normal([hidden_layer, num_output]), name = 'weight')
}

bias = {
	'bie' : tf.Variable(tf.random_normal([num_output]), name = 'bias')
}","dump=plt.hist(np.log10(hum_data[hum_data.inequality=='='].meas.values),bins=50,log=True)",,Neural Network->3. Implementation in python->3.1. A basic neural network with `numpy`,Stock Price->Apple stock price prediction->Importing data and spliting into test and training data,Explore->No duplicates !!!->Equals are nicely distributed
188120,"with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    num_examples = len(X_train)
    
    print(""Training..."")
    print()
    for i in range(EPOCHS):
        X_train, y_train = shuffle(X_train, y_train)
        for offset in range(0, num_examples, BATCH_SIZE):
            batch_counter += 1
            end = offset + BATCH_SIZE
            batch_x, batch_y = X_train[offset:end], y_train[offset:end]
            _, l =sess.run(
                [training_operation, loss_operation], 
                feed_dict={x: batch_x, y: batch_y, pkeep: DROP_OUT})
            if not batch_counter % log_batch_step:
                loss_batch.append(l)
                batches.append(batch_counter)
                train_acc_batch.append(evaluate(X_train, y_train))
                valid_acc_batch.append(evaluate(X_validation, y_validation))
        
        training_accuracy = evaluate(X_train, y_train)
        validation_accuracy = evaluate(X_validation, y_validation)
        print(""EPOCH {} ..."".format(i+1))
        print(""Training Accuracy = {:.3f}"".format(training_accuracy))
        print(""Validation Accuracy = {:.3f}"".format(validation_accuracy))
        print()
    
    
    saver.save(sess, './lenetmodified')
    print(""Model saved"")
    
    test_accuracy = evaluate(X_test, y_test)
    print(""Test Accuracy = {:.3f}"".format(test_accuracy))
    
loss_plot = plt.subplot(211)
loss_plot.set_title('Loss')
loss_plot.plot(batches, loss_batch, 'g')
loss_plot.set_xlim(batches[0], batches[-1])

acc_plot = plt.subplot(212)
acc_plot.set_title('Accuracy')
acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')
acc_plot.plot(batches, valid_acc_batch, 'g', label='Validation Accuracy')
acc_plot.set_xlim(batches[0], batches[-1])
acc_plot.legend(loc=4)
plt.tight_layout()
plt.show()","with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    num_examples = len(X_train)
    
    print(""Training..."")
    print()
    for i in range(EPOCHS):
        X_train, y_train = shuffle(X_train, y_train)
        for offset in range(0, num_examples, BATCH_SIZE):
            batch_counter += 1
            end = offset + BATCH_SIZE
            batch_x, batch_y = X_train[offset:end], y_train[offset:end]
            _, l =sess.run(
                [training_operation, loss_operation], 
                feed_dict={x: batch_x, y: batch_y, pkeep: DROP_OUT})
            if not batch_counter % log_batch_step:
                loss_batch.append(l)
                batches.append(batch_counter)
                train_acc_batch.append(evaluate(X_train, y_train))
                valid_acc_batch.append(evaluate(X_validation, y_validation))
        
        training_accuracy = evaluate(X_train, y_train)
        validation_accuracy = evaluate(X_validation, y_validation)
        print(""EPOCH {} ..."".format(i+1))
        print(""Training Accuracy = {:.3f}"".format(training_accuracy))
        print(""Validation Accuracy = {:.3f}"".format(validation_accuracy))
        print()
    
    
    saver.save(sess, './lenetmodified')
    print(""Model saved"")
    
    test_accuracy = evaluate(X_test, y_test)
    print(""Test Accuracy = {:.3f}"".format(test_accuracy))
    
loss_plot = plt.subplot(211)
loss_plot.set_title('Loss')
loss_plot.plot(batches, loss_batch, 'g')
loss_plot.set_xlim(batches[0], batches[-1])

acc_plot = plt.subplot(212)
acc_plot.set_title('Accuracy')
acc_plot.plot(batches, train_acc_batch, 'r', label='Training Accuracy')
acc_plot.plot(batches, valid_acc_batch, 'g', label='Validation Accuracy')
acc_plot.set_xlim(batches[0], batches[-1])
acc_plot.legend(loc=4)
plt.tight_layout()
plt.show()","# inter = list(zip(train_images, train_labels))
# random.shuffle(inter)
# images, labels = zip(*inter)
# train_data = DataSet( np.array(train_images), np.array(train_labels), reshape=False)
# del train_images, train_labels",,Traffic Sign Classifier,Traffic Sign Classifier,"Cogs181-Final-Checkpoint->Setup, Download and Preprocess->Data Exploratory->Loading Train Data"
311244,"df.LotFrontage.fillna(0, inplace=True)
df.loc[df['CentralAir'] == 'Y', 'CentralAir'] = 1
df.loc[df['CentralAir'] == 'N', 'CentralAir'] = 0
df.CentralAir.fillna(df['CentralAir'].mean(), inplace=True)","# Feature 41, CentralAir: Central air conditioning
data.CentralAir.value_counts()","left_fit, right_fit = sliding_window(warped)
plot_poly_on_thresholded(warped, left_fit, right_fit)",,House Pricing->Improvement of Lasso,Final Project 3->Target (SalePrice)->3. GarageCars,Advanced Lane Finding->Detect lane lines
85501,"# predict against FAQ test set 
featureNames = faq.columns[1:width-1] #remove the first ID col and last col =classifier
feqPreds = clf.predict(faq[featureNames])

predout = pd.DataFrame({'id':faq['id'],'predicted':feqPreds,'actual':faq['class']})","preds = clf.predict(test[features])
predout = pd.DataFrame({ 'id' : test['id'], 'predicted' : preds, 'actual' : test['class'] })","f = lambda x: x**2

print(f(3))",,4->1. Load Data->Flaws in the Approach and Further Validation->Predict the Class of Sentence with Previously Built Model,4->1. Load Data->Generate Predictions from the Test Data-Set,Unitt 10 - Importing Packages And List Comprehensions-Checkpoint->Exercise - odd multiples of 3
144877,"# Author - Ritvik Khanna 
# Date - 04/05/18 
# Version - 2.3


import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import pandas as pd
from sklearn.datasets import load_iris
import numpy as np
from sklearn import preprocessing
import matplotlib.pyplot as plt #importing graph plotting functionality
# import os
# print(os.listdir(""../input""))
# Load dataset
df = pd.read_csv(""Datasets/car_evaluation.csv"", names = [""buying"",""maint"", ""doors"", ""persons"", ""lug_boot"",""safety"",""class""])","import numpy as np
import torch
import torch.nn as nn
from torch.autograd import Variable
import torch.optim as optim
import numpy as np
import matplotlib
#matplotlib.use('Agg')
import matplotlib.pyplot as plt","def get_clean_tokens(vol):
    if vol.language != 'eng':
        raise
    tl = (vol.tokenlist(case=False, pages=False, pos=False)
                .reset_index('section', drop=True)
                .reset_index()
           )
    tl['vol'] = vol.id
    return tl",,Script,Torch Ts Pred->Generate sine,Naive+Bayes+Classification->HTRC: Easy classification from HathiTrust collections->Example Use: classification
456829,"pos = np.where(y == 1)
neg = np.where(y == 0)
p = plt.scatter(X[pos,0], X[pos,1], marker = '+')
n = plt.scatter(X[neg,0], X[neg,1], marker = 'o')
plt.xlabel('Microchip Test 1')
plt.ylabel('Microchip Test 2')
plt.title('Scatter plot of training data')
plt.legend((p, n), ('Accepted', 'Rejected'), loc = 'upper right')
plt.show()","y= df.iloc[0:100, 4].values
y= np.where(y == 'Iris-setosa', -1, 1) # Convert the classes into -1/+1

X = df.iloc[0:100, [0,2]].values # sepal and petal length for the two flowers.

# TODO: Convert into proper plt.Figure()
# graph the two flowers as they are, based on their two features
plt.scatter(X[:50, 0], X[:50, 1], color='red', marker='o', label='setosa')
plt.scatter(X[50:100, 0], X[50:100, 1], color='blue', marker='x', label='versicolor')
plt.xlabel('sepal length')
plt.ylabel('petal length')
plt.legend(loc='upper left')
plt.show()","fig = plt.figure()
fig.set_size_inches(6,6)
ax = fig.add_subplot(111)
fixed(ax)
heights = levels(ax,0)
plt.plot(0,0,'ko', ms = 8);
plt.text(-2,0.5,r'energy $\rightarrow$',rotation=90,va='center')
plt.text(1.1,1.9,r'$\rightarrow$ allowed level',size=20);",,"Regularized Logistic Regression->Regularized Logistic Regression->Visualize Data <a name=""plot""></a>",Chapter 2 Training Ml Algorithms For Classification->1. Perceptron,"Quantum Dot Widget->Quantum dot intro->Don't get scared by the code, just scroll down to the pictures :)"
314259,df.category[20],df['Capital_Net']=df.Capital_Gain-df.Capital_Loss,"## Perform matrix factorization through stochastic gradient descent, for different numbers of features
# (grid search on the number of feature)
# keep the best number of feature, and the associated factorization

## Parameters for the matrix factorization
# maximum number of features fixed
max_K = 3

# incrementation step for K
step_K = 2

# number of full iterations of the stochastic gradient descent
max_epochs = 1

# regularization parameters (regularization of the loss function in Gradient Descent)
lambda_user = 1e-4
lambda_item = 1e-4

# descent step size
gamma = 0.1

# initialization of variables refering to the matrix factorization for the best number
best_rmse_test_sgd = float('Inf')
best_k_sgd = 0
best_user_feat_sgd = np.zeros(1)
best_item_feat_sgd = np.zeros(1)

# for each K of the grid, compute the matrix factorization (training set) and the rmse (test set), 
# If it improves the results, update the rmse

for K in range(2,max_K,step_K):
    
    print(""matrix factorization for the number of features : "", K)
    
    # compute the stochastic gradient descent matrix factorization
    user_features, item_features, rmse_test = matrix_factorization_SGD(train, test, K, max_epochs, lambda_user, lambda_item, gamma)
    if rmse_test < best_rmse_test_sgd:
        # better rmse => update the references
        best_rmse_test_sgd = rmse_test
        best_k_sgd = K
        best_user_feat_sgd = user_features
        best_item_feat_sgd = item_features

# print the results
print(""best number of features found : "", best_k_sgd)
print(""rmse on the test set for this number of features : "", best_rmse_test_sgd)",,Data Exploration-Checkpoint->Categories,"Project->Building,Training and Predicting Using Various Model->Univariate Analysis->Race Countplot",Marc Notebook->4) Matrix Factorization through Stochastic Gradient Descent
300313,client.cluster_status(instance_id),"cluster = LocalCluster()
client = Client(cluster)
client","dtree.train(X_train, cv2.ml.ROW_SAMPLE, y_train)",,Development->Example Code->Using the IBM Analytics Engine Library,"Parallel Time Series Forecasting In Python With Dask->Getting the Data->Note: If you don't want to or are having troubles downloading the M4Dataset file, you can also create a dataframe of random time series by using the following code for demonstration purposes. However, I did design the notebook to work with the M4 Dataset so you will have to edit the cells to work with this dataframe instead if you want to run this yourself",05->Building the decision tree
84478,"data = pd.Series([1, np.nan, 2, None, 3], index=list('abcde'))

data","# data for our examples
df = pd.DataFrame({'Label':list(""AABBCC""),
                  'Values':(1,2,3,4,np.nan,8)})
df","myGBM.detector_plot(radius=10,projection='ortho',lon_0=20,fermi_frame=True)
myGBM.detector_plot(radius=10,projection='ortho',lon_0=200,fermi_frame=True,fignum=3)
myGBM.detector_plot(radius=10,projection='ortho',lon_0=0,lat_0=40,fignum=2,fermi_frame=True)",,4->Handling Missing Data,06->Grouping and Aggregating Data->Filtering Groups,Demo->GBM Geometry Demo->Working with the GBM class->In Fermi GBM coodinates
290790,"wcss = []
for i in range(1,11):
    kmeans = KMeans(n_clusters = i, init ='random', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)","wcss = []
for i in range(1,11):
    kmeans = KMeans(n_clusters = i, init ='random', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)","eng_train['BldgTypeSimple'] = 'multifam'
eng_test['BldgTypeSimple'] = 'multifam'
eng_train.loc[eng_train.BldgType == '1Fam', 'BldgTypeSimple'] = '1Fam'
eng_test.loc[eng_test.BldgType == '1Fam', 'BldgTypeSimple'] = '1Fam'

nominal.append('BldgTypeSimple')",,K-Means Clustering->Using Elbow method to find the optimal number of clusters,K-Means Clustering->Using Elbow method to find the optimal number of clusters,Attempt4->EXPLORING DATA->Reducing levels in categorical features->Reducing levels in BldgType
101145,"ret = [round(elem, 2) for elem in ret]","ret = [round(elem, 2) for elem in ret]","# Print the first few records of the data set
titanic_df.head(15)",,Project->Table of Contents->Evaluating performance:,Project->Table of Contents->Evaluating performance:,Titanic Code->Nosipho Lekoba: TITANIC PROJECT->Read Titanic file
71852,"### TODO: Obtain bottleneck features from another pre-trained CNN.
bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')
train_TL = bottleneck_features['train']
valid_TL = bottleneck_features['valid']
test_TL = bottleneck_features['test']
print (train_TL[0].shape)","### TODO: Obtain bottleneck features from another pre-trained CNN.
bottleneck_features = np.load('bottleneck_features/DogXceptionData.npz')
train_tl = bottleneck_features['train']
valid_tl = bottleneck_features['valid']
test_tl = bottleneck_features['test']","X = np.linspace(-10,10,21)
y = np.zeros((X.shape[0],1))
y[5:16] = 1
plt.plot(X[np.where(y == 0)[0]],y[np.where(y == 0)[0]],'ro')
plt.plot(X[np.where(y == 1)[0]],0*y[np.where(y == 1)[0]],'b+')",,Dog App->Convolutional Neural Networks->Predict Dog Breed with the Model,Dog App->Predict Dog Breed with the Model,Kernel Methods Intro->Demonstration of Kernel Methods->1D Case
102566,"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
f.tight_layout()
ax1.imshow(img3)
ax1.set_title('Original Image', fontsize=20)
ax2.imshow(region_of_interest(img3)) # Using undistort() function as defined above
ax2.set_title('Region of Interest', fontsize=20)
plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)
plt.show()
#plt.savefig('./outputs/roi.jpg')","f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 13))
f.suptitle('Movie poster comparison', fontsize=24)

ax1.imshow(sample_movie['imdb']['poster'])
ax1.set_title('IMDB')
ax1.axis('off')
ax2.imshow(sample_movie['tmdb']['poster'])
ax2.set_title('TMDB')
ax2.axis('off')

f.tight_layout()
plt.subplots_adjust(top=1.12)
plt.show()","# Include fit with poly_reg
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y)",,Project->Advanced Lane Finding Project->4. Finding region of interest,Cs109B - Milestone 1->CS109B - Milestone 1->Look up the favourite movie,[Python] Polynomial Regression->Polynomial Regression model with degree 3
1077,klp.P_in(triple),import kozai.kl_period as klp,"fname_ic2 = res.download[0]
f2 = fits.open(fname_ic2)
f2.info()",,Tutorial->A stroll through the `kozai` python package->The `kl_period` module,Tutorial->A stroll through the `kozai` python package->The `kl_period` module,Jsoc Data Export->JSOC data export->Export FITS files with metadata
159790,"from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train_vectorized, Y_train)","regress = sm.OLS(y, x_new)
modelfit = regress.fit()","import pandas as pd
import glob
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline",,Sentiment Analysis,Assignment2 Ys2808,Capstone Project->_Why can't YouTubers keep their comments to themselves?_->_How to remove the noise?_
242662,"train_df = pd.read_csv(""train_processed.csv"",index_col=""PassengerId"")
ytrain = train_df[""Survived""]

feature_names = [""Pclass"",""Age"",""SibSp"",""Parch"",""Fare"",""IsMale"",""Ticket-4digit"",""Ticket-5digit"",""Ticket-6digit""]
Xtrain = train_df[feature_names]","xtrain_sample = pd.read_csv('data/SAT6/X_train_sat6.csv', header=None, nrows=100, dtype='uint8') # load pixel table 
ytrain_sample = pd.read_csv('data/SAT6/y_train_sat6.csv', header=None, nrows=100) # load label table
annotations = pd.read_csv('data/SAT6/sat6annotations.csv', header=None) # load label annotation table","X.shape
X1.shape",,"03->Index->search ""best C"" with cross-validation",Satellite Image Classification->Satellite Image Classification->Explore and Prepare Data->Initial Data Exploration,Fast Ica->First experiments
122234,"plt.bar(list(np.arange(len(av_abcences))),av_abcences)

#plt.labels(['a', 'b', 'c', 'd', 'e'],ha='left')","av_abcences_without_outliers_by_sum = filter_abcences.groupby(""health"").sum()[""absences""]
plt.bar(list(np.arange(len(av_abcences_without_outliers_by_sum))),av_abcences_without_outliers_by_sum)
plt.xlabel()","analyse_fire_theft_regression(scale='quadratic', learning_rate=LEARN_RATE, num_of_epochs=NUM_OF_EPOCHS)",,01->Alcohol consumption,01->Alcohol consumption,"E63 Assign09 Nn Tensor Flow-Shanaka De Soysa->E-63 Big Data Analytics - Assignment 09 - TensorFlow->Shanaka De Soysa->Problem 3.->Similarly, vary parameter learning_rate through values 0.001, 0.005, 0.01, 0.02 and 0.05. Report and plot changes in the execution time and accuracy."
360280,"def get_neuron(x):
    W = tf.Variable(np.array([1.,1.]), 
                    dtype=tf.float32,
                    name=""Weight"")
    
    # add summaries
    variable_summaries(W, ""Weight"")
    
    b = tf.Variable(np.array([0.],),
                    dtype=tf.float32,
                    name=""bias"")
    
    # add summaries
    variable_summaries(b, ""bias"")

    prod = tf.reduce_sum(W * x) + b
    neuron = 1 / (1 + tf.exp(-prod))
    
    return neuron","def effective_angles(state):
    return b+np.inner(np.transpose(W),state)

def Psi_M(state,a,b,W):
    return np.exp(np.inner(a,state)) * np.prod(2*np.cosh(effective_angles(state)))","t = sp.symbols('t')
x = sp.functions.DiracDelta
y = sp.Function('y')

eq = sp.Eq(4*sp.Derivative(y(t), t, t) + y(t), sp.Derivative(x(t), t) - 0.5*x(t))
sol = sp.dsolve(eq)
# print(sol)
sol",,8->Network,Nqs->Solving the quantum many-body problem with NQS,Signal And System Experiment->Signal and System Experiment->Experiment Jobs->Lab 1.2 - Differential Equation
194939,"### Run the predictions here.
### Feel free to use as many code cells as needed.
test_new_images=np.asarray(test_new_images)

prediction = tf.nn.softmax(logits)
predicted_label = tf.argmax(prediction, 1)
with tf.Session() as sess:
    saver.restore(sess, tf.train.latest_checkpoint('.'))
    #temp_store=[]
    #temp_store.append(image1)
    p=sess.run(predicted_label, feed_dict={x: test_new_images})
    print(""Label = {:}"".format(predicted_label))
    test_accuracy = evaluate(test_new_images, label_new_images)
    print(""Test Accuracy = {:.3f}"".format(test_accuracy))
    saver.save(sess, save_file)
    print(""Predicted labels"",p)
    print(""Actual Labels"", label_new)","# Run rhw predictions
prediction = tf.nn.softmax(logits)
with tf.Session() as sess:
    saver.restore(sess, tf.train.latest_checkpoint('.'))
    new_figs_class = sess.run(prediction, feed_dict={x: test_figs_data, keep_prob : 1.0})
    
for i in range(len(predicts[0])):
    predict_label = np.argmax(new_figs_class[i])
    print('Image', i, 'prediction:',  predict_label, ', the true label is', true_label[i],'.')",potholes.shape,,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Predict label on new Images,Traffic Sign Classifier->Question 6,Ml Diagnostic Part 3->Potholes Database
231844,"#your turn: create some other scatter plots

# scatter plot between *TAX* and *PRICE*
plt.scatter(bos.TAX, bos.PRICE)
plt.xlabel(""full-value property-tax rate per $10,000 (TAX)"")
plt.ylabel(""Housing Price"")
plt.title(""Relationship between TAX and Price"")","plt.scatter(rets.PEP, rets.KO)
plt.xlabel('Returns PEP')
plt.ylabel('Returns KO')","import sqlite3
import sys

db_filename = 'todo.db'
project_name = ""pymotw""

with sqlite3.connect(db_filename) as conn:
    cursor = conn.cursor()

    query = """"""
    select id, priority, details, status, deadline from task
    where project = ?
    """"""

    cursor.execute(query, (project_name,))

    for row in cursor.fetchall():
        task_id, priority, details, status, deadline = row
        print('{:2d} [{:d}] {:<25} [{:<8}] ({})'.format(
            task_id, priority, details, status, deadline))",,Mini Project Linear Regression,1->Pandas basics (Data analysis / Idea generation)->Optional: If there is time...,Sqllite->Using Variables and Queries
301410,"# Mean of each channel as provided by VGG researchers
#Question:What does 3,1,1 do? And try out x[:, ::-1]
vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((3,1,1))

def vgg_preprocess(x):
    x = x - vgg_mean     # subtract mean
    return x[:, ::-1]    # reverse axis bgr->rgb","# This mean of channel is provided by VGG researchers
vgg_mean = np.array([123.68, 116.779, 103.939]).reshape((3, 1, 1))

def vgg_preprocess(x):
    x = x - vgg_mean    # subtract mean
    return x[:, ::-1]   # reverse axis bgr -> rgb","### perform classification
output = net.forward()
print output",,Lesson1->Create a VGG model from scratch in Keras->Model creation,Nps- Vgg16 Scratch->Create a VGG model from scratch in Keras->Model Creation->Creating model architecture,Caffe Classification
216999,"from lib.commons import Network
from lib.keys import PrivateKey

# Create a random private key
priv_key = PrivateKey()
print(""Private Key: "", priv_key.key)

pub_key = priv_key.create_pub_key()
print(f""Public key: {pub_key}"")

# Create a bech32 Address for main net
address_main_net = pub_key.get_segwit_address(Network.MAIN_NET)
print(f""Address (MainNet): {address_main_net}"")","lm2 = smf.ols(formula = 'IncomePerCapita ~ pub_only + priv_only + BothCoverage', data = data2).fit()
print(lm2.summary())","def compute_basket_returns(factor_data, forward_returns, number_of_baskets, month):
    data = pd.DataFrame(factor_data.iloc[month-1]).join(forward_returns.iloc[month-1])
    # Rank the equities on the factor values
    data.columns = ['Factor Value', 'Month Forward Returns']
    data.sort('Factor Value', inplace=True)
    
    # How many equities per basket
    equities_per_basket = np.floor(len(data.index) / number_of_baskets)

    basket_returns = np.zeros(number_of_baskets)

    # Compute the returns of each basket
    for i in range(number_of_baskets):
        start = i * equities_per_basket
        if i == number_of_baskets - 1:
            # Handle having a few extra in the last basket when our number of equities doesn't divide well
            end = len(data.index) - 1
        else:
            end = i * equities_per_basket + equities_per_basket
        # Actually compute the mean returns for each basket
        basket_returns[i] = data.iloc[start:end]['Month Forward Returns'].mean()
        
    return basket_returns",,Examples->Create Segwit native address (Bech32),"Dp1618 Homework2->Homework 02 --- Session 3, Multilinear regression->EX 2, Income vs Insurance coverage (35+10pt -- 10+10+5+10+10)",Ranking Universes By Factors->Ranking Universes by Factors->Basket Returns
458241,"Sessions=pd.DataFrame.from_csv('/Users/attiladobi/Udacity/Udacity_Data_Interview/Sessions.csv')
#view sessions
#Sessions",covariates = pd.DataFrame.from_csv(path + session + '/params/covariates.csv'),pm.summary(trace_beck).round(2),,Udacity,"Imaging-Traces->*svoboda lab / tactile navigation*->Load raw data->Load params
Imaging-Raw->*svoboda lab / tactile navigation*->Setup evnironment->Load params",Mbsr Trial Analysis->Comparison of 6th followup to 3rd->Beck anxiety model
159182,"x_label=[i[0] for i in genre_count]
y_axis=[i[1] for i in genre_count]
x_axis=range(len(genre_count))
plt.bar(x_axis, y_axis, color='g')
plt.xticks(x_axis, x_label)
plt.xticks(rotation=90, fontsize = 10)
plt.yticks(fontsize = 15)
plt.title(""Number of movies produced according to genre"")
plt.ylabel(""No. of movies made"", fontsize = 10);","# beware! each image in here is using different color scale
fig = plt.figure(figsize=(10, 15))

# from the bottom to the top:
f1 = aplpy.FITSFigure('./images/uninjected.ms.afterclean.withmask.cont.fits', figure=fig, subplot=[0.0,0.0,0.5,0.3333])
f1.set_tick_labels_font(size='x-small'); f1.set_axis_labels_font(size='small')
f1.show_colorscale()
f1.add_label(0.5, 0.95, ""uninjected - clean - with mask"", relative=True)

f2 = aplpy.FITSFigure('./images/injected.ms.afterclean.withmask.cont.fits', figure=fig, subplot=[0.5,0.0,0.5,0.3333])
f2.set_tick_labels_font(size='x-small'); f2.set_axis_labels_font(size='small')
f2.hide_yaxis_label(); f2.hide_ytick_labels()
f2.show_colorscale()
f2.add_label(0.5, 0.95, ""injected - clean - with mask"", relative=True)


f3 = aplpy.FITSFigure('./images/uninjected.ms.afterclean.cont.fits', figure=fig, subplot=[0.0,0.3333,0.5,0.3333])
f3.set_tick_labels_font(size='x-small'); f3.set_axis_labels_font(size='small')
f3.hide_xaxis_label(); f3.hide_xtick_labels()
f3.show_colorscale()
f3.add_label(0.5, 0.95, ""uninjected - clean - no mask"", relative=True)

f4 = aplpy.FITSFigure('./images/injected.ms.afterclean.cont.fits', figure=fig, subplot=[0.5,0.3333,0.5,0.3333])
# f4.set_tick_labels_font(size='x-small'); f4.set_axis_labels_font(size='small')
f4.hide_yaxis_label(); f4.hide_ytick_labels()
f4.hide_xaxis_label(); f4.hide_xtick_labels()
f4.show_colorscale()
f4.add_label(0.5, 0.95, ""injected - clean - no mask"", relative=True)

f5 = aplpy.FITSFigure('./images/uninjected.ms.withoutclean.cont.fits', figure=fig, subplot=[0.0,0.6666,0.5,0.3333])
f5.set_tick_labels_font(size='x-small'); f5.set_axis_labels_font(size='small')
f5.hide_xaxis_label(); f5.hide_xtick_labels()
f5.show_colorscale()
f5.add_label(0.5, 0.95, ""uninjected - before cleaning"", relative=True)

f6 = aplpy.FITSFigure('./images/injected.ms.withoutclean.cont.fits', figure=fig, subplot=[0.5,0.6666,0.5,0.3333])
# f6.set_tick_labels_font(size='x-small'); f6.set_axis_labels_font(size='small')
f6.hide_xaxis_label(); f6.hide_xtick_labels()
f6.hide_yaxis_label(); f6.hide_ytick_labels()
f6.show_colorscale()
f6.add_label(0.5, 0.95, ""injected - before cleaning"", relative=True)

fig.canvas.draw()","indices = np.arange(len(trainX[use_features].columns))
indices_nz = np.nonzero(lasso.coef_)
indices_z = np.setdiff1d(indices, indices_nz)
set([use_features[i] for i in indices_z])",,"Investigate A Dataset->First, I will examine whicn genre of movies is made most over time?",Completeness Test 01->Completeness Test - 01->Result of the first test,"Home Prices Ver 2->Predicting home prices in Ames, IA->4 Data exploration->5.2 Lasso"
399202,"from genderclassifierfunctions import unzipfile
from genderclassifierfunctions import loadjsonfiletolist
from genderclassifierfunctions import writecsvfile
from genderclassifierfunctions import clean
from genderclassifierfunctions import feature_processing
from genderclassifierfunctions import feature_selection_classifier_1
from genderclassifierfunctions import feature_selection_classifier_2
from genderclassifierfunctions import feature_selection_classifier_3
from genderclassifierfunctions import kmeans_classification
from genderclassifierfunctions import minibatchkmeans_classifier
from genderclassifierfunctions import plot3d
from genderclassifierfunctions import plot2d
from genderclassifierfunctions import eigen_decomposition
from genderclassifierfunctions import concatenate_result
import genderclassifierfunctions
from sklearn.decomposition import PCA","image_df_clean.prediction_1 = image_df_clean.prediction_1.str.title()
image_df_clean.prediction_2 = image_df_clean.prediction_2.str.title()
image_df_clean.prediction_3 = image_df_clean.prediction_3.str.title()","import cv2
import numpy as np

img = cv2.imread('messi5.jpg')
 
res = cv2.resize(img,None,fx=2, fy=2, interpolation = cv2.INTER_CUBIC)
 
#OR

height, width = img.shape[:2]
res = cv2.resize(img,(2*width, 2*height), interpolation = cv2.INTER_CUBIC)

cv2.imshow('image',img)
cv2.waitKey(0)
cv2.destroyAllWindows()",,"Genderclassifier-Checkpoint->Gender Labelling with Python->Import libraries
Genderclassifier->Gender Labelling with Python->Import libraries",Wrangle Act->Data Cleaning->Issue #6: the first letter of all dog names are not all capitalized.,Lecture 4 Image Processing->Scaling
473356,"import os

cwd = os.getcwd()
years = list(range(2012, 2017))
data_file_names = []
for _1, _2, files  in os.walk(cwd+""\\data""):
    data_file_names = files
data_file_names = {year:file for year, file in zip(years, data_file_names)}","import os

cwd = os.getcwd()
years = list(range(2012, 2017))
data_file_names = []
for _1, _2, files  in os.walk(cwd+""\\data""):
    data_file_names = files
data_file_names = {year:file for year, file in zip(years, data_file_names)}","bp_table = pd.crosstab(index=df_bp[""Base""], 
                       columns=df_bp[""Protein""])",,Parsing Data->Parsing the data,Parsing Data->Parsing the data,Cava-Items Sold-Labelname
190599,"plot_imgs(X_train, y_train, 5, 5)","plot_img(x_train.data[0:12], np.argmax(y_train.data[0:12], axis=1), labels)","evaluate(simp_model, data_loader_val)
evaluate(model_from_scratch, data_loader_val)",,Traffic Sign Classifier->Include an exploratory visualization of the dataset,Convolutional Neural Network (Modified Mnist)->Modified MNIST Challenge->Data Splitting,Res Net And Fine Tuning->Model fine-Tuning
230819,"from pylab import *

# your turn: create some other scatter plots
feature_lst = bos.columns[0:-1]

n_row = 7
n_col = 2

fig, ax = plt.subplots(figsize=(20,50))

for r in range(len(feature_lst)):
        subplot(n_row,n_col,r+1)
        sns.regplot(x=feature_lst[r], y=""PRICE"", data=bos)","col = da.drop(['price','date'],axis =1).columns

for i in range(len(col)):
    plt.subplot(6,3,i+1)
    sns.regplot(da[col[i]],da['price'])
plt.subplots_adjust(top=3, bottom=0, left=0, right=2, hspace=0.5,
                    wspace=0.35)","pivoted_control_samples.hist()
sns.despine(offset=10, trim=True)",,Mini Project Linear Regression->EDA and Summary Statistics->Scatterplots,House Price Prediction->Regression Problem: Predicting House Prices->Plotting a Scatter plot for all columns to find relevant features,Analyse Hsa-Mi R-124A-3P Transfection Time-Course->Analyse hsa-miR-124a-3p transfection time-course
124826,marketData.tail() # retrieve last 5 rows to look at market data,return_market.tail() # Market S&P 500 return array,"statistic, p_val = normaltest(df.temperature)

print('p value =', p_val)",,Final Project->Introduction,Final Project->Introduction,"8->1.Is the distribution of body temperatures normal?->Investigate if the body temperatures are normal distibuted
8->5.At what temperature should we consider someone's temperature to be ""abnormal""?->Background"
27915,"import pandas as pd

def make_empty_table(variables, subset_keys = None, fill_value = 0):
    """"""Creates an empty table-dataframe with rows for all variables.
    
        variables (dict): lists of values for every variable
        subset_keys: list of variables to include in the table
        fill_value: initialization value
        
        returns: empty table-dataframe with a row for every variable combination
    """"""
    
    # if list of subset is set empty: return table with only one entry
    if subset_keys == []:
        return pd.DataFrame(fill_value, index = [0], columns = ['value'])
    
    # filter variable subset
    if subset_keys:
        variables = {key: variables[key] for key in subset_keys}
    
    # create a new pandas dataframe
    # one row for every combination of variable values in the subset
    # (by taking the cartesian product)
    varnames = sorted(variables.keys())
    varvalues = [variables[var] for var in varnames]
    i = pd.MultiIndex.from_product(varvalues, names = varnames)
    df = pd.DataFrame(fill_value, index = i, columns = ['value']).reset_index()
    return df","#nsd=[(n,s,d) for n in rec_items_list for s in strategies for d in data_sets]
#nsdk=[(n,s,d,k) for n in rec_items_list for s in strategies for d in data_sets for k in kpi]
#index = pd.MultiIndex.from_tuples(nsd, names=[""n_items"",'Strategy','Train/Test'])
#index = pd.MultiIndex.from_tuples(nsdk, names=[""n_items"",'Strategy','Train/Test','kpi'])
index = pd.MultiIndex.from_product( [rec_items_list, strategies, data_sets,kpi], 
                                       names=[""n_items"",'Strategy','Train/Test','KPI'])
strategy_df=pd.DataFrame(index=index,columns=[""value""])
for n in rec_items_list:
    for s in strategies:
        for d in data_sets:
            strategy_df.loc[(n,s,d,""k_avg_rating"")]=strategies_0[s].loc[n,(""sum"",d)]/strategies_0[s].loc[n,(""count"",d)]
            strategy_df.loc[(n,s,d,""k_n_ratings"")]=strategies_0[s].loc[n,(""count"",d)]
            strategy_df.loc[(n,s,d,""k_total_rating"")]=strategies_0[s].loc[n,(""sum"",d)]
            strategy_df.loc[(n,s,d,""k_score_15"")]=strategies_0[s].loc[n,(""sum"",d)]-2.5*strategies_0[s].loc[n,(""count"",d)]
strategy_df_ri=strategy_df.reset_index()
strategy_df_ri","import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
figure(num=None, figsize=(12, 9), dpi=80, facecolor='w', edgecolor='k')

R = calculate_cv(output_images_links, target_images_links, 'r')
G = calculate_cv(output_images_links, target_images_links, 'g')
B = calculate_cv(output_images_links, target_images_links, 'b')
Gray = calculate_cv(output_images_links, target_images_links, 'gray')


plt.plot(R, label= 'R') 
plt.plot(G, label= 'G') 
plt.plot(B, label= 'B') 
plt.plot(Gray, label= 'Gray') 

plt.ylabel('RMSE')
plt.xlabel('Day')
plt.legend()
plt.show()


print(f'R:    Min   {np.amin(R)}               Avg   {np.mean(R)}                Max   {np.amax(R)}')  
print(f'G:    Min   {np.amin(G)}               Avg   {np.mean(G)}                Max   {np.amax(G)}')
print(f'B:    Min   {np.amin(B)}               Avg   {np.mean(B)}                 Max   {np.amax(B)}')
print(f'Gray: Min   {np.amin(Gray)}               Avg   {np.mean(Gray)}                Max   {np.amax(Gray)}')",,Notebook->Cluster Graph Belief Propagation,Ml-100->Recommendation->3rd strategy: Liked items from the same user category->(3a) Segment by sex,Run2 Evaluation->Coefficient of Variation
405970,"rhyme = ('There was a croOKed man, and he walked a crooked mile.;'+
         'He found a cROOKed sixpence upon a crooked stile.;'+
         'He bought a crOoked cat, which caught a CROoked mouse,;'+
         'And they all lived together in a little crooKEd house.')
         
rhyme = rhyme.replace(',', '').replace(';', '\n')
rhyme = rhyme.lower().replace('crooked', 'CROOKED')
print rhyme","clean = clean.lower().split()
print(list(clean))","#model_vae = VAE().to(device) - if cuda is available, but not the case, yet
model_vae = VAE(encoder,decoder)
optimizer = Optimizer(model_vae,optimizer = optim.Adam(model_vae.parameters(), lr=1e-4),early_stopping = True)
#optim.RMSprop(model_vae.parameters(), lr = 1e-3, momentum=0.9)",,Solutions 4,Model-06->Cleaning and preprocessing,Mnist Autoencoder->VAE Encoder/Decoder architecture
474800,"# PROPERTY ID - DO NOT EDIT !  
DOC.set_id('cmip6.atmos.gravity_waves.subgrid_scale_orography')  

# PROPERTY VALUE(S): 
# Set as follows: DOC.set_value(""value"")  
# Valid Choices: 
#      ""effect on drag""  
#      ""effect on lifting""  
#      ""enhanced topography""  
#      ""Other: [Please specify]""  
# TODO

# PROPERTY QUALITY CONTROL STATUS:  
# 0=draft, 1=in-review, 2=reviewed  
DOC.set_qc_status(0)","# PROPERTY ID - DO NOT EDIT !  
DOC.set_id('cmip6.toplevel.key_properties.tuning_applied.fresh_water_balance')  

# PROPERTY VALUE: 
# Set as follows: DOC.set_value(""value"")  
# TODO

# PROPERTY QUALITY CONTROL STATUS:  
# 0=draft, 1=in-review, 2=reviewed  
DOC.set_qc_status(0)","tf.reset_default_graph()
tflearn.config.init_training_mode()
net = tflearn.input_data([None, MAX_SENTENCE_LENGHT])
net = tflearn.embedding(net, input_dim=NUM_DICTIONARY_WORDS+1, output_dim=100)
net = tflearn.lstm(net, 128, dropout=0.8)
net = tflearn.fully_connected(net, 2, activation='softmax')
net = tflearn.regression(net, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')

LSTM_model = tflearn.DNN(net, tensorboard_verbose=0)
LSTM_model.load('model_stanford_lstm_2classes.tfl')

sentence = 'djimon hounsou and anthony hopkins turn in excellent performances'
print('1. ', sentence, GetPolarity(sentence))
sentence = 'they turn in excellent performances'
print('2. ', sentence, GetPolarity(sentence))
sentence = 'and turn in excellent performances'
print('3. ', sentence, GetPolarity(sentence))",,Atmos->29. Cloud Scheme --> Sub Grid Scale Water Distribution->34.3. Subgrid Scale Orography,Toplevel->4. Key Properties --> Tuning Applied->4.6. Fresh Water Balance,Report
80748,"# these are the corresponding beta values for sqrt, bdroms, and age
lr.coef_","#Intercept
lr.intercept_","training_set_group_by_class = {}
test_set_group_by_class = {}
for k, v in uuids_group_by_class.iteritems():
    
    total = len(v)
    training_size = int(total * training_percentage)
    test_size = int(total * test_percentage)
    
    training_set = v[:training_size]
    test_set = v[-1*test_size:]
    
    training_set_group_by_class[k] = training_set
    test_set_group_by_class[k] = test_set
    
    print '{}: training set size: {}'.format(k, len(training_set))
    print '{}: test set size: {}'.format(k, len(test_set))",,4->Introduction->Interpreting Linear Regression,Iowa Liquor->Iowa Liquor Project->Model Building - Linear Regression,Seti Img To-Binary->Train/Test
345838,"param_test1 = {
 'min_child_weight':range(1,6,2)}","xgb2.GridSearch(param_test1 , n_jobs=8 , iid=False , cv=5)","from sklearn.svm import SVC
from sklearn.cross_validation import cross_val_score

svm = SVC(C = 2000)

# fit model to training data
svm.fit(X_train, y_train)

# evaluate model's training performance
y_train_pred = svm.predict(X_train)
print('Training accuracy:', accuracy_score(y_train, y_train_pred))

# evaluate model's validation performance
print('Validation accuracy:', cross_val_score(estimator = svm, X = X_train, y = y_train, cv = 5).mean())",,Process->Results->Best Random Forest result with 75% threshold->gridsearchcv:,Model Xgboost Tune->Xgboost model tuning->Model tuning,6->One of the commandments of predictive analysis is you never change your model after seeing the testing performance.
173450,a.binary_img(),"# not sure why this is how I named outputs...
#savepath = wmap_path.replace('.nii.gz', '_prismahc_diffsp.nii.gz')

wmap_path = 'agg_95pct_wmap_masked.nii.gz'

wmap_avg_path = 'agg_wmap.nii.gz'

# Giovanni's blobs that we want to display in with tracks for Marilu's talk
spmblob_path = '/Volumes/macdata/groups/language/kesshi/SPM/kesshi_test/kesh_disconn_masks/disconn/cluster_size_thr100.nii.gz'

# mask provided by PET group (Renaud & Adrienne)
petmask_path = '/Users/kesshijordan/ref_data/PET_MNI_MASKS/mask_sum_GM-WM_sup0p5_without_crblm-pons.nii'
wmap, wmap_aff = loadnii(wmap_path)

wmap_resliced = nil.image.resample_to_img(wmap_path, mnipath_fa, interpolation='nearest').get_data()
spm_resliced = nil.image.resample_to_img(spmblob_path, mnipath_fa, interpolation='nearest').get_data()
petmask_resliced = nil.image.resample_to_img(petmask_path, mnipath_fa, interpolation='nearest').get_data()
wmap_avg_resliced = nil.image.resample_to_img(wmap_avg_path, mnipath_fa, interpolation='nearest').get_data()","# Preprocess training set 
train_set = data[(data['TIME_ID'] >= start_train_dt) & (data['TIME_ID'] <= end_train_dt)]
test_set  = data[(data['TIME_ID'] >= start_test_dt)  & (data['TIME_ID'] <= end_test_dt)]",,H1B Taxrent Indeed Analysis->Application Amount by State,"Superimpose Mni Results On Tractography->Now transform results into the ""diffusion template"" space",Capstone Project->Data exploration part
449564,"df = pd.DataFrame.from_dict(normalized_word_frequencies, orient='index')
word_freq = df.nlargest(25, 0).plot(kind='bar', title='Normalized Word Frequency\n%s' % (artist), legend=False)
word_freq.set_xlabel(""Word"")
word_freq.set_ylabel(""Distribution"")","all_sentiments.pop('compound', 0)
df = pd.DataFrame.from_dict(all_sentiments, orient='index')
sent_plot = df.plot(kind='bar', title='Sentiment Distribution\n%s' % (artist), legend=False)
sent_plot.set_xlabel(""Sentiment"")
sent_plot.set_ylabel(""Distribution"")
sent_df = df",!ls $model_dir/checkpoints,,Initial Analysis->Most negative  album,Initial Analysis->Clean the lyrics,Tutorial - Tensor Flow From Estimators To Keras->TensorFlow: From Estimators to Keras->Building a Keras Model->4. Run experiment
36849,"# Concat the dummy DataFrames Together
dummies = pd.concat([occ_dummies,hus_occ_dummies],axis=1)","# rename colums of both dummy variables
occ_dummies.columns = ['occ1', 'occ2', 'occ3', 'occ4', 'occ5', 'occ6']
hus_occ_dummies.columns = ['hocc1', 'hocc2', 'hocc3', 'hocc4',
                           'hocc5', 'hocc6']

# drop old categorical variables and number of years had affair
x1 = df.drop(['occupation','occupation_husb','had_affair'],axis=1)

# concatenate dummies together and combine with rest of data
dummies = pd.concat([occ_dummies,hus_occ_dummies],axis=1)
x1 = pd.concat([x1,dummies],axis=1)
x1.head()","import IPython.display as disp
disp.Image(url='http://bit.ly/1MmivWM')",,"Logistic Regression->Supervised Learning: Logistic Regression->Data Preparation
Supervised Learning - Logistic Regression",Lec76-79-Logistic-Regression->Part 5: Data Preparation,Mempy Nate->Python for Community Analysis->Neighborhood Design->Total Appraisal per Acre by Neighborhood
106013,"y, y.shape","y = [2.4, 0.7, 2.9, 2.2, 3.0, 2.7, 1.6, 1.1, 1.6, 0.9]","def heatmap_cluster(n, ax):
    km = KMeans(n_clusters=n, random_state=0)
    users['cluster'] = km.fit_predict(user_matrix)
    cluster_matrix = users.groupby('cluster').mean().values
    sns.heatmap(cluster_matrix, cmap='Reds', ax=ax)
    ax.set_title(str(n) + ' clusters')",,Introduction To Python->Lists->2-D plot formation,Pca Tutorial->Principal component analysis tutorial->Data,User-Clustering-Siming-Topics->Cluster Users based on Topics->Find optimal number of clusters
475320,"# Remove Digits 
import re
data['full_text_proc'] = data['full_text_proc'].apply(lambda x : re.sub(""\d+"", """", x))","import re
from nltk.stem import *
stemmer = PorterStemmer()

proc_text = []

for i in data.index:
    tmp_text = re.sub(""\d+"", """", data.text[i]) # removes numbers  
    tmp_text = re.sub(r'[?|$|.|!]',r'', tmp_text) # removes puncutation
    tmp_text = tmp_text.lower() # makes text lowercase
    tmp_text = ([i for i in tmp_text.split() if i not in stoplist]) # tokenizes and removes stopwords
    tmp_text = ' '.join(tmp_text) # joins words back to stopwords
    tmp_text = stemmer.stem(tmp_text) # stems words
    proc_text.append(tmp_text)
    
data.proc_text = proc_text","from __future__ import print_function, division

from torchvision import models, datasets, transforms
import torchvision
import torch.nn as nn
import torch
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
import matplotlib.pyplot as plt
import time
import os
import copy

plt.ion()
%matplotlib inline",,Main Notebook->Training->Training: CNN+BiLSTM->Note: we will not run common and rare word removal cells. This will be taken care of with TF-IDF later.,Scip Classification 4-5->scip classification->preliminary processing,Fine Tuning
364662,"np.random.seed(1)
embed_size = 10

# word embeddings
embed = (np.random.rand(len(vocab),embed_size) - 0.5) * 0.1

# embedding -> embedding (initially the identity matrix)
recurrent = np.eye(embed_size)

# sentence embedding for empty sentence
start = np.zeros(embed_size)

# embedding -> output weights
decoder = (np.random.rand(embed_size, len(vocab)) - 0.5) * 0.1

# one hot lookups (for loss function)
one_hot = np.eye(len(vocab))","#split into training and test datasets 
np.random.seed(1818)
msk = np.random.rand(len(df)) < 0.66
dftrain = df[msk]
dftest = df[~msk]","conf = Config('GaAs_slab.yml')
sim = Simulator(conf)
sim = setup_sim(sim)",,Chapter12->Let's Train it!,7 Classification Models->Data set up,Ga As Slab Consistency-Checkpoint->Introduction
398763,r = praw.Reddit('Test by u/_Daimon_'),r = praw.Reddit('comment scraper'),"cross_val_score(clf, X_std, y)",,Week1->Scrape comments from reddit,Reddit Scrapper->Reddit Comment Scraper,Nested Cross Validation->Create Outer Cross Validation (For Model Evaluation)
443290,"def print_eval_scores(model, X_train, y_train, cv):
    accuracy = cross_val_score(model, X_train, y_train, cv=cv,scoring='accuracy')
    log_loss = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_log_loss')
    roc_auc = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc')

    print('Mean Accuracy: %s' % accuracy.mean())
    print('Mean Log Loss: %s' % log_loss.mean())
    print('Mean Area Under ROC Curve: %s' % roc_auc.mean())
    
print_eval_scores(lr_model, X_train, y_train, kfold)","for clf in classifiers:
    name = clf.__class__.__name__
    Accuracy = cross_val_score(clf, train, np.ravel(target), scoring = 'accuracy',cv=5).mean()
    LogLoss = cross_val_score(clf, train, np.ravel(target), scoring = 'neg_log_loss',cv=5).mean()
    ROC_AUC = cross_val_score(clf, train, np.ravel(target), scoring = 'roc_auc',cv=5).mean()
    
    print(""=""*30)
    print(name)
    
    print('****Results****')
    print(""Accuracy: {}"".format(Accuracy))
    print(""LogLoss: {}"".format(LogLoss))
    print(""ROC_AUC: {}"".format(ROC_AUC * 100))
    
print(""=""*30)","#Creating y1
y1= df['loc1']

#Encoding categorical data
labelencoder = LabelEncoder()
y1= labelencoder.fit_transform(y1)
y1= pd.DataFrame(y1, columns=['loc1'])
y1",,Flight Delays->Flight Delays->Exploratory Data Analysis->Logistic Regression,Predict Blood Donations->This kernel contains the solution codes for Warm Up: Predict Blood Donations competion of www.drivendata.org competition website,Iteration-1 Model->Encoding y1 (x-coordinate)
469912,"print(""Augmented Dickey-Fuller test for air passenger data"")
adf_test(log_pass)","print(""Augmented Dickey-Fuller test for air passenger data"")
adf_test(pass_stationary)","do_rec = input(""Test by recording sound [y/n]: "")

FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE = 8000 # low rate so that the audio resembles phone call quality
CHUNK = 1024
RECORD_SECONDS = 5
WAVE_OUTPUT_FILENAME = ""test_audio.wav""
IMG_EXT = "".png""

if do_rec.lower() == 'y':
    audio = pyaudio.PyAudio()

    # start Recording
    stream = audio.open(format=FORMAT, channels=CHANNELS,
                    rate=RATE, input=True,
                    frames_per_buffer=CHUNK)
    print (""recording..."")
    frames = []

    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
        data = stream.read(CHUNK)
        frames.append(data)
    print (""finished recording"")


    # stop Recording
    stream.stop_stream()
    stream.close()
    audio.terminate()

    waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')
    waveFile.setnchannels(CHANNELS)
    waveFile.setsampwidth(audio.get_sample_size(FORMAT))
    waveFile.setframerate(RATE)
    waveFile.writeframes(b''.join(frames))
    waveFile.close()
    
    plt.figure(figsize=(12, 12))
    y, sr = librosa.load(WAVE_OUTPUT_FILENAME)
    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=256, fmax=4000)
    librosa.display.specshow(librosa.logamplitude(S,ref_power=np.max),
                            fmax=4000)
    output_filename = WAVE_OUTPUT_FILENAME[:-4] + ""_spec"" + IMG_EXT
    plt.savefig(output_filename, bbox_inches='tight', pad_inches = 0)
    print(""\nSaved spectrogram:"")",,"Time Series Analysis Arima, Sarimax","Time Series Analysis Arima, Sarimax",Pocket Dial Vs Regular Call Cnn->Pocket Dial vs. Regular Call Classification->Part 2: Training or Testing the Convolutional Neural Net Model
458737,"%%time
# Create bottleneck features for test images
datagen = ImageDataGenerator(rescale=1. / 255)

# build the VGG16 network without the FC layers
model = VGG16(include_top=False, weights='imagenet')

test_generator = datagen.flow_from_directory(
    test_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode=None,
    shuffle=False)

bottleneck_features = model.predict_generator(test_generator, nb_test_samples//batch_size)
print(""VGG16 test bottleneck features calculated."")

np.save(open(bottleneck_features_test, 'wb'), bottleneck_features)
print(""Training samples saved."")","%%time
# Create bottleneck features for test images
datagen = ImageDataGenerator(rescale=1. / 255)

# build the VGG16 network without the FC layers
model = VGG16(include_top=False, weights='imagenet')

test_generator = datagen.flow_from_directory(
    test_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode=None,
    shuffle=False)

bottleneck_features = model.predict_generator(test_generator, nb_test_samples//batch_size)
print(""VGG16 test bottleneck features calculated."")

np.save(open(bottleneck_features_test, 'wb'), bottleneck_features)
print(""Training samples saved."")",'abc' + 'bc',,Cat Detector,Cat Detector,1->Checkpoint->Strings
444804,"print('Conditional log likelihood, when two parameters are fixed to their max likelihood value.')
npoints = 8
vars_d2 = [[0, 1], [2, 3], [0, 2], [1, 3]]
axs = [[0,0], [0,1], [1,0], [1,1]]
fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(14,8))
for j, var_d2 in enumerate(vars_d2):
    x = np.linspace(0, 1, npoints)
    y = np.linspace(0, 1, npoints)
    xx, yy = np.meshgrid(x, y)
    xx, yy = xx.reshape((-1,)), yy.reshape((-1,))
    samples = fixed_p.reshape((1,4))*np.ones((npoints**2,4))
    samples[:,var_d2[0]] = xx
    samples[:,var_d2[1]] = yy
    zz = model.log_like(data, samples)
    tmp = np.array([x_norm2real(s) for s in samples])
    x, y = tmp[:,var_d2[0]], tmp[:,var_d2[1]]
    ax_j = ax[axs[j][0],axs[j][1]]
    t = ax_j.contourf(x.reshape((npoints,npoints)),y.reshape((npoints,npoints)),zz.reshape((npoints,npoints)), 20)
    ax_j.set_title('Conditional log likelihood')
    ax_j.set_xlabel(var_names[var_d2[0]])
    ax_j.set_ylabel(var_names[var_d2[1]])
    cbar = plt.colorbar(t, ax=ax_j)
plt.show(fig)","#This function allows us to see the model
def plot_decision_boundary(model, X, y):
    X_max = X.max(axis=0)
    X_min = X.min(axis=0)
    xticks = np.linspace(X_min[0], X_max[0], 100)
    yticks = np.linspace(X_min[1], X_max[1], 100)
    xx, yy = np.meshgrid(xticks, yticks)
    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = ZZ >= 0.5
    Z = Z.reshape(xx.shape)
    fig, ax = plt.subplots()
    ax = plt.gca()
    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)
    ax.scatter(X[:,0], X[:,1], c=y,s=40, alpha=0.4)
    plt.xlabel(""Feature One"")
    plt.ylabel(""Feature Two"")","# execute gridsearch and get best score
rf_grid = GridSearchCV(scale_pipeline, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring= 'accuracy')

# fit on ALL grid.fit(X, y) and NOT grid.fit(X_train, y_train) because the GridSearchCV will automatically 
# split the data into training and testing data (this happen internally).
rf_grid.fit(X, y)

print(""RF Grid search Best Score:"", rf_grid.best_score_)
print(""RF Grid search Cross Validation Results:"", rf_grid.cv_results_)",,Parameter Estimation - Material Homogenization->Parameter estimation in material homogenization->Maximum likelihood with UQpy,"Logistic Regression Lesson (In Class)-><b>Logistic Regression</b>-><b> Can you use Spotify data to predict whether or not I will like a song? </b>-><b>Probability, odds, e, log, log-odds</b>",Model Eval Rf-Remix->Random Forest->Execute GridSearch
59113,"# Define the train model function and train the model
def train_model_with_lr (iter, lr_list):
    
    # iterate through different learning rates 
    for i, lr in enumerate(lr_list):
        model = linear_regression(1, 1)
        optimizer = optim.SGD(model.parameters(), lr = lr)
        for epoch in range(iter):
            for x, y in trainloader:
                yhat = model(x)
                loss = criterion(yhat, y)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
        # train data
        Yhat = model(train_data.x)
        train_loss = criterion(Yhat, train_data.y)
        train_error[i] = train_loss.item()
    
        # validation data
        Yhat = model(val_data.x)
        val_loss = criterion(Yhat, val_data.y)
        validation_error[i] = val_loss.item()
        MODELS.append(model)

train_model_with_lr(10, learning_rates)","# Practice create model1. Train the model with batch size 30 and learning rate 0.1, store the loss in a list <code>LOSS1</code>. Plot the results.

data_set = Data2D()
train_loader = DataLoader(dataset = data_set, batch_size = 30)
model1 = linear_regression(2, 1)
optimizer = optim.SGD(model1.parameters(), lr = 0.1)
LOSS1 = []
epochs = 100
def train_model(epochs):    
    for epoch in range(epochs):
        for x,y in train_loader:
            yhat = model1(x)
            loss = criterion(yhat,y)
            LOSS1.append(loss.item())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()     
train_model(epochs)
Plot_2D_Plane(model1 , data_set)  
plt.plot(LOSS1)
plt.xlabel(""iterations "")
plt.ylabel(""Cost/total loss "")","midi_file = ""../tests/resources/midi/Bomfunk_MCs_-_Freestyler.mid""
reader = MidiReader(note_mapper)
df = reader.convert_to_dataframe(midi_file)",,2->Different `lr` and Data Structures to Store results for different Hyperparameters,2,Basics->Load Mappings File->Convert a MIDI file to a DataFrame
206866,"df['TF_IDF_score_lemmatized'] = apply_tf_idf('Tweets_tokens_remove_special_chars_remove_stop_words_lemmatized', df)
df['TF_IDF_score_lemmatized_highest'] = df['TF_IDF_score_lemmatized'].apply(find_highest_scores_word)
df.head(2)","df['TF_IDF_score_lemmatized'] = apply_tf_idf('Tweets_tokens_remove_special_chars_remove_stop_words_lemmatized', df)
df['TF_IDF_score_lemmatized_highest'] = df['TF_IDF_score_lemmatized'].apply(find_highest_scores_word)
df.head(2)","# Setup sample data
import random, string

def randomword():
    return ''.join([random.choice(string.ascii_lowercase) for 
                    _ in range(random.randrange(1, 8))])

sample = ' '.join([randomword() for _ in range(10000)]);",,Exploratory Data Analysis->Follow-up Questions:->Minor Follow-up Question: Would Lemmatization help to improve quality for identifying topics further?,Exploratory Data Analysis->Follow-up Questions:->Minor Follow-up Question: Would Lemmatization help to improve quality for identifying topics further?,Timing->Timing
488252,"test_matrix[0,0] = 100
test_matrix","test_matrix[::-2,:]","plt.figure(figsize=(16, 32))
# comparing number of neurons needed in the hiden layers  
hidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]
for i, nn_hdim in enumerate(hidden_layer_dimensions):
    plt.subplot(5, 2, i+1)
    plt.title('Hidden Layer size %d' % nn_hdim)
    model = build_model(nn_hdim)
    plot_decision_boundary(lambda x: predict(model, x))
plt.show()",,Cpds 02-Checkpoint->Workshop 2: Matrix and Data Frame->Matrix and Array->Matrix,Cpds 02-Checkpoint->Workshop 2: Matrix and Data Frame->Matrix and Array->Advanced Indexing,3 Backprop And Finish->Training and analysing our results->Backpropagation
204609,"y_train = data_train['SalePrice']

#Index for splitting combined data into train and split later
index_train = data_train.shape[0]
index_test = data_test.shape[0]

comb_data = pd.concat((data_train, data_test)).reset_index(drop=True)
comb_data.drop('SalePrice', axis=1, inplace=True)

print(""Comb_data shape: {}"".format(comb_data.shape))","# remove and save the Id field..
train_ID = train['Id']
test_ID = test['Id']

#Now drop the  'Id' 
train.drop(""Id"", axis = 1, inplace = True)
test.drop(""Id"", axis = 1, inplace = True)

# get the target variable 
y_train = train.SalePrice.values

# combine train and test and remove the target variable
ntrain = train.shape[0]
ntest = test.shape[0]

all_data = pd.concat((train, test)).reset_index(drop=True)
all_data.drop(['SalePrice'], axis=1, inplace=True)
print(""all_data size is : {}"".format(all_data.shape))
print "" \n The rows have increased as we have combined both the train and test data""","rf1=RandomForestClassifier(n_estimators=500, max_depth=5, min_samples_split=2, 
                           min_samples_leaf=1, max_leaf_nodes=None, min_impurity_decrease=0.0, 
                           bootstrap=True, oob_score=True, n_jobs=-1, random_state=2, verbose=0, 
                           warm_start=False, class_weight={0:1,1:3})",,Code->Scatter plots for some of the most correlated variables with Target Variable,Price Prediction->2) Combine test and train data,Fraud Detect Rf->Baseline Random Forest Model
298359,"calculate_distance(formatted_rent_info, cbd_lat, cbd_lng, suburb_stations)
calculate_distance(formatted_sale_info, cbd_lat, cbd_lng, suburb_stations)","formatted_rent_info = data_format(rent_info, 'rent')
formatted_sale_info = data_format(sale_info, 'sale')","import seaborn as sns

pca1 = PCA(5)
pca1.fit(whole.iloc[:,1:10])
projected = pca1.fit_transform(whole.iloc[:,1:10])

def draw_densities(proj1, dt):
    ncols = nrows = len(proj1[0])
    fig,axes=plt.subplots(nrows,ncols,sharex=False, figsize=(20,20))
    c=[0,0,'#0000FF', 0, '#FF0000']
    color=[c[x] for x in dt['class']]
    #rng = np.random.RandomState(0)
    
    """"""We couldn't manage to set the size according to the number of overlapping points. There
    always seems to be concentrict circles of the same color, which wouldn't be the case in a 
    proper plot (each slot should have only one circle of each color with the size correspoding
    to its frequency. So we fixed the size and tried to deduce the number of occurances by the color
    intensity. """"""
    size=20 # 120#[x/4 for x in range(800)]#[[[x1*y1*10 for x1 in range(11)] for y1 in range(11)]])#120

    for i in range(nrows):
        axes[4,i].set_xlabel('Component '+str(i+1))
        axes[i,0].set_ylabel('Component '+str(i+1))
        #y = proj1[i]

        for j in range(ncols):
            #x = proj1[j]
            
            if j==i:
                sns.set_style('whitegrid')
                sns.kdeplot(proj1[dt['class']==2,i], color=c[2], ax=axes[i,j])
                sns.kdeplot(proj1[dt['class']==4,i], color=c[4], ax=axes[i,j])
            else:
                axes[i,j].scatter(proj1[:,j],proj1[:,i], alpha=0.3, color=color, s=size)
        
    fig.text(0.5,0.04,'Variables', ha='center', va='center', fontsize=30)
    fig.text(0.05,0.5,'Frequency', ha='center', va='center', \
                 rotation='vertical',fontsize=30)
    
    hndl = [Line2D([0], [0], marker='.', color=c[2], label='Scatter',markerfacecolor=c[2], markersize=15),\
          Line2D([0], [0], marker='.', color=c[4], label='Scatter',markerfacecolor=c[4], markersize=15)]
    
    fig.legend(handles=hndl,labels=('Bening','Malignant'),loc='upper right',prop={'size': 16})
    plt.show()",,Domain Crawler,Domain Crawler,"Vclsi-02-Duitama-Lacerda->Visual Computing in the Life Sciences->Exercise 2 (Principal Component Analysis, 25 Points)"
189897,"DIR = 'models'
if not os.path.exists(DIR) :
    os.makedirs(DIR)","import pickle
import os 

dest = os.path.join('movieclassifier', 'pkl_objects')

if not os.path.exist(dest):
    os.makedirs(dest)
    
pickle.dump(stop, 
            open(os.path.join(dest, 'stopwords.pkl'), 'wb'), 
            protocol=4)

pickle.dump(clf, 
            open(os.path.join(dest, 'classifier.pkl'), 'wb'), 
            protocol=4)","start_time = time.time()

#validation sets --------------------------------------------------------------------------
cv = 5

#model -------------------------------------------------------------------------------------
my_model = GradientBoostingRegressor(
    min_samples_split = par_min_samples_split
)

#model parameters --------------------------------------------------------------------------
#list(range(60,120,40)
my_model_parameters = {
    ""n_estimators"":[400, 600, 800],
    ""learning_rate"":[0.1],
    ""max_depth"":[6,8,10]  
}

#grid search -------------------------------------------------------------------------------
clf = GridSearchCV(
    estimator = my_model, 
    param_grid = my_model_parameters,
    cv = cv,
    n_jobs = 6,
    scoring = my_scorer
)
clf.fit(X, y)

print(""Model learning: % .4f minutes ---"" % ((time.time() - start_time)/60))",,Traffic Sign Classifier->Model Architecture,Note->9 Embedding a Machine Learning Model into a Web Application->9.1 Serializing fitted scikit-learn estimators,Gradient Boosting Regressor->Boosting parameter tuning
465011,"ByM2, BxM2 = fpdp.rotate_vector(np.array([p.localBy, p.localBx]), theta=90, axis=0)

f, (ax1, ax2) = plt.subplots(1, 2)
ax1.matshow(ByM)
ax2.matshow(ByM2)","# plot the unwarped image and the error image
f, (ax1, ax2) = plt.subplots(1, 2)
ax1.matshow(imuw)
ax2.matshow(imuw-im)

print(nrmse(im, np.concatenate([imw[None], imuw[None]], 0)))","city_info = {'Washington': './data/Washington-2016-Summary.csv',
             'Chicago': './data/Chicago-2016-Summary.csv',
             'NYC': './data/NYC-2016-Summary.csv'}

for city in city_info:
    print(number_of_trips(city_info[city]))",,01 Fpd Overview->FPD->Notebook history->BLAS libraries,01 Fpd Overview->FPD->Interactive notebook plotting,Bike Share Analysis->Exploring and Analysing 2016 US Bike Share Data
277165,"# Create the plot
# Set a marker size variable
globSize = 2
globColSc = 'Bluered'

plotBin_1Trace = go.Scatter(x = plotBin_1.air_temp
                            , y = plotBin_1.totaldemand_mean
                            , mode = 'markers'
                            , marker = dict(size = globSize, color = plotBin_1.air_temp, cmin = atMin, cmax = atMax
                                            , colorscale = globColSc)
                            , name = hrLabel[0])
plotBin_2Trace = go.Scatter(x = plotBin_2.air_temp
                            , y = plotBin_2.totaldemand_mean
                            , mode = 'markers'
                            , marker = dict(size = globSize, color = plotBin_2.air_temp, cmin = atMin, cmax = atMax
                                           , colorscale = globColSc)
                            , name = hrLabel[1])
plotBin_3Trace = go.Scatter(x = plotBin_3.air_temp
                            , y = plotBin_3.totaldemand_mean
                            , mode = 'markers'
                            , marker = dict(size = globSize, color = plotBin_3.air_temp, cmin = atMin, cmax = atMax
                                           , colorscale = globColSc)
                            , name = hrLabel[2])
plotBin_4Trace = go.Scatter(x = plotBin_4.air_temp
                            , y = plotBin_4.totaldemand_mean
                            , mode = 'markers'
                            , marker = dict(size = globSize, color = plotBin_4.air_temp, cmin = atMin, cmax = atMax
                                           , colorscale = globColSc)
                            , name = hrLabel[3])
plotBin_5Trace = go.Scatter(x = plotBin_5.air_temp
                            , y = plotBin_5.totaldemand_mean
                            , mode = 'markers'
                            , marker = dict(size = globSize, color = plotBin_5.air_temp, cmin = atMin, cmax = atMax
                                           , colorscale = globColSc)
                            , name = hrLabel[4])
plotBin_6Trace = go.Scatter(x = plotBin_6.air_temp
                            , y = plotBin_6.totaldemand_mean
                            , mode = 'markers'
                            , marker = dict(size = globSize, color = plotBin_6.air_temp, cmin = atMin, cmax = atMax
                                           , colorscale = globColSc)
                            , name = hrLabel[5])

# Define a figure for the plots
fig = tools.make_subplots(rows = 2, cols = 3, shared_yaxes=True, shared_xaxes=True, subplot_titles=hrLabel)

fig.append_trace(plotBin_1Trace, 1, 1)
fig.append_trace(plotBin_2Trace, 1, 2)
fig.append_trace(plotBin_3Trace, 1, 3)
fig.append_trace(plotBin_4Trace, 2, 1)
fig.append_trace(plotBin_5Trace, 2, 2)
fig.append_trace(plotBin_6Trace, 2, 3)

fig['layout'].update(title = ""Relationship between air temperature at "" + stationName + "" vs "" + sqlRegion + "" energy demand"")
fig['layout']['xaxis2'].update(title='Air temp (deg C)')
fig['layout']['yaxis2'].update(title='Demand (MW)')

po.iplot(fig)","fig = tools.make_subplots(rows=2, cols=2,
#                           shared_xaxes=True,
#                           shared_yaxes=True,
#                          subplot_titles=('Months',None,None, 'Third Subplot'),
                         specs=[[{}, None],
                                 [{}, {}]]
                                 )

tickvals=len(lista_ore)

trace0 = go.Bar(
    x=lista_ore,
    y=count_ora,
    yaxis='y2',
    marker=dict(
        color='Gray'
    ),
    name='settimana'
    
)
fig.append_trace(trace0, 1, 1)

trace1 = go.Heatmap(
        z=zuma,
        x=lista_ore,
        y=lista_giorni,
        name='prelievi',
        hoverinfo = 'z',
        colorscale='RdBu',
        ygap=3,
        xgap=3
    )
fig.append_trace(trace1, 2, 1)


trace2 = go.Bar(
    x=freq_giorno['IDBadge'],
    y=lista_giorni,
    orientation='h',
    yaxis='y3',
        marker=dict(
        color='Gray'
    ),
    name='giorno'
)
fig.append_trace(trace2, 2, 2)

layout = go.Layout(
    width = 800,
    height = 200,  
    showlegend=False #non funziona
)
fig['layout']['xaxis1'].update(title=None, domain=[0, 0.9],visible=False )
fig['layout']['xaxis2'].update(tickfont=dict(size=10),ticks='', autotick=False,nticks=24,domain=[0, 0.9],showgrid=False )
fig['layout']['xaxis3'].update(tickfont=dict(size=10),domain=[0.905, 1] )
fig['layout']['yaxis1'].update(domain=[0.61, 1] )
fig['layout']['yaxis2'].update(domain=[0, 0.6],tickmode='array',tickvals=lista_giorni, ticktext=lista_giorni_st, autorange='reversed',showgrid=False,tickcolor='white',autotick=False, )
fig['layout']['yaxis3'].update(tickfont=dict(size=10),domain=[0, 0.6], visible=False, autorange='reversed' )

fig['layout'].update(height=450, width=900, title='Temporal matrix (day)', showlegend=False)
iplot(fig)","import random
sample_list = [i for i in range(0, 10)]

print(random.random()) # Get a float value (range is 0 to 1.0)
print(random.uniform(0, 10)) # Get a float value (range is 0 to 10 )
print(random.randint(0,10)) # Get a int value (range is 0 to 10)
print(random.choice(sample_list)) # Get a value from sample_list randomly
random.shuffle(sample_list) # shuffle sample_list
print(sample_list)",,Exploratory Analysis->Analysis of NSW Climate Data,Open Lab 02-05->Open lab DVVA - esercitazione->Calendar Heatmap della giornata->con colorscale lineare,General-Python->How to use Random module
229566,"np.ceil(np.dot(u,v))","# dot products
np.dot(u,v)","print(doc2vec.alpha)
print(doc2vec.epochs_trained)
print(doc2vec.best_epoch)
#best_model = Doc2Vec.load(f'tmp_{doc2vec.epochs_trained}')
best_model = Doc2Vec.load('/var/tmp/best_doc2vec')
model_name = f'doc2vec_{snippet_length}_{dm}_{dim}_{window_size}_{alpha}_{INCLUDE_BOOK_VECTORS}_{glove}'
best_model.save(f'../res/{model_name}')
print(best_model)

doc2vec = best_model",,05->Setup->Calculating the magnitude of the projection of the input vector onto the parameter vector:,"Lab4->Vectors, Matrices, Linear Algebra->1.1 Vectors",Doc2Vec->Load the best model
128289,"def leastGoalsInSeason():
    min_goals = 1000
    counter = 0
    ### check each total goals dataframe and looking for lowest season goals total
    while counter < len(total_goals_array_of_seasons):
        if total_goals_array_of_seasons[counter]['total goals'].max() < min_goals:
            min_goals = total_goals_array_of_seasons[counter]['total goals'].min()
        counter += 1
    print(""The least amount of goals scored by a team in a season is "" +str(min_goals))

leastGoalsInSeason()","total_localPoints = season_local_visitor.groupby(['season', 'localTeam'])[['localPoints']].sum()
total_localPoints.head()","import numpy as np
from sklearn import neighbors, datasets",,Final Project->Intro to Programming: Extracurricular - Data Analysis->Interesting Facts from Data->Goals  (cont'd),The Spanish League->The Spanish League: Exploratory Data Analysis->Check the summary of the dataframe:,Melek Ml - Knn->K-Nearest Neighbor->SKLEARN
96925,"#Starter Code

from bs4 import BeautifulSoup
import requests


url = '' #Put the URL of your AZLyrics Artist Page here!

html_page = requests.get(url) #Make a get request to retrieve the page
soup = BeautifulSoup(html_page.content, 'html.parser') #Pass the page contents to beautiful soup for parsing


#The example from our lecture/reading
data = [] #Create a storage container
for album_n in range(len(albums)):
    #On the last album, we won't be able to look forward
    if album_n == len(albums)-1:
        cur_album = albums[album_n]
        album_songs = cur_album.findNextSiblings('a')
        for song in album_songs:
            page = song.get('href')
            title = song.text
            album = cur_album.text
            data.append((title, page, album))
    else:
        cur_album = albums[album_n]
        next_album = albums[album_n+1]
        saca = cur_album.findNextSiblings('a') #songs after current album
        sbna = next_album.findPreviousSiblings('a') #songs before next album
        album_songs = [song for song in saca if song in sbna] #album songs are those listed after the current album but before the next one!
        for song in album_songs:
            page = song.get('href')
            title = song.text
            album = cur_album.text
            data.append((title, page, album))
data[:2]","data = [] #Create a storage container
for album_n in range(len(albums)):
    #On the last album, we won't be able to look forward
    if album_n == len(albums)-1:
        cur_album = albums[album_n]
        album_songs = cur_album.findNextSiblings('a')
        for song in album_songs:
            page = song.get('href')
            title = song.text
            album = cur_album.text
            data.append((title, page, album))
    else:
        cur_album = albums[album_n]
        next_album = albums[album_n+1]
        saca = cur_album.findNextSiblings('a') #songs after current album
        sbna = next_album.findPreviousSiblings('a') #songs before next album
        album_songs = [song for song in saca if song in sbna] #album songs are those listed after the current album but before the next one!
        for song in album_songs:
            page = song.get('href')
            title = song.text
            album = cur_album.text
            data.append((title, page, album))
data[:2]","# Set up lists to store training errors and test errors.
max_depth_train_error = []
max_depth_tests_error = []

# Train models with different max depths and record
# scores on the models.
minimum_depth = 1
maximum_depth = 200
num_trials = 2
for max_depth in range(minimum_depth, maximum_depth + 1):
    # Run trials for each max depth and average the values.
    train_trial_errors = []
    tests_trial_errors = []
    for i in range(0, num_trials):
        clf = RandomForestClassifier(max_depth=max_depth)
        clf.fit(trainingX, trainingY)
        train_trial_errors.append(1 - clf.score(trainingX, trainingY))
        tests_trial_errors.append(1 - clf.score(testX, testY))
    max_depth_train_error.append(np.mean(train_trial_errors))
    max_depth_tests_error.append(np.mean(tests_trial_errors))",,"Index->Question 1  <a id=""q1""></a>",Index->Web Scraping->Pulling out the Song Titles and Pages,Lokos Models->Varying Tree Depth
290016,"X_full = ds.drop(['target'], axis=1).values","ds.drop([0.0], dim='lon')",my_atom_A23ca.z,,Preprocessing->Preprocessing the OttoProduct dataset,03-Indexing-And-Selecting-Data->Indexing and Selecting Data->Dropping Labels,"1 Ampal Introduction And Selections->An Introduction to the AMPAL Framework->1. Converting PDB files to AMPAL Objects
1 Ampal Introduction And Selections->An Introduction to the AMPAL Framework->1. Converting PDB files to AMPAL Objects"
107614,"np.diff(z).min(), np.diff(z).max()","print ""The minimum difference between any two consecutive timestamps is: "" + str(np.min(np.diff(temperature['timestamp'])))
print ""The maximum difference between any two consecutive timestamps is: "" + str(np.max(np.diff(temperature['timestamp'])))",df_remove_dups.to_csv('data/html_extract.csv'),,1,Final Project->12-752 Course Project->7. Energy Prediction Model with Occupancy->2.2 Occupancy Data->T-test and R-square,"Step2 Web Scraping Script Bs4-Checkpoint->Web Scraping List of URL's->List below Imports a file with All URL's to Scrape Through. 26,000 Records were updated.->Export the File for further EDA"
114017,"#Drop Duplicates
sh_df.drop_duplicates(inplace=True, subset='SH_event_id')
#Drop Parking Passes
sh_df = sh_df.loc[~sh_df['SH_venue'].str.lower().str.contains('parking')]
#Map timezone column from state
sh_df['SH_timezone'] = sh_df['SH_venue_state'].map(state2timezone)
#Convert to local timezones
sh_df = convert_times(sh_df,['SH_date'],'SH_timezone')
sh_df.head()","#change all strings to lower case letters
lower = df_vw['VerseWords'].str.lower()
#name column
lower.name = ""VerseWords""
#join new column to original dataframe
df_vw_lower = complete_df.join(lower)
#remove Words column
df_vw_lower.drop(df_vw_lower.columns[[7]], axis=1, inplace=True)
#check df_vw_lower
df_vw_lower.head()","# True, False
type(True)",,"1->Table of Contents->3.3 Convert to datetimes, drop duplicates",Capstone Notebook-Visualization->Visualization->Load all the data needed,1-Python-In-One-Hour-Or-So->Objects have types:->Booleans:
184998,"url = ""https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv""
response = requests.get(url)","url_request = requests.get('http://api.us.socrata.com/api/catalog/v1/domains') 
#this is an API call of basically only domain URLs

url_json = url_request.json() #turn this requests.get object into a JSON-style dict
url_df = pd.io.json.json_normalize(url_json, record_path = 'results')","# Create a new feature to handle the zero values of TotalBsmtSF
#all_data[""ZeroBsmt""] = 0
#all_data.loc[all_data[""TotalBsmtSF""] == 0,""ZeroBsmt""] = 1",,Wrangle Act->Data Gathering->Get the image predictions tsv file from the provided url using request library.,"Original Lda Code - Socrata Api Dataset - Gensim Lda Groupings->Access All Socrata Open Data Portal Meta-Data with Series of API Calls->First, get a list of all Socrata open data portals (for governments in the U.S.)",Houseprice-Root->Some feature engineering
416123,"distance(dA1, dA1_numeric)","def hartl_deltad(d,r,n): # Hartl & Taubes 1996, eq. 3
    delta = lambda theta: d/2 - sqrt(0.25 * d**2 + r**2 - r * d * cos(theta))
    numer = integrate(lambda x: delta(x) * sin(x)**(n-2), 0, arccos(r/d))[0]
    denom = integrate(lambda x: sin(x)**(n-2), 0, arccos(r/d))[0]
    return numer/denom
hartl_deltad(d,r,n)","for i in range(1,1000000):
    val = int(i*(i+1)/2)
    if(div(val) > 500):
        print(val)
        break",,Neural Network->Batch learning,Expected Improvement->Setting up->Expected $\Delta d$,012
405882,"ys = kernel_PCA(kernel, l=3)

fig = plt.figure()
ax = fig.gca(projection='3d')
ax.scatter(ys[:,0], ys[:,1],ys[:,2], c = color)
ax.set_xlabel(""X1"")
ax.set_ylabel(""X1"")
ax.set_zlabel(""X3"")
plt.show()","fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

#defense plot
xs = X_r_def[:,0]
ys = X_r_def[:,1]
zs = X_r_def[:,2]
ax = fig.add_subplot(111, projection='3d')
ax.scatter(xs, ys, zs, c=label_color_def)

ax.set_xlabel('PC I')
ax.set_ylabel('PC II')
ax.set_zlabel('PC III')

plt.show()","pca = PCA(4)
pca.fit(X)
X_PCA = pca.transform(X)
# sns.lmplot('1 PCA', ""2 PCA"", X_r[:, 0:1])
fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=600)
for cl in classes:
    x = np.array([x_ for y_, x_ in zip(y, X_PCA) if y_ == cl[""cl_id""]])
    ax.scatter(x[:,0], x[:, 1], s=10, cmap=""jet"", label=cl[""cl_name""])
ax.set_ylim(-25, 25)
ax.set_xlim(-25, 25)
ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);",,Pca->Projection in higher dimensional space,Making A Super Team Ii,Gesture Classification->Dimension reduction->PCA
91535,"plt.plot(time_samples, signal, 'b')
plt.plot(tmin, xmin, 'go', label='minima')
plt.plot(tmax, xmax, 'ro', label='maxima')
plt.vlines(tmax[2], -2, 3, 'r','dashdot', label='axis of symmetry')
plt.vlines(tmin[-3], -2, 3, 'g', 'dashdot', label='axis of symmetry')
plt.legend(loc=4)","dx = 0.01                    # x increment [0.01]
dt = 0.01                    # t increment [0.01]
tmin = 0.0                   # start time [0.0]
tmax = 10.0                  # stop time [10.0]
xmin = 0.0                   # minimum x value [0.0]
xmax = 10.0                  # maximum x value [10.0]
c = 1.0                      # speed of sound [1.0]
rsq = (c*dt/dx)**2           # finite difference solution
k = 2*np.pi/xmax             # wavevector - do not change
kn = k/2                     # used to calculate harmonics
global fstr                  # function string
nx = int((xmax-xmin)/dx) + 1 # number of points on x grid
nt = int((tmax-tmin)/dt) + 2 # number of points on t grid
u = np.zeros((nt, nx))       # solution to WE","%%time
analyze(lambda depth, state: negamax_αβ(depth, -0xbeef, 0xbeef, state), REFERENCE_MATCH, 7)",,Index,07-Animation-1D-Waves->One Dimensional Wave Equation->The parameters,Gobblet Gobblers->Quick Aside: Alpha-Beta Pruning
448668,"batch_size = 128 # number of samples per each training in the dataset
num_classes = 10 # number of Y
epochs = 2 # number of training; assign 30 or more for better accuracy","batch_size = 5
learning_rate = 0.001
num_epoch = 1","# Copy the 3 columns we need
# Note since TokenTweetFrame had ""time_stamp"" as an index we are going to need to reset it before
# we can copy it over
CategoryTweetFrame = TokenTweetFrame.reset_index()[['time_stamp','tokens','count_tokens']].copy()

# Saving the file away
save_frame(CategoryTweetFrame, ""CategoryTweetFrame"")

# Read the csv using the made function that deals with converting and covert ""tokens"" column
CategoryTweetFrame = read_frame_convert_list(""CategoryTweetFrame"", ['tokens'], length=4)

# Set index 
CategoryTweetFrame = CategoryTweetFrame.set_index(""time_stamp"")

# See first 5 entries
CategoryTweetFrame[:5]",,Keras-Checkpoint->Set Training Parameters,3 Vgg Net->1. Setting->1-2 Hyperparameters,Analyzing Twitter Tweet #Gamer Gate Case-Checkpoint->Creating the Catgories Dataframe
240679,"def standard_shuffle(x):
    permute_indices = np.arange(x.shape[0])
    np.random.shuffle(permute_indices)
    return x[permute_indices]","def exp_argument_reduce(y):
    m = 0
    while y > 1:
        y /= 2.0
        m += 1        
    return exp_unit_interval(y)**(2**m)

print exp_argument_reduce(2)
print np.exp(2)",df.keys() #df.columns,,"Lecture Live->Hypothesis Testing->Multiple Regression->Step 3 aside: Plot the null distribution->Create the Function
Lecture Breakout->Review->Inverting the Regression Model->FDR Correction: The Benjamini-Hochberg Procedure->Combining Regions A & B",Lab03 Answers->Exercise 2,Data Handling->Pandas Dataframes
102615,"# Calculate total of males and females who died and survived
titanic_df.groupby(['Survived', 'Sex'])[[""Sex""]].count()","sex_count = df[['PassengerId','Sex']].groupby('Sex').count()
sex_pc = sex_count *(100./len(df))
sex_pc","regr.coef_
df2 = pd.DataFrame({'type':['TV','Radio','Newspaper'], 'coef':regr.coef_})
df2.groupby('type')['coef'].mean().plot(kind='bar')
plt.show()",,Project->Titanic Data Analyst Nano-Degree Project->5.Analyze data,Titanic Passengers And Survival-Checkpoint,Homework 1 - Github And Pandas->Bad Learning Rate
443008,show_doc(series2cat),show_doc(children),train.Ticket.describe(),,"Core->core
Core->Basic core->Check functions","Torch Core
Torch Core->Torch Core->Functions to get information of a model","Titanic Data Exploration->Descriptive statistics->Univariate->- Ticket
Titanic Data Exploration->Descriptive statistics->Multivariate->- Name"
194513,"import imgaug as ia
from imgaug import augmenters as iaa
import random
brighter = iaa.Multiply((1.2, 1.3), per_channel=0.2)
fig, ax = plt.subplots(7, 7, figsize=(14, 14))
print (ax.shape)
for label in unique_labels:
    condition  = (np.average(X_train,axis = (1,2,3) )>50)&((np.average(X_train,axis = (1,2,3) )<150))
    index_array = np.array(np.where(y_train[condition] == label))
    np.random.shuffle(index_array)  
    index = index_array.squeeze()[0]
    image = X_train[condition][index].squeeze()
    if np.average(image)<100:
        image = brighter.augment_image(image)
    plot_index_1 = int((label)/7.)
    plot_index_2 = label - (int((label)/7.))*7
    ax[plot_index_1][plot_index_2].set_title('label: '+str(label),fontsize = 12)
    ax[plot_index_1][plot_index_2].imshow(seq.augment_image(image))
    ax[plot_index_1][plot_index_2].axis('off')
for label in range(len(unique_labels),49):
    plot_index_1 = int((label)/7.)
    plot_index_2 = label - (int((label)/7.))*7
    ax[plot_index_1][plot_index_2].axis('off')
    
fig.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.1)

fig.savefig(r'report_images\stopsign_examples.jpg')","import imgaug as ia
from imgaug import augmenters as iaa
import random
brighter = iaa.Multiply((1.2, 1.3), per_channel=0.2)
fig, ax = plt.subplots(7, 7, figsize=(14, 14))
print (ax.shape)
for label in unique_labels:
    condition  = (np.average(X_train,axis = (1,2,3) )>50)&((np.average(X_train,axis = (1,2,3) )<150))
    index_array = np.array(np.where(y_train[condition] == label))
    np.random.shuffle(index_array)  
    index = index_array.squeeze()[0]
    image = X_train[condition][index].squeeze()
    if np.average(image)<100:
        image = brighter.augment_image(image)
    plot_index_1 = int((label)/7.)
    plot_index_2 = label - (int((label)/7.))*7
    ax[plot_index_1][plot_index_2].set_title('label: '+str(label),fontsize = 12)
    ax[plot_index_1][plot_index_2].imshow(seq.augment_image(image))
    ax[plot_index_1][plot_index_2].axis('off')
for label in range(len(unique_labels),49):
    plot_index_1 = int((label)/7.)
    plot_index_2 = label - (int((label)/7.))*7
    ax[plot_index_1][plot_index_2].axis('off')
    
fig.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.1)

fig.savefig(r'report_images\stopsign_examples.jpg')",y_predicted=clf.predict(X_test),,Traffic Sign Classifier->show examples of the label based data set pictures,Traffic Sign Classifier->show examples of the label based data set pictures,Activity Recognition Of Weight Lifting Excercises->Data Splitting (Training and Testing)->Decision Trees
338050,"# train more
model1.optimizer.lr /= 10
print(""phase 3, lr="", model1.optimizer.get_config()['lr'])
train_with_data(model1, X_train, y_train, resample_and_augumentation, nb_epoch=2, aug_factor=2.5)","def ques_1a(epochs, s):
    train_arr = []
    train_acc = []
    test_arr = []
    test_acc= []
    
    train_loader_1a, test_loader_1a = get_loader(len(train_data), len(test_data))
    model1 = Net1()

    if(use_cuda):
        model1 = model1.cuda()
    
    optimizer = optim.SGD(model1.parameters(), lr = 0.01, momentum = 0.9)
    if(s == 'a'):
        criterion = F.binary_cross_entropy
    elif(s=='f'):
        criterion = nn.MSELoss()
    
    for epoch in range(epochs):
        print('Epoch # {}'.format(epoch))
        train_loss, train_c = train(model1, 
                                          train_loader_1a, optimizer, criterion)
        test_loss, test_c = test(model1, 
                                       test_loader_1a, optimizer, criterion)
        train_arr.append([train_loss])
        test_arr.append([test_loss])
        train_acc.append([(100.0*train_c)/len(train_loader.dataset)])
        test_acc.append([(100.0*test_c)/len(test_loader.dataset)])
    return train_arr, test_arr,train_acc, test_acc","# drop the source column
df = df.drop(columns=['source'])

# view the data
df",,Traffic Signs Recognition->Data Exploration Visualization->Question 4,Assignment2-Checkpoint->Assignment 2,Data Analysis->Final Steps of Data Preparation
85684,"z1 = add_print(5, 10)
print(z1)","z1 = add(3, 9)",2 <= 2,,4->Return keyword and argument,"Introduction To Python For Data Science->Python Language Basics->List, Set, and Dict Comprehension","Lecture 1->Let's begin with the tools->Building lists and list comprehension
Introduction To Python For Data Science->Python Language Basics->set
Python101->Python 101->Functions
Unit 02->Informatik 1 - Biomedical Engineering->Booleans
Comparison Operators->Comparison Operators->Less than or Equal to
Operators And Branching->Python Operators and Branching->Comparison Operators on int and float
9-Comparison,Chained Comparison"
69443,"### TODO: Train the model.
checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Resnet50.hdf5', 
                               verbose=1, save_best_only=True)

resnet50.fit(train_resnet50, train_targets, 
          validation_data=(valid_resnet50, valid_targets),
          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)","### TODO: Train the model.
checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Resnet50.hdf5', 
                               verbose=1, save_best_only=True)

Resnet50_model.fit(train_Resnet50, train_targets, 
          validation_data=(valid_Resnet50, valid_targets),
          epochs=25, batch_size=32, callbacks=[checkpointer], verbose=1)","make_map(model.Obama, ""P(Obama): Poll Aggregation"")",,Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Train the Model,"Dog App
Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Model Architecture","Desperately Seeking Silver->Gallup Party Affiliation Poll->Logistic Regression->The accuracy of the model has gone more better and the precision has also improved over the Logistic Regression model
Hw2 User->Logistic Considerations->Gallup Party Affiliation Poll->Carry out a logistic fit using the `cv_and_fit` function developed (& found in hw2.py)."
459591,from dataframer import grab_data,"data = grab(""Friday"", ""Queen / Spadina"", ""S"", ""NB"")",complaintdates = pd.DataFrame(complaintdates),,Data Visualization->data visualization->grabbing data,Anomaly Detection Analysis->**Volume Trend Deviation**->The `anomolous` Function->Examples of 15 Minute Anomaly Detection->**Example 3: Detecting Anomalies for Richmond and Bathurst**,Hack For Heat #3->SQL-ing this->Complaints over time
267691,"# Convert totals into percentages 
hod_dept_pc = pd.DataFrame()
for column in hod_dept.columns:
    hod_dept_pc[column] = (hod_dept[column] / hod_dept[column].sum() * 100).astype(float)

# Create the line plot
plt.figure(num=8,figsize=(16,8))
plt.plot(hod_dept_pc)
plt.legend(hod_dept_pc.columns, loc='center left', bbox_to_anchor=(1, 0.5))
plt.xlabel('Hour of day')
plt.ylabel('Percent products bought')
plt.title('Percent products bought by department and hour')
plt.xticks(range(24))
plt.xlim(0,23)
plt.ylim(0,10)
plt.show()","# get the na count per column
naCountPerColumn = np.sum(x_raw == -999, axis=0)
naCountPerColumnWithPred = np.sum(x_raw[prediction==1,:] == -999, axis=0) # where higgs boson was predicted
nbColumns = x_raw.shape[1]
rowNumbers = np.arange(1, nbColumns + 1)

f = plt.figure()
plt.scatter(rowNumbers, naCountPerColumn, label='all observations', color='r')
plt.scatter(rowNumbers, naCountPerColumnWithPred, label='Higgs boson predicted', color='g')
plt.xlim(xmin=-0.5, xmax=nbColumns+0.5)
plt.ylim(ymin=-10000, ymax = 250000)
plt.legend(loc='best')
#plt.legend(bbox_to_anchor=(1.6, 0.5))
plt.xlabel('column')
plt.ylabel('# NAs per column')
plt.show()
f.savefig('Images/NAPerColumn.pdf')

naCountPerColumn","colorspaces('output_images/project_video_warped/frame1044.jpg', cspaces = ['hls'])",,Data Exploration->big merged dataset,Task->Data Exploration,Lanes->Detecting Lane Pixels
342279,"matches = bf.knnMatch(des1,des2,k=2)

#Ratio test
good = []
for m,n in matches:
    if m.distance < 0.7*n.distance:
        good.append([m])","def detect_and_match_keypoint(image_1,image_2):
    # image_1 = query image
    # image_2 = train image
    # Initiate SIFT detector
    sift = cv2.SIFT()
    # find the keypoints and descriptors with SIFT
    kp1, des1 = sift.detectAndCompute(image_1,None)
    kp2, des2 = sift.detectAndCompute(image_2,None)
    # BFMatcher with default params
    bf = cv2.BFMatcher()
    matches = bf.knnMatch(des1,des2, k=2)
    # Apply ratio test
    match_details = []
    image_1_keypoint = []
    image_2_keypoint = []
    for m,n in matches:
        if m.distance < 0.2*n.distance:
            match_details.append([m])
            image_1_keypoint.append(kp1[m.queryIdx])
            image_2_keypoint.append(kp2[m.trainIdx])
    return match_details,np.float32([kp.pt for kp in image_1_keypoint]),np.float32([kp.pt for kp in image_2_keypoint])",list(range(1)),,Sift Detector->Feature matching using SIFT with OpenCV,Image Stitcher->Function to find Keypoint and match between 2 images,Cadet Seating For Test Project->Occupancy in a Classroom->Recursion->Another Recursion
43442,"seen_list = [""seen_1"",""seen_2"",""seen_3"",""seen_4"",""seen_5"",""seen_6""]

seen_counts = {}
for column in seen_list:
    count = star_wars[column].sum()
    seen_counts[column] = count

print(seen_counts)","star_wars[[seen_column, fan_column]].head()",model.evaluate?,,Basics->Finding The Highest Ranked Movie,Understanding Star Wars Fans->How men and women ranked each movie 1-6.->Some data cleaning,4->About Keras models
68453,"network_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis=0))) for feature in test_network]

# report test accuracy
test_accuracy = 100*np.sum(np.array(network_predictions)==np.argmax(test_targets, axis=1))/len(network_predictions)
print('Test accuracy: %.4f%%' % test_accuracy)","### TODO: Calculate classification accuracy on the test dataset.
Resnet50_predictions = [np.argmax(transfer_model.predict(np.expand_dims(feature, axis = 0))) for feature in test_Resnet50]
test_accuracy = 100 * np.sum(np.array(Resnet50_predictions) == np.argmax(test_targets, axis = 1)) / len(Resnet50_predictions)
print('Test accuracy: %.4f%%' % test_accuracy)","for i in range(3):
    # compute log - likelihood
    loglike_scores[('test score','MAP: l=' + str(ll[i]))] = log_likelihood_score(
        rbfX_test[i],y_test, maprbfbeta_train[i])
    loglike_scores[('training score','MAP: l=' + str(ll[i]))] = log_likelihood_score(
        rbfX_train[i],y_train, maprbfbeta_train[i])",,Dog App->Artificial Intelligence Nanodegree->Test the Model,Dog App->(IMPLEMENTATION) Test the Model,Mlsalt1 Coursework->Task 13: Best Model vs Nominal Model->Training and Test scores for MAP
218417,"Image(""figs/preview-lightbox-1e5model_TestOn_1e5.png"")",np.imag(C) # same as: C.imag,"from scipy.stats import tmean, tvar, tstd, iqr, tmax, tmin


data = [20, 45, 68, 900, 57, 45, 33, 35, 45, 22]

print('Mean = {:,}'.format(tmean(data)))
print('Var = {:,}'.format(tvar(data)))
print('StD = {:,}'.format(tstd(data)))
print('IQR = {:,}'.format(iqr(data)))
print('Range = {:,}'.format(tmax(data) - tmin(data)))",,Report->DnCNN performance,Numpy-Checkpoint->Numpy -  multidimensional data arrays->Introduction->Array/Matrix transformations,Statistics->Probability and Statistics->4. Descriptive Statistics->4.2 Variability
271388,"distant_reconst = DISTANT_POINTS.dot(pca_top_2.components_)
reshaped_distant_reconst = distant_reconst.reshape((len(DISTANT_POINTS), *resized_array.shape[1:]))
reshaped_distant_reconst += image_mean","img_pca_tranf_new = pca_matrix_normal.T.dot(img_matrix_shifted) # Apply on all the data

# Reconstruct through PCA Matrix and Mean Vector
img_matrix_reconst_new = pca_matrix_normal.dot(img_pca_tranf_new) + component_mean.reshape(height*width,1)","def test3():
    for i in range(len(lst)):
        if not compare(C1[i], f_3(W1, L1[:2], X[i, :]), 0.0001):
            return False
    return True

test3()",,0->CS156 LBA->The reconstructions all look fairly similar to each other,Pca On All Data - Gaussian And Reconstruction Error Methods-Checkpoint->Apply PCA for Dimension Reduction->Method 2: PCA Reconstruction Error->Reconstruct the Data with the new PCA Matrix,Quiz1->3.
426018,feat_info.head(),"# Assess categorical variables: which are binary, which are multi-level, and
# which one needs to be re-encoded?
feat_categorical = feat_info[feat_info[""type""] == ""categorical""].attribute.tolist()","class BaseClass(param.Parameterized):
    x                       = param.Parameter(default=3.14,doc=""X position"")
    y                       = param.Parameter(default=""Not editable"",constant=True)
    string_value            = param.String(default=""str"",doc=""A string"")
    num_int                 = param.Integer(50000,bounds=(-200,100000))
    unbounded_int           = param.Integer(23)
    float_with_hard_bounds  = param.Number(8.2,bounds=(7.5,10))
    float_with_soft_bounds  = param.Number(0.5,bounds=(0,5),softbounds=(0,2))
    unbounded_float         = param.Number(30.01)
    hidden_parameter        = param.Number(2.718,precedence=-1)

class BaseClass_(param.Parameterized):
    #custom syntax highlighting using pygments would be really awesome. Red is hard to read
    layout = ['r$N=baseclass',[['c$N=text_col',['ft$d=X&v=3.14&doc=""X position""',
                                            'ft$d=Unbounded float&v=30.01',
                                            'it$d=Unbounded int&v=23',
                                            'text$d=String value&v=str&doc=""A string""']
                               ],
                               ['c$N=sliders_col',['(7.5,10,0.1,8.2)$D=Float with hard bounds',
                                                   '(0.,2.,0.1,0.5)$D=Float with soft bounds&hb=(0.,5.)',
                                                   '(-200,100000,1,50000)$d=Num int&v=23',
                                                   'text$d=y&v=Not editable&disabled=True']
                               ]
                              ]
             ]",,Identify Customer Segments->Project: Identify Customer Segments->Step 0: Load the Data,Identify Customer Segments->Project: Identify Customer Segments->Step 3: Clustering->Step 1.2: Select and Re-Encode Features->Step 1.2.1: Re-Encode Categorical Features,Brainstorming -Merging Paramnb With Shaolin->3. Brainstorming: How to add this features to paramnb->widget definition
411266,status_df.head(),df['State_Code'].isin(df['State_Code'].head(3)).head(),"def calibrate_camera(calibration_folder=None,show_image=True):
    # prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
    objp = np.zeros((6*9,3), np.float32)
    objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)

    # Arrays to store object points and image points from all the images.
    objpoints = [] # 3d points in real world space
    imgpoints = [] # 2d points in image plane.

    # Make a list of calibration images
    images = glob.glob(calibration_folder+'/calibration*.jpg')
    if show_image:
        fig=plt.figure(figsize=(10,70))
    num = 1
    gray = None 
    
    # Step through the list and search for chessboard corners
    for fname in images:
        img = cv2.imread(fname)
        img_cp = np.copy(img)
        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

        # Find the chessboard corners
        ret, corners = cv2.findChessboardCorners(gray, (9,6),None)

        # If found, add object points, image points
        if ret == True:
            objpoints.append(objp)
            imgpoints.append(corners)

            # Draw and display the corners
            if show_image:
                draw_img = cv2.drawChessboardCorners(img, (9,6), corners, ret)
                y = fig.add_subplot(17,2,num)
                y.set_title(""Original Image"",size=18)
                y.imshow(img_cp)
                
                num+=1
                
                x = fig.add_subplot(17,2,num)
                x.set_title(""Chessboard Drawn with Corners"",size=18)
                x.imshow(draw_img)
            num+=1
    if show_image:        
        plt.show()
        fig.savefig(""reference_images/finding-corners-images"")
    
    return objpoints,imgpoints

objpoints,imgpoints = calibrate_camera(CAMERA_CALIBRATION_IMAGES_FOLDER,show_image=True)",,"Apache Log->Server status codes
Apache Log Analysis->Traffic analysis->Analyzing Server status codes",Dw Pandas->Pandas->Unique values,Advanced Lane Finding->Advanced Lane Finding Project->Step -1 Camera Calibration and Distortion Correction
278897,print(greetings),"print(greet('Cody', 'Pavillion, WY'))",dataset = dict_parser.convert_into_df(data_dict),,06->Rose's Function->Dina's Function,Functions->Functions->Arguments,Outlier Identification->Outlier Identification->Importing data
371495,"df['Hour'] = df['timeStamp'].apply(lambda y:y.hour)
df['Month'] =df['timeStamp'].apply(lambda y:y.month)
df['Day_of_week'] =df['timeStamp'].apply(lambda y:y.dayofweek)",df.timeStamp.apply(lambda x: x.hour).head(),! ls /Users/grp/datacampNotebooks/pythonCourses/persist_models,,911 Calls Data Capstone Project->911 Calls Capstone Project->Data and Setup,911 Calls Data Capstone Project,21->_2. Optimizing a Neural Network with Backward Propagation:_->step4 => classification - make predictions:
161036,"import pandas as pd

from IPython.core.display import display, HTML
from IPython.display import Image


# # pip install requests_oauthlib
import requests
from requests_oauthlib import OAuth1

#OAuth ~ simple way to to publish & interact with data

%matplotlib inline
%config InlineBackend.figure_format = 'retina'","from __future__ import print_function
import requests
from requests_oauthlib import OAuth1

import pandas as pd
import os, ast","Kf_pca = PCA(10).fit_transform(fourier(X, 400))",,Sentiment Analysis->Text Visualizations,Twitter Api-Checkpoint->Twitter API->Rest API vs Streaming API:->Rest API,Non Linear Component Analysis->Non-linear Component Analysis
54008,print(train_images.shape),"print max_maks_img
print np.imag(S)[max_maks_img]","from urllib import urlopen as ureq
from bs4 import BeautifulSoup

my_url = 'http://www.detectiveconanworld.com/wiki/Anime'

#openning connection grabbing page
uclient = ureq(my_url)
html_page = uclient.read()
uclient.close()

#html parsing
soup = BeautifulSoup(html_page, 'html.parser')",,"2->2.2.3 Matrices (2D Tensors)
95-865 Model Validation->95-865: Model Validation->Data preparation
Ch2 Build Block Nn->broadcasting",Complete S Local Maxima->Local Maxima,Detective Conan Episodes – Web Scraping And Sql->Table of Contents
366332,"def expand(X):
    X0 = tf.transpose(tf.gather(tf.transpose(X), [0]))
    X1 = tf.transpose(tf.gather(tf.transpose(X), [1]))
    X_ = tf.concat([X, X ** 2, X0 * X1, tf.ones_like(X0)], axis=1)
    return X_

def classify(X, w):
    """"""
    Given feature matrix X [n_samples,2] and weight vector w [6],
    return an array of +1 or -1 predictions
    """"""
    X_ = expand(X)
    X_ = tf.cast(X_, w.dtype)
    dot_product = tf.matmul(X_, tf.reshape(w, shape=[6, 1]))   
    ones = tf.ones_like(dot_product)
    ans = tf.where(tf.greater(dot_product, 0.0), ones, -ones)
    
    with tf.Session() as sess:
        return sess.run(ans)","def run_on_dev(dev=""/gpu:0""):
    with tf.Session() as sess:
        with tf.device(dev):
            A = tf.constant([[3.,3.]])
            B = tf.constant([[2.],[2.]])
            product = tf.matmul(A, B)
            result = sess.run(product)
            print(result)

dev1 = CPU
dev2 = GPU
run_on_dev(dev1)
run_on_dev(dev2)",np.linalg.cond(inputs['A']),,Task 1,Tensor Flow->Simple Regression Example->Backpropagation Training,Hessian Vector Product->Stability comparison->Method 2: Finite differences
24798,test_slice = 0.30,"train_train_indices = slice(0, 4 * 1260 // 5)  # This is for slicing in order to save memory
train_test_indices = slice(4 * 1260 // 5, 1260) # Could also use np.arange!!",player_data_bin[:2],,Titanic->Data->Adult/Child feature,Lecture Breakout->Decoding (Time Permitting)->Voxel Selection,Ex2->Task 2->K-Means Clustering
466673,"layout = scatter + hv.Histogram(np.histogram(station_info['opened'], bins=24), kdims=['opened'])
layout","hv.Histogram(np.histogram(economic_data.growth, 
                          normed=True), 
             kdims='Growth')",bool(' '),,1-Introduction->Introduction->Compositional ``Layouts``,Day11 And 12 Data Viz Annotating Data->Data Visualization [Source: PyViz](http://pyviz.org/tutorial/index.html)->HoloViews Elements->Casting between elements,Week-01-Unit-03-Loops-And-Conditionals->Loops and Conditionals->Conditionals
293180,"# read_csv has many other functions for reading in CSV data
# such as custom parsing functions, or custom delimiters
pd.read_csv?","f = some_other_function()
print(f)
print(type(f))","df_for_swiss_categories[""language""] = languages[""Regione linguistica""]",,Pandas->Grouping and aggregating categories,Lesson 1 - Data Types & Functions,Interactive Viz-Checkpoint->Bonus 4->THIRD PART - MAPS
344511,scatter_visual(),"def visual_NN_boundaries(X, y, Nsamp = 2000):
    for i in range(len(y)):
        if y[i] == 1:
            plt.scatter(X[i][0],X[i][1], color = 'r', s = 500, marker = '*')
        else:
            plt.scatter(X[i][0],X[i][1], color = 'b', s = 500, marker = '*')
    x = np.random.random((Nsamp, 2))
    for j in range(len(x)):
        z = nn.predict(x[j])
        if z > 0.5:
            plt.scatter(x[j][0], x[j][1], color = 'r')
        else:
            plt.scatter(x[j][0], x[j][1], color = 'b')
        
    plt.xlim(-0.5, 1.5)
    plt.ylim(-0.5, 1.5)
    plt.show()

visual_NN_boundaries(X, y)","exp_m = ('male + age0 + age_rs0 + age_rs1 + age_rs2 + cd40 + cd40_sq + cd40_cu + dvl0 + '
         'cd4 + cd4_sq + cd4_cu + dvl + enter + enter_sq + enter_cu')
mcgf.exposure_model(exp_m, restriction=""g['lag_art']==0"")",,Toxic Comment Classifier - Keras-Tf->Toxic Comment Classifier->4. Training->6.2 Scatter Plot,Comp Phys-Ii Week08-2 Multi-Layer Forward-Feed Nn->Multi-layer Forward-feed NN with Stochastic Gradient Descent->Breakout Exercise:,1 Monte Carlo G-Formula->G-formula: Monte Carlo Estimator->Estimating the Exposure/Treatment Model
35808,coeffs,"from pymks.tools import draw_coeff

draw_coeff()",pd.DataFrame(H).head(),,"Logistic Regression->Classification->Run On Data
Linear Regression->Multiple linear regression
01-02-Simple-And-Multivariate-Linear-Regression->Linear Regression->Simple Linear Regression->The Results
Part-4-Model-Training-And-Evaluation->Model creation->Train-Test Split->San Juan
Python3Sympy-To-Gap-Singular->Example: SCSCP client in Python3 calculates Groebner basis with Singular
Lecture09 Code->HEADS' UP!->Coeff data structure.
Block Idxmat Shuffle For K-Fold Cross-Validation->Eyeball the estimated coefficients
Pandas Linear Regression->Linear regression in bare numpy and pandas",Elasticity 2D->Resizing the Coefficients to use on Larger Microstructures,"Nmf-Nnls->Defining matrices W and H
Nmf-Out-Matrix->Defining matrices W and H (learning on M-1 users and N movies)"
392595,"sm_pipe2 = make_pipeline(SMOTE(random_state=1), SVC())
sm_grid2 = GridSearchCV(sm_pipe2, param_grid=svc_params)
sm_model2 = sm_grid2.fit(X_train, y_train)

sm_pred2 = sm_model2.predict(X_test)

model_scores(y_test, sm_pred2)","param_grid = {""C"": [0.10, 0.15, 0.20],
              ""class_weight"": ['balanced', None],
              ""degree"": [2,3,4,5],
              ""kernel"" : ['linear', 'poly', 'rbf', 'sigmoid'],
              ""probability"" : [True, False],
              ""shrinking"" : [True, False],
              ""random_state"" : [42]}

grid_search = GridSearchCV(SVC, param_grid=param_grid)
grid_search.fit(X_train_s, y_train_s)

SVCGrid = grid_search.best_estimator_

y_pred_train = SVCGrid.predict(X_train_s)
y_pred_test = SVCGrid.predict(X_test_s)

SVCAccuracy = accuracy_score(y_test, y_pred_test)

print(accuracy_score(y_train, y_pred_train))
print(accuracy_score(y_test, y_pred_test))","n_bins = 64
hist, bins = np.histogram(df['N_matches'], bins=n_bins)
cumsum = np.cumsum(hist)
cumsum = np.insert(cumsum, 0, 0)
cumsum = hist.max() * (cumsum - cumsum.min()) / (cumsum.max() - cumsum.min())
cumsum_x = np.linspace(0, df['N_matches'].max(), n_bins+1)

bars = np.percentile(df['N_matches'], [10, 25, 50, 75, 100])
bar_heights = np.interp(bars, cumsum_x, cumsum)

fig, ax = plt.subplots(figsize=(12, 5))
sns.distplot(df['N_matches'], bins=n_bins, kde=False, rug=True, hist_kws={'rwidth': 0.9}, ax=ax);
ax.plot(cumsum_x, cumsum);
ax.vlines(bars, np.zeros(bars.size), bar_heights, colors='C1', linestyle='--');",,Credit Card Fraud Detection->Credit Card Fraud Detection->SVC with SMOTE - oversampling,Test->4. Developing and Training models->2.4 Dummies Encoding->4.2.2 Second model: <code>SVC()</code>:,I Cat-Alignment->iCAT Alignment->3 Refine Problem Tiles
266500,"plt.figure(figsize=(5,12))
plt.boxplot(nums)
plt.yscale('symlog', linthreshy = 10**2)
plt.show()","sns.boxplot(x=df['Product_Category'], y=df['Sales'])
plt.yscale('log')
plt.show()",random.seed(123),,Data Exploration->Data Exploration,Case Study And Time Series->Boxplots,"Random Forest->Set a seed for reproducability
Python Jupyter->Create a Metric function"
188354,""""""" Not Implemented """"""

# Visualize Augmentation 
import os
from keras.preprocessing.image import ImageDataGenerator

# Define augmentation parameters 
datagen = ImageDataGenerator(
        rotation_range=15,
        width_shift_range=0.1, # Low w,h shift because of appling higher zoon. 
        height_shift_range=0.1,
        shear_range=0.8,
        zoom_range=[.8, 0.5],# [lower, upper]: Don't want to zoom out, only in. 
        channel_shift_range=0.9,
        fill_mode='nearest')


# Execute augmentation generator on sample images for vizualization.

def vizualize_image(image):
    """""" Processess randomly squeezed image from above, and saves to specified dir. """"""
    # Reshape: ImageDataGenerator.Flow() takes rank 4 data, input image.shape = (32, 32, 3) 
    image = image.reshape((1,) + image.shape) # New size: (1, 32, 32, 3)
    
    # Generate random batches of transformed images with ImageDataGenerator.flow()
    i = 0
    for batch in datagen.flow(image, batch_size=1,
                              save_to_dir='preview',
                              save_format='png'):
        i += 1
        if i > 19:
            break 


# Determine if function call is still required or not. 
if os.listdir('preview') == []:
    vizualize_image(image) 
else:
    print(""Images already exist!"")","# Save augmented images to file

# define data preparation
batch_size = 16


def image_augmentation(method,prefix,msg):

    # perform the specified augmentation
    datagen = method
    datagen.fit(trainData)
    i = 0

    generator = datagen.flow_from_directory('data/train/', target_size=(100,100),
        shuffle=False, batch_size=batch_size,
        save_to_dir='data/train',save_prefix=prefix)
   
    for batch in generator:
        i += 1
        if i > 20: # save 20 images
            break  # otherwise the generator would loop indefinitely
    
    images = generator.filenames
    classes = generator.classes
    print(""Class Indices:"",generator.class_indices)
    print(msg)
    print(len(listdir('data/train/'))-3,"" total augmented images"")
    
    return images,classes","Rd=287 #J/kg/K
rho=the_press/(Rd*xymean)
fig,ax=plt.subplots(1,1,figsize=(6,6))
out=ax.plot(rho,the_height*meter2km)
out=ax.set(xlabel=r'$\rho\ (kg/m^3)$',ylabel='height (km)',title='density of dry air')",,Traffic Sign Classifier,Capstone->Results->Part 2: Image Augmentation,"Lec3->Assignment for Wednesday, Sept. 16: 11am->My solution->convert kinematic flux to W/m^2"
366120,"import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import seaborn as sns
from enum import Enum","# Load the necessary libraries here:
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

import seaborn as sn

# If you require additional libraries to answer any questions 
# then import them as necessary.","# with error bars

fig = pl.figure(figsize=(10,5)) 
ax = fig.add_subplot(111)

scatter = ax.scatter(df_['Average total all civilian firearms'],
                     df_['Number of homicides by firearm'],
                    color = (df_['Country Name'] == 'United States').map({0:'b', 1:'r'})
)
ax.errorbar(df_['Average total all civilian firearms'], df_['Number of homicides by firearm'], 
            yerr = np.sqrt(df_['Number of homicides by firearm']*1000), fmt = '.', color='grey')
    
ax.set_xlabel(""Average total all civilian firearms"")
ax.set_ylabel(""Homicide by fire arms"")
ax.set_ylim()
pl.show()",,Implementation->Implementation of Langton's Ant,"2017J Tma01 Q2->TMA 01, question 2 (40 marks)
2017J Tma01 Q2",Hw5->Assignment 3: regression exercise->plot the number of  homicides by fire arm against the number of civilian firearms. plot the  US in a different color
346104,"dictionary=gensim.corpora.Dictionary.load('dictionary.gensim')
corpus=pickle.load(open('corpus.pkl', 'rb'))
lda=gensim.models.ldamodel.LdaModel.load('model5.gensim')

import pyLDAvis.gensim
lda_display=pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)
pyLDAvis.display(lda_display)","dictionary = gensim.corpora.Dictionary.load('dictionary_ds.gensim')
corpus = pickle.load(open(file='corpus_ds.pkl', mode='rb'))  # 'rb' --> Read Bytes
lda = gensim.models.ldamodel.LdaModel.load('model3_ds.gensim')","fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))
bins_rng = range(0,85,10)
survived = df_train[df_train.Survived==1]
notsurvived = df_train[df_train.Survived==0]

ax = sns.distplot(survived[survived.Sex=='female'].Age.dropna(), 
                  bins=bins_rng, label = 'Survived', ax = axes[0], kde =False, color='green')
ax = sns.distplot(notsurvived[notsurvived.Sex=='female'].Age.dropna(), 
                  bins=bins_rng, label = 'Not survived', ax = axes[0], kde =False, color='red')
ax.legend()
ax.set_title('Female')
ax = sns.distplot(survived[survived.Sex=='male'].Age.dropna(), 
                  bins=bins_rng, label = 'Survived', ax = axes[1], kde = False, color='green')
ax = sns.distplot(notsurvived[notsurvived.Sex=='male'].Age.dropna(), 
                  bins=bins_rng, label = 'Not survived', ax = axes[1], kde = False, color='red')
ax.legend()
_ = ax.set_title('Male')",,Topic Modelling,Job Lda Data Science->Combining into a one TEXT PREPROCESSING FUNCTION,Kaggle - Titanic->4. Visual analysis->5.6. Final input for the modelling chapter
429750,"query = '''
SELECT schemaname, tablename
FROM pg_tables
WHERE schemaname IN ('public', 'kcmo_lehd', 'kcmo_water', 'ada_kcmo')
'''

tables = pd.read_sql(query, conn)
print(tables)","print(pd.read_sql('departments', conn))","import csv, googlemaps",,1 1 Variables->Variables: Analyzing your Datasets->Load the Data->Establish a Connection to the Database,2017-07-02-Working-With-Sql-Databases->4. Read from Database,Tesla Supercharger Geocoding
280857,data_sheet = wb.get_sheet_by_name('Data'),"sh1 = wb.get_sheet_by_name('Sheet1')
sh1['A1'].value","print(""Bootstrap sample proportion = "", bs_reps.mean())
print(""Bootstrap sample std dev = "", bs_reps.std())
print(""Bootstrap sample margin of error = "", 1.96 * bs_reps.std())
print(""Bootstrap 95% confidence interval = "", np.percentile(bs_reps,[2.5, 97.5]))",,6->Reading CSV files->Using [csv.reader](https://docs.python.org/3.6/library/csv.html#csv.reader),Day1 Python Deal Excel->How to Get Value in Sheet,"Sliderule Dsi Inferential Statistics Exercise 2->3. Compute margin of error, confidence interval, and p-value."
198562,df.plot();,"print df[ ['TV'] ].plot(kind = 'box')
print df[ ['Radio'] ].plot(kind = 'box')
print df[ ['Newspaper'] ].plot(kind = 'box')
print df[ ['Sales'] ].plot(kind = 'box')","constants = solve((soln[0].subs(t,0).subs(x(0),5), soln[1].subs(t,0).subs(y(0),10)),{C1,C2})
constants",,"Matplotlib->Plotting graphs with matplotlib->First, some useful Python->Destructuring
Visualization->Scatter Plot
Exploring Data->Exploring Data with Pandas.plot()
Exploring Data->Exploring Data with Pandas.plot()
Python-Data-Viz-Sol->Seaborn `heatmap`->Line Plot With a `DataFrame`
Dynamic->Plotly->Generating some random data
01 First Datacrunching->E 01 Read some data and look at it->Get the data
01 First Datacrunching->E 01 Read some data and look at it->Read the data
Python-Data-Viz-Lab->Line Plot With a `DataFrame`
Pandas-Highcharts-Examples->Pandas DataFrame plotting with Highcharts->Build a DataFrame with a timeseries
09 Netcdf->Exersise->Plot with Pandas
Initial Temperature Values->A preliminary look at sensor data
Visualizing Data With Matplotlib - Slides->Bubble (scatter) plots
Getting Started Notebook Gis->Jupyter Notebook GIS: getting started->Plots
Week 4 - Applied Visualizations->DataFrame.plot","Lab 3-1 Nicholas Fong->Lab 03 - 1 by Nicholas Fong, worked with Vivian Duong->Let's do some boxplots!",Coupled Differential Equations->Solving multiple linear ordinary differential equations in Sympy
206841,"# Modifying since we want to do it per class.
fig, ax = plt.subplots(1,1 , figsize=(16,9))
sns.pointplot(x='Survived', y='Fare', hue='Pclass', data=train_data, ax=ax)","fig,ax = plt.subplots()
sn.pointplot(data=hour_df[['hour',
                           'total_count',
                           'season']],
             x='hour',y='total_count',
             hue='season',ax=ax)
ax.set(title=""Season wise hourly distribution of counts"");","def waiting_time_more_than(events_per_minute, t, quiet=False):
    p = np.exp(-events_per_minute * t)
    if not quiet:
        print(f'{int(events_per_minute*60)} Meteors per hour. Probability of waiting more than {t} minutes: {100*p:.2f}%.')
    return p
    
def waiting_time_less_than_or_equal(events_per_minute, t, quiet=False):
    p = 1 - waiting_time_more_than(events_per_minute, t, quiet=quiet)
    if not quiet:
        print(f'{int(events_per_minute*60)} Meteors per hour. Probability of waiting at most {t} minutes: {100*p:.2f}%.')
    return p

def waiting_time_between(events_per_minute, t1, t2):
    p1 = waiting_time_less_than_or_equal(events_per_minute, t1, True)
    p2 = waiting_time_less_than_or_equal(events_per_minute, t2, True)
    p = p2-p1
    print(f'Probability of waiting between {t1} and {t2} minutes: {100*p:.2f}%.')
    return p

assert waiting_time_more_than(events_per_minute, 15, True) + waiting_time_less_than_or_equal(events_per_minute, 15, True) == 1",,Exploratory Data Analysis->Explore the fare paid within each class by those who survived and those who did not.,"Bike Sharing Demand->Bike Sharing demand prediction->Visualize Attributes, Trends and Relationships->Hourly distribution of Total Counts",Poisson->Waiting Time
144435,"df2=df.drop(['case_status'], axis=1)
df3=pd.get_dummies(df2)
df3['case_status']=df.case_status","df2=df.drop(['case_status'], axis=1)
df3=pd.get_dummies(df2)
df3['case_status']=df.case_status","fig = plt.figure(figsize=(6,6))
ax = plt.axes()

ph_test_predict = pd.DataFrame({'test':y_test.values,
                                'predict': y_test_pred_gr_sugar}).set_index('test').sort_index()

ph_test_predict.plot(marker='o', ls='', ax=ax)
ax.set(xlabel='Test', ylabel='Predict', xlim=(0,35), ylim=(0,35));",,Full Project->Dummify Variables,Full Project->Dummify Variables,Wk8 Hw->Question 11->Question 5
160531,"b = nltk.FreqDist(HT_negative)
e = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})
# selecting top 10 most frequent hashtags
e = e.nlargest(columns=""Count"", n = 10)   
plt.figure(figsize=(16,5))
ax = sns.barplot(data=e, x= ""Hashtag"", y = ""Count"")
ax.set(ylabel = 'Count')
plt.show()","fig,ax= plt.subplots()
fig.set_size_inches(20,5)
windspeed = pd.DataFrame(data.windspeed.value_counts()).reset_index()
plt.xticks(rotation=45) 
sn.barplot(data=windspeed,x=""index"",y=""windspeed"",ax=ax)
ax.set(xlabel='Windspeed Values', ylabel='Count',title=""Count Of Windspeed Values Before Imputing"",label='big')","import networkx as nx

g = nx.karate_club_graph()",,Sentiment Analysis->Racist/Sexist Tweets,Eda & Ensemble Model->Filling 0's In windspeed Using Random Forest,C Dlib->Table of Contents->2.E.2 Community fitness/comparison visualization
225751,"X_res.shape, X.shape","## Type Your Answer Below ##
# reshape X from row vector in shape(100, ) to column vector in shape (100, 1)
X_re = X.reshape(X.shape[0], 1)
X_re.shape",np.mean(deviations),,Solution->Predicting user adoption,"Ds Hw2 Huimin Qian 060117->3. Use linear regression model to predict y, with only one feature--x. Please print out the training and validation score of your model and the mathematical formula of your model.","Variability->The Rough Size of Deviations from Average
Spread->Variability->Standard units"
420575,"# let p1 be white proportion, p2 be black proportion
p1 = float(wcalled / wnotcalled)
p2 = float(bcalled / bnotcalled)
np1 = len(df.race[df.race == 'w'])
np2 = len(df.race[df.race == 'b'])
print(p1, np1, p2, np2)","bcall = data[data.race=='b'].call
wcall = data[data.race=='w'].call
mean_dif = bcall.mean() - wcall.mean()
b_sigma = (bcall.std() ** 2) / bcall.count()
w_sigma = (wcall.std() ** 2) / wcall.count()
moe = b_sigma + w_sigma * 1.96
ci = (mean_dif - moe , mean_dif + moe)
print(""Margin of Error: %f"" %moe)
print(""Confidence Interval:"")
print(ci)","snp_counts = ds.aggregate_rows(agg.counter(hl.Struct(ref=ds.alleles[0], alt=ds.alleles[1])))
pprint(snp_counts)",,"Sliderule Dsi Inferential Statistics Exercise 2->3. Compute margin of error, confidence interval, and p-value.",Sliderule Dsi Inferential Statistics Exercise 2->Examining Racial Discrimination in the US Job Market->What test is appropriate for this problem? Does CLT apply?,Hail-Overview->Loading data from disk
217993,"eval_fixed = mv_utils.get_fixed_eval(classes_dist.index, eval_res)
v_fixed = eval_fixed.groupby(""true_destination"").ndcg_5.mean()
v_fixed = v_fixed.sort_values(ascending=False)","def normalize_audio(source_files):
    fixed_lengths = [lb.util.fix_length(y, PADDED_LENGTH) for y, _ in source_files]
    return [(f - np.mean(f)) / np.std(f) for f in fixed_lengths]","df.sub(s, axis='index') # subtract s from each column",,Report->Evaluation->Comparison with Simple Decision rules: Random and Fixed,Svm->SVM Classifier->Helper functions,10Min->Operations->Stats
206157,clf.grid_scores_,"### check on k paramter
clf.steps[1]","# Set lab variable (e.g., lab = ""CS105x-lab0"")
lab = ""CS120x-lab4""",,"Exploratory Data Analysis->New Feature Visualization->Comparison->Lapse 7
Homework 4->Applied ML->Question 2: Applied ML->Task 6 - A less naive approach
Modeling->Neural Network
Modeling->SVM->Bootstrap Dataset
Bike-Sharing-Demand->Bike sharing demand competition->Finding the optimum parameters for randomforest regressor
Data Science In Python->$f(x) = \sum_{i=1}^{N-1} 100(x_i - x_{i-1}^2)^2 + (1 - x_{i-1})^2$->Grid Search Cross Validation
Beer Analysis Final->Step 7:  Extract Topic Key Words->Beer Group Classification->Hyperparameter Tuning
Grid Search Gmm Test",Training Main->Exploring Algorithms->Tuning on SelectKBest,Cs105 Autograder->![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)->Ignore the following cell. You will fill it in after Part 2 after you have received the email from the autograder->** Part 3: Submit a lab to the course autograder **
369417,"def f(x):
    return x ** 2 - 10 * np.exp(-10000 * (x - np.pi) ** 2)","def numpy_invlogit(x):
    return 1 / (1 + np.exp(-x))",List2 = [x for x in range(10) if x%2 == 0],,Optimization->Data Analysis and Machine Learning Applications for Physicists->Optimization,"Talk
Talk",01->A Delete Assign->Basically range returns a list of elements (In the Basic format) from 0 to $n-1$
62221,tbwriter = SummaryWriter(),writer.commit(),"def color_hist(img, nbins=32, bins_range=(0, 256)):
    rhist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)
    ghist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)
    bhist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)

    bin_edges = rhist[1]
    bin_centers = (bin_edges[1:] + bin_edges[0:len(bin_edges)-1])/2
    
    hist_features = np.concatenate((rhist[0], ghist[0], bhist[0]))
    return rhist, ghist, bhist, bin_centers, hist_features",,2->PyTorch->Our First Neural Net,"Lesson 8 - Pdf Search App-Checkpoint->Lesson 8 - Practical: Searching your own PDF library->Indexing our PDFs
Text Search->Exercise: Building and Loading Text Search in Python Whoosh->4) Parse with our defined functions in place.",Untitled-Checkpoint->Car Detection with SVM->Feature Extraction functions
69637,"### TODO: Load the model weights with the best validation loss.
Resnet50_model.load_weights('saved_models/weights.best.Resnet50_model.hdf5')","### Load the model weights with the best validation loss.

Resnet50_model.load_weights('saved_models/weights.best.Resnet50.hdf5')","#Looking at each division
grouped = df.groupby('Division')

for name,group in grouped:
    print(name)
    print(group)
    print('--------------------------------')",,"Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Load the Model with the Best Validation Loss
Dog App->(IMPLEMENTATION) Load the Model with the Best Validation Loss
Dog App->(IMPLEMENTATION) Load the Model with the Best Validation Loss","Dog App->Train the Model
Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Load the Model with the Best Validation Loss
Dog App->Artificial Intelligence Nanodegree->(IMPLEMENTATION) Load the Model with the Best Validation Loss
Dog App->(IMPLEMENTATION) Load the Model with the Best Validation Loss",Powerlifting Plots (8) (3) (1) (1) (1)
204767,factorization_model,"mod = LinearFactorModel(portfolios, factors)
print(mod.fit(cov_type='kernel'))","# Coin picking probabilities.
p_pick_c1 = .5
p_pick_c2 = 1. - p_pick_c1
# Coing 1 probabilities.
p_c1_h = .5         
p_c1_t = 1. - p_c1_h
# Coing 2 probabilities.
p_c2_h = .9         
p_c2_t = 1. - p_c2_h
# Sequence ""H, T"" proabability
print  p_pick_c1 * p_c1_h * p_c1_t + p_pick_c2 * p_c2_h * p_c2_t",,Code->Factorization Model,Asset-Pricing Examples->Examples->2-Step Estimation,Udacity - Statistics->Normal Distribution
62916,"N = 10
[jx, jy, jz] = jspin(N)
jp = jspin(N, ""+"")
jm = jp.dag()

w0 = 20
g = 0
h0 = w0 * jz + g * jx 
gCE = 1.0
gD = .0
gP = N/2 * gCE
n_therm = 1

gE_list = [0, gP*(1+n_therm)]
system = Dicke(N = N)
gE = 0

system.hamiltonian = h0
system.collective_emission = gCE
system.emission = gE
system.pumping = gP
system.dephasing = gD
L = system.liouvillian()
rhoss = steadystate(L)
print(system)
td = np.log(N)/(N*gCE)
tmax = 100 * td
t = np.linspace(0, tmax, 2000)
dw = 10
wmin = w0 - dw
wmax = w0 + dw
nw = 100
wlist = np.linspace(wmin, wmax, nw)
lw = 3
#1
g2_norm, G2 = coherence_function_g2(L, None, t, [], jm, solver=""me"", args={},
                          options=Options(ntraj=[20, 100]))

g1_norm, G1 = coherence_function_g1(L, None, t, [], jm, solver=""me"", args={},
                          options=Options(ntraj=[20, 100]))

print(""ok"")
#swss = spectrum(L, wlist, [], jp, jm)
wlist, swss = spectrum_correlation_fft(t, g1_norm)

print(""ok 2"")
#2
g2_ss = correlation_2op_1t(L, rhoss, t, [], jp, jm)
#3
rho0 = dicke(N, N/2, -N/2)
#g2(tau)
rhoB = jm*rhoss*jp
result1 = mesolve(L, rhoB, t, [], e_ops = [jp*jm], options = Options(store_states=True))
print(""ok 3"")
g2t = result1.expect[0]
result2 = mesolve(L, rho0, t, [], e_ops = jp*jm, options = Options(store_states=True))
print(""ok 4"")
rhot = result2.states
jpjmt = result2.expect[0]
#g2t nat
Bn = jp * jm
jpjm_ss = expect(Bn, rhoss)
rhoBn = Bn * rhoss","N = 10
[jx, jy, jz] = jspin(N)
jp = jspin(N, ""+"")
jm = jp.dag()

w0 = 20
g = 0
h0 = w0 * jz + g * jx 
gCE = 1.0
gD = .0
gP = N/2 * gCE
n_therm = 1

gE_list = [0, gP*(1+n_therm)]
system = Dicke(N = N)
gE = 0

system.hamiltonian = h0
system.collective_emission = gCE
system.emission = gE
system.pumping = gP
system.dephasing = gD
L = system.liouvillian()
rhoss = steadystate(L)
print(system)
td = np.log(N)/(N*gCE)
tmax = 100 * td
t = np.linspace(0, tmax, 2000)
dw = 10
wmin = w0 - dw
wmax = w0 + dw
nw = 100
wlist = np.linspace(wmin, wmax, nw)
lw = 3
#1
g2_norm, G2 = coherence_function_g2(L, None, t, [], jm, solver=""me"", args={},
                          options=Options(ntraj=[20, 100]))

g1_norm, G1 = coherence_function_g1(L, None, t, [], jm, solver=""me"", args={},
                          options=Options(ntraj=[20, 100]))

print(""ok"")
#swss = spectrum(L, wlist, [], jp, jm)
wlist, swss = spectrum_correlation_fft(t, g1_norm)

print(""ok 2"")
#2
g2_ss = correlation_2op_1t(L, rhoss, t, [], jp, jm)
#3
rho0 = dicke(N, N/2, -N/2)
#g2(tau)
rhoB = jm*rhoss*jp
result1 = mesolve(L, rhoB, t, [], e_ops = [jp*jm], options = Options(store_states=True))
print(""ok 3"")
g2t = result1.expect[0]
result2 = mesolve(L, rho0, t, [], e_ops = jp*jm, options = Options(store_states=True))
print(""ok 4"")
rhot = result2.states
jpjmt = result2.expect[0]
#g2t nat
Bn = jp * jm
jpjm_ss = expect(Bn, rhoss)
rhoBn = Bn * rhoss","def remove_bad_shapes(shape):
    if shape.type == 'Multipolygon':
        shape_list = shape.geoms
    elif shape.type == 'Polygon':
        shape_list = [shape]
    else:
        return None
    
    polygon_list = []
    
    for g in shape_list:
        print g
        print explain_validity(g)

        hole_list = []
        for hole in g.interiors:
            hole_shape = shapely.geometry.Polygon(hole)

            if ""Too few points"" not in explain_validity(hole_shape):
                hole_list.append(hole)
            else:
                print ""Removed hole: ""
                print ""\t"" + str(hole)


        if ""Too few points"" not in explain_validity(shapely.geometry.Polygon(g.exterior.coords)):
            print ""\t"" + explain_validity(shapely.geometry.Polygon(g.exterior.coords))
            polygon_list.append(shapely.geometry.Polygon(g.exterior.coords, hole_list))
        else:
            print ""Removed: "" 
            print ""\t"" + str(g.exterior.coords)
            
    return shapely.geometry.MultiPolygon(polygon_list)",,"2->1) Time evolution->Emission spectrum->$N>1$: Steady-state superradiance, cooperative resonance fluorescence","2->1) Time evolution->Emission spectrum->$N>1$: Steady-state superradiance, cooperative resonance fluorescence",Tech Fest Notebook->There is one error that is better if treated differently: polygons without enough points
471957,lust1 = lust.where(lambda row: row['S'] != 'R'),cleanDamaged = unlcrimes.where(lambda row: row['Damaged'] != None),"# table_intervals_df = pd.read_csv('table_intervals_results.txt', sep='\t')",,Homework 2,Data Smells Walkthrough,Storyline For Aied Poster->Learning through inquiry->Sensitivity analysis on CVS criteria using BIC
4070,"%%time
body_test_transformed = issue_body_proc2.transform_parallel(body_test)","from yellowbrick.features import ParallelCoordinates

# Instantiate the visualizer
visualizer = ParallelCoordinates()

visualizer.fit(x_train, y_train)      # Fit the data to the visualizer
visualizer.transform(x_train)   # Transform the data
visualizer.poof()         # Draw/show/poof the data","sns.lmplot(x='comp1', y='comp2', data=df_tsne5n1500, hue='label', fit_reg=False)
plt.title(""t-SNE, 1500 Iterations"", size=16, y=1.05)",,Tutorial->Initilize **processor** object->`transform_parallel` method,Uci Contraceptive Yellowbrick->Read the CSV->Yellowbrick Radviz prep work->Looks like lots of noise. It's going to be hard to get separation and good predictive results on this classifier problem.,Homework 5-Checkpoint->Homework 5
464453,"speaker_file_name = ctypes.c_char_p(b'vtlapi-2.1b/JD2.speaker')
gesture_file_name = ctypes.c_char_p(b'vtlapi-2.1b/example-hallo.ges')
wav_file_name = ctypes.c_char_p(b'example-hallo.wav')
feedback_file_name = ctypes.c_char_p(b'example-hallo.txt')","speaker_file_name = ctypes.c_char_p('vtlapi-2.1b/JD2.speaker'.encode())
failure = VTL.vtlInitialize(speaker_file_name)
if failure != 0:
    raise ValueError('Error in vtlInitialize! Errorcode: %i' % failure)","# Import the fertility.csv data: data
data = pd.read_csv(""fertility.csv"", encoding = 'latin2')

fertility_latinamerica = data[data[""Continent""] == ""LAT""][""fertility""]
female_literacy_latinamerica = data[data[""Continent""] == ""LAT""][""female literacy""]

fertility_africa = data[data[""Continent""] == ""AF""][""fertility""]
female_literacy_africa = data[data[""Continent""] == ""AF""][""female literacy""]

fertility_europe = data[data[""Continent""] == ""ASI""][""fertility""]
female_literacy_europe = data[data[""Continent""] == ""ASI""][""female literacy""]

fertility_asia = data[data[""Continent""] == ""EUR""][""fertility""]
female_literacy_asia = data[data[""Continent""] == ""EUR""][""female literacy""]",,Ex Vlt Gestures->Synthesize from a gestural score.,Ex Vlt Core->Initialize vtl,"Interactive Data Visualization With Bokeh - Part Ii->Layouts, Interactions, and Annotations->Creating gridded layouts"
107152,display_grouped_patient_characteristics('Alcoholism'),"patient_groups=[""control"", ""viral"", ""bacterial"", ""fungal""]
group_id = lambda name: patient_groups.index(name)

X = pd.DataFrame.from_csv(""combineSV_WTcpmtable_v2.txt"", sep=""\s+"").T
y = [group_id(""bacterial"")] * 29 \
    + [group_id(""viral"")] * 42 \
    + [group_id(""fungal"")] * 10 \
    + [group_id(""control"")] * 61

print ""Complete data set has %d samples and %d features."" % (X.shape[0], X.shape[1])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)
print ""Training set has %d samples. Testing set has %d samples."" % (len(X_train), len(X_test))","yahoo = pd.read_csv('/data/YHOO.csv', index_col='Date', parse_dates=True)
yahoo = yahoo.reset_index()
mask = (yahoo['Date'] > start) & (yahoo['Date'] <= end)
yahoo = yahoo.loc[mask]
yahoo = yahoo.set_index(['Date'])
yahoo.head()",,"Project Investigate No-Show Appointments->Project: Investigate No-Show Appointments->Data Cleaning->Examination description
Project Investigate No-Show Appointments->Project: Investigate No-Show Appointments->Data Cleaning->Examination description->Display percentages and write description of those",Pathogen Prediction->Predicting Pathogen from RNAseq data,Analysis1 Final->Analysis 1->Loading data for the required securities/shares into dataframes
289548,"# Timestamp in seconds
df = df.withColumn( 
    'TIME_unix_time', 
    unix_timestamp(df['time'], ""yyyy-MM-dd'T'HH:mm:ss'Z'"")
)

# Partitioning on <athlete> and <activity_type>
window = Window.partitionBy('athlete', 'activity_type').orderBy('TIME_unix_time')","trip_window = Window.partitionBy(""trip_id"").orderBy(unix_timestamp('arrival_time','HH:mm'),
                                                    unix_timestamp('departure_time','HH:mm'))","# Alternative method (about 1.5 ms faster):
# df_themes_named = df_themes.sort_values(['code','name'])
# df_themes_named['name'] = df_themes_named.name.replace('',method='bfill')
# df_themes_named = df_themes_named.sort_index()
# df_themes_named",,Preprocessing->Preprocessing Notebook->Processing->Tracking point dataset,Final Notebook 2->Section 3: Implement a robust route planning algorithm->using the lag function on specific windows,"Sliderule Dsi Json Exercise->JSON example, with string->3.   Dataframe from #2 with all missing names filled in:"
351000,"X_train.issueArea = X_train.issueArea.apply(lambda x: issue_consolidate(x))
X_train.petitioner = X_train.petitioner.apply(lambda x: party_categorizer(int(x)))                                  
X_train.respondent = X_train.respondent.apply(lambda x: party_categorizer(int(x)))
X_train.caseSource = X_train.caseSource.apply(lambda x :'1' if x in ['300', '301', '301'] else '0')
X_train.caseOrigin = X_train.caseOrigin.apply(lambda x :'1' if x in ['300', '301', '301'] else '0')","train['HasPool'] = train['PoolArea'].apply(lambda x: 'N' if x == 0 else 'Y')
plot_category_distribution(train, 'HasPool')","with Model() as varying_intercept_slope:
    
    # Priors    
    mu_a = Normal('mu_a', mu=0., tau=0.0001)
    sigma_a = Uniform('sigma_a', lower=0, upper=100)
    tau_a = sigma_a**-2
    
    mu_b = Normal('mu_b', mu=0., tau=0.0001)
    sigma_b = Uniform('sigma_b', lower=0, upper=100)
    tau_b = sigma_b**-2
    
    # Random intercepts
    a = Normal('a', mu=mu_a, tau=tau_a, shape=len(set(sites)))
    # Random slopes
    b = Normal('b', mu=mu_b, tau=tau_b, shape=len(set(sites)))
    
    # Model error
    sigma_y = Uniform('sigma_y', lower=0, upper=100)
    tau_y = sigma_y**-2
    
    # Expected value
    y_hat = a[sites] + b[sites] * data.week
    
    # Data likelihood
    y_like = Normal('y_like', mu=y_hat, tau=tau_y, observed=data.biophony)
    y_sim = Normal('y_sim', mu=y_hat, tau=tau_y, shape=y_hat.tag.test_value.shape)",,Technical Notebook,Eda->Analysis->Has Pool,Regression Model - Exploration->Regression model - exploration->Plot data
44005,autos['price'].value_counts().sort_index(),auto[auto['horsepower'] > 100].count(),"def init_keys_dict():
    return dict(
        [(k, {'default_cv': None, 'tuned_cv': None, 'default_test': None, 'tuned_test': None, 'trials': None}) for k in
         CANDIDATES.keys()])


trackers = {}
custom_metrics = [RocAuc(), Accuracy()]
all_metrics = init_keys_dict()
trainer = Trainer(opt_evals=N_PROBES, n_estimators=N_ESTIMATORS)",,Basics->Exploring Ebay Car Sales Data->Exploring the Odometer and Price Columns,Day+13+-+Pandas+Lab+Exercise+(Kaggle+Automobile+Dataset)->Pandas Lab Exercise (Kaggle Automobile Dataset),Model Search->Run them all
375487,"dump=plt.hist(np.log10(hum_data[hum_data.inequality=='>'].meas.values),bins=50,log=True)","logT90 = np.log10(data['T90'])
plt.hist(logT90, bins=30)
plt.xlabel('$\log_{10}\mathrm{T90}$ (sec)');","# This code will...
two_layer_model = models.Sequential()  # this is Keras's way of specifying a model that is a single sequence of layers
two_layer_model.add(layers.Dense(250, activation='relu', input_shape=(6,))) #optimized to get best accuracy at 250
two_layer_model.add(layers.Dense(2, activation='softmax'))

print(two_layer_model.summary())


two_layer_model.compile(optimizer='rmsprop',
                        loss='categorical_crossentropy',
                        metrics=['accuracy'])",,Explore->Larger than are not,Final->Distribution of `T90`,Final Report Rev5->(4) Neural network (NN)->(b) 2-layer neural net
482086,"import os
import csv

from keras.models import Sequential, Model
from keras.layers import Cropping2D, Lambda, Dense, Conv2D, Activation, Flatten, MaxPooling2D, Dropout
from sklearn.model_selection import train_test_split
from random import shuffle
import cv2
import numpy as np
import sklearn","import os
import csv

from keras.models import Sequential, Model
from keras.layers import Cropping2D, Lambda, Dense, Conv2D, Activation, Flatten, MaxPooling2D, Dropout
from sklearn.model_selection import train_test_split
from random import shuffle
import cv2
import numpy as np
import sklearn","tuned_parameters_lr = {'C': scipy.stats.expon(scale=100), 'class_weight': [{True: 10}]}
scores_lr = ['f1']
trained_estimator_lr = train_model_by_random_search_cv(
    X_train, 
    X_test, 
    y_train, 
    y_test, 
    estimator_lr, 
    tuned_parameters_lr, 
    ts_iterable, 
    num_iter=10,
    scores=scores_lr,
    probability=True
)",,Behavior Cloning->Behavior Cloning Project,Behavior Cloning->Behavior Cloning Project,"Train Static Annual Lr 12->A3) Logistic Regression - Objective=f1, class_weight=10"
167909,"keras.layers.recurrent.LSTM(output_dim, init='glorot_uniform', inner_init='orthogonal', 
                            forget_bias_init='one', activation='tanh', 
                            inner_activation='hard_sigmoid', 
                            W_regularizer=None, U_regularizer=None, b_regularizer=None, 
                            dropout_W=0.0, dropout_U=0.0)","# 1.
# Create the encoder (set the output_dim to hidden_dim which we defined earlier).

(encoder_output_h, encoder_output_c) = LSTM_layer(input_sequence, hidden_dim)

# 2.
# Set num_layers to something higher than 1 and create a stack of LSTMs to represent the encoder.
num_layers = 2
output_h = alias(input_sequence) # get a copy of the input_sequence
for i in range(0, num_layers):
    (output_h, output_c) = LSTM_layer(output_h.output, hidden_dim)

# 3.
# Get the output of the encoder and put it into the right form to be passed into the decoder [hard]
thought_vector_h = sequence.first(output_h)
thought_vector_c = sequence.first(output_c)

thought_vector_broadcast_h = sequence.broadcast_as(thought_vector_h, label_sequence)
thought_vector_broadcast_c = sequence.broadcast_as(thought_vector_c, label_sequence)

# 4.
# Reverse the order of the input_sequence (this has been shown to help especially in machine translation)
(encoder_output_h, encoder_output_c) = LSTM_layer(input_sequence, hidden_dim, future_value, future_value)","# Sample of variables we deleted because majority of the rows are empty. 
b.sort_values(ascending=False).head(5)",,3->LSTM,Cntk 204 Sequence To Sequence->Step 2: define the network->Exercise 1: Create the encoder,Data Cleaning->Cleaning Data->Cleaned Data
2265,"%matplotlib inline
import matplotlib.pylab
import matplotlib.pyplot as plt
matplotlib.pylab.rcParams['figure.figsize'] = (18, 4)

logMagSpecgram = madmom.audio.spectrogram.LogarithmicFilteredSpectrogram(stft) # logfilt for better visualization
plt.imshow(logMagSpecgram[:, :200].T, aspect='auto', origin='lower') # plot the spectrogram 
plt.xlabel('Frame')
plt.ylabel('Frequency bin')","filt_spec = madmom.audio.spectrogram.FilteredSpectrogram(spec, filterbank=madmom.audio.filters.LogFilterbank,
                                                         num_bands=24)
plt.imshow(filt_spec.T, origin='lower', aspect='auto')","djia_index_names = ['Index']
djia_index_dtypes = {'Index':numpy.float}
djia = pandas.read_csv(""djia/index.csv"", index_col = 0, header = 0, names = djia_index_names, dtype = djia_index_dtypes, 
                       na_values = '.', parse_dates = True)",,Tutorial->Basic Data Processing->Magnitude Spectrogram,Onset Detection->Onset detection tutorial,M36101P-Assignment1->Census data->Days left till election->Dow Jones Industrial Average index data set
97838,"A2 = 0.577
D3 = 0
D4 = 2.115
# For Xbar Control Chart we have
LCL_X = X_grand_average - (A2*R_average)
UCL_X = X_grand_average + (A2*R_average)
# For R control Chart we have 
LCL_R = D3*R_average
UCL_R = D4*R_average
print 'LCL for Xbar Control chart is ',LCL_X, 'and UCL is ',UCL_X
print 'LCL for R Control chart is ',LCL_R, 'and UCL is ',UCL_R","figure(figsize=(6,4))
plot(sample_j,R_mi,'.-')
plt.axhline(y=UCL_R, color='r', linestyle='-')
plt.axhline(y=LCL_R, color='r', linestyle='-')
plt.axhline(y=R_m_bar, color='r', linestyle='-')
ylabel('Temperature Range')
xlabel('Sample')
title('Rm Chart')","# First perform Canny edge detection to use as the input
# Note that the output `canny_edges` is in grayscale(single-channel)
canny_edges = cv2.Canny(img, 50, 100)

# Then find lines with Hough transform
lines = cv2.HoughLines(canny_edges, 1, np.pi/180, 200)
print(""rho theta"")
print(lines)
print(lines.shape)",,Homework 3->Problem 6.4,Homework 4->ORI 397: Statistical Methods in Manufacturing->Problem 9.24,1 Hough Transform->Hough Transform->1. Hough Line Transform
471527,"customer = {
    ""Name"": [""Krisna"", ""Adit""],
    ""Address"": [""Salemba"", ""Jaya""]
}","#example of using slice addressing on series objects
series_custom[5:10]","sql_query = \
""SELECT ID, CLINIC, DATE FROM DATA WHERE VARIABLES->>'gen_1' = '1' AND VARIABLES->>'nat_1' = '1'"" 
# ->> is used to access a key-value pair in a JSON column. The value will be returned as character string


session.commit() # These commit lines make sure the session is in its default state even if something goes wrong
try:
    result = session.execute(sql_query)
    for r in result:
        print(""ID:"" ,r.id, "" clinic:"", r.clinic, "" date:"", r.date)
except ProgrammingError as e:
    print(""SQL command wasn't valid:"", e)
session.commit()",,Python Basic->Machine Learning Package is Starting,Dq Data Structures->Pandas internals: Series,"Sql Basics->SQL Basics->Exercise 2: Querying JSON objects in PostgreSQL
Sql Basics->SQL Basics->Exercise 2: Querying JSON objects in PostgreSQL->b)"
477599,"#Try best combination of parameters from grid search results
from sklearn.naive_bayes import GaussianNB
clf_pipe = Pipeline(
    [('PCA_TJ_$$',RandomizedPCA(n_components=50)),
     ('CLF_TJ_$$',GaussianNB())]
)

for train, test in cv:
    clf_pipe.fit(X[train],y[train])
    yhat[test] = clf_pipe.predict(X[test])
    
total_accuracy = mt.accuracy_score(y, yhat)
print 'Gaussian Naive-Bayes Classifier, Refined pipeline accuracy', total_accuracy","# Naive Bayes
# from sklearn.svm import SVC
# from sklearn.linear_model import LogisticRegression
# from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import GaussianNB
from sklearn.decomposition import PCA 


# setup pipeline to take PCA, then fit a different classifier
clf_pipe = Pipeline(
    [('PCA',PCA(n_components=5,svd_solver='randomized')),
     ('CLF',GaussianNB())]
)

# now iterate through and get predictions, saved to the correct row in yhat
for train, test in cv.split(X,y):
    clf_pipe.fit(X[train],y[train])
    yhat[test] = clf_pipe.predict(X[test])

total_accuracy = mt.accuracy_score(y, yhat)
conf = mt.confusion_matrix(y, yhat)

print ('Pipeline accuracy', total_accuracy)
print(conf)
#plot_class_acc(y,yhat,title=""Naive Bayes + PCA"")","GDP_INDICATOR = 'SP.POP.TOTL'
YEAR = 2016

popWB = download(indicator=GDP_INDICATOR, country='all', start=YEAR, end=YEAR)
popWB.shape # (264, 1)",,Assignment3->Gaussian Naive-Bayes,Project2 Jjstjw(1)->Analysis of the Dallas Crime Dataset using Classification and Regression Techniques,Project 3 Life Expectancy Versus Gdp - Mjr With Map->Project 3: GDP versus life expectancy->Dataset->4. Data Analysis->Including GDP per capta on dataset
305953,"def extract_text(boiler):
    js = json.loads(boiler)
    if ('body' in js and js['body'] != None):
        body = js['body']
        # Remove non-letters        
        letters_only = re.sub(""[^a-zA-Z]"", "" "", body) 
        # Convert to lower case, split into individual words
        words = letters_only.lower().split()
        body_text = ( "" "".join(words))
    else:
        body_text = """"
        
    if ('title' in js and js['title'] != None):
        title = js['title']
        # Remove non-letters        
        letters_only = re.sub(""[^a-zA-Z]"", "" "", title) 
        # Convert to lower case, split into individual words
        words = letters_only.lower().split()
        title_text = ( "" "".join(words))
    else:
        title_text = """"
        
    if ('url' in js and js['url'] != None):
        url = js['url']
        # Remove non-letters        
        letters_only = re.sub(""[^a-zA-Z]"", "" "", url) 
        # Convert to lower case, split into individual words
        words = letters_only.lower().split()
        url_text = ( "" "".join(words))
    else:
        url_text = """"
    
    return body_text + title_text + url_text","def text_cleaner(text):
    text = re.sub(r'--', ' ', text)
    text = re.sub(r'\d', '', text)
    #text = re.sub(r'\.', '. ', text)
    text = text.lower()
    text = ' '.join(text.split())
    return text",data[data.sex == 'Female'].shape[0] + data[data.sex == 'Male'].shape[0] == data.shape[0],,Project-Dor->Pre Processing->Bag of Words,"4->Classification Models
New York Times Decades->New York Times Over the Decades->Text Selection and Cleaning",Hw1
408989,"import numpy as np

mdiff = np.ones((num_topics, num_topics))
np.fill_diagonal(mdiff, 0.)
    
plot_difference(mdiff, title=""Topic difference (one model) in ideal world"")","# difflib_differ.py

import difflib
# from difflib_data import *

d = difflib.Differ()
diff = d.compare(text1_lines, text2_lines)
print('\n'.join(diff))","Refereestandings = pd.concat([Referees,Refereesshownredaway,Refereesshownredathome,Refereesshownyellowaway,Refereesshownyellowathome],axis=1,sort=True)
Refereestandings['Yellowcards_shown'] = Refereestandings.AY+Refereestandings.HY
Refereestandings['Redcards_shown'] = Refereestandings.AR+Refereestandings.HR
Refereestandings.sort_values(by='Redcards_shown',ascending=False)
ToughestReferee=Refereestandings.rename(columns={'Referee':'Matches_officiated','AR':'Red Card shown to away player','HR':'Red Card shown to home player','AY':'Yellow Card shown to away player','HY':'Yellow Card shown to home player'})
ToughestReferee.sort_values(by='Redcards_shown',ascending=False).fillna(0).astype(int)
#.sort_values(['Points'], ascending=[False])",,Lda Model Difference->Comparison of two LDA models & visualize difference->Case 1: How topics in ONE model correlate with each other.,Difflib - Compare Sequences->difflib - Compare Sequences->Comparing Bodies of Text,Premier League-Update-2015-16->Relegation analysis
178168,"#twitter clearenece
consumer_key = 'XXXXXXXXXXXXXXXXXXX'
consumer_secret = 'XXXXXXXXXXXXXXXXXXX'
access_token = 'XXXXXXXXXXXXXXXXXXX-XXXXXXXXXXXXXXXXXXX'
access_secret = 'XXXXXXXXXXXXXXXXXXX'

auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_secret)

api = tweepy.API(auth)","# This handles Twitter authentication and the connection to Twitter Streaming API
auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = API(auth, wait_on_rate_limit=True)","from sklearn.neural_network import MLPRegressor
scaler = StandardScaler()
# Fit only to the training data
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
# Puedo variar el hidden_layer a mas grande o el random_state a mas grande
print (""---- Start algorithm ---- "")
MLP = MLPRegressor(
    hidden_layer_sizes=(100,100,100,100),  activation='relu', solver='sgd', alpha=0.1, batch_size=2000,
    learning_rate='adaptive', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True,
    random_state=0, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,
    early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)

print (""Fitting ..."")
MLP.fit(X_train,y_train)
prediction = MLP.predict(X_test)
print (prediction)
print (""Error cuadratico medio: %.2f "" % np.mean((prediction - y_test)**2))",,Wrangle Act,Mozsprint Data Wrangling-Checkpoint->Using Python for Event's data analysis->Final touch : The Word Cloud,Some Analysis - Turi->Gradient Boost Regressor->MLP REGRESSOR
283506,"np.max(CleanAnaerobic[CleanAnaerobic[""PlasmidsPerCell""]==""0""].MeanPixel)","X = np.linspace(np.log(data05_clean['NormSourceEUI'].min()),np.log(data05_clean['NormSourceEUI'].max()),10)
Y = ghg.params[0] + ghg.params[1]*X","def processNames(name1, name2):
    names = list((set(name1.middle.split(' ')) | set(name2.middle.split(' ')) | set(name1.last.split(' ')) | set(name2.last.split(' '))) - {''})
    for name in names:
            if(len(name) == 1):
                for othername in names:
                    if(othername[0] == str(name) and othername!=name):
                        names.remove(name)
                        break
    if(name1.last in names):
        names.remove(name1.last)
        name1.middle = ' '.join(names)
    elif(name2.last in names):
        names.remove(name2.last)
        name1.last = name2.last
        name1.middle = ' '.join(names)",,"Homework3
Group3 Hw3 Prob1",370 Jay Street Eui Estimation->6. Estimating Greenhouse Gas Emissions (Total GHG Emissions)->How this faires with other office buildings:,Name De Duplicator->Name Deduplicator->processNames
256248,"petition_ids = list(range(1, 25))","def eval_agg_clust(clusterer, config, rang, distance, data):
    aggBouldins = []
    aggDunns = []
    aggSilhouette = []
    aggCalinskis = []
    
    for i in rang:
        config['n_clusters'] = i
        aggClusterer = clusterer(**config)
        aggClusterer.fit(data)

        clusters = []
        means = []
        for cluster_id in set(aggClusterer.labels_):
            cluster = data[np.argwhere(aggClusterer.labels_ == cluster_id)[:,0]]
            clusters.append(cluster)
            means.append(cluster.mean(axis=0))

        aggBouldins.append(jq.davisbouldin(clusters, means))
        aggDunns.append(jq.dunn_fast(data, aggClusterer.labels_))
        aggSilhouette.append(silhouette_score(distance, aggClusterer.labels_, metric='precomputed'))
        aggCalinskis.append(calinski_harabaz_score(data, aggClusterer.labels_))
    
    return aggBouldins, aggDunns, aggSilhouette, aggCalinskis","# get RMSE for RF model 
RMSE = sqrt(mean_squared_error(y_test, y_predicted))
print(RMSE)",,Michael Brown Combined Episodes Analysis,Clustering->Clustering the users based on consupmtions,Modeling Step With Output->EDA->Train Model and make predictions on the test set
7790,"# look at the shape of the dataframe (rows = observattions, columns = fields)
data_df.shape",data_df[:10],"Yield_agb= ( k_N/1.35 * (1**-1.35 - 8.**-1.35) ) * 0.1
Yield_massive= ( k_N/1.35 * (8.**-1.35 - 30**-1.35) ) * 0.1",,Data Preprocessing->Data Preprocessing->The Data,Amazon Phone Reviews Nltk->Sentiment Analysis on Amazon Unlocked Mobile Phones Using NLTK->There are 385 brands in this data set.,Sygma Ssp H Yield Input->Regression test suite: Test of basic SSP GCE features->Outline:->Test of distinguishing between massive and AGB sources:
59800,"from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()

clf.fit(X=X_train, y=y_train)

pred = clf.predict(X=X_test)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
print('Confusion Matrix:\n'+str(confusion_matrix(y_test, pred)))
print('\nClassification Report:\n'+str(classification_report(y_test, pred)))
print('Accuracy = {0:6.2f}'.format(accuracy_score(y_test, pred)))","# Gaussian Naive Bayes
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train, y_train)
print(metrics.classification_report(y_test, clf.predict(X_test)))
print('Accuracy: {}'.format(round(clf.score(X_test, y_test),2)))","from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam

## TODO: Compile the model
model.compile(optimizer ='rmsprop', loss='mean_squared_error', metrics=['accuracy'])


## TODO: Train the model
hist = model.fit(X_train, y_train, validation_split = 0.2, epochs = 70, batch_size=16, verbose=2)

## TODO: Save the model as my_model.h5
model.save('my_model.h5')",,2->Class Exercise,Story 5 - Validity->SDMT Validity->2.1 Predict MS Via Classification,Facial-Keypoints-Cnn->(IMPLEMENTATION) Compile and Train the Model
90667,"def plot_decorator(f):
    points = np.genfromtxt('data.csv', delimiter=',')
    
    return lambda a, b: f(a, b, points)


cost = plot_decorator(compute_error)","hatches = ""\/|+x-.o*""
for h in hatches:
    Decor({'component': Component({'hatch':h}), 'hatch': h, 'colour': '#eeeeee'}).plot()","import trading_env as te

stayflat     = lambda o,e: 1   # stand pat
buyandhold   = lambda o,e: 2   # buy on day #1 and hold
randomtrader = lambda o,e: e.action_space.sample() # retail trader

# to run singly, we call run_strat.  we are returned a dataframe containing 
#  all steps in the sim.
bhdf = env.run_strat(buyandhold)

print bhdf.head()

# we can easily plot our nav in time:
bhdf.bod_nav.plot(title='buy & hold nav')",,Linear Regression->Plot Cost funstion,Display Objects->Display objects->Decor,Trading Env->utility methods: running strategies once or repeatedly
199711,"# while loops
s = 0
i = 0
v = range(5)
while i < len(v):
    s += v[i]
    i += 1
print(s)","i = 0
s = 'xyz'
while i < len(s) and not (s[i] in 'aeiouAEIOU'):
    print(s[i])
    i = i + 1","# test pipeline image 2
global heatmap_memory 
heatmap_memory = np.zeros_like(test_images[0][:,:,0]).astype(np.float) # create a image memory of old cars

image = test_images[2]
draw_image = process_image(image)


plt.subplot(121)
plt.imshow(draw_image)
plt.title('Car Positions')
plt.subplot(122)
plt.imshow(heatmap_memory, cmap='hot')
plt.title('Heat Map')",,Python Basics->Python Basics->Loops in Python,"Week5 While List Mutate-Checkpoint->While Loops, Lists, and Mutability->Loops Conditions and Lazy Evaluation",P5 Vehicle Detection->P5 Vehicle Detection->Define the video pipeline->Test the pipeline on all test images
52681,"payment_activities_RAW = deepcopy(dfs[6])
payment_uid = set(payment_activities_RAW[""T:concept:name""])","result_ids = create_to_payment_uid
for i in range(0,len(dfs)):
    if i not in [2,6]:
        helper = deepcopy(dfs[i])
        helper = set(helper[""T:concept:name""])
        result_ids = result_ids - helper","import os
import numpy as np
import cv2
import glob
import matplotlib.pyplot as plt
%matplotlib qt

out_dir = 'output_images/step1/'
# Make a list of calibration images
images = glob.glob('camera_cal/calibration*.jpg')",,2->CROSS SECTIONAL ANALYSIS->Get all CaseIDs from payment events:,2->CROSS SECTIONAL ANALYSIS->Definition of the E:dismissal variable:->Delete all CaseIDs which are part of any other set,Step1-Camera Calibration
253918,"start_time = time.time()
clflog_ros_30 = LogisticRegression(class_weight='balanced')

parameters = {""C"": [0.01, 0.1, 1, 10, 100, 1000],
             ""penalty"":['l2','l1']}

clflog_ros_30, Xtrain, ytrain, Xtest, ytest = do_classify(clflog_ros_30,parameters,X_train_ros_30,
                                                   X_test,y_train_ros_30,y_test,
                                                          score_func='roc_auc')

make_roc(""clflog"",clflog_ros_30, ytest, Xtest, None, labe=200)

print(""--- %s seconds ---"" % (time.time() - start_time))","# BASE CASE
print('\n\n-------------------------------------------------')    
print('BASE CASE')
print('-------------------------------------------------')
c, e, m = regress(xtrain, ytrain, xtest, ytest)
c.index = ['ones', 'surface', 'arrondissement']
c.columns = ['coefficient']
m.columns = ['surface', 'arrondissement', 'true_value', 'prediction', 'error']
# print results
printMe()","# now let's convert our GeoDataFrame into a GeoJson
juarez_2013_gjson = merged_datasets.to_json()",,Untitled->Problem 2->2.5 Building Classification models->2.5.1 Defining some important functions for Classification->2.5.1.3 Functions for performing Resampling->2.5.3.1.4 Fitting LR on ROS dataset having a sampling ratio of 30:70 (Minority:Majority),Tp 1->2- Machine learning->2.2- The base case,Juarez Electoral Sections->Ciudad Juarez Electoral Sections->Plotting the electoral sections from Juarez
106606,"# One way to visualize the effect of gender on churn -- i.e. interdependence between variables
from plotnine import *
(ggplot(df_churn, aes(x='Churn', fill='gender')) + geom_bar(position='fill'))","# Grouped bar plot with animal and outcome on x-axis and age on y-axis
from plotnine import *
ggplot(data, aes(x='animal', y='age', fill='outcome')) + geom_bar(stat='identity', position='dodge')","'''
32 = integer
3.14 = float
'32' = string
'''
# str -> int 
print (type(int('32')) )

# int -> str
print (type(str(32)))

# int / str -> float 
print (type(float(32)))
print (type(float('32')))",,Systematic-Approach-To-Visualizing-Data->A Systematic Approach to Visualizing Data->6. Visualize Relationships Between Categorical Features->2d. Numerical Features,Day 3 - Exploring And Analyzing Data In Python->Describing and summarizing data->Plotting in seaborn->ggplot2 for Python,Lecture01-><font color=red>Comments</font>->Type conversion
158454,"# getting the list of countries having missing data in income classification for 1990
"", "".join(list(mat_mort_regions[pd.isnull(mat_mort_regions['1990_income'])]['country'].sort_values()))","#handling missing values

mat_mort_100k_lbirths = mat_mort_100k_lbirths.replace(0, pd.np.nan)
mat_mort_100k_lbirths.info()","#today = datetime.date.today()
#file_name = stock_name+'_stock_%s.csv' % today
#df.to_csv(file_name)",,Investigate A Dataset->Extending the data set: income variables,Investigate A Dataset,Oldversion3->Download data and normalize it
147260,"#Lets import the imputer from sklearn
from sklearn.preprocessing import Imputer
#we have to instantiate the imputer object with parameters that we need
imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)#NaN means not a number. axis means which direction rows = 1, columns = 0
#imputer has some nice functions such as fit_transform where it fits to our data and fills in the missing data
#be careful it, you only want to transform the column values that are numbers if the strategy is 'mean'. if its categorical you want to use 'most_frequent'
X = imputer.fit_transform(X)
#what does your data look like?","# cek head dari data_categorical_imputed
x_train_cat_imput.head()","def defaultEnergyFunction(hidden, observation, alpha=1, beta=1, eta=1):
    Tb = np.asarray([[0,1.0,0], [1.0,0,1.0], [0, 1.0, 0]])
    matB = conv2d(hidden, Tb, 'same')
    mapE = hidden * (alpha - beta * matB - eta * observation)
    E = np.mean(mapE)
    dE = -2 * mapE
    
    return E, dE",,Lecture 2->Part 1 - Code Review->Missing Data,Dsb - Exercise - Muhammad Fahry-Checkpoint->Categorical Dummy,Exercise 3->EECS 491: Probabilistic Graphical Models Assignment 2->Energy Function
92672,"def term_df_dy(term):
    pass","def slow_function(i):
    '''
    wait 1s and then print argument passed in
    '''
    from time import sleep
    print(""Sleeping ..."")
    sleep(1)
    print(""argument is: {}"".format(i))
    return i","probability_dog = preds[:,1]
probability_dog[:5]",,Index->Using our partial derivative rule,Interact Manual,Hw1->fast-ai homework from lesson 1->Run the data against the test set and generate a kaggle submission csv
101437,"def initial_jam_empty(x):
    """"""
    Lane 1: density 1 for x < 0
    Lane 2: line with negative slope (roughly, the result of some time passing 
            from lane 1's state)
    """"""
    p1 = (1.0 * (x <= 0))
    # Smoothed
    #p1 = (0.0 + (1.0 * (x <= 0)) + 
    #      (0.5 + 0.5 * numpy.cos(10 * numpy.pi * x)) * ((x > 0) & (x <= 0.1)))
    
    # Simple line
    p2 = -0.5 * x + 0.5
    # More accurate 'earlier'
    #p2 = (1.0 * (x <= -0.5) + (0.5 - x) * ((x > -0.5) & (x <= 0.5)))
    # Smoothed
    #p2 = 0.2 + (0.2 * (x < 0) + 
    #     0.2 * numpy.cos(numpy.pi * x) * (x >= 0))
    
    return numpy.array([p1, p2])

problem = TwoLanesTraffic(a=0.5)
x, hist = fvsolve2systembc(
                problem.riemann_hll, initial_jam_empty, problem.bc_periodic,
                source=problem.source_lane_changes, n=100, tfinal=3, args=())

plot_densities(problem, x, hist, until=3)
plot_speeds(problem, x, hist, until=3)","rho_l = 0.9; rho_r = 0.6
v_l = 1.0; v_r = 0.8
traffic_variable_speed.phase_plane_plot(rho_l,rho_r,
                                        v_l,v_r,connect=False)","metrics.roc_auc_score(y_test, y_pred_prob)

# the area under ROC curve can als obe calculated; the closer to one the better
# the entire graph is 1",,Project->Final Project->Traffic in Two Lanes->Scenario: Closed Road Reopens,Traffic Variable Speed->LWR Traffic flow with varying speed limit->Case 2: speed limit decrease with $\rho_l < \rho_r$,Scikit-Learn->Data Modeling with Scikit-Learn->Creating a Model->K-nearest neighbors (KNN) classification
367168,"ln_selection_function = sim_info['ln selection function']
# assert np.isclose(np.sum(np.exp(ln_selection_function) * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)

fig = plt.figure(figsize=(n_types*len(colors), len(colors)))
for t in range(n_types):
    plt.subplot(1, n_types, t+1)
    plt.pcolormesh(z_mids, mu_mids, ln_selection_function[t].T, cmap='viridis')#, vmin = 0., vmax = 3.)
    plt.title('SN '+types[t]+' log selection function distribution')
    plt.xlabel(r'$z$')
    plt.ylabel(r'$\mu$')
    plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])
    plt.colorbar()
plt.savefig('plots/full_selection_function.png')","interim_ln_prior = sim_info['interim ln prior']
# assert np.isclose(np.sum(np.exp(interim_ln_prior) * z_difs[np.newaxis, :, np.newaxis] * mu_difs[np.newaxis, np.newaxis, :]), 1.)

fig = plt.figure(figsize=(n_types*len(colors), len(colors)))
for t in range(n_types):
    plt.subplot(1, n_types, t+1)
    plt.pcolormesh(z_mids, mu_mids, interim_ln_prior[t].T, cmap='viridis')#, vmin = 0., vmax = 3.)
    plt.title('SN '+types[t]+' log interim prior distribution')
    plt.xlabel(r'$z$')
    plt.ylabel(r'$\mu$')
    plt.axis([z_bins[0], z_bins[-1], mu_bins[0], mu_bins[-1]])
    plt.colorbar()
plt.savefig('plots/full_interim_prior.png')","f, ((ax1, ax2, ax3, ax4)) = plt.subplots(1,4, sharex='col', sharey='row'
                                         , figsize=(14,6))
cols = ['L_error', 'R_error', 'M_error', 'T_error']
titles = ['Left endpoint', 'Right endpoint', 'Midpoint', 'Trapezoidal']
axes = [ax1, ax2, ax3, ax4]

for i in range(len(cols)):
    ax = axes[i]
    ax.bar(df['n'], df[cols[i]], width=3, alpha=0.33, fc='b')
    ax.set_title(titles[i])
    ax.set_ylabel('Error')
    ax.set_xlabel('$n$ approximations')
    ax.axhline(0, c='black', lw=1, ls='dashed')
    ax.grid(ls='dashed', alpha=0.5)",,Inference->Introducing the log-interim posteriors and interim hyperparameters,Inference->Introducing the log-interim posteriors and interim hyperparameters,Approximate Integration-Checkpoint->Approximations
316004,"scores = np.average(
    comparison_vectors.values,
    axis=1,
    weights=[30, 10, 5, 10, 30, 15])
scored_comparison_vectors = comparison_vectors.assign(score=scores)
scored_comparison_vectors.head(5)","peak = FloatVector(peak[~np.isnan(peak)])
r.assign('peak', peak)","frame = OrbitPlotter3D()

frame.set_attractor(Earth)
frame.plot_trajectory(positions, label=""ISS"")

frame.show()",,Slides->1 + 1 = 1 or Record Deduplication with Python->3/4 - Classification->Threshold-Based Classification,Assignment-01->Use rpy2 to access R from within python->Convert the format from python to R,Propagation Using Cowell'S Formulation->Cowell's formulation->First example
10541,"ufos = pd.read_csv(""data/ufo-sightings/scrubbed.csv"", sep=',', low_memory=False)
flights = pd.read_csv(""data/flights/flights.csv"", sep=',', low_memory=False)
mthPassengerFlights = pd.read_csv(""data/flights/flight_passenger.csv"", sep=',', low_memory=False)
mthCargoFlights = pd.read_csv(""data/flights/flight_cargo.csv"", sep=',', low_memory=False)
#airports = pd.read_csv(""data/airports.csv"", sep=',', low_memory=False)","# specify how many rows to read
ufo = pd.read_csv('http://bit.ly/uforeports', nrows=3)
ufo","g = 10 #m/s^2
n0 = 0.1 #porosity
beta0 = 1 #
k0 = 9.869233e-16 #m^2
eta0 = 1e-3 #Pa s


depth = 1e4 #m
dz_tmp = 3e2

z0 = np.linspace(0, depth, depth//dz_tmp+1)
z = z0
# z = z0**1.2 / depth**(1.2-1) ##variable grid space
dz = z[1] - z[0]
nz = len(z)

nt = 100
dt = 0.1 * (n0 * beta0) * dz**2 / (k0/eta0)
h = dz #intergral step

T0 = (depth)**2 * (n0 * beta0) / (k0/eta0) #characteristic time
print('characteristic t: ', T0)

solver = Solver() ##for implicit updating
coeff = Coeff(n0, beta0, k0, eta0, nz)",,Analysis->Read CSVs,"Data Wrangling->7. When reading from a file, how do I read in only a subset of the rows?
Data School Pandas->Python pandas Q&A video series by [Data School](http://www.dataschool.io/)->10. Your pandas questions answered! ([video](https://www.youtube.com/watch?v=B-r9VuK80dk&list=PL5-da3qGB5ICCsgW1MxlZ0Hq8LL5U3u9y&index=10))",Fluid-Diffusion-Equation->Set parameters
122625,open(path).readline(),Image.open(PATH+'sr.png'),"print('NACA 6409 max thickness: ', naca_6409.max_thickness(n_interpolated_points=None))
print('NACA 6409 max camber: ', naca_6409.max_camber(n_interpolated_points=None))
print('NACA 6409 chord length: ', naca_6409.chord_length)
print('NACA 6409 leading edge: ', naca_6409.leading_edge)
print('NACA 6409 trailing edge: ', naca_6409.trailing_edge)",,"01->Data Analyse in Python->Obtaining data
Chapter 2->usa.gov data from bit.ly
Analysis Usagov Data Bitly->Analysis on USA.GOV data from bit.ly->Read a line from the text file",Super Resolution->Super Resolution - Keras->Build Super Resolution Network->Create Super Res Model ( a.k.a Image Transform Net) Architecture,Tutorial-1-Generate Foils->BladeX->Tutorial 1: Prepare a blade 2D sectional profile - Generate foils->1.1 Demonstration with NACA-4 profiles
174036,"x = 3.7

# your solution here","def dummy_cluster(x):
    if x=='Yes':
        return 1
    else:
        return 0","# For each article in each cluster, map the article to its main category.
cluster_both_categories = {}
for k,v in clusters.iteritems():
    cats = [main_category[article] + ' : ' + sub_category[article] for article in v]
    count = Counter(cats)
    cluster_both_categories[k] = count",,"04->Booleans, Tuples, and Dictionaries->Exercise 1",K Means Clustering Project->Evaluation,Analyzing Wikipedia Featured Articles->Clustering->Let's see them together.
471900,nx.diameter(G_simple),"g = nx.Graph()

g.graph['title']  = 'Test Graph'
g.graph['author'] = 'John'

g.graph",go.Histogram(),,Script365,"Network Analysis With Module Network X-Checkpoint->Network analysis with module NetworkX->Nodes and edges: creation and attributes->Dictionaries of node and edge attributes->CAUTION: Use these dictionaries only to read attributes, not to change them.","Plotly Mini->Plotly->Data, Layout, Figure"
371890,"scatter_plot(data_reduced_tsne,target_np)","scatter_plot(second_hid_reduced_tsne,target_np)","abplot(3843, 3843, 0.10, 0.02, show_power=True)",,Part1->Test data after reducing with TSNE(),Part1->The below code is refereced from http://pytorch.org/docs/0.3.1/notes/cuda.html?highlight=argparse#memory-management,Walkthrough->Calculate the minimum sample size
418238,"# And here's another, a tree method
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(random_state=30) # reset the operator

#Then just like before
classifier.fit(xtrain,ytrain)

y_predict = classifier.predict(xtest)
print(classifier.score(xtest,ytest))","model2 = RandomForestClassifier(n_estimators=100, random_state=0)
model2.fit(Xtrain, ytrain)
ypred2 = model2.predict(Xtest)
accuracy_score(ytest, ypred2)","def opt_step(k_grid, obj_fun, x0, a_ch, k_min, k_max, pm):     
    """"""
    Function that loops over all states (= elements in k_grid) and solves the respective optimization problem (obj_fun, initial guess x0)
    for a given vector of Chebyshev basis coefficients a_ch
    -> uses Scipy's optimize module
    """"""
    ## initialize policy and value functions
    V = np.zeros(m)
    K = np.zeros(m)
    Hy = np.zeros(m)
    Hm = np.zeros(m)
    
    ## loop
    for (ind, k) in enumerate(k_grid):
    ## optimization step
        res = scipy.optimize.minimize(obj_fun, x0, args = (k, a_ch, k_min, k_max, pm), method ='BFGS') 
        V[ind] = - res.fun
        K[ind], Hy[ind], Hm[ind]  = np.exp( res.x )
        if res.success is not True:
            print(res.message)
        
    return V, K, Hy, Hm",,Code Coffee Ml-Checkpoint->Machine Learning->Multi-class Classification,"05 - Modeling-Checkpoint->Part 1 - Sentiment Analysis on Stock Market->Features->Random Forest Classifier
06 - Presentation-Checkpoint->06 - Presentation: My Attempt at Predicting the Stock Market->The Data->Features->Random Forest Classifier",Ps Dp-Solutions->Question 3 (N)->Answer->Solve model with value function iteration
227728,"c = np.arange(8).reshape(2,4)
c","km_8 = KMeans(8).fit(zc_dataC.iloc[:,2:])",weather_df = weather_df.drop(1015),,Num Py->10. Finding elements satisfying conditions,"Hw11 2 Xy1002->Homework11_2 CLUSTERING BUSINESS by xy1002->2. cluster the NUMBER OF ESTABLISHMENTS time series with K-means in **a few** clusters (as discussed there is no real good, sound way to decide what a good number is here. try a few options, keeping in mind a few is more than a couple, but i recommand you stay within the single digit numbers)->Kmeans (I used 5 and 8 clusters)",Week 5 Assignment Mk->Week 5 Assignment->Drop bad row data
459762,"# d(out)/dx
x.grad","def dummy_cluster(x):
    if x=='Yes':
        return 1
    else:
        return 0","def adjust_dist(in_dist, out_dist, report=False):
    '''This function takes in a distribution and adjusts its std and mean to that of the out distribution
    The report mode outputs the std and mean of the adjusted distribution'''
    adjusted_dist = np.float64(np.mean(out_dist) + (in_dist - np.mean(in_dist))*(np.std(out_dist)/np.std(in_dist)))
    
    if report==True:
        mean = np.mean(adjusted_dist)
        std = np.std(adjusted_dist)
        print(""The mean is {}, the std is {}"".format(mean, std))
        return adjusted_dist
    else:
        return adjusted_dist",,Pytorch-Tut-Checkpoint->Example complete process,K Means Clustering Project->Evaluation,A1Q1 - Final->ELE2769 -  Remote Sensing Image Analysis - Assignment 1->1st Problem - Image Pan-Sharpening Correction->Function Definitions
365077,"# Perform watershed
# Use the function watershed from the module skimage.morphology.
# Use the labeled nuclei seeds and the smoothed nuclei image as input.
from skimage.morphology import watershed
ws = watershed(smoothed_image,seeds_labeled)","# (i) Perform watershed
# Use the function watershed from the module skimage.morphology.
# Use the labeled cell seeds and the smoothed membrane image as input.
from skimage.morphology import watershed
ws = watershed(green_smooth,seeds_labeled)","mylist = numpy.unique([re.findall(pattern = ""href=\""/store/apps/details.*?\"""",
                                  string =contents)]) #Getting the link reference
wholeUrl = [str(shortUrl + link[6:-1] + ""&hl=en"")  for link in mylist]
##Doing the same for each link to get more links
contents2 = [urllib.request.urlopen(link).read().decode('utf8') for link in wholeUrl]

newlist2 =[]
for link in contents2:
    newlist2= newlist2 + re.findall(pattern = ""href=\""/store/apps/details.*?\"""",
                                    string =link)   #Getting the link reference

newlist2 = numpy.unique(newlist2)
wholeUrl2 = [str(shortUrl + link[6:-1] + ""&hl=en"")  for link in newlist2 
             if not ""reviewId"" in link]
wholeUrl2 = numpy.unique(wholeUrl2)",,06-Cell Analysis Solution->Computing cell mask->Exercises 4 - Seeded watershed,Tutorial Pipeline Solutions->Image Processing with Python -- <font color='orange'>Tutorial Pipeline Solutions</font>->Importing Modules & Packages->Expansion by Watershed-><font color='orange'> Exercise Solution </font>,L1-Carsa564-Joshu107-Checkpoint->Query Process
205155,"# Build a graph.
a = tf.constant(5.0)
b = tf.constant(6.0)
c = tf.multiply(a, b)

# First way
# Launch the graph in a session.
sess = tf.Session()

# Evaluate the tensor `c`.
print(sess.run(c))

# Using the `close()` method.
sess.close()

# Second way
# Using the context manager.
with tf.Session() as sess:
    
    # Evaluate the tensor `c`.
    print(sess.run(c))
    
# Third way
sess = tf.InteractiveSession()

# Evaluate the tensor `c`.
# If you have a Tensor t, calling t.eval() is equivalent 
# to calling tf.get_default_session().run(t)
print(c.eval())

# Using the `close()` method.
sess.close()","# Build a graph.
a = tf.constant(5.0)
b = tf.constant(6.0)
c = tf.multiply(a, b)

# First way
# Launch the graph in a session.
sess = tf.Session()

# Evaluate the tensor `c`.
print(sess.run(c))

# Using the `close()` method.
sess.close()

# Second way
# Using the context manager.
with tf.Session() as sess:
    
    # Evaluate the tensor `c`.
    print(sess.run(c))
    
# Third way
sess = tf.InteractiveSession()

# Evaluate the tensor `c`.
# If you have a Tensor t, calling t.eval() is equivalent 
# to calling tf.get_default_session().run(t)
print(c.eval())

# Using the `close()` method.
sess.close()",bwamem_2_job = job_data[job_data['file'].isin(normal_tasks.file)],,Code->1. Graph and session,Code->1. Graph and session,General Analysis->repeat of above process for jobs rather than tasks
232363,"# create dataframe with residuals
X = bos.loc[:, ['PRICE','CRIM','RM','PTRATIO']]
X['Residual'] = residuals
X.head()",bo.get_locs().head(),"fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 10))
axlist = [x for ax in axes for x in ax]

for audio, ax in zip(tru, axlist):
    ax.plot(timeline, audio)
    
plt.show()",,Mini Project Linear Regression->Part 4: Comparing Models->Akaike Information Criterion (AIC),Brain Objects->The structure of a brain object,Example->Features retrieval->The data
366386,"x = []
yc = []
yf = []

for node in g.Nodes():
    farness = snap.GetFarnessCentr(g, node.GetId())
    closeness = snap.GetClosenessCentr(g, node.GetId())
    print ""node: %d closeness: %f farness: %f"" % (node.GetId(), closeness, farness)
    x.append(node.GetId())
    yc.append(closeness)
    yf.append(farness)
    
plt.bar(x, yc)
plt.show()
plt.bar(x, yf)
plt.show()","# Betweenness
nodes = TIntFltH()
edges = TIntPrFltH()
GetBetweennessCentr(G1, nodes, edges, 1.0)
normalize = (len(nodes) - 1) * (len(nodes) - 1)
for node in nodes:
    print ""Node: {} Betweenness Centrality: {}"".format(node, nodes[node] / normalize)
    
# Closeness
for node in G1.Nodes():
    print ""Node: {} Closeness Centrality: {}"".format(node.GetId(), GetClosenessCentr(G1, node.GetId()))
    
# PageRank
page_rank = TIntFltH()
GetPageRank(G1, page_rank)
for node in page_rank:
    print ""Node: {} PageRank: {}"".format(node, page_rank[node])","# bf indicates building-floor

X_crossval_bf = {}
y_crossval_bf = {}
X_holdout_bf = {}
y_holdout_bf = {}

for building in buildings:
    for floor in floors:
        
        # Finding index of samples with the building and floor
        
        index_crossval_bf = y_crossval[(y_crossval.BUILDINGID == building) & (y_crossval.FLOOR == floor)].index
        index_holdout_bf = y_holdout[(y_holdout.BUILDINGID == building) & (y_holdout.FLOOR == floor)].index
        
        if len(index_crossval_bf) == 0:
            continue
            
        key = (building,floor)
        
        X_crossval_bf[key] = X_pca_crossval.loc[index_crossval_bf]
        y_crossval_bf[key] = y_crossval.loc[index_crossval_bf,['LATITUDE','LONGITUDE']]
        
        X_holdout_bf[key] = X_pca_holdout.loc[index_holdout_bf]
        y_holdout_bf[key] = y_holdout.loc[index_holdout_bf,['LATITUDE','LONGITUDE']]
        
        print(""Building = {}, Floor = {}"".format(building,floor))
        print(""Crossval shape"", len(index_crossval_bf))
        print(""Holdout shape"", len(index_holdout_bf))
  
X_crossval_bf.keys(), X_holdout_bf.keys(),",,Task 1->Task 1->Closeness and farness centrality:,Social Network Analysis Tutorial With Snap->Tutorial: Social Network Analysis in Python->Basic Example,Uji Indoor Loc-Response-Classification->UJIIndoorLoc Response EDA and Classification - Part 2->5. Per-Building Per-Floor Regression->5.1 Data Transformation
298818,"#health -- red = bad
kw = dict(column='tree_score', k=10, cmap='RdYlGn')
hp = villages.plot(scheme='QUANTILES', **kw)
fig = hp.get_figure()
fig.savefig('health-map.png')","statsd = OrderedDict()
trustd = OrderedDict()
for mobility in ['static','all_mobile']:
    fig=plot_lines_of_throughput(get_mobility_stats(mobility))
    fig.tight_layout(pad=0.1)
    fig.savefig(""img/throughput_sep_lines_{}.pdf"".format(mobility),  transparent=True,facecolor='white')","def rotate_two(arr, d, n):
    """"""This function receives an array, value to rotate the position by (number), and the size of the array, 
    rotates the array""""""
    
    d %= n
    
    i = 0
    
    while(i < d):
        
        temp = arr[0]
        
        for j in range(n - 1):
            
            arr[j] = arr[j + 1]
            
        arr[n - 1] = temp
        
        i += 1
        
    return arr",,Project11->Exploring Trees and Shadows->.. and finally plot..->.. the density of trees per villages..,Data Processing For Trust Com->Plot Throughput Lines,Programs For Array Rotation->Method 2: Rotating the array one element at a time
63969,"train_data = products.iloc[train_data_index[0]]
validation_data = products.iloc[validation_data_index[0]]","pairs_train = data_to_index(pairs_train, input_lang, output_lang)
pairs_valid = data_to_index(pairs_valid, input_lang, output_lang)
pairs_test = data_to_index(pairs_test, input_lang, output_lang)","# There are 5 (5, 12, 3) CV test scores because there are 5 folds
get_test_data(grid).shape",,Hw2->2. Train Valiation datasets splitting,Gcn->Reading the data,Cv Hyperparameter->Hyperparameter Tuning with GridSearchCV->Helper Functions to get Data from grid.cv_results
206773,"df = pd.read_csv(""final_merged_df.csv"")
df = df.drop(""Unnamed: 0"", axis = 1)","df = pd.read_csv('dffinal.csv')
df.drop('Unnamed: 0', axis = 1, inplace = True)
df.drop('Response', axis = 1, inplace = True)
df.drop('Type', axis = 1, inplace = True)","ciphertext = caesar_encipher('This is a test message.', 4)

for key in range(26):
    plaintext = caesar_decipher(ciphertext, key)
    print(plaintext, '<=', key)",,Exploratory Data Analysis->Train/Test Split->user_region & item_region,Project6Cleaning,2-Breaking-Ciphers->A language model->Breaking Caesar ciphers
488872,"with open(""test.png"", ""wb"") as FILE:
    img.save(FILE)","with open(""test.png"", ""wb"") as FILE:
    img.save(FILE)","contour_analyzer.contours += missing_contours

contour_analyzer.compute_contour_bounding_boxes()
contour_analyzer.find_empty_cells(imgThreshInv)

contour_analyzer.find_corner_clusters()
contour_analyzer.compute_cell_hulls()
contour_analyzer.find_fine_table_corners()",,Lesson02->Now our example lesson 02,Lesson02->Now our example lesson 02,Interactive->Insert clusters into main contours & perform second run
354386,"# Your turn.

occur = np.zeros(0)
for columns in X.T:
    count = columns.count_nonzero()
    occur = np.append(occur,count)

occur = occur
freq = np.zeros(occur.max())
for i in range(int(occur.max())):
    freq[i] = (occur <=i).sum()
    
plt.plot(freq)
plt.xlim([0,80])","# Plot the national percentages
def plot_national_percentages(data, years,name):
    ''' Plot the percentages of occurrences in the given range of years '''
    percent_occur = []
    for x in data:
         percent_occur.append(x[0]*100.0/x[1])
    
    plt.figure(figsize=(10, 8))
    plt.plot(years,percent_occur,linewidth=2,label=name)
    plt.xlabel('Years',fontsize=20)
    plt.ylabel('Percentages of Occurrences',fontsize=20)
    plt.legend(loc='upper right')
    plt.show()
    
years = range(1880, 2015)
data  = count_occurrences_in_years(NATIONAL_DATA_DIR,'Peter',years)
plot_national_percentages(data, years,'Peter')","N = 100  # The number of points in any dimension, we'll make this bigger later
x = np.linspace(-5,5,N)  # a linear space from -5 to 5
y = np.linspace(-5,5,N)  # same as x, though doesn't have to be",,Mini Project Naive Bayes,Babynames->Baby Names Dataset,Laplace Equation In Class->Laplace Equation
146484,"image_number = 7
scale_factor = .9
rotation = 5 #In degrees

# Unnormalize the keypoints
keypoints = y_train[image_number] * 48 + 48

# Use openCV to get a rotation matrix
M = cv2.getRotationMatrix2D((48,48),15, .9)
dst = cv2.warpAffine(np.squeeze(X_train[image_number]),M,(96,96))
new_keypoints = np.zeros(30)
for i in range(15):
    coord_idx = 2*i
    old_coord = keypoints[coord_idx:coord_idx+2]
    new_coord = np.matmul(M,np.append(old_coord,1))
    new_keypoints[coord_idx] += new_coord[0]
    new_keypoints[coord_idx+1] += new_coord[1]

# Plot the image and the augmented image
fig = plt.figure(figsize=(12,12))
ax = fig.add_subplot(121)
ax.imshow(np.squeeze(X_train[image_number]), cmap='gray')
ax.scatter(keypoints[0::2], 
        keypoints[1::2], 
        marker='o', 
        c='c', 
        s=20)
ax2 = fig.add_subplot(122)
ax2.imshow(dst, cmap='gray')
ax2.scatter(new_keypoints[0::2], 
        new_keypoints[1::2], 
        marker='o', 
        c='c', 
        s=20)","#Randomly Rotate Slightly
random_degree = np.random.uniform(low=-1, high=1) * 22.5
M = cv2.getRotationMatrix2D((32/2,32/2),random_degree,1)
image1 = cv2.warpAffine(X_train[_ix],M,(32,32))

plt.subplot(1, 2, 1)
plt.imshow(X_train[_ix], aspect='auto')
plt.subplot(1, 2, 2)
plt.imshow(image1, aspect='auto')","import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = (20.0, 10.0)
%matplotlib inline

loans = pd.read_pickle( 'C:\lc data\lc.pickle' )",,Cv Project->Data Augmentation->Rotation and Scaling,Traffic Sign Classifier,When Loans Get Charged Off
155722,"levels = np.arange(np.min(z),np.max(z),0.005)

plt.contour(p,s,z, levels=levels)
plt.xlabel('probability')
plt.ylabel('score')
plt.title('Correlation between event probabilities and experts scores')
plt.colorbar();","levels = np.arange(np.min(z),np.max(z),0.005)

plt.contour(p,s,z, levels=levels)
plt.xlabel('probability')
plt.ylabel('score')
plt.title('Correlation between event probabilities and experts scores')
plt.colorbar();","ipmag.common_mean_watson(norms,rev_antis,plot='yes')",,10->Brier score->Brier score special cases:,10->Brier score->Brier score special cases:,Essentials Ch 12->Jupyter Notebook  problems in the Essentials of Paleomagnetism Textbook by L. Tauxe->Problem 3c
158906,"ax=table.plot.bar()
ax.set(title='Scholarship influence on the attendance',xlabel='Scholarship',ylabel='Show',ylim=[0,100])
ax.set_xticklabels(['No','Yes'],rotation=0)
plt.show()","#Here we will check if the Gender has effect on the appointments 
table=show_data.groupby('Gender')['Show'].mean()*100

#plot
ax=table.plot.bar()
ax.set(title='Gender influence',xlabel='Gender',ylabel='Show',ylim=[0,100])
ax.set_xticklabels(['Female','Male'],rotation=0)
plt.show()","x = [1,2,3,4]
out = []
for num in x:
    out.append(num**2)
out",,Investigate A Dataset->The number of people who has Scholarship and attend their appointments = 8283 and their percentage equals about 9.4%,Investigate A Dataset,Loops & List Comprehension->List Comprehension
239613,"num_bins = 20
plt.hist(df['Steering Angle'].values, bins=num_bins , range=(-1 ,1)) 
plt.title(""Steering angle Data spread"")
plt.show()
# hist, bins = np.histogram(df['Steering Angle'].values, num_bins)
# hist
df.shape","bin_edges = [0,20,30,40,50,60,70,80,90,100]
_=plt.hist(df_swing['pct_dem'], bins = bin_edges)
_=plt.xlabel('votes for obama')
_=plt.ylabel('number of counties')
plt.show()","import gurobi as gb
import networkx as nx
import matplotlib.pyplot as plt
import random

from networkx import bipartite
from IPython.display import SVG",,Behavioral Cloning->Lets define our generator->Data exploration,2008Us Aelectionproject->swing_states lab,Assignment1-Checkpoint->Network Optimization
265219,distFile,"distFile = spark.read.csv(""/Users/rajanikant/Documents/tmp/traffic/TRAFFIC.log"",header=True, mode=""DROPMALFORMED"")",stationJSON['STATION'][0]['OBSERVATIONS'],,01-Spark-Essentials->3. Creating an RDD->3.2. Creating RDDs from a file,Spark Introduction->Spark Introduction,"Ex0 Hello World->Ex. 0 API Hello World!->Information about the observations found at a particular station is always in the `OBSERVATIONS` key. In this case, we can access that information like:"
223365,"def process_image(image):
    # NOTE: The output you return should be a color image (3 channel) for processing video below
    # TODO: put your pipeline here,
    # you should return the final output (image with lines are drawn on lanes)
    result = procimagefinal(image)
    return result","def classify_one_argument(row):
    return classify(row, train_20, genres, 5)

# When you're done, this should produce 'Hip-hop' or 'Country'.
classify_one_argument(test_20.row(0))","# d_g = discriminador(generador(z))
discriminator.trainable = False

z = Input(shape=(latent_dim,))
img = generator(z)
decision = discriminator(img)
d_g = Model(inputs=z, outputs=decision)

d_g.compile(Adam(lr=0.0004, beta_1=0.5), loss='binary_crossentropy',
            metrics=['binary_accuracy'])",,P4->Test on Videos,Music Ml->2. K-Nearest Neighbors - a Guided Example->3.2. A classifier function,02 Dcgan Cifar10->3. Compile model->Combined network
374883,"import matplotlib.pyplot as pl
pl.style.use('default')","%matplotlib inline

import numpy as np
import matplotlib.pyplot as pl
pl.style.use('ggplot')
from scipy.optimize import minimize
from revrand.optimize import sgd, AdaDelta, Adam
from revrand import basis_functions as bf
from revrand.metrics import smse","model_params = {
    ""Langmuir"": {""M"": 10.0, ""K"": 10.0},
    ""Quadratic"": {""M"": 10.0, ""Ka"": 10.0, ""Kb"": 10.0 ** 2 * 3},
    ""BET"": {""M"": 10.0, ""Ka"": 10.0, ""Kb"": .2},
    ""DSLangmuir"": {""M1"": 10.0, ""K1"": 1.0,
                   ""M2"": 30.0, ""K2"": 30.0}, # warning: 1/2 is arbitrary
    ""Henry"": {""KH"": 10.0},
    ""TemkinApprox"": {""M"": 10.0, ""K"": 10.0, ""theta"": -0.1}
}",,Intro To Matplotlib->Plotting in python with `matplotlib`,Sg Demo->Simple stochastic gradients optimisation demo,Isotherm Tests->Test isotherm fitting
236588,"class StaticRNN(tf.keras.Model):
    def __init__(self, h, cell):
        super(StaticRNN, self).__init__()
        if cell == 'lstm':
            self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=h)
        elif cell == 'gru':
            self.cell = tf.nn.rnn_cell.GRUCell(num_units=h)
        else:
            self.cell = tf.nn.rnn_cell.BasicRNNCell(num_units=h)
        
        
    def call(self, word_vectors, num_words):
        word_vectors_time = tf.unstack(word_vectors, axis=1)
        outputs, final_state = tf.nn.static_rnn(cell=self.cell, inputs=word_vectors_time, sequence_length=num_words, dtype=tf.float32)
        return outputs","def RNN(x, weights, biases):

    # Prepare data shape to match `rnn` function requirements
    # Current data input shape: (batch_size, timesteps, n_input)
    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)

    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)
    x = tf.unstack(x, timesteps, 1)

    # Define a lstm cell with tensorflow
    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)

    # Get lstm cell output
    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)

    # Linear activation, using rnn inner loop last output
    return tf.matmul(outputs[-1], weights['out']) + biases['out']","fig, axs = plt.subplots(1, 2, figsize=(12,5))

# v represents ""bush - powell""
v = model.decision_function(np.vstack([bush, powell]))
v = v[:,ibush] - v[:,ipowell]

# random variable to plot on y
y = np.random.normal(size=len(v))

axs[0].scatter(v, y, s=10, color=['b']*len(bush) + ['r']*len(powell));
axs[0].set_title('decision function');

# v represents ""bush-ness""
v = model.predict_log_proba(np.vstack([bush, powell]))
v = v[:,ibush] - v[:,ipowell]

axs[1].scatter(v, y, s=10, color=['b']*len(bush) + ['r']*len(powell));
axs[1].set_title('log probability');

for ax in axs:
    ax.set_yticks([])
    ax.set_xlabel('Bush - Powell')
    ax.axvline(x=0)",,Rnn->P1: Embedding Model->Zeroing out outputs past real sentence length!,"Keras Mnist->Handwritten Digit Recognition->Load data->Installing additional modules
02 Sequential Mnist->Simple RNN Modles for Sequential MNIST with Tensorflow->Row-by-row Sequential MNIST->Basic LSTM Model","Powell Goggles->Powell Goggles->Now that we have a working facial recognition program, we can try to fool it"
261899,"IVtw.PutIVtable.drop_duplicates().plot(figsize=(13,6),grid=True,
                                      color=[i.hex for i in list(Color(rgb=(0.45,0.55,0.75)).range_to(Color(rgb=(0.75,0,0)), len(IVtw.PutIVtable.columns.tolist())))])","cols = ['SSN', 'LAST', 'FIRST']

# Use temporary dataframe where none of the above columns are NaN
df = orig_df.dropna(subset=cols)

duplic = df.duplicated(keep=False, subset=cols)

addl_dupe_eids = {repeat_value: set(frame.index) for repeat_value, frame
                    in df[duplic].groupby(cols)}

addl_dupe_eids = [{eid: {other_eid for other_eid in idxs if other_eid != eid}}
                    for repeat_value, idxs in addl_dupe_eids.items()
                    for eid in idxs]

addl_dupe_eids = {eid: other_eids for d in addl_dupe_eids
                    for eid, other_eids in d.items() if other_eids}

# Instead of using something like this, which will overwrite existing key: value pairs in `master_dupe_eids`,
# `master_dupe_eids = {**master_dupe_eids, **addl_dupe_eids}`,
# use this function, which updates the global `master_dupe_eids` dictionary
def update_master_dupe_eids(new_dupe_eids):
    '''Updates global `master_dupe_eids` dictionary so as not to overwrite previous predictions,
    which is what would happen with dict.update or {**d, **other_d}-type methods
    
    Parameters:
        - new_dupe_eids: dict, similar to `master_dupe_eids`'''
    for eid in new_dupe_eids:
        if eid not in master_dupe_eids:
            master_dupe_eids[eid] = new_dupe_eids[eid]
        else:
            for new_eid in new_dupe_eids[eid]:
                master_dupe_eids[eid].add(new_eid)

update_master_dupe_eids(addl_dupe_eids)","res2 = f.fit()
print(res2.format())",,Example->Practice for applying the method created in this post->Plot IV as timeseries,Pma Exploratory->Patient-matching algorithm data challenge->Appendix->Phone numbers,"Simulating A 2D Image And A Bit Of Error Analysis->Region Projection (this time with two variables)
Simulating And Fitting A 2D Image (This Time With A Bayesian Approach)->Where's the Bayes?->Fitting the data"
408049,"#Loading vehicle and non-vehicle images

cars = glob.glob('resources/vehicles/**/*.png', recursive=True)
notcars = glob.glob('resources/non-vehicles/**/*.png', recursive=True)

images_of_car = [mpimg.imread(impath) for impath in cars]
images_of_noncar = [mpimg.imread(impath) for impath in notcars]

print(""{0} images of car found"".format(len(images_of_car)))
print(""{0} images are non-car images"".format(len(images_of_noncar)))","car_images = glob.glob('./vehicles/**/*.png')
noncar_images = glob.glob('./non-vehicles/**/*.png')

print('The number of car image is {},and number of non-car images is {}'.format(len(car_images),len(noncar_images)))","# read raw volumes
df = pd.read_csv(files[1])

# cleanup col names
for c in df.columns:
    if df[c].dtypes == 'object':
        df[c] = df[c].map(lambda x: re.sub(""[^0-9]"", """", x))
        
# re-org by id        
vols = df.pivot(index='id', columns='log_feature', values='volume').fillna(0)
cnames = ['log_feature_'+str(x) for x in vols.columns]

# scale or min-max
vols_scl = vols.apply(lambda x: preprocessing.MinMaxScaler().fit_transform(x))
vols_scl.columns = cnames",,Vehicle Detection->Loading Vehicle and Non-Vehicle Images,Project5->Udacity Self-Driving Car Engineer Nanodegree Program->Apply this pipeline on the Advanced Lane Line detection output video.,Exploration->Attribute Summaries->Sequencing
367961,"plt.hist(scores)
plt.xlabel('Score')
plt.xlim([0, 7])
plt.title('Histogram of rolling %d dices %d times' %(N_dices, N_trials))
plt.show()","plt.hist(scores)
plt.xlabel('Score')
plt.xlim([0, 7])
plt.title('Histogram of rolling %d dices %d times' %(N_dices, N_trials))
plt.show()","def change_col_names(data, colnames):
    '''
    Parameters
    ----------------------------------------------------------
        data: data we want to change column names
        colnames: column names we want to be replace 
        
    Output
    ----------------------------------------------------------
        data with updated column names for easier calling
    '''
    data.rename_column('col1', colnames[0])
    data.rename_column('col2', colnames[1])
    data.rename_column('col3', colnames[2])
    data.rename_column('col4', colnames[3])
    #data.write(new_filename, format='ascii')
    return data",,Central Limit Theorem->The Central Limit Theorem->Here comes the CLT...,Central Limit Theorem->The Central Limit Theorem->Here comes the CLT...,"Desc Hack -- Hubble Diagram, Sn, Pd Fs->Loading SN rates"
215199,"X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'], test_size=0.30, random_state =101)","X_train, X_test, y_train, y_test = train_test_split(scale_X, y, test_size=0.3, random_state=42)","te = lp_train.map(lambda lp: lp.label).zip(model.predict(lp_train.map(lambda lp: lp.features)))
train_err = te.filter(lambda (v, p): v != p).count() / float(te.count())
1-train_err",,K Nearest Neighbors Project->Train Test Split,News Popularity- Classification Model Evaluation->2. Read data->4. Data split->4.3 Split to train and test dataset,Titanic->Calculate Model Training Data Prediction Accuracy
331443,"test_3= test[test['Junction']==3]
test_others= test[test['Junction']!=3]","#drop the columns we don't need
cols_recoded = [
 'AgeuponOutcome',
 'AnimalType',
 'Breed',
 'Color',
 'DateTime',
 'Name',
 'SexuponOutcome',
 'Agecategory',
 'Breedtype',
 'Colortype',
 'unknown',
 'Ampm']


train_data = train_data.drop(cols_recoded, axis=1)
test_data = test_data.drop(cols_recoded, axis=1)

other_cols = [
  'ID',
 'OutcomeSubtype',
 'OutcomeType']
train_data = train_data.drop(other_cols, axis=1)
test_data = test_data.drop(other_cols, axis=1)",fin_dat.groupby(['Domain']).size()[fin_dat.groupby(['Domain']).size() > 100].sum(),,Model->Visulatisation->Inference from the Data Visualisation,Subha Explorations->That's it for feature engineering. Now split the train and test data back.,Pwned->Analysis of HIBP/YouGov Data
280211,enums[1],enum.max(),"dtype = {
    'Id': int,
    'RiderId': int,
    'Date': str,
    'Time': str,
    'Average_Gradient': float,
    'Distance': float,
    'Highest_point': float,
    'Lowest_point': float,
    'Meadured_time': int,
    'Moving_time': int,
    'Average_heart_rate': float,
    'Max_heart_rate': float,    
}
data_train = pd.read_csv(path_data_train, sep="";"", dtype=dtype)
data_train.head()",,6->6. Reactor->6.2. MetaReactions (reactions on CGRs).,Enum Field->Enum field->Field value,Garmin->Reset index->Selecting a row
387783,km = KMeans(),"tots = mta.transpose(2,0,1).reshape(mta.shape[2], mta.shape[1]*mta.shape[0]).T
tots = tots[tots.std(1)>0]

km = KMeans(n_clusters=10)

# standardize the lightcurves before clustering 

vals = ((tots.T - tots.mean(1))/tots.std(1)).T

mycluster = km.fit(vals)","import time

# Init x to store random numbers
x = []

# Start time to measure
str_time = time.clock()
#str_time = time.time()

for i in range(10000):
    x.append(random.choice([1,2,3,4,5,6,7,8,9]))

# Ending point to measure the time    
end_time = time.clock()
#end_time = time.time()

# Different time 
print(end_time - str_time)",,Mini Project Clustering->K-Means Clustering->How do the clusters look?,Subway Timeseries Instructions->HW6 TIME SERIES ANALYSIS->Extra Credit,P4 Randomness And Time->Week 2->4.4 Measuring Time
173342,"FROM = ""kenneth.lay@enron.com""
# Get the recipient lists for each message
recipients_per_msg = list(db.mbox.aggregate([
    {""$match"": {""From"": re.compile(r"".*{0}.*"".format(FROM), re.IGNORECASE)}},
    {""$project"": {""From"": 1, ""To"": 1}},
    {""$group"": {""_id"": ""$From"", ""recipients"": {""$addToSet"": ""$To""}}}
]))
recipients_per_message = recipients_per_msg[0]['recipients']
all_recipients = [recipient for message in recipients_per_message for recipient in message]
print ""Num total recipients on all messages:"", len(all_recipients)
print 'First 25 recipients'
all_recipients[:25]","# In a Mongo shell, you could try this query for the same effect:
# db.mbox.find({""To"" : {""$regex"" : /.*enron.com.*/i} }, 
#              {""To"" : 1, ""_id"" : 0})

senders = [ i 
            for i in mbox.distinct(""From"") 
                if i.lower().find(""@enron.com"") > -1 ]

receivers = [ i 
              for i in mbox.distinct(""To"") 
                  if i.lower().find(""@enron.com"") > -1 ]

cc_receivers = [ i 
                 for i in mbox.distinct(""Cc"") 
                     if i.lower().find(""@enron.com"") > -1 ]

bcc_receivers = [ i 
                  for i in mbox.distinct(""Bcc"") 
                      if i.lower().find(""@enron.com"") > -1 ]

print ""Num Senders:"", len(senders)
print ""Num Receivers:"", len(receivers)
print ""Num CC Receivers:"", len(cc_receivers)
print ""Num BCC Receivers:"", len(bcc_receivers)","print train.iloc[0:10,:]",,Enron->Enron email database,Chapter 6 - Mining Mailboxes,Json-Processing->Reading json
395026,"from sklearn.cross_validation import train_test_split
X = df[[""density""]]
Y = df[[""alcohol""]]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20 , random_state = 0)","from sklearn.model_selection import train_test_split

print('train/test split: {}/{}={:.2f}'.format(len(X_train), len(X_valid), len(X_valid)/len(X_train)))
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=0)
print('train/test split: {}/{}={:.2f}'.format(len(X_train), len(X_valid), len(X_valid)/len(X_train)))","#
# A = scsearch(keyword='silures',verbose=True,nonspinning=True)
# A = scsearch(keyword='q1.2_base',verbose=True)
A = scsearch(q=[9.9,11],verbose=True,nonspinning=True)",,Assignment 2->Accuracy and Cross Validation,Traffic Sign Classifier-Solution->Balance and augment dataset,Dev Bam Extraction Radius->Dev for Handling BAM Extaction Radii
458846,newscratch.close(),"# For the purpose of pretty printing

bitcoin[['Close','30_day_mean', '50_day_mean', '100_day_mean', '30_day_volatility']].plot(figsize=(10,8));
plt.title('Bitcoin Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

bitcoin_cash[['Close','30_day_mean', '50_day_mean', '100_day_mean', '30_day_volatility']].plot(figsize=(10,8));
plt.title('Bitcoin Cash Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

dash[['Close','30_day_mean', '50_day_mean', '100_day_mean', '30_day_volatility']].plot(figsize=(10,8));
plt.title('Dash Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

ethereum_classic[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Ethereum Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

bitconnect[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Bitconnect Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

litecoin[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Litecoin Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

monero[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Monero Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

nem[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('NEM Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

neo[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('NEO Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

numeraire[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Numeraire Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

omisego[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Omisego Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

qtum[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Qtum Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

ripple[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Ripple Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

stratis[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Stratis Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()

waves[['Close','30_day_mean', '50_day_mean', '100_day_mean','30_day_volatility']].plot(figsize=(10,8));
plt.title('Waves Closing Price with 30 Day Mean & Volatility')
plt.ylabel('Price')
plt.show()",dfTrain = pd.get_dummies(dfTrain[list_features]),,03 Raster->Categories->Write a raster map->Query a raster using a Point object,Time Series Analysis - Cryptocurrencies->Time Series Analysis: Cryptocurrencies,Mcmc Classification Titanic Ann One Hidden Layer->Predicting passenger survival with MCMC->I. Preparatory steps: Reading and cleaning->II.B. Predictions for Kaggle
482369,print(my_dict),"# %load solutions/single_dict.py
my_dict = {'blade_runner':'title'}
print(my_dict)","import numpy as np
from keras.models import Model, Input
from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Activation, Average
from keras.utils import to_categorical
from keras.losses import categorical_crossentropy
from keras.callbacks import ModelCheckpoint, TensorBoard
from keras.optimizers import Adam
from keras.datasets import cifar10",,"Dictionaries->Dictionaries->Converting Other Sequence Types to Dictionary
Series->Pandas->Series->Series into Dictionary
6 Programming Basics->Programming Basics->Built-In Data Types->Strings
List Tuple Comparison->List and tuple comparison->List
Dictionary And Operators->Dictionary
Dictionary And Operators->Dictionary->3. Dictionary pop()
Unit03 05->Comprehension dicts",01->Data Types,Ml Project->Explore Different ConvNets Models->Import the data
80618,"# create a multiplication table
for row in range(1,11):
    for col in range(1, 11):
        print('{:4}'.format(row * col), end='')
    print()","row, col = df.shape
print('Number of rows: {}'.format(row))
print('Number of cols: {}'.format(col))","import scda
import pprint
import logging
import os
import numpy as np
scda.configure_log()
import pprint

import matplotlib
import matplotlib.pyplot as plt
%pylab inline
matplotlib.rcParams['image.origin'] = 'lower'
matplotlib.rcParams['image.interpolation'] = 'nearest'
matplotlib.rcParams['image.cmap'] = 'gray'
matplotlib.rcParams['axes.linewidth'] = 1.
matplotlib.rcParams['lines.linewidth'] = 2.5
matplotlib.rcParams['font.size'] = 15",,4->Control Flow->Case Study: Calculating $\pi$->Nested Loops,"Kpj Mta Combined->Read the text file into a DataFrame->Convert columns (datetime, etc.)",Wfirst Splc Demo->WFIRST SPLC - mini design survey varying the support strut thickness
136752,"import csv

mat = np.zeros((943, 1682))
with open('ratings.txt') as f:
    for row in csv.reader(f):
        if len(row) == 3:
            mat[int(row[0])-1, int(row[1])-1] = float(row[2])
recommend = MapMatComp()
recommend.fit(mat)","import numpy as np
import csv

with open(""/home/netbug/Helene - Sheet1.csv"", 'r') as f:
    mtrx = list(csv.reader(f, delimiter="",""))
    print(mtrx)
    row = mtrx[2]
    item = int(row[1]) + float(row[2]) + float(row[3])
    print(row[0], item)
    
    iterrows = iter(mtrx)
    next(iterrows)
    for row in iterrows:
        item = float(row[1]) + float(row[2]) + float(row[3])
        print(row[0], '%.1f' % round(item, 1))","def sum_diff(m,n):
    
    m, n = m+n, m-n
    
    return m,n

print(sum_diff(3,5))",,Hw4->Problem 2 (Matrix factorization)->Load Data & Training,Lesson2 (1)->Numpy,Python Intro-Answer Key->Functions
5855,"import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt # package to make plot
%matplotlib inline
from brownian_function import brownian
import random","import numpy as np
from scipy.integrate import odeint
import matplotlib.pyplot as plt # package to make plot
%matplotlib inline
from brownian_function import brownian
import random",df[df['#_killed'] > 5],,Tutorial->Tutorial about stochastic gene expression using Gillespie algorithm->Structure:,Tutorial->Tutorial about stochastic gene expression using Gillespie algorithm->Structure:,Breakout Questions-Checkpoint->Pandas Breakout Questions->7. How many casualties occured in each Month?
19837,"sns.countplot(titanic_df['Sex'])
plt.title(""Number of Males vs Females"")","plt.figure(figsize=(12,6))
sn.countplot(x='season', hue='toss_decision', data=match_df)
plt.xticks(rotation='vertical')
plt.show()","img_rows, img_cols = 28, 28
X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)
X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)
    
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0",,Titanic->Feature Exploration->Distribution of Sex,Ipl Data Analysis->Win Percentage for each team (wins/total),09->Training a Deep Neural Net to Classify Handwritten Digits Using Keras->Preprocessing the MNIST dataset
212118,"#Summary statistic
df.temperature.describe()","df['STDANYYR1'].replace(2,0,inplace=True)
df['HEPBCEVER1'].replace(2,0,inplace=True)
df['HIVAIDSEV1'].replace(2,0,inplace=True)
df['CANCEREVR1'].replace(2,0,inplace=True)
df['INHOSPYR'].replace(2,0,inplace=True)
df['AMDELT'].replace(2,0,inplace=True)
df['AMDEYR'].replace(2,0,inplace=True)
df['ADDPR2WK1'].replace(2,0,inplace=True)
df['DSTWORST1'].replace(2,0,inplace=True)
df['IMPGOUTM1'].replace(2,0,inplace=True)

df['IMPSOCM1'].replace(2,0,inplace=True)
df['IMPRESPM1'].replace(2,0,inplace=True)
df['SUICTHNK1'].replace(2,0,inplace=True)
df['SUICPLAN1'].replace(2,0,inplace=True)
df['SUICTRY1'].replace(2,0,inplace=True)
df['PNRNMLIF'].replace(2,0,inplace=True)
df['PNRNM30D'].replace(2,0,inplace=True)
df['PNRWYGAMT'].replace(2,0,inplace=True)
df['PNRRSHIGH'].replace(2,0,inplace=True)

df['TXYRRESOV1'].replace(2,0,inplace=True)
df['TXYROUTPT1'].replace(2,0,inplace=True)
df['TXYRMHCOP1'].replace(2,0,inplace=True)
df['TXYREMRGN1'].replace(2,0,inplace=True)
df['TXCURRENT1'].replace(2,0,inplace=True)
df['TXLTYPNRL1'].replace(2,0,inplace=True)
df['AUOPTYR1'].replace(2,0,inplace=True)
df['AUINPYR1'].replace(2,0,inplace=True)
df['AUALTYR1'].replace(2,0,inplace=True)


df.head()",station_16 = station_16[station_16.Terminal != 'NRHQ '],,Sliderule Dsi Inferential Statistics Exercise 1->1. Is the distribution of body temperatures normal?,Bda-Project-Data->BDA_Fall17: Data for Final Project->3.2 Recode values for selected features:,Nr Station Cleaning-Checkpoint->Station 16
460290,"# close tensorflow session (Note, the graph is still open)
sess.close()",sess.close()  # closes the session,G.edges(data= True) #list of all edges with attributes,,"Notebook Cnn Tf->calculate loss with L2-regularization and build an optimizer
Notebook Mlp Epistasis->Train the MLP->train MLP->let's close the session, so we can walk through an example of how to load the parameters
Notebook Cnn Rn Acompete->Sequence specificities of RNA-binding proteins with convolutional neural networks->Intro to Convolutional Neural Networks->let's close the session, so we can walk through an example of how to load the parameters",Lecture 1:Graphsandsessions->**TENSORFLOW**,Note Week 1->Example - Weighted Graph
444025,"#examine mjtheme_namecode and find that we have nested fields 
df.mjtheme_namecode.head()","#Show first 10 entries of the 'mjtheme_namecode' column
json_df.mjtheme_namecode.head(10)","plt.figure()

ax = plt.axes(projection=ccrs.PlateCarree())
kwargs = dict(ax=ax, transform=pc, x='longitude', y='latitude', cbar_kwargs=dict(orientation='horizontal'))
xds['mean_surface_temperature'][0].plot.contourf(**kwargs)
ax.coastlines()
ax.set_global()

plt.show()",,Json Exercise->Question 2:,Sliderule Dsi Json Exercise->Data Wrangling with JSON->Part 1 - Import and Inspect Data,Netcdf And Xarray->Cartopy tutorial: Appendix for raster to raster->Using xarray and cartopy
403927,"yy2=0
for i in range(1,len(W2)):
    yy2=yy2+ W2[i]*(xx**i)","# compute corresponding feature values for our mesh grid (test data)
xx2 = xx ** 2
yy2 = yy ** 2
xx_yy = xx * yy
xx2_yy = (xx ** 2) * yy
xx_yy2 = xx * (yy ** 2)","PAYEMS['PAYEMS_DIFF'] = PAYEMS.PAYEMS.diff()
PAYEMS['PAYEMS_PCT_DIFF'] = PAYEMS.PAYEMS_DIFF / PAYEMS.PAYEMS * 100
PAYEMS['YR'] = PAYEMS.index.year
PAYEMS['MONTH'] = PAYEMS.index.month

sns.lmplot(x=""YR"", y=""PAYEMS"", hue=""MONTH"", data=PAYEMS[PAYEMS.index.year>=2012], size=14, ci=95)
plt.show()",,Polynomial Regression->Error->Warning,Talwar Harkar Ps4->INFX574 Problem Set 4->2.3 Feature Engineering->Step 1. Use these two features to compute some new ones.,Us-Employment->PAYEMS (Jobs Report) February 2017->May Have Been Phony in the Past?
298908,"#navigate up one folder with ""../"" prefix
#we need to deserialize, extract the data structure from byte code, using the pickle module
enron_data = pickle.load(open(""../final_project/final_project_dataset.pkl"", ""r""))","import pickle

enron_data = pickle.load(open(""../ud120-projects/final_project/final_project_dataset.pkl"", ""r""))

for i in enron_data:
    if enron_data[i]['bonus'] == 97343619.0:
        print ('The biggest Enron outlier is ""%s"".'% i)","plt.figure(figsize=(10,8))
sns.barplot(x=""department"", y=""orders_ratio"", hue=""cluster"", data=common_departments)
plt.show();",,Poi Id->Task 0:  Explore the data,Lesson 07 Outliers->Quiz: Identify the Biggest Enron Outlier,Eda->Data visualization->Number of products per order
185880,"# Load pickled data
import pickle

#paths
training_file = ""data/train.p""
validation_file= ""data/valid.p""
testing_file = ""data/test.p""

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(validation_file, mode='rb') as f:
    valid = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)

#Set Train, Validation and Test Data
X_train, y_train = train['features'], train['labels']
X_valid, y_valid = valid['features'], valid['labels']
X_test, y_test = test['features'], test['labels']","# Load pickled data
import pickle

# TODO: Fill this in based on where you saved the training and testing data
training_file = r'traffic-signs-data\train.p'
validation_file= r'traffic-signs-data\valid.p'
testing_file = r'traffic-signs-data\test.p'

with open(training_file, mode='rb') as f:
    train = pickle.load(f)
with open(validation_file, mode='rb') as f:
    valid = pickle.load(f)
with open(testing_file, mode='rb') as f:
    test = pickle.load(f)
    
X_train, y_train = train['features'], train['labels']
X_valid, y_valid = valid['features'], valid['labels']
X_test, y_test = test['features'], test['labels']","%%time
CHARS_SIZE = len(charindex)
SEQUENCE_LENGTH = 100
X_train = []
Y_train = []
for i in range(0, len(Text_Data)-SEQUENCE_LENGTH, 1 ): 
    X = Text_Data[i:i + SEQUENCE_LENGTH]
    Y = Text_Data[i + SEQUENCE_LENGTH]
    X_train.append([charindex.index(x) for x in X])
    Y_train.append(charindex.index(Y))

X_train = np.reshape(X_train, (len(X_train), SEQUENCE_LENGTH))

Y_train = np_utils.to_categorical(Y_train)",,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree,Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree,Ai Generated Star Wars->Create Sequences
6780,"from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))","predstfidf=classif.predict(X_test_tfidf)
print X_test_tfidf.shape, Y_test.shape, predstfidf.shape, len(genre_names)
print genre_names
print classification_report(Y_test, predstfidf)","def compute_region(row):
    if row['state'] == 'ON' or row['state'] == 'QC':
        return 'canada'
    if row['latitude'] > 43 and row['longitude'] < -108:
        return 'pnw'
    if row['latitude'] < 43 and row['longitude'] < -108:
        return 'west'
    if row['latitude'] < 38 and row['longitude'] > -108:
        return 'south'
    if row['latitude'] > 38 and row['longitude'] > -108 and row['longitude'] < -84:
        return 'midwest'
    if row['latitude'] > 38 and row['longitude'] > -84:
        return 'northeast'",,"Tutorial->Introduction->Build Machine Learning Models->Use classification_report to see how well it classifies each class
Datacafe Hackathon->Predicting mobile application user churn->4. Evaluating Model->4.2. Compute precision, recall, F-measure and support
Website User Conversion Rate->Predictive modeling
Abhishek Air Bnb->Airbnb Data Analysis by Abhishek Kopparapu",Deep Learning Project-Pytorch->Building a dataset out of the scraped information!,Data Preparation->Constructing dialect labels
361836,"print df.groupby(""prestige"").size()
print df.groupby([""prestige"", ""admit""]).size()

#there should be 8 groups (i.e. 8 keys)
#same thing as groupby... print pd.crosstab(df.prestige, df.admit)","# let's get a count of how many PNEUMONIA vs NO-PNEUMONIA admits we have
pneumonia_admit_count_df = pat_admit_pneumonia_df.groupby('Encounter_Pneumonia_Diagnosis').size()
pneumonia_admit_count_df","print(pca.components_)
CorPC = pd.DataFrame(
    [[np.corrcoef(X[:, j], PC[:, k])[0, 1] for j in range(X.shape[1])]
        for k in range(K)],
            columns = df.columns[:4],
    index = [""PC %i""%k for k in range(K)]
)

print(CorPC)",,Project1-Starter->3b. How will you test for outliers?,03A Mimicii Chest X Ray Reports Short->Exploring the MIMIC Chest X-ray reports->now let's get a dataframe of Patient/Admit/Pneumonia,Decomposition Solutions->Dimension reduction and feature extraction->Apply PCA on iris dataset
313527,import entropy,import scipy.interpolate,"#Confusion matrix, Accuracy, sensitivity and specificity
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(preds.observed,preds.predicted)
sensitivity1 = (float(cm[2,2])/(cm[2,2]+cm[0,2]+cm[1,2]))
print('Sensitivity : ', sensitivity1 )

specificity1 = (float(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1]+cm[0,2]+cm[1,2]))
print('Specificity : ', specificity1)",,"Titanic Survival Exploration->Look at Entropy
Saami
Modern Greek","Pypick->PyPick: seismic event picking using Matplotlib and machine learning->Application: Tomo2D
Sky Camera->Sky Camera
Pnc 03Dc3Eval->Interpolation not KDTree
Orbits->Initialize the data->Forward projection
Orbits->Initialize the data->Forward projection",Nba-Tweets->NBA Tweets: Natural Language Processing and Network Analysis->Part 3: Natural Language Processing Analysis
394944,"df = pd.read_csv(""train_set.csv"")","df = pd.read_csv('College_Data', index_col =0)","# show observation parameters
print(""detector: %s"" % dm.meta.instrument.detector)
print(""channel: %s"" % dm.meta.instrument.channel)
print(""band: %s"" % dm.meta.instrument.band)",,Assignment 2->Assignment 2 Key->First let's load the dataset into dataframe df,K Means Clustering Project->Get the Data,"Mrs Tutorial->Run level 2A, 2B and 3 steps on MRS data using jwst step classes->MRS_IMATCH step ======================================->MRS_IMATCH: run step"
198274,"from PIL import Image
import numpy as np
# import matplotlib.pyplot as plt
from pylab import *

im = np.array(Image.open('img/batman.jpg'))
print(type(im), im.shape)

imshow(im)

# plot the points
x = [100,100,400,400]
y = [100,200,100,200]

plot(x,y,'r*')
print('x[:2] : ', x[:2])
plot(x[:2], y[:2])

title('Plotting: batman.jpg')
axis('off')
show()","%matplotlib inline
'''
The main reason for additional (minor) components: a diagonal can only be approximated by the square pixels 
of the image, hence, additional frequencies are needed to compose the image.

'''


import matplotlib.pyplot as plt
import matplotlib.image as mpimg
im = mpimg.imread('sonnet.gif')
im = im[:,:,0]
print type(im), im.shape, im.dtype


plt.figure(figsize = (8, 5))
plt.imshow(im, cmap = plt.cm.Greys_r )

ft2d, kx, ky = FT2D(im)
plt.figure()
#To see better, use sqrt scale
plt.imshow(np.log(np.abs(ft2d)), cmap = plt.cm.gray)
plt.colorbar()

plt.figure()
plt.plot(kx)

plt.show()","plot_digits(gmm_means, 'Cluster Center Images')

confusion_matrix_stats(gmm, X_test_flat, y_test, ['N/A']*10)

gmm_covars = gmm.covariances_
samples = gen_samples(gmm_means, gmm_covars)

plot_digits(samples, 'Generated Sample Images')",,Matplotlib->Matplotlib->Images,"Comp-Phys-Wk12-Day2->Simple application: Remember we said for character recognition, it's not ideal to have the rotated version -- often for a text, we can figure out whether it's rotated or not.
Week-12Pt2 Fourier Transform-2D-Images->Simple application: Remember we said for character recognition, it's not ideal to have the rotated version -- often for a text, we can figure out whether it's rotated or not.",Submit3-Checkpoint->GMM
347804,"np.arange(40,50,.25)","#(Xtrain==np.inf).sum()
Xtrain=Xtrain.replace([np.inf, -np.inf], np.nan).dropna()",df['CASE_STATUS'].unique(),,"5->Arrays
5->Manipulating Data with Numpy, Scipy, and Pandas->Arrays
Data Transformation And Visualization In Python->Seaborn->Pivot Table // Average flight time from origin to destination","Fred Data->Classifying USD/JPY price direction
Fred Data->Classifying USD/JPY price direction",Data Exploration->Eduardo's take on the H1-B Visa
79339,"plots(x_imgs[:8], titles=y_valid[:8])   # show the first 8 samples and labels","def img_normalize(data):
    return (data / 122.5) - 1

### Shuffle the data

X_train, y_train = shuffle(X_train, y_train)

### Normalize data

### This suggested approach was bad as the performance of the model didn't exceed more than 0.753
#X_train = (X_train - 128) / 128
#X_valid = (X_valid - 128) / 128
#X_test  = (X_test - 128) / 128

### This formula for normalizing the images, combined with two Drop-Outs, boosted the model up to 0.961 :-)
### Formula from Discussions SDCND Term 1 no. 236031
X_train = img_normalize(X_train)
X_valid = img_normalize(X_valid)
X_test  = img_normalize(X_test)","# Close doesn't count in RSA encryption. You need the exact private key.
rsa_decrypt(ciphertext,rsa_number,private_key-1)",,Ml4-Mnist Sgd->Defining Logistic Regression Ourselves,Traffic Sign Classifier->Save 32x32 png images of all signs in a data set->Shuffle and normalize the images,Rsa->RSA tutorial->RSA encryption
142110,"# boxplt for each genre showing different audio feature distribution --> interaction between audio feature and
# genre could be useful
boxplot_audio_features_by_genre(df)","low_rating_filter = joined_tables['rating'] < 2.5
med_rating_filter = (joined_tables['rating'] >= 2.5) & (joined_tables['rating'] < 4)
high_rating_filter = joined_tables['rating'] >= 4
ratings_range_dict = {genr : {'Low_rating': joined_tables[joined_tables[genr] & low_rating_filter]['rating'].count(),
                      'Mid_rating': joined_tables[joined_tables[genr] & med_rating_filter]['rating'].count(),
                      'High_rating': joined_tables[joined_tables[genr] & high_rating_filter]['rating'].count()}
               for genr in genres}","si_all_angstrom = [(const.h*const.c/s.to(units.Joule)).to(units.Angstrom) 
                   for s in si_all]
si_err_all_angstrom = [(const.h*const.c/s.to(units.Joule)).to(units.Angstrom) 
                       for s in si_err_all]

si_err_all_angstrom[0] = 0.0*units.Angstrom
si_err_all_angstrom[1] = 0.0*units.Angstrom",,Eda->EDA->Exploratory analysis by genre->Distribution of audio features by genre,Movie Data Analysis,Shifty Lines->A simple model for Doppler-shifted Spectra->A Shifted Spectrum with Emission/Absorption Lines with Variable Amplitudes and Signs->Sampling the Model
188795,"### Load the images and plot them here.
### Feel free to use as many code cells as needed.


import os
import cv2
_new=tf.placeholder(tf.float32,(None,32,32,3))
image=[]

#load test1.jpg
fp1=os.getcwd()+'/test1.jpg'
print(fp1)
_image =cv2.imread(fp1)
# cv2.imshow('img',_image)
#_image=np.array(_image)
_image = cv2.resize(_image, (32, 32))
print(_image.shape)
image.append(_image)

#load test2.jpg
fp2=os.getcwd()+'/test2.jpg'
print(fp2)
_image =cv2.imread(fp2)
# cv2.imshow('img',_image)
#_image=np.array(_image)
_image = cv2.resize(_image, (32, 32))
print(_image.shape)
image.append(_image)

#load test3.jpg
fp3=os.getcwd()+'/test3.jpg'
print(fp3)
_image =cv2.imread(fp3)
# cv2.imshow('img',_image)
#_image=np.array(_image)
_image = cv2.resize(_image, (32, 32))
print(_image.shape)
image.append(_image)

#load test4.jpg
fp4=os.getcwd()+'/test4.jpg'
print(fp4)
_image =cv2.imread(fp4)
# cv2.imshow('img',_image)
#_image=np.array(_image)
_image = cv2.resize(_image, (32, 32))
print(_image.shape)
image.append(_image)

#load test5.jpg
fp5=os.getcwd()+'/test5.jpg'
print(fp5)
_image =cv2.imread(fp5)
# cv2.imshow('img',_image)
#_image=np.array(_image)
_image = cv2.resize(_image, (32, 32))
print(_image.shape)
image.append(_image)

#load test6.jpg
fp6=os.getcwd()+'/test6.jpg'
print(fp6)
_image =cv2.imread(fp6)
# cv2.imshow('img',_image)
#_image=np.array(_image)
_image = cv2.resize(_image, (32, 32))
print(_image.shape)
image.append(_image)

image=np.array(image)
for index in range(0,len(image)):
    visulize_image= image[index].squeeze()

    plt.figure(figsize=(1,1))
    plt.imshow(visulize_image)
# print(len(imagesTogether))
# imagesTogetherNP=np.asarray(imagesTogether)
# print(imagesTogetherNP.shape)


# reader = tf.read_file(fp)
# image = tf.image.decode_jpeg(reader,channels=3)
# resized_image = tf.image.resize_images(image, (32,32))
# plt.figure(figsize=(1,1))
# plt.imshow(resized_image)","# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
objp = np.zeros((6*9,3), np.float32)
objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)

# Arrays to store object points and image points from all the images.
objpoints = [] # 3d points in real world space
imgpoints = [] # 2d points in image plane.

# Make a list of calibration images
images = glob.glob(os.getcwd() + '\camera_cal\calibration*.jpg')
print(images[0])
# print first image
img1 = cv2.imread(images[0])
plt.imshow(img1)","from sklearn.decomposition import PCA
from visualization_helper import plot_dataset

pca = PCA(n_components=2)
X_new = pca.fit_transform(X)

plot_dataset(X_new, iris.target_names[y])",,Traffic Sign Classifier->Implementation,"Car Nd-Advanced-Lane-Lines-P4->1. Evaluate functions on test images->1.1 First, I'll compute the camera calibration using chessboard images",03-Intro-To-Scikit-Learn->Introduction to scikit-learn \#1->Estimator object->Exercise:
216645,"from opencvutils import contours
# import cvu.contours as contours

# load the shapes image clone it, convert it to grayscale, and
# detect edges in the image
image = cv2.imread(""images/shapes.png"")
orig = image.copy()
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
edged = cvu.auto_canny(gray)

# find contours in the edge map using OpenCV 2.4.X
if cvu.is_cv2():
    (cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,
        cv2.CHAIN_APPROX_SIMPLE)

# find contours in the edge map using OpenCV 3
elif cvu.is_cv3():
    (_, cnts, _) = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,
        cv2.CHAIN_APPROX_SIMPLE)

# loop over the (unsorted) contours and label them
for (i, c) in enumerate(cnts):
    orig = contours.label_contour(orig, c, i, color=(240, 0, 159))

# show the original image
plt.imshow(orig);","gray = cv2.cvtColor(logo, cv2.COLOR_BGR2GRAY)
edgeMap = cvu.auto_canny(gray)
plt.imshow(edgeMap, cmap='gray');","for key in stats_dic.keys() :
    avg_N_over_obj = []
    for obj in objects:  
        avg_N_over_obj.append(stats_dic[key][obj]['N'])
    print('For min Mag %.2f, the mean N is %.2f'%(key, np.mean(avg_N_over_obj)))
    
print(' ')
for obj in objects: 
    avg_N_over_mag = [] 
    for key in stats_dic.keys():
        avg_N_over_mag.append(stats_dic[key][obj]['N'])
    print('For obj %s, the mean N is %.2f'%(obj, np.mean(avg_N_over_mag)))",,Examples->Examples in `opencvutils`->More Advanced Operations,Examples->Examples in `opencvutils`->Basic Image Operations,Ptf D Fig 2 Ptf Sel R Cut->Making the 4-panel plot of uncorrected SF
334982,"# the data, shuffled and split between train and test sets
(x_train, y_train), (x_test, y_test) = mnist.load_data()","# Keras took care of splitting the data
(x_train, y_train), (x_test, y_test) = mnist.load_data()",# your code goes here!,,"Mnist Dataset
Mnist Keras->Training a simple Convolutional Neural Network on the MNIST database->4. Define model architecture
Week+12 Mlnn-Part2->Part 2: With noise (np.random.normal)->Multi Layer Neural Network
Dcn - Mnist->Weight Matrix Plots->Set cmap
Create Dnn Models->1. Setup Training / Test Data->A. Load MNIST Dataset","Ml Algorithms, Part 9, Neural Networks->ML Algorithms, Part 9, Neural Networks->Imports",Hands On12->Recall: Boosting->Exploring bagging->Evaluating train/test with and without bagging
207912,"# Need for Cumulative Distribution Function (CDF)
# We can visually see what percentage of versicolor flowers have a 
# petal_length of less than 5?
# How to construct a CDF?
# How to read a CDF?

#Plot CDF of petal_length

counts, bin_edges = np.histogram(iris_setosa['petal_length'], bins=10, 
                                 density = True)
pdf = counts/(sum(counts))
print(pdf);
print(bin_edges);
cdf = np.cumsum(pdf)
plt.plot(bin_edges[1:],pdf);
plt.plot(bin_edges[1:], cdf)


counts, bin_edges = np.histogram(iris_setosa['petal_length'], bins=20, 
                                 density = True)
pdf = counts/(sum(counts))
plt.plot(bin_edges[1:],pdf);

plt.show();","# Plots of CDF of petal_length for various types of flowers.

# Misclassification error if you use petal_length only.

counts, bin_edges = np.histogram(iris_setosa['petal_length'], bins=10, 
                                 density = True)
pdf = counts/(sum(counts))
print(pdf);
print(bin_edges)
cdf = np.cumsum(pdf)
plt.plot(bin_edges[1:],pdf)
plt.plot(bin_edges[1:], cdf)


# virginica
counts, bin_edges = np.histogram(iris_virginica['petal_length'], bins=10, 
                                 density = True)
pdf = counts/(sum(counts))
print(pdf);
print(bin_edges)
cdf = np.cumsum(pdf)
plt.plot(bin_edges[1:],pdf)
plt.plot(bin_edges[1:], cdf)


#versicolor
counts, bin_edges = np.histogram(iris_versicolor['petal_length'], bins=10, 
                                 density = True)
pdf = counts/(sum(counts))
print(pdf);
print(bin_edges)
cdf = np.cumsum(pdf)
plt.plot(bin_edges[1:],pdf)
plt.plot(bin_edges[1:], cdf)


plt.show();","variables = ['Astapor', 'King\'s Landing', 'Winterfell', 'iPhone', 'avg_dist', 'avg_rating_by_driver', 'avg_rating_of_driver', 'surge_pct', 'avg_surge', 
        'trips_in_first_30_days', 'ultimate_black_user', 'weekday_pct']

Xtrain, Xtest, ytrain, ytest = train_test_split(udf[variables].values, udf['active'], random_state = 42, test_size = 0.2)

log_clf = LogisticRegression()
cv_score(log_clf, Xtrain, ytrain)",,"Exploratory Data Analysis->(3.4) Histogram, PDF, CDF","Exploratory Data Analysis->Univariate analysis (CDF)
Exploratory Data Analysis->PDF AND CDF
Exploratory Data Analysis->(3.4) Histogram, PDF, CDF",Ultimate Challenge
218468,"pop_growth_USA = pd.read_sql_query('SELECT Year, Value FROM Indicators WHERE CountryCode=""USA"" AND IndicatorCode=""SP.POP.GROW""', connection)
ax = pop_growth_USA.set_index('Year').plot(kind='bar', figsize=(10,5), title='Population growth \n', legend=False)
ax.set(xlabel='\nYear', ylabel='Population growth in %\n')
plt.show()","attack_perpetrators_USA = pd.read_sql_query('SELECT gname, num_attacks FROM (SELECT gname, COUNT(gname) num_attacks FROM Attacks WHERE iso_code=""USA"" GROUP BY gname) ORDER BY num_attacks DESC LIMIT 10', connection)
ax = attack_perpetrators_USA.set_index('gname').plot(kind='barh', figsize=(10,5), title='Perpetrators attack distribution \n', legend=False)
ax.set(xlabel='\nNumber of attacks', ylabel='Perpetrators\n')
plt.show()","group_by(application_train,'ORGANIZATION_TYPE','TARGET')",,Report->Website->What's next?,Report->Impact of Terrorism on World Development->Data exploration,Eda Kernel 0815->Types Of Features
431379,"# can be also done that way:
x = 2
y = 3

x += y # returns the sum of x and y and assigns it to x
# it is equal to x = x+y

print(x)","x = 5
y = 6

if x < y:
    print('x is less than y')
elif x > y:
    print('x is greater than y')
else:
    print('x and y are equal')","q, r = divmod(128, 37)
print(""q ="", q, ""r ="", r)",,Introduction To Programming Using Python #01 - Basics->Introduction to programming using Python->Operators for all the numeric types->Addition operator (x+y),"Lecture+Two+May+30Th+With+Exercise->Lecture Two - May 30th, 2017->Strings->Alternative execution->EXERCISE: assign values to X and Y to create different outcomes",Cs103-06->Lecture 6: Statements->Assignment Statements->Simultaneous Assignment
305826,T_Data.shape,"T = data[:,13:]
T.shape","from our_project.models import Human

# create a Human and add data
h = Human()
h.name = 'Anton'
h.age = 34

# save data to database
h.save()",,Ujwala Dharanikota Capstone Project->Data Analysis:,Deng-A2->Data,13 Django Introduction->Relational database->How to use the Django Models?
176502,"#Check for directory before load the data
os.getcwd()
os.chdir('/Users/michellehsu/Desktop/CMU/Spring 2018/95-828 ML for Problem Solving/HW/HW3/Data/loans')","os.chdir('/Users/lawerencelee')  # Go to my home directory
os.getcwd()","# which possibly have both?
both = list(set(diff[""bin""]).intersection(set(same[""bin""])))
print(both)",,"Hw3->4 Applied: Random Forest->In this question, we use Random Forest model to predict loan default. Download the data loans.zip from Canvas. The zip file contains two files: loans train.csv and loans test.csv. They have the same format, where the last column is a binary default label (that we aim to predict) and other columns are observational features. The feature names are provided in the header.",File Systems->Python for File Systems->The `cd` of Python's OS Module,Cpr Dpann Rubisco-Revision->PRK/CBB analysis->exploring genomes with the bifunctional FBA/FBPase
132234,"import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

lyrics_enriched = pd.read_csv('../data/Lyrics_enriched.csv')
df = pd.read_csv('../data/Track_Features.csv')","import pandas
import seaborn as sn
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
london = pandas.read_csv(""/home/vivek/Documents/mydata/iimb/regression/regression_input_post_code_year_month_aggregate.csv"")","from dl import authClient
token = authClient.login('USERNAME',getpass.getpass()) # replace USERNAME with your username, enter password when prompted",,Data Analysis,"Linear Regression->Global Imports
Lasso Regression With Grid Search->Global Imports
Xtreme Gradient Boost Grid Search->Global Imports",Datalab Basic Access Withexercises->Log-in and obtain an authentication token
493183,"c[1:3, -1]",C.covariant_derivative_components,"picked_stocks = np.zeros([Xtest.shape[0],num_stocks])
portfolio_value = np.zeros(Xtest.shape[0])

money = 100000.0",,Numpy->NumPy Arrays,Gravi Py - Tutorial->GraviPy - tutorial->Derivatives->The _Riemann_ tensor,Ai Stock Picker->Trading with Deep Learning->__Introduction__->__Test the Trading Strategy__
193235,"with tf.Session() as sess:
    saver.restore(sess, chosen_version)
    get_test_images_predictions(preprocessed_esp_test_images, 
                                esp_test_images,
                                esp_test_images_labels,
                                esp_test_images_names,
                                sess)","with tf.Session() as sess:
    saver.restore(sess, chosen_version)
    get_test_images_predictions(preprocessed_esp_test_images, 
                                esp_test_images,
                                esp_test_images_labels,
                                esp_test_images_names,
                                sess)",era_ds = era_ds.resample(time='1D').mean(),,"Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Image Enhancements->Especial Cases->`MoNet0.1.5` Test
Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Image Enhancements->Data Augmentation","Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Image Enhancements->Especial Cases->`MoNet0.1.5` Test
Traffic Sign Classifier->Self-Driving Car Engineer Nanodegree->Image Enhancements->Data Augmentation","Surface Forcing Year Selection->Selection of ""normal"" surface flux year from Tamura and eraInterim datasets->EraInterim surface stress"
2908,S.get_fields(main_only = True),"r = s.reader()

print(r.doc_count())
print(r.doc_frequency('doc',b'lincoln'))
print(r.doc_field_length(21, 'doc')) 
print(r.avg_field_length('doc'))","custom_query_dict = {
    'name': 'HelloWorld',
    'id': response.json()['id']
}

response = api.query(query_dict=custom_query_dict, auth=auth)",,Tutorial->Tuorial on json validation and json schema inspection via `omdata` package->Inspecting a given json schema->Checking top level fields,Py Index->Wrap Index in an IndexReader and get statistics needed for BM25,Quickstart->Quickstart->Query jobs in Cromwell/Cromwell-as-a-Service->Get the workflows
120813,"a = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])
print(a)
a[[1, 3, 2]]","y_enc = (np.arange(np.max(y) + 1) == y[:, None]).astype(float)

print('one-hot encoding:\n', y_enc)","logData = []
for row in rec_log:
    if row[-1] == 'l2' and row[-2] == '0.1':
        logData.append([row[1],row[3]])
logData = np.array(logData)
logData = tb.sortByColumn(logData,0)
logData = np.vstack((tb.sortByColumn(logData,0)[:-3],tb.sortByColumn(logData,0)[-2:]))",,01->Fancy indexing,Softmax Regression->Softmax Regression->Examples->Example 1 - Gradient Descent->Predicting Class Probabilities,"Showcase Figures->Showcase figures->Figures 4,7 and 9"
130594,"def fast_distances(test_row, train_rows):
    """"""An array of the distances between test_row and each row in train_rows.

    Takes 2 arguments:
      test_row: A row of a table containing features of one
        test song (e.g., test_20.row(0)).
      train_rows: A table of features (for example, the whole
        table train_20).""""""
    assert train_rows.num_columns < 50, ""Make sure you're not using all the features of the lyrics table.""
    counts_matrix = np.asmatrix(train_rows.columns).transpose()
    diff = np.tile(np.array(test_row), [counts_matrix.shape[0], 1]) - counts_matrix
    distances = np.squeeze(np.asarray(np.sqrt(np.square(diff).sum(1))))
    return distances","# Just run this cell to define fast_distances.

def fast_distances(test_row, train_rows):
    """"""An array of the distances between test_row and each row in train_rows.

    Takes 2 arguments:
      test_row: A row of a table containing features of one
        test movie (e.g., test_20.row(0)).
      train_rows: A table of features (for example, the whole
        table train_20).""""""
    assert train_rows.num_columns < 50, ""Make sure you're not using all the features of the movies table.""
    counts_matrix = np.asmatrix(train_rows.columns).transpose()
    diff = np.tile(np.array(test_row), [counts_matrix.shape[0], 1]) - counts_matrix
    np.random.seed(0) # For tie breaking purposes
    distances = np.squeeze(np.asarray(np.sqrt(np.square(diff).sum(1))))
    eps = np.random.uniform(size=distances.shape)*1e-10 #Noise for tie break
    distances = distances + eps
    return distances","import numpy as np
import timeit
import matplotlib.pyplot as plt",,Lab5,"Project3->1. The Dataset->1.1. Word Stemming->Question 1.1.1
Project3->3. Features->Question 3.1.2",Calculating Variance
133626,"titanic_data = pd.read_csv('./titanic_data.csv')
titanic_data.head()","titan = pd.read_csv('train.csv')#, index_col='PassengerId')
titan.info
titan['Sex_bool'] = pd.get_dummies(titan.Sex, prefix='Sex_bool', drop_first=True)
titan2 = titan.drop(['Name', 'Cabin', 'Sex'], axis=1)
titan2 = titan2.dropna(axis=0, how='any')","hist, bins = np.histogram(actions, bins=3)
width = 0.7 * (bins[1] - bins[0])
center = (bins[:-1] + bins[1:]) / 2
plt.bar(center, hist, align='center', width=width)
plt.show()",,Data Analysis->Data Wrangling,Titanic,06-Function-Approximation->Function Approximation
162002,"import pandas as pd
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()
%matplotlib inline","%matplotlib inline
import numpy as np
import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt
import matplotlib as mpl
sn.set(context='notebook', style='ticks', color_codes=True, palette='deep')","sns.countplot(x='sex', data=tips) #y is already calculated = count",,"3
3->Load and clean
3->Load & clean",Bat-Casey-Exploratory->Bat AGN Exploratory Analysis: Casey 2012 Model Fits,Seaborn2 Categorical Plots
276637,"fig, axes = plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(12, 10)
sn.boxplot(data=dailyData,y=""count"",orient=""v"",ax=axes[0][0])
sn.boxplot(data=dailyData,y=""count"",x=""season"",orient=""v"",ax=axes[0][1])
sn.boxplot(data=dailyData,y=""count"",x=""hour"",orient=""v"",ax=axes[1][0])
sn.boxplot(data=dailyData,y=""count"",x=""workingday"",orient=""v"",ax=axes[1][1])

axes[0][0].set(ylabel='Count',title=""Box Plot On Count"")
axes[0][1].set(xlabel='Season', ylabel='Count',title=""Box Plot On Count Across Season"")
axes[1][0].set(xlabel='Hour Of The Day', ylabel='Count',title=""Box Plot On Count Across Hour Of The Day"")
axes[1][1].set(xlabel='Working Day', ylabel='Count',title=""Box Plot On Count Across Working Day"")","vgg_loss_train = pd.read_csv('csv/50words/run_50w_vgg-tag-Loss.csv')
vgg_loss_val = pd.read_csv('csv/50words/run_50w_vgg-tag-Loss_Validation.csv')

cnn_loss_train = pd.read_csv('csv/50words/run_50w_cnn-tag-Loss.csv')
cnn_loss_val = pd.read_csv('csv/50words/run_50w_cnn-tag-Loss_Validation.csv')

lstm_loss_train = pd.read_csv('csv/50words/run_50w_lstm-tag-Loss.csv')
lstm_loss_val = pd.read_csv('csv/50words/run_50w_lstm-tag-Loss_Validation.csv')

fig, ax = plt.subplots(3, 2)
fig.set_size_inches(15,15)

ax[0][0].plot(vgg_loss_train['Value'], 'r')
ax[0][1].plot(vgg_loss_val['Value'], 'g')
ax[0][0].set_title('VGG Finetune - Loss Train')
ax[0][1].set_title('VGG Finetune - Loss Val')

ax[1][0].plot(cnn_loss_train['Value'], 'r')
ax[1][1].plot(cnn_loss_val['Value'], 'g')
ax[1][0].set_title('Alexnet - Loss Train')
ax[1][1].set_title('Alexnet - Loss Val')

ax[2][0].plot(lstm_loss_train['Value'], 'r')
ax[2][1].plot(lstm_loss_val['Value'], 'g')
ax[2][0].set_title('LSTM - Loss Train')
ax[2][1].set_title('LSTM - Loss Val')
plt.show()","prop_df['scores_scaled'] = prop_scaled['scores_scaled']
prop_df.head()",,"Kernel->Outliers Analysis
Bike Sharing->Outliers Analysis",Results->Loss,05-Rank-Properties-Using-Features->Score and rank properties using intrinsic and spatial features->Scale data->Inverse transform data
221908,"sns.boxplot(data=res_house, y='SalePrice', x='MSZoning')
# FV (floating village residential) has the highest median price
# The most expensive properties are RL (residential low-density)
# RL (residential low-density) has the largest variation in price","plt.figure(figsize=(15,8))
sn.boxplot(x='STABBR', hue='Enrollment Gender wise', y='Enrollment', data=student_melt, fliersize=3, showmeans = True);",sns.boxplot(train.trip_duration),,Machine Learning->Machine Learning->6. EDA: Boxplots of Categorical Variables,Gender+Differences+In+University+Enrollment->Gender Differences in University Enrollment->Gender based on Enrollment,02-Eda
279172,"act_obj = LeakyReLU()
nn = CustomNeuralNetwork(layers,act_obj)","nn = CustomNeuralNetwork(layers,act_obj)","city_ride.groupby(['city','type']).fare.agg(['count']).reset_index()
total_city_rides = city_ride.fare.count()

total_rides = pd.DataFrame(city_ride.groupby(['type']).fare.agg(['count']).reset_index())
total_rides = total_rides.rename(columns = {'count': 'Rides'})
total_rides['Percent of Total'] = total_rides['Rides']/total_city_rides*100
total_rides",,Scratch Neural Net->With different activation->Leaky ReLU activation?,Scratch Neural Net->Normalize dataset (preprocessing for Neural Net),Pyberhw->Pie Chart - % Total Rides by City Type
110776,tokens = word_tokenize(raw_text),"tokens = word_tokenize(text)
lowercase_tokens = []
for token in tokens:
    lowercased_version = token.lower()
    lowercase_tokens.append(lowercased_version)","Pop_df['Immigration_Rate_%']=Pop_df['Immigration_Rate_%'].round(2)
Pop_df.head(6)",,1->Reading texts->Tokenization,01-Preprocessing-Solutions->Preprocessing->Text normalization->Challenge,"Analysis On Immigration, Stock Market, And Gdp->Analysis on Immigration, Stock market, and GDP->To get the data on Net immigration as a dataframe(value in %)->Rounding off the immigration rate to two decimal places"
399523,"import os
root_path = '/Users/raz/Documents/machineLearning/data/presidents/'
s = [(root.replace(root_path,''),root, files) for root, dirs, files in os.walk(root_path) if not root == root_path]
s
president_data = []
president_id = []


ids = {}
id_index = 0
for newsgroup, path, files in s:
    print ('PROCESSING ' + newsgroup)
    temp_counter = 0
    if not newsgroup in ids:
        ids[newsgroup] = id_index
        id_index += 1
    for file in files:
        if file != '.DS_Store':
            tmp = os.path.join(path, file)
            print(tmp)
            message = open(tmp, ""r"", errors='ignore')
            # TO BE DONE
            message.close()
            
president_data[:5]","def messages (maildir_root):
    """"""
    (Generator) Given a mailbox directory name, yields an
    email object for each message therein.
    """"""
    for base, dirs, files in os.walk (maildir_root):
        for filename in files:
            filepath = os.path.join (base, filename)
            email_file = open (filepath)
            msg = email.parser.Parser ().parse (email_file)
            email_file.close ()
            yield msg","# Example 1.6: addition
a = 1
b = 2.0
x = a+b
x",,"Feature Selection->FEATURE SELECTION->now normalize->So our new feature, BMI looks like it will improve performance","04--Comps-Gens-Sparse->CSE 6040, Fall 2015 [04]: List comprehensions, generators, and sparse data structures->Example: Generating email objects",Chapter 1 - An Introduction To Python->1.3 Arithmetic
203956,"numerical_cols = list(df_train.dtypes[df_train.dtypes != 'object'].index)
numerical_cols","# Group the dataset by the key columns (make/model/year/component)
grouped = clean_train_df.groupby(key_cols)

# Perform aggregate functions on the columns for each vehicle type.
# Aggregate with the mode for categorical columns
# Aggregate with the mean for numerical columns
grouped_df = pd.DataFrame()
for col in cat_cols:
    grouped_df[col] = grouped[col].agg(aggMode)
for col in numer_cols:
    grouped_df[col] = grouped[col].mean()

# Add in a column for the number of complaints for each vehicle-component.
grouped_df['COMPLAINTS'] = grouped.size()
numer_cols.append('COMPLAINTS');
all_cols.append('COMPLAINTS');

print('done')","# Write the lambda function using replace
tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))

# Write the lambda function using regular expressions
tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x)[0])",,"Kaggle Titanic Old->This script attempts to predict who will survive on the titanic->Find numerical and non-numerical columns to one-hot encode or categorize each column
Kagglel Titanic Full->Try XGBoost",Data Analysis - David - Backup->Data cleaning for prediction->Group complaints by make/model/year and perform aggregate functions on the column values,Python Cleaning Data->Tidy Data->Apply
374466,"# read spaceborn SR data
if platf == ""gpm"":
    sr_data = wrl.io.read_gpm(gpm_file, bbox)
elif platf == ""trmm"":
    sr_data = wrl.io.read_trmm(trmm_2a23_file, trmm_2a25_file, bbox)
else:
    raise(""Invalid platform"")","filename = wrl.util.get_wradlib_data_file('rainbow/2013070308340000dBuZ.azi')
rbdict = wrl.io.read_rainbow(filename)",cash_register = CashRegister(),,Wradlib Match Workflow->Match spaceborn SR (GPM/TRMM) with ground radars GR->Data handling->Read and organize the SR data,Wradlib Load Rainbow Example->Load and inspect data from a Rainbow file->Load Rainbow file,Index->Python Object Oriented Cash Register Lab->Instructions
89702,"# Least-squares directly using linalg.lstsq
fit_quad = np.linalg.lstsq(X_quad, y)[0]
fit_cube = np.linalg.lstsq(X_cube, y)[0]
fit_silly = np.linalg.lstsq(X_silly, y)[0]

# Show fit params
print(""Fit quad. Model: "", fit_quad)
print(""Fit cubic Model: "", fit_cube)
print(""Fit silly Model: "", fit_silly)","print np.linalg.lstsq(A, y)","fig = plt.figure(figsize=(25,7))
sns.violinplot(x='Sex', y='Age', hue='Survived', data=data, split=True,
              palette={0:""r"", 1: ""g""});",,Linear Regression->Making Predictions: Linear Regression->Simple Example->Wakeup-Task: Brute-Force Search,Linear Algebra,Notebook->Hyperparameters Tuning
219039,"from ggplot import *
df = titanic_obj.titanic_data
df['Salutation'] = df.apply(lambda x: titanic_obj.get_salutation(x['Name']), axis=1)
ggplot(df, aes(x='Age', y='Salutation')) + geom_boxplot()","ggplot(aes('yr', 'ecological.footprint'), data=data) + geom_boxplot()","AAPL_df = AAPL_df[['Close']]
print(AAPL_df.head())",,Report->Investigate a Dataset - Titanic Data->What factors made people more likely to survive?->Data Wrangling,Data Viz Case Study 4->Ecological Footprint,Project
211812,"#Import needed packages.
import numpy as np
import numpy.random as random
import pandas as pd
import matplotlib.pyplot as plt
import scipy.stats as stats
import pylab
import seaborn as sns

sns.set()

df.info() 
df.head()
#variables: temperature, gender, heart_rate","%matplotlib inline
import matplotlib.pyplot as plt, seaborn as sn, mpld3
import pandas as pd, numpy as np
sn.set_context('notebook')","x = 4
print(x)

if x < 5:
  print(""x is less than five!"")
else: 
  print(""x is not less than five!"")
if x%2 == 0:
    print(""even"")
else:
    print(""odd"")",,Sliderule Dsi Inferential Statistics Exercise 1->What is the True Normal Human Body Temperature?,Ecoreg Prelim Analysis2,04-Containers-Exercises->Exercises 4
402206,"from sklearn.cross_validation import train_test_split

ratings_train, ratings_test = train_test_split(
    beer_reviews, test_size=0.2, random_state=0)","review_train, review_test, label_train, label_test = train_test_split(result['pos'],result['Labels'], test_size=0.2,random_state=42)","# slice ocupations columns for first hierarchy level
hier_level1 = occup_full[['Managers',
                    'Prof_occup',
                    'Thecnical',
                    'Administ',
                    'Skl_trades',
                    'Care_other',
                    'Sales',
                    'Operative',
                    'Elementary']]
hier_level1.sum()",,Deep Beers Blog Post Part 1->Explicit feedback Recommender System,Hotel Review->Test the SVM model on 2 reviews from Yelp. The SVM model performs a lot better than Multi Naive Bayes since it predicts the labels for these 2 reviews correctly,Histograms - London->London Census data analysis->Qualification groups->Histograms QS606EW
312621,"air_quality.O3.plot(grid=True, figsize=(12, 2))","O3.plotGrotrian(thresh_int=0)
N2.plotGrotrian()","feature_configs = buildExtractionConfigurations();
print (len(feature_configs))
for feature_config in feature_configs:
    trainClassifierForFeatureConfig(vehicles, non_vehicles, feature_config)",,"17-Pandas-Intro-Solution->Basic visualisation->Exercise
17-Pandas-Intro->Introduction to pandas->Basic visualisation->Exercise",Nc 2 1->LINE EMISSIVITIES->Emissivities of lines from O III->Compare the values of the Eistein coefficients A's for the various available sources of atomic data.,Vehicle-Detection->Vehicle Detection Project->Feature extraction and classifier training->Samples of images with corresponding HOG features:->Run classification training for each of the configurations
208242,"import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


df_total=pd.read_excel(""DATA/dades_excels/base de datos seguiments130618.xlsx"")
df_total.set_index('Códigobiobanco', inplace=True)
COL=df_total.columns
l=[]
for x in COL:
    if (x[:4]=='plac'):
        l.append(x)
        
l=l[:10] # contains all column names with plaque
print(l)
cols_clas_placa=l+['EventoCV_Si_No','muerte',""sexo""]
df= df_total.loc[:,cols_clas_placa]
df['Totalplaques']=df[l[0]].fillna(0)+df[l[1]].fillna(0)+df[l[2]].fillna(0)+df[l[3]].fillna(0)+df[l[4]].fillna(0)+df[l[5]].fillna(0)+df[l[6]].fillna(0)+df[l[7]].fillna(0)+df[l[8]].fillna(0)+df[l[9]].fillna(0)
df['CCAplaca']=df['placas_cc_d']+df['placas_cc_i']
df['bulb_placa']= df['placas_med_bif_d']+df['placas_med_bif_d']
df['mortCV']=df['muerte']=='CV death'","import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


df_total=pd.read_excel(""DATA/dades_excels/base de datos seguiments130618.xlsx"")
df_total.set_index('Códigobiobanco', inplace=True)
COL=df_total.columns
l=[]
for x in COL:
    if (x[:4]=='plac'):
        l.append(x)
        
l=l[:10] # contains all column names with plaque
print(l)
cols_clas_placa=l+['EventoCV_Si_No','muerte',""sexo""]
df= df_total.loc[:,cols_clas_placa]
df['Totalplaques']=df[l[0]].fillna(0)+df[l[1]].fillna(0)+df[l[2]].fillna(0)+df[l[3]].fillna(0)+df[l[4]].fillna(0)+df[l[5]].fillna(0)+df[l[6]].fillna(0)+df[l[7]].fillna(0)+df[l[8]].fillna(0)+df[l[9]].fillna(0)
df['CCAplaca']=df['placas_cc_d']+df['placas_cc_i']
df['bulb_placa']= df['placas_med_bif_d']+df['placas_med_bif_d']
df['mortCV']=df['muerte']=='CV death'","# Data Sets and Cuts

fbins_min = data['egg-header']['minimum_frequency'][0]/1e6
fbins_max = data['egg-header']['maximum_frequency'][0]/1e6
fdata = data['combined']['StartFrequency']/1e6+fbins_min # MHz
pdata = data['combined']['TotalPower']*1e9 # nW/Hz
sdata = data['combined']['Slope']/1e9 # MHz/ms
tldata = data['combined']['TimeLength']*1e3 # in ms
tldata_events = data['combined']['TimeLength']*1e3 # in ms
uid_data = data['combined']['UniqueEventID']
mpt_total_data = data['combined']['MultiPeakTracks_Total']
orderf_data = data['combined']['MultiPeakTracks_OrderByFreq']
# ... and RP variables
average = data['combined']['Average']
cpf = data['combined']['CentralPowerFraction']
kurtosis = data['combined']['Kurtosis']
maxcentral = data['combined']['MaximumCentral']
meancentral = data['combined']['MeanCentral']
normcentral = data['combined']['NormCentral']
rms = data['combined']['RMS']
rms_away = data['combined']['RMSAwayFromCentral']
sigmacentral = data['combined']['SigmaCentral']
skewness = data['combined']['Skewness']
npeaks = data['combined']['NPeaks']
isvalid = data['combined']['IsValid'].astype(bool)
# extras
t_range_margins = np.array([-0.05,0.2]) # for StartTimeInAcq cut
t_range = (pretrigger_time*1e3)+t_range_margins
cut_t = (data['combined']['StartTimeInAcq']*1e3>=t_range[0]) & (data['combined']['StartTimeInAcq']*1e3<=t_range[1])
cut_track_seq0 = (data['combined']['EventSequenceID']==0)
cut_base = (cut_track_seq0 & cut_t)
trackid_rp = data['combined']['TrackID']
og_numtracks = trackid_rp.size",,Data Exploration Part 1->Data Exploration / Visualization (part 1):,Data Exploration Part 1->Data Exploration / Visualization (part 1):,Rp Ps Fromraw->3.5 Final Training Set Selection
262935,"statuses = np.array( uniqueLinks.status )

f, ax = pl.subplots(1, 1)
ax.hist( statuses, np.arange(-1.5,6,1), edgecolor=(0.2,0.2,0.2), lw=3, 
            color=(0.2,0.6,1.0) )


ax.set_ylabel('Count')
ax.set_xticks([-1, 1, 3, 4])
ax.set_xticklabels( ['Time out', 'Accessible', 'Redirected', 'Not found'], rotation=-45 )

pl.show()","fig = pl.figure(figsize=(10,5)) 
ax = fig.add_subplot(111)
ax.hist(df_['homicidepp'], color='darkred')
ax.set_xlabel(""Homicides per 1000 People"")
ax.set_ylabel(""Frequency"")
ax.set_title(""Homicides per person histogram"")","##Error measure: RMSE
#Minimum MSE on validation
import numpy as np
np.sqrt(0.00271) * 48",,Figures Using Links->Importing and parsing data,Hw5 Assignment 3->exploration->plot the average number of homicide by fire arms per person,Facial Keypoints V3 Output-Deep Learning->Facial Keypoints Detection->First model: a single hidden layer
433423,"# Compute the maximum string length
Txmax = len(max(x_traindev, key=len))
print(Txmax)

# We set desired string length to 448 which is 5 char shorter than Txmax. 
# This simplifies the dimensions during convolution later, without significant performance loss
Tx = 448
# Set the number of classes
n_c = 12","### Replace each question mark with the appropriate value.

# TODO: Number of training examples
n_train = len(X_train)

# TODO: Number of testing examples.
n_test = len(X_test)

# TODO: What's the shape of an traffic sign image?
image_shape = X_train[0].shape

# TODO: How many unique classes/labels there are in the dataset.
n_classes = max(y_train)+1

print(""Number of training examples ="", n_train)
print(""Number of testing examples ="", n_test)
print(""Image data shape ="", image_shape)
print(""Number of classes ="", n_classes)","c = '\w*_id'
check_values(dfs['trans_details'], c, id_pattern)",,Challenge->Deep Learning for Time Series and NLP,Traffic Sign Classifier,Clean All Files->Cleaning of All Files->Load Schemas Dict
64183,"df = pd.DataFrame(simulation)
df.columns = ['simulationMean']
df.describe()",import simul,"wd1 = widgets.Select(
    options= datestr,
    description='earlier date:',
    disabled=False
)
wd1.observe(seld1, names='value')

wd2 = widgets.Select(
    options= datestr,
    description='later date:',
    disabled=False
)
wd2.observe(seld2, names='value')

g=widgets.Button(
    description='Plot and make file',
    disabled=False,
    button_style='success',
    tooltip='Click to plot')
g.on_click(go)

widgets.HBox([wd1, wd2, g])",,Hw2->Problem 2(b),Genop->Cost function,Whiteisland Levelling Image
436916,"import json
import random
import pandas as pd
import tensorflow as tf
import numpy as np
from sklearn.model_selection import train_test_split

df = pd.read_csv(""eeg-data.csv"")
subs=pd.read_csv(""subject-metadata.csv"")","import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import tensorflow as tf

from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils
import pandas as pd

#load the data
train = pd.read_csv(""train.csv"")
test = pd.read_csv(""test.csv"")","import pandas as pd
df = pd.read_csv(""sonar.all-data.csv"", header=None)
df.head()",,Classification,"Multi Layer Perceptron
Multi Layer Perceptron",Random Forest-Checkpoint->Random Forests from Scratch->The next four code boxes are copied nearly directly from the tutorial linked at the beginning. Run them all through before continuing.
260689,"import datetime
import pandas as pd
# import the Socrata Open Data API
from soda import SODA

# import my application token from a file
from settings import APP_TOKEN",data.set_token(APP_TOKEN),"import numpy as np

data = np.loadtxt('data1.txt')

x = data[:,0:2]
y = data[:,2]",,Example->Python Wrapper for the Socrata Open Data API->Imports,Example->Python Wrapper for the Socrata Open Data API->Set the Token,Training Neural Network Example->Training Neural Network
175173,"import itertools

predict_input_fn = lambda: csv_input_fn(files_name_pattern= TEST_DATA_FILES_PATTERN, 
                                      mode= tf.estimator.ModeKeys.PREDICT,
                                      batch_size= 5)

predictions = list(itertools.islice(estimator.predict(input_fn=predict_input_fn),5))

print("""")

print(""* Predicted Classes: {}"".format(list(map(lambda item: item[""class""]
    ,predictions))))

print(""* Predicted Probabilities: {}"".format(list(map(lambda item: list(item[""probabilities""])
     ,predictions))))","import itertools

predict_input_fn = generate_pandas_input_fn(file_name=TEST_DATA_FILE, 
                                      mode= tf.estimator.ModeKeys.PREDICT,
                                      batch_size= 5)

predictions = estimator.predict(input_fn=predict_input_fn)
values = list(map(lambda item: item[""predictions""][0],list(itertools.islice(predictions, 5))))
print()
print(""Predicted Values: {}"".format(values))","ts_logrtX = pd.read_csv('saved_dat/ts_logrtX.csv', index_col=0)
ts_logrtX.index = pd.to_datetime(ts_logrtX.index)
arma_all_resid = np.load('saved_dat/arma_all_resid.npy')
split_date = '2011-06-18'
lrt_train = ts_logrtX.ix[:split_date, ['sp500_logrt']]
lrt_test = ts_logrtX.ix[split_date:, ['sp500_logrt']]",,04->7. Prediction,02->7. Prediction,Sp500 Pred With Ml - Chapter 2->Chapter 2: machine learning for S&P500 prediction->2.3 K-nearest neighbors->2.3.1 Direct approach
295862,"import zarr
print('zarr', zarr.__version__)",genotype_zarr,"plt.plot(xList,p1.sequence(.4, 3.5441),'r.')
plt.show()
plt.plot(xList,p1.sequence(.3, 3.5441),'g.')
plt.show()
plt.plot(xList,p1.sequence(.2, 3.5441),'b.')
plt.show()",,Multithreaded Copy->Measure copy times when using multi-threading->Zarr arrays (in-memory),2016-04-14-To-Hdf5-And-Beyond->Post-script: Performance with real genotype data,Midterm->PHYS227 Midterm->Problem 2
116244,"image = np.zeros((512,512,3), np.uint8)

# Let's define four points
pts = np.array( [[10,50], [400,50], [90,200], [50,500]], np.int64)

# Let's now reshape our points in form  required by polylines
pts = pts.reshape((-1,1,2))
# take image , points , polygon closed or not, collor, thickness
cv2.polylines(image, [pts], True, (0,0,255), 3)
cv2.imshow(""Polygon"", image)
cv2.waitKey(0)
cv2.destroyAllWindows()","# Create a window holder to show you image in
cv2.namedWindow(""PyData Tutorial"", cv2.WINDOW_AUTOSIZE)
cv2.imshow(""PyData Tutorial"", frame)

# Press any key to close external window
cv2.waitKey() 
cv2.destroyAllWindows()","print(stemmed_tokens[0:30])
print('We have obtained ' + str(len(stemmed_tokens)) + ' stemmed tokens.')",,1->Drawing Images and Shapes->create a polygon,01 Manipulation Of Images And Videos->Fix Window,Word Cloud->Taming text->Part 2->Stemming
9256,"opt, snr = os.compute_noise_marginalized_os(pp.chain, N=10000)","# C = 10 p=3  ACC = 0.637 tol=0.01 eps=0.01  /255
# C = 10 p=3  ACC =0.6155  tol=0.01 eps=0.01  /255
# C = 10 p=3  ACC =0.584  tol=0.01 eps=0.01  /255
computAcc(testlabel,RESULT,'OVA')","for i in range(10):
    ver_imagen(X_test,y_test,model)",,Analysis->11-year NANOGrav Stochastic Background Analysis->Optimal Statistic Analysis->Compute noise marginalized OS,Comp5318 Assignment1->Method one: Get models data and train OVA model,Leer Datasets - Galaxy Zoo-Checkpoint->All the question as one output
17295,"#create a new dataframe to hold complete information
newdf = pd.DataFrame()

#loop through each theme code in the dataframe
for code in wbdf.code.unique():
    
    #create a temporary dataframe for each theme code
    subdf = wbdf[wbdf[""code""] == code]
    
    #fill the NaN values with the correct strings, forward and backward
    subdf = subdf.fillna(method=""ffill"")
    subdf = subdf.fillna(method=""bfill"")
    
    #add the corrected dataframe to the newly created dataframe, newdf
    newdf = pd.concat([newdf, subdf])
    
#sort and display head for a sanity check against previous printout    
newdf.sort_index().head(15)","# Combine indicies of both dataframes
combine_df = pd.concat([cb_duration_df1, kb_duration_df1])

# Only concerned with duration column for visualization
plot_df = pd.DataFrame()
plot_df['kb_duration'] = combine_df['kb_duration_category']
plot_df['cb_duration'] = combine_df['cb_duration_category']
plot_df = plot_df.sort_index()

plot_df.head()",plt.imshow(arr[rng][1].astype('uint8')),,"Sliderule Dsi Json Exercise->JSON example, with string->3. Dataframe with the missing names filled in.",A2 Arbitrage Analysis->Cryptocurrency Arbitrage - Frequency and Duration Analysis->5. Data Analysis - Arbitrage Frequency,Imagenet-Parallel->Imagenet Processing in parallel->Image -> Image
171138,nrows,"nrow = 150
ncol = 150","df_75 = pd.concat([df_st1['75%'],df_st2['75%'],df_st3['75%'],df_st4['75%']],axis=1,keys=['st1', 'st2','st3','st4'])
df_75.head(2)",,"Multiple Linear Regression->Load the File
English Wikipedia Nlp Aws->3.5 Bags of words->What are the topics of the top 20 communities?
English Wikipedia Nlp Aws->2. Notebook setup->3.2 Named entity recognition
Nsf Network->Cliques
Multiple Linear Regression3->Load the File",Dogs2->Dog breed classifier,Scatter Temp Vs Co2-Matrix-Checkpoint->Scatter Matrix->Time Series
346040,"from pathlib import Path
d = Path().resolve().parent.parent.parent

d = str(d) + ""/data/original/papers.csv""

papers = pd.read_csv(d)
papersArray = papers.paper_text
papersId = papers.id","d = Path().resolve().parent.parent
d = str(d) + ""/data/generated/Google and Micorsoft hindex.csv""

hindexGoogle = pd.read_csv(d)
print(hindexGoogle.head(5))

hindexGoogle = hindexGoogle[['author_id', 'google_hindex' ]]


hindexGoogle = hindexGoogle.sort_values('google_hindex',ascending=False)
hindexGoogle = hindexGoogle.head(num)
print(hindexGoogle.head(5))

topGooglehindexIDs = hindexGoogle[['author_id']]
print(topGooglehindexIDs.head(5))",minmax_scaler.fit_transform(X),,Topic Modelling->LDA topic modelling,Spearman Evaluation->Spearman evaluation,"Preprocessing->Preprocessing->Standardize, Centering->Simple Sequence"
446440,"random.seed = 1234

gr = np.arange(0, 1.05, 0.05)
res = []

for g in gr:
    res.append(kFoldValidation(srch_train, proccessData, RFDropOut, [g, {'n_estimators':30, 'n_jobs':4}]))
    
plt.figure(figsize=(12,8))
plt.plot(gr, res)","gr = np.arange(100, 1100, 100)
res = []

for g in gr:
    res.append(kFoldValidation(srch_train, proccessData, XGB,
                               [0, {'max_depth':2, 'n_estimators':g, 'learning_rate':0.05}]))
    
plt.figure(figsize=(12,8))
plt.plot(gr, res)","import numpy as np
def chess(n):
    return np.array([[(i+j)%2 for i in range(n)] for j in range(n)])

print(chess(8))",,Test->Job test->Code->Brief exploratory analysis->XGBoost->Tuning->Random Forest with Drop Out,Test->Job test->Code->Brief exploratory analysis->XGBoost->Tuning->XGBoost,"23Rd May, 2018(Lab)->LAB Assignments 23rd May->LA-34. Chess Pattern"
417291,"widgets.HBox([widgets.Label(value=""The $m$ in $E=mc^2$:""), widgets.FloatSlider()])","from ipywidgets import HBox, Label, IntSlider

HBox([Label('A too long description'), IntSlider()])","df.at[dates[0],'A'] = 0
df",,"1Er Widget->lista de widget->Label
Widget List",Widget Styling->Layout and Styling of Jupyter widgets->The `layout` attribute.->Description,Ten Minutes To Pandas->10 Minutes to pandas->Object Creation->Setting
489595,"# Main training loop
for epoch in range(nr_epochs):
    print('Epoch {}'.format(epoch+1))
    # training
    tra_losses = []
    tra_accs = []
    print('training...')
    for b in tqdm_notebook(range(0, nr_iterations_per_epoch), leave=False):
        X, Y = batch_ex_train.get_random_batch_balanced()     
        loss, accuracy = train_fn(X.astype(np.float32), Y.astype(np.float32))
        tra_losses.append(loss)
        tra_accs.append(accuracy)
    tra_loss_lst.append(np.mean(tra_losses))
    tra_acc_lst.append(np.mean(tra_accs))
            
    # validation
    val_losses = []
    val_accs = []
    print('validation...')
    for b in tqdm_notebook(range(0, nr_validation_samples, validation_batch_size), leave=False):
        X = validation_X[b:min(b+validation_batch_size, nr_validation_samples)]
        Y = validation_Y[b:min(b+validation_batch_size, nr_validation_samples)]
        loss, accuracy = validation_fn(X.astype(np.float32), Y.astype(np.float32))
        print loss
        val_losses.append(loss)
        val_accs.append(accuracy)
    val_loss_lst.append(np.mean(val_losses))
    val_acc_lst.append(np.mean(val_accs))   
    #continue
    if np.mean(val_accs) > best_val_acc:
        best_val_acc = np.mean(val_accs)
        # save network
        params = L.get_all_param_values(network)
        np.savez(os.path.join('./', network_name+'.npz'), params=params)

    # plot learning curves
    fig = plt.figure(figsize=(10, 5))
    tra_loss_plt, = plt.plot(range(len(tra_loss_lst)), tra_loss_lst, 'b')
    val_loss_plt, = plt.plot(range(len(val_loss_lst)), val_loss_lst, 'g')
    tra_acc_plt, = plt.plot(range(len(tra_acc_lst)), tra_acc_lst, 'm')
    val_acc_plt, = plt.plot(range(len(val_acc_lst)), val_acc_lst, 'r')
    plt.xlabel('epoch')
    plt.ylabel('loss')
    plt.legend([tra_loss_plt, val_loss_plt, tra_acc_plt, val_acc_plt], 
                ['training loss', 'validation loss', 'training accuracy', 'validation accuracy'],
                loc='center left', bbox_to_anchor=(1, 0.5))
    plt.title('Best validation accuracy = {:.2f}%'.format(100. * best_val_acc))
    display.clear_output(wait=True)
    display.display(plt.gcf())
    time.sleep(.2)","### Replace None with your code ###

def train_network(network, x_training, y_training, x_validation, y_validation, n_epoch, batch_size, network_filepath):

    # lists where we will be storing values during training, for visualization purposes
    tra_losses = [] # list for training loss
    tra_accs = [] # list for training accuracy
    val_losses = [] # list for validation loss
    val_accs = [] # list for validation accuracy

    # we want to save the parameters that give the best performance on the validation set
    # therefore, we store the best validation accuracy, and save the parameters to disk
    best_validation_accuracy = 0 # best validation accuracy

    for epoch in range(1, n_epoch):
        st = time.time()

        # Train your network
        results = network.fit(x_training, y_training, epochs=epoch, batch_size=batch_size, verbose=1)

        # Get training loss and accuracy
        training_loss = results.history['loss']
        training_accuracy = results.history['acc']

        # Add to list
        tra_losses.append(training_loss[-1]) # Changed training_loss to training_loss[-1]
        tra_accs.append(training_accuracy[-1]) # Changed training_accuracy to training_accuracy[-1]

        # Evaluate performance (loss and accuracy) on validation set
        scores = network.evaluate(x_validation, y_validation, batch_size=batch_size)     
        validation_loss = scores[0]
        validation_accuracy = scores[1]

        # Add to list
        val_losses.append(validation_loss)
        val_accs.append(validation_accuracy)

        # (Possibly) update best validation accuracy and save the network
        if validation_accuracy > best_validation_accuracy:
            best_validation_accuracy = validation_accuracy
            network.save(network_filepath)

        # Visualization of the learning curves
        fig = plt.figure(figsize=(10, 5))
        tra_loss_plt, = plt.plot(range(len(tra_losses)), tra_losses, 'b')
        tra_accs_plt, = plt.plot(range(len(tra_accs)), tra_accs, 'c')
        val_loss_plt, = plt.plot(range(len(val_losses)), val_losses, 'g')
        val_acc_plt, = plt.plot(range(len(val_accs)), val_accs, 'r')
        plt.xlabel('epoch')
        plt.ylabel('loss')
        plt.legend([tra_loss_plt, tra_accs_plt, val_loss_plt, val_acc_plt], 
                  ['training loss', 'training accuracy', 'validation loss', 'validation accuracy'],
                  loc='center left', bbox_to_anchor=(1, 0.5))
        plt.title('Best validation accuracy = {:.2f}%'.format(100. * best_validation_accuracy))
        display.clear_output(wait=True)
        display.display(plt.gcf())
        time.sleep(.2)","djiadf = pd.read_csv('Class05_DJIA_data.csv',parse_dates=[0])
print(djiadf.dtypes)
djiadf.head()",,Assignment 7->Vessel segmentation in retina fundus images (revisited)->Main training loop,Assignment 5->Task 2: Train your network,Class05->Class 05->Subsetting Data
168876,"import community_louvain
partition = community_louvain.best_partition(G.to_undirected())",partitions = community_louvain.best_partition(U),"from sklearn.model_selection import GridSearchCV
parameters = {'C':[i/10 for i in range(1,10)]}
gridsearch = GridSearchCV(svc,param_grid = parameters,scoring='accuracy',cv=10)
gridsearch = gridsearch.fit(X_train, y_train) 
best_accuracy = gridsearch.best_score_
best_paramters = gridsearch.best_params_",,"3-><span style=""color:navy""> Alternative 2 : Classifying articles using Community Louvain <span>->** Importing corresponding package and generating the clusters out of the best partition of the Graph**
3-><span style=""color:navy""> Alternative 2 : Classifying articles using Community Louvain <span>->**Creating cluster dictionary in order to use article ids as keys**",Project2 Final->Communities Within  the User Nodes,Kaggle Ecommerce Challenge->Kaggle ecommerce challenge (Improvisation)->(1) Linear Support Vector Classifier with TF IDF Vectorized data->Let's perform hyperparameter tuning for the penalty factor of support vector classfier and then use the best penalty factor (C) for our prediction
486108,"# Function for calculating a matrix element of the dipole-dipole interaction operator
def getDipoleDipole(state_f, state_i, distance):
    q = state_f.getM()-state_i.getM()
    
    if q[0] == 0 and q[1] == 0:
        prefactor = -2
    elif q[0] == 1 and q[1] == -1:
        prefactor = -1
    elif q[0] == -1 and q[1] == 1:
        prefactor = -1
    else:
        return 0
    
    return prefactor * pi.coulombs_constant / distance**3 * \
        cache.getElectricDipole(state_f.getFirstState(), state_i.getFirstState()) * \
        cache.getElectricDipole(state_f.getSecondState(), state_i.getSecondState()) 

# Define Rydberg states
state_i = pi.StateTwo([""Rb"", ""Rb""], [61, 61], [0, 1], [0.5, 0.5], [0.5, 0.5])
state_f = pi.StateTwo([""Rb"", ""Rb""], [61, 61], [1, 0], [0.5, 0.5], [0.5, 0.5])

# Get the matrix element at an interatomic distance of 10 um
distance = 10 # um
matrixelement = getDipoleDipole(state_f, state_i, distance)
print(""The matrix element has the value {} GHz."".format(matrixelement))","state = pi.StateTwo([""Rb"", ""Cs""], [61, 60], [0, 1], [0.5, 1.5], [-0.5, 1.5])","## Create a view that consists of the year and fatalities
df1 = df[['year','fatalities_total']]
#Taking all values except null
df_1 = df1[df1.fatalities_total != 'NULL']
df_1['fatalities_total']=df_1['fatalities_total'].astype(int)


# Create Histograms bin   |   value
by_year = df_1.groupby('year')
df_by_decade = by_year.sum()['fatalities_total']
df_by_decade.columns = ['year', 'fatalities_total']

print len(df_by_decade)
df_by_decade[:10]",,Introduction->Introduction to the pairinteraction Library->Application 2: Matrix Elements,Introduction->Introduction to the pairinteraction Library->Application 5: Effective Hamiltonians,Part 3- Aircrash Analytics- Data Mining->General Group By Logic
23733,df['FamilySize'] = df['SibSp'] + df['Parch'] + 1,"'Total speeches: %d, at rate of - speech every %d days' %  (df.shape[0],  (10*365.) / df.shape[0])","grid_searchXGB = grid_searchXGB.fit(X_train, y_train)",,Titanic->Feature Engineering->Feature Engineering/ FamilySize,"Consuming-Government-Data->PM Speeches->So, how many were there?",Predict Criminals
224646,"RGB_data=np.load(""rgb_data.npy"")
RGB_flatten_data=RGB_data.reshape(30602,1080,3)
R_data=RGB_flatten_data[:,:,0]
G_data=RGB_flatten_data[:,:,1]
B_data=RGB_flatten_data[:,:,2]","# Composite the two RGB images by finding the maximum R, G, and B value between the two
# Then create the new RGB and colorTuple
composite_R = np.maximum(R, TC_R)
composite_G = np.maximum(G, TC_G_true)
composite_B = np.maximum(B, TC_B)

composite_RGB = np.dstack([composite_R, composite_G, composite_B])

# Create a color tuple for pcolormesh

# Don't use the last column of the RGB array or else the image will be scrambled!
# This is the strange nature of pcolormesh.
composite_rgb = composite_RGB[:,:-1,:]

# Flatten the array, becuase that's what pcolormesh wants.
composite_colorTuple = composite_rgb.reshape((composite_rgb.shape[0] * composite_rgb.shape[1]), 3)

# Adding an alpha channel will plot faster, according to Stack Overflow. Not sure why.
composite_colorTuple = np.insert(composite_colorTuple, 3, 1.0, axis=1)","xs2 = np.linspace(stats.lognorm.ppf(0.01, 0.7, loc=-.1), stats.lognorm.ppf(0.99, 0.7, loc=-.1), 150)

lognorm = stats.lognorm.pdf(xs2, 0.4)

Z = lognorm/2 + lognorm[::-1]

#Your code goes here",,Vae->Import KIIT data,Mapping Goes16 Fire Temperature->Fire Temperature/TrueColor Composite,Notebook->Exercise 3: Skew and Normality
395162,"pred_accuracy_decesion_tree_unscaled = metrics.accuracy_score(y_test_unscaled, y_pred_decesion_tree_unscaled)
print (pred_accuracy_decesion_tree_unscaled)","y_pred = svc.predict(x_validation)

y_true = y_validation

svc.score(x_validation,y_validation)

accuracy = metric.accuracy_score(y_true, y_pred, normalize = False )

accuracy = (accuracy/x_validation.shape[0])*100
print(accuracy)","import lasagne
from lasagne import layers
from lasagne.updates import nesterov_momentum
from nolearn.lasagne import NeuralNet




net1 = NeuralNet(
    layers=[('input', layers.InputLayer),
            ('hidden', layers.DenseLayer),
            #('hidden2', layers.DenseLayer),
            ('output', layers.DenseLayer),
            ],
    # layer parameters:
    input_shape=(None, 65),
    hidden_num_units=1,  # number of units in 'hidden' layer
     hidden_nonlinearity = lasagne.nonlinearities.linear,
    #hidden2_num_units=30,  # number of units in 'hidden' layer

    output_nonlinearity=lasagne.nonlinearities.sigmoid,
    output_num_units=65, 

    # optimization method:
    update=nesterov_momentum,
    update_learning_rate=.01,
    update_momentum=0.9,

    max_epochs=100,
    verbose=1,
     regression=True,
    objective_loss_function = lasagne.objectives.squared_error
    )

    # Train the network
print(""training..."")
data = np.array(hcad).astype(float32)

net1.fit(data, data)

# # Try the network on new data
# print(""Feature vector (100-110): %s"" % data['X_test'][0])
# print(""Label: %s"" % str(data['y_test'][0]))
# print(""Predicted: %s"" % str(net1.predict([data['X_test'][0]])))",,Assignment 2->Assignment 2 - Exploratory Data Analysis and Classification on Diabetes Dataset->Unscaled decesion tree,Bank Marketing With Svm->LINEAR MODEL->Calculating the Accuracy,Hcad 1 Dimension Exploration-Checkpoint
178374,twitter_archive_final.info(),twitter_archive_final.head(),"#allDataClassifInv = getAllDataClassif(getInvertedCriteria(allData, totalTimesCriteria + completionTimesCriteria))
allDataClassifInv = getAllDataClassif(getInvertedCriteria(allData, totalTimesCriteria + completionTimesCriteria))
scoresInv, standardScalerInv, modelInv, featuresInv, targetInv, unscaledFeaturesInv = getFeaturesTarget(allDataClassifInv)",,Wrangle Act->Assess->Test,Wrangle Act->Assess->Test,Classification->Can the score of a player be predicted with their RedMetrics data?
290090,check_duplicates(sport_news_df),check_duplicates(sport_news_df),"# Getting the solution and cost from the largest component of the optimal quantum state

max_value = max(data.values())  # maximum value
max_keys = [k for k, v in data.items() if v == max_value] # getting all keys containing the `maximum`

x_quantum=np.zeros(n)
for bit in range(n):
    if max_keys[0][bit]=='1':
        x_quantum[bit]=1
        
best_cost_quantum = 0
for i in range(n):
    for j in range(n):
        best_cost_quantum+= w[i,j]*x_quantum[i]*(1-x_quantum[j])
        
        
# Plot the quantum solution
colors = []
for i in range(n):
    if x_quantum[i] == 0:
        colors.append('r')
    else:
        colors.append('b')
nx.draw_networkx(G, node_color=colors, node_size=600, alpha = .8)

print('Best solution from the quantum optimization is = ' +str(x_quantum)+ ' with cost = ' + str(best_cost_quantum))",,"Preprocessing->Save datasets->- build sport news dataframe (channel ""esportes"")","Preprocessing->Save datasets->- build sport news dataframe (channel ""esportes"")",Classical Optimization->MaxCut on 4 Qubits->Running it on quantum computer
154584,"names = ['viridis', 'jet', 'rainbow', 'spring', 'hsv']
fig, axs = plt.subplots(1, len(names), figsize=(20, 4), sharex=True, sharey=True)
x = np.linspace(0, 1, 100)
y = x + np.random.randn(len(x)) * .2
for name, ax in zip(names, axs):    
    ax.scatter(x, y, c='k', s=80, alpha=.2)
    ax.scatter(x, makeitpop(y, name, 40), c=y, s=40, alpha=.8, cmap=plt.get_cmap(name))
    ax.set(title=name)","fig = plt.gcf()
ax= fig.add_subplot(111, projection='3d')
x = np.random.randn(50)
y = np.random.randn(50)
z = np.random.randn(50)
size = np.random.randn(50)*500
size = size.astype(int)

colors = np.random.randn(len(x))
colors2 = np.arange(len(x))

s1 = ax.scatter(x, y, z, c=colors, s=size)
s1 = ax.scatter(x*1.5, y*1.5, z, c=colors2, s=size)","pdat[""rounded_AVG""] = np.array([round(a,-1) for a in pdat[""AVG""].values])",,Demo->Warping 1-dimensional data!,Matplotlib Note-Checkpoint->Plot in 3D->scatter point,Lecture-Regression-Prediction
54879,"drug_to_drop  = ['Drug_999_IC50', 'Drug_1047_IC50', 'Drug_1049_IC50',
                'Drug_1050_IC50', 'Drug_1052_IC50', 'Drug_1053_IC50']
dummy = ic50.drop_drugs(drug_to_drop)
data = ic50.hist()","# The n column is dropped because do not measure the drug use on a percentage scale. 
drug_only = drug.drop(['n'], axis=1)","fig, ax = plt.subplots(1, 1)

__=ax.hist(np.log10(milky_way_centrals['stellar_mass']), 
           bins = np.linspace(9.5, 11.25, 50), normed=True)",,2->Reading data sets->IC50,"Project2 Sat Eda->6.3.3->7.2 Do a high-level, initial overview of the data->Before doing so, we must separate the data points into two groups: 1) Drug use and 2) Drug Frequency","Working With Sims Noanswers->Working with Simulations->What is the stellar mass of a Milky Way halo?
Halotools Kitp Demo->Halotools Demo->What is the stellar mass of a Milky Way halo?"
331326,"def test(model):
    """"""
        Test the model's accuracy
        Inputs:
            None
        Outputs: 
            Prints the test output results
    """"""
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()
    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))","def test(verbose=True):
    model.eval()
    test_loss = 0
    correct = 0
    for data, target in test_loader:
        if USE_CUDA: 
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        #test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss
        test_loss += torch.nn.BCELoss()(torch.exp(output), target, size_average=False).data[0] # sum up batch loss
        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    if verbose:
        print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
            test_loss, correct, len(test_loader.dataset),
            100. * correct / len(test_loader.dataset)))
    return [float(test_loss), correct]","def pRF_size(x, y):
  return 2.0*np.log(np.sqrt(np.e+x**2+y**2))

def apply_pRF(par): 
  x = par[0]
  y = par[1]
  model = pRF_model(x, y, pRF_size(x, y), pos)
  return stim_slow*model",,Model->How will we test the model,Kaggle-Porto Seguro Nn V01->Model Building->Second approach : embedded neural networks->One Hot Encoding,P Rf Estimation Simulation Colab-Checkpoint->1. Simulation Data->1.3. Applying receptive fields
304465,"df[(df.timestamp < end)&(df.timestamp > start)].count()['email'] / float(df.count()['email']), 'answer'","df.sort_values(by=[""Store Number"", ""Date""], inplace=True)

start_date = pd.Timestamp(""20150101"")
end_date = pd.Timestamp(""20151231"")

sales_mask_2015 = (df['Date'] >= start_date) & (df['Date'] <= end_date)
sales_2015_df = df[sales_mask_2015]","# execute this cell first to import all necessary libraries

from MOU import MOU, make_rnd_connectivity, classfy
from scipy.io import loadmat
import numpy as np
from scipy.stats import pearsonr
from scipy.linalg import logm, expm
import matplotlib.pyplot as plt
import seaborn as sns


sns.set_context('talk')",,Pandas Tutorial->Pandas Tutorial Information->Question 1,Project 3 V1-Checkpoint->Project 3->Filter the Data->Calculating Sales per Store for 2015,Mou Bayes V Sgradient->Bayesian estimation for multivariate Ornstein-Uhlenbeck (MOU) model
51857,"import operator

sorted(city_temps, key=operator.itemgetter('city'))","def oper_function(x,y):
    summ = x+y
    product = x*y
#    return summ, product    #comment out the return function

add, multiplication = oper_function(1,2)             #assign the value of summ to add, product to multiplication
print('sum = ', add, 'product = ', multiplication)","# ""e"" or ""E"" is allowed for scientific notation

x = 1e6
print(x, type(x))",,2->Functions can call other functions->Aside: Good coding style for accessing attributes of dictionaries,Functions (20180821)->1.0 Basic Facts About Functions,Supapyt-Introduction To Python->Assignment to variables
285582,"from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Convolution2D, MaxPooling2D
from keras.callbacks import EarlyStopping
from keras.utils import to_categorical","from keras.layers.convolutional import Convolution2D, MaxPooling2D
from keras.layers.core import Activation, Dense, Dropout, Flatten","model = load_saved_rnn_model()

# predict on held-out test data
y_pred = model.predict(x_test_sequences_padded)",,Deep Learning->Deep Learning Challenge->Preprocess data->Normalization,"Cnn Tutorial With Keras->Load Data - MNIST Dataset->Create Model
Cnn Tutorial With Keras-Checkpoint->Create Model",Toxic Comments Classification->Toxic Comment Classification->Prediction on Test Data using the Selected Model
107417,rows[-1],"# Retuns ""MANDANTE"" if the home team wins the macth,
# ""VISITANTE"" if visitor wins or ""EMPATE"" in case of tied match
def resultado(row):
    if (row[""gols_mandante""] > row[""gols_visitante""]):
        return ""MANDANTE""
    elif (row[""gols_mandante""] < row[""gols_visitante""]):
        return ""VISITANTE""
    else:
        return ""EMPATE""","# Compute the global mean and global standard deviation: global_mean, global_std
global_mean = cars.mean()
global_std = cars.std()

# Filter the US population from the origin column: us
us = cars[cars['origin'] == 'US']

# Compute the US mean and US standard deviation: us_mean, us_std
us_mean = us.mean()
us_std = us.std()

# Print the differences
print(us_mean - global_mean)
print('----------')
print(us_std - global_std)",,"1
Capston Project W3 Toronto Part 1->Capstone Project - Coursera IBM Data Science->Read Data from Wiki",Step 1-Match Results->2. Data Parsing->2.5 Create a result column->2.5.1 Defines a function to parse score,Pandas Foundations->Case Study - Sunlight in Austin->Separating populations with Boolean indexing->Separate and summarize
159066,"# Use this, and more code cells, to explore your data. Don't forget to add
#   Markdown cells to document your observations and findings.

figure=plt.figure()
ax=figure.add_subplot(111)
ax.set_title(""Diabetes "",fontweight='bold',fontsize=14)
ax.set_xlabel('Patients')
df_hospital['Diabetes'].plot(kind='hist');","fig = plt.figure()
ax = fig.add_subplot(111)
lines = ax.plot([1, 2, 3])
text = ax.set_xlabel(""X"")","decoder = rosenbrock_decoder
obj_fun = rosenbrock_OF

ga_options = dict(
    num_eras=100, population_size=40, chromosome_length=20, 
    crossover_probability=0.35, mutation_probability=0.04
)

plot_ga(obj_fun, decoder, min_or_max=MIN, ga_opts=ga_options, 
        title=""Rosenbrock Function (minimization)"")",,Investigate A Dataset->Performing Query on No Show Appointment dataset,3->Putting it all together->Plotting Time-Series->Spines,Genetic Algorithms->Benchmark Performance->Rosenbrock Function
37790,"from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(token_pattern=r'\b\w+\b')
train_matrix = vectorizer.fit_transform(X_train['review_clean'])
## We can't use the previous fit_transform because we have to use the same words that train_matrix
test_matrix = vectorizer.transform(X_test['review_clean'])","from sklearn.feature_extraction.text import CountVectorizer

# Instantiate the CountVectorizer method
count_vector = CountVectorizer(stop_words=""english"", token_pattern=u'(?u)\\b\\w\\w+\\b')

# Fit the training data and then return the matrix
training_data = count_vector.fit_transform(X_train)

# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()
testing_data = count_vector.transform(X_test)","def compute_U_D_M_R(mu,h,r):
    U=np.exp(mu+h)
    D=np.exp(mu-h)
    M=np.exp(mu)
    R=np.exp(r)
    return U,D,M,R
def compute_q_Q(p_Q,U,D,M,R):
    q_Q=np.divide(R-M,D-M)+np.dot(p_Q,np.divide(U-M,D-M))
    return q_Q",,Logistic Regression->Facial Emotion->Sentiment Model from scratch,Bag->Project Title : Author Labeling by text classification->Applying Bag of Words processing to our dataset,Pricing Trinomial Condor->Pricing options in the Trinomial Model->Computing Equivalent Martingale Measure in Trinomial Model
116837,"softmax_data = [0.7, 0.2, 0.1]
one_hot_data = [1.0, 0.0, 0.0]

softmax = tf.placeholder(tf.float32)
one_hot = tf.placeholder(tf.float32)

cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))

with tf.Session() as sess:
    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, one_hot: one_hot_data}))","softmax_data = [0.7, 0.2, 0.1]
one_hot_data = [1.0, 0.0, 0.0]

softmax = tf.placeholder(tf.float32)
one_hot = tf.placeholder(tf.float32)

# cross entropy = - sum(y * log(y-hat))
cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))

with tf.Session() as sess:
    print(sess.run(cross_entropy, feed_dict = {softmax: softmax_data, one_hot: one_hot_data}))",df.truncate(after='5/3/2014'),,1->Intro to TensorFlow->Linear Classifier for MNIST dataset->Cross Entropy Cost Function,How Softmax Works-Checkpoint->Understanding Cross entropy,"Time Series-Checkpoint->Pandas Time Series->Slizing with dates->Truncation
Pandas Time Series Basics
Pandas Time Series Basics->Truncation observations after May 2nd 2014"
358364,ppmi.nearest_neighbors('blue'),ppmi.nearest_neighbors('fire'),"frame[0, :].shape[0] * 4, frame[0, :].view(dtype=dtype).view(dtype=np.uint8).shape",,Py Dsm Meetup Presentation->Practical introduction to distributional semantic models using PyDSM->Common operations,Query Expansion Using Distributional Semantics->Query Expansion using Distributional Semantics->Non-weighted vs. PPMI-weighted,Capture Numpy
154261,"fig, ax = plt.subplots(10, 10, figsize=(20, 20))
for i, axi in enumerate(ax.flat):
    axi.imshow(faces.images[i], cmap='bone')
    axi.set(xticks=[], yticks=[], xlabel=faces.target_names[faces.target[i]])","%matplotlib inline
fig, ax = plt.subplots(4, 8, figsize=(12, 10))
for i, axi in enumerate(ax.flat):
    axi.imshow(figures.images[i], cmap=""bone"")
    axi.set(xticks=[], yticks=[], xlabel=figures.target_names[figures.target[i]])","# Egg 

Cp_egg = 2.68 # KJ/Kg K
k_egg = 0.40

# Water for Heating

mu_Hwater = Visco_water((T_h3 + T_h4)/2)
Cp_Hwater = 4.187
k_Hwater = 0.4
rho_Hwater = density_water((T_h3 + T_h4)/2)

mu_bHwater = Visco_water((T_h3 + T_h4)/2)
mu_wHwater = Visco_water((T_h3 + T_h4)/2 - 2)

# Water for Cooling

mu_Cwater = Visco_water((T_c1 + T_c2)/2)
Cp_Cwater = 4.187
k_Cwater = 0.397
rho_Cwater = density_water((T_c1 + T_c2)/2)

mu_bCwater = Visco_water((T_c1 + T_c2)/2)
mu_wCwater = Visco_water((T_c1 + T_c2)/2 + 2)

# Stainless Steel AISI 316

Kw = 16.3

# Fouling Factors

R_fh = 0.0002
R_fc = 0.0005",,Demo->Example: Faces,Exam Fall 17->Exam->Training an SVM Classifier->15. Plot again the confusion matrix of model. Are there any changes in the best fits?,Heat Exchangers->Properties
281576,"mask = np.zeros(image.shape, dtype = ""uint8"")
(cX, cY) = (image.shape[1] // 2, image.shape[0] // 2)
cv2.rectangle(mask, (cX - 75, cY - 75), (cX + 75 , cY + 75), (255, 255, 255), -1)
plt.imshow(mask)","labeled_img = np.zeros(img_open_close.shape + (3, ), np.uint8)
colors = ((0,0,255),(0,255,0),(255,0,0),(0,255,255),(255,0,255))
pnts_list = []
mask_list = []
for ind, contour in enumerate(contours):
    mask = np.zeros(img_open_close.shape, np.uint8)
    cv2.drawContours(mask, [contour], 0, 255, -1, 8)
    pixel_points = cv2.findNonZero(mask)#(x,y)

    labeled_img[mask == 255] = colors[ind]
    pnts_list.append(pixel_points)
    mask_list.append(mask)

plt.subplot(121); plt.imshow(labeled_img); plt.title('Labeled Image')
plt.subplot(122); plt.bar(range(len(pnts_list)), [len(pnts) for pnts in pnts_list]); plt.title('Pixels per blob');","strength.sort_values().plot.barh(figsize=(4,8))",,6->Masking->Masking allows us to focus only on parts of an image that interest us.,Blob Analysis->Morphology And Blob Analysis->Blob Analysis->Filling Blobs,Pandas Tidy->Question: Days of Rest->Data Read
200789,"# Load dataset
filename = 'Data/SCDB_2017_01_justiceCentered_LegalProvision.csv'
raw_dataset = read_csv(filename,encoding='mac_roman',low_memory=False)","# Load dataset
filename = 'Data/SCDB_2017_01_justiceCentered_LegalProvision.csv'
raw_dataset = read_csv(filename,encoding='mac_roman',low_memory=False)","import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from StringIO import StringIO",,Case Outcome Prediction->Treatment of vote->Treatment of lawType,Case Outcome Prediction->Treatment of vote->Treatment of lawType,Lec 03 Introduction To Hypothesis Testing
40562,"import csv
f= open(""guns.csv"", ""r"")
file= csv.reader(f)","import csv
f = open('askreddit_2015.csv','r')
file = csv.reader(f)
posts_with_header = list(file)
posts_header = posts_with_header[0]
posts = posts_with_header[1:]
posts_header","# split into train and test set
train = data[data['year'] < 2017]
test = data[data['year'] >= 2017]",,Basics,5 Askreddit 2015 ( Regex & Datetime )->Regular Expressions,2->Data Cleaning->3. Split into training and test dataset
302347,"pdata.Comment = [sanitize_wo_stopwords(x[1: -1]) for x in pdata.Comment]
# full_df.Comment = [sanitize_with_words(x[1: -1]) for x in full_df.Comment]
# full_df.Comment = [sanitize_with_lemma(x[1: -1]) for x in full_df.Comment]


# pdata.Comment = [x[1: -1] for x in pdata.Comment]","#Get rid of refs which represent a role as a player
def clean_roles(wikis):
    wo_basketball_wikis = []
    for w in wikis :
        #Add to the list only if none of the keywords are contained
        if any([x in w for x in roles_keywords]):
            continue
        else: 
            wo_basketball_wikis.append(w)
    return wo_basketball_wikis",early_stopper??,,Presentation,Ntds-Project->2. Clustering methods,Wine-Quality->decision trees->simple tree->plot the tree
286752,"titanic_df['Deck'] = titanic_df.Cabin.str[:1]
titanic_df.head(2)","#4
a = len(titan[(titan.Age >= 18) & (titan.Parch > 0)]) #the amount of the parents (vague charachteristic)
b = len(titan[(titan.Age < 18) & (titan.Parch == 0)]) #the amount of the kids aboard
c = titan[(titan.Sex == 'male') & (titan.SibSp > 1)].SibSp.sum() #the amount of siblings of male people aboard
d = titan[(titan.Sex == 'female') & (titan.SibSp > 1)].SibSp.sum() #the amount of siblings of female people aboard
#print (a,b,c,d)","inp = input(""Enter a string\n"")

inot = inp.find('not')          # index of 'not' word

ipoor = inp.find('poor')         # index of 'poor' word

if (inot < ipoor):             # if 'not' occurs before 'poor'
    
    new = inp.replace(inp[inot:ipoor+4],'good')                # replace the phrase from not to poor with good
    print(new)

else:
    print(""String is not good"")",,"Decision Trees->3) Hyper parameter tuning->3.1) GridSearchCV for cross validation (cv) = 5 and different hyper parameters->3.2.3) Deck
Titanic Logistic Regression->1) Data Preparation->2) Logistic Regression",Lab1,Strings->Strings->Additional Exercises->(B)
11369,"raw_artwork_df = pd.read_csv(os.path.join(data_folder, ""artworks.csv"") , names=[
        'title', 
        'artist_name', 
        'constituent_id', 
        'artist_bio', 
        'nationality', 
        'begin_date', 
        'end_date', 
        'gender', 
        'date', 
        'medium', 
        'dimensions', 
        'credit_line', 
        'accession_number', 
        'classification', 
        'department', 
        'date_acquired', 
        'cataloged', 
        'object_id', 
        'url', 
        'thumbnail_url', 
        'circumference_cm', 
        'depth_cm', 
        'diameter_cm', 
        'height_cm', 
        'length_cm', 
        'weight_kg', 
        'width_cm', 
        'seat_height_cm', 
        'durcation_seconds']).drop(0, axis=0)

# get rows of artists from artwork_df
artist_df = raw_artwork_df[['constituent_id', 'artist_name', 'nationality', 'gender']].drop_duplicates()

# run split_rows on artist_df to split rows where multiple artists are in one row
artist_df = util.split_rows(artist_df)

# remove parentheses from nationality and gender 
artist_df['nationality'] = artist_df['nationality'].apply(lambda x: x.replace('(', '').replace(')', ''))
artist_df['gender'] = artist_df['gender'].apply(lambda x: x.replace('(', '').replace(')', ''))

# replace empty strings with NaNs
artist_df = artist_df.replace('', np.nan, regex=True)

# remove if artist is_entity
artist_df = artist_df[artist_df['artist_name'].apply(util.get_is_not_entity_from_full_name)]

# show data
display(artist_df.head())
print(artist_df.shape)","artwork_df = raw_artwork_df[[
        'constituent_id',
        'nationality',
        'date_acquired']]

artwork_df[['male', 'female', 'white', 'black', 'asian', 'aian', 'mix', 'hispanic']] = \
    artwork_df['constituent_id'].apply(util.join_race_and_gender, args=(artist_df,)).apply(pd.Series)

display(artwork_df.head())
print(artwork_df.shape)","def pf_stats(df):    
    
    y = pf_return(df)
    x = pf_stdev(df)
    rf = risk_free.mean()
    var = pf_var(df)
    sharpe = (y-rf)/x
    
    print(""Expected Portfolio Stats\n"") + 40 * ""-"" + ""\n""
    print(""Annual Return = %3.2f"") %y
    print (""Risk-free rate = %3.4f"") %rf
    print(""Variance = %3.4f"") %var
    print(""Standard deviation = %3.4f"") %x
    print ('Sharpe Ratio = %3.2f') %sharpe
    
pf_stats(n4_log)",,Analysis->An Analysis of the MoMA's Collection by Race and Gender->Load artworks dataset and transform into an artists dataset,Analysis->An Analysis of the MoMA's Collection by Race and Gender->Exploring the Artworks dataset,Nickolas4 Modern Portfolio Theory->1.) Modern Portfolio Theory->Portfolio Stats
232497,"from scipy.stats import probplot
probplot(m2.resid, plot=plt)
plt.show()","from scipy import stats
# Constructing the probability plot
probplt = stats.probplot(df.temperature, plot=plt)
plt.show()","# helper function for labeling facilities as fellowship training sites 
def fellow_site_generator(df):
    if df[""Facility Name""] in fellow_sites:
        return True
    else: 
        return False

# add column to each entry indicating whether happened at institution where fellows work
CAD_cath_df['Fellows'] = CAD_cath_df.apply(fellow_site_generator, axis=1)",,Mini Project Linear Regression->Part 4: Comparing Models->Akaike Information Criterion (AIC),Inferential Statistics-Human Temperature->1. Is the distribution of body temperatures normal?,Sparcs->Sites where fellows work
203489,"periods = ['2015Q1', '2015Q2', '2015Q3', '2015Q4', '2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4']",period+2,"vidcap = cv2.VideoCapture('test_video.mp4')
success,image = vidcap.read()
count = 0
success = True
while success:
    success, image = vidcap.read()
    print('Read a new frame: ', success)
    cv2.imwrite(""./test_video_frames/frame%d.jpg"" % count, image)     # save frame as JPEG file
    count += 1",,Tyler Fording Suplari Data Science Project->Suplari Data Science Project->Variables,"T Ime Series Pandas Manipulate->Calculate one-period percent change->Alternatice,",Search And Classify->Video Processing->Extract frames from test video
301375,"plt.plot(india.column(""time""), india.column(""population_total""))
plt.title(""India's Population Growth"")
plt.xlabel(""Year"")
plt.ylabel(""Population (billions)"")","India_yearly_data.plot(title=""Indian Rupees per 1 EURO"",figsize=(10,5) , linewidth=3.2)
plt.ylabel(""Exchange Rate"")
plt.xlabel(""Average year statistics from year 2009 - 2018 "")","plt.xlim(0,50)
plt.vlines(10,0,55,lw=2, linestyles=""--"")
plt.xlabel('time')
plt.title('Births and deaths of our population, at $t=10$')
plot_lifetimes(observed_lifetimes, event_observed=observed)
print(""Observed lifetimes at time %d:\n""%(current_time), observed_lifetimes)",,Lesson1->Lesson 1: Why Data Science?->Population Growth and Fertility over Time,Data Analysis Currency Exchange->Task 4:->Line graph to analyse the Annual data of Indian Currency (INR),Kaplan->Censorship
142834,"sns.set_context(""notebook"", font_scale=2.5)
sns.pairplot(num_df[['MasVnrArea', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'SalePrice']].dropna(), size=5)","corrDim = house_train[['SalePrice','LotArea','MasVnrArea','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','YearBuilt','YearRemodAdd','gar_type', 'driveway']].corr()
corrDim","num_bureau_reports = new_features['NUM_BUREAU_REPORTS']
num_bureau_reports = num_bureau_reports.apply(lambda x: np.log(x + 1))
num_bureau_reports = np.nan_to_num(num_bureau_reports)

#visualize the new distribution
explore_numeric(num_bureau_reports)

log_transform_features.append('NUM_BUREAU_REPORTS')",,Eda->Numerical Variables->Missing Values,Data Exploration,Step 8 - Data Exploration - Pos Cash Balance->SUM_CREDIT_ACTIVE_CODE
241393,"# ASIDE: UGH so much repeated code, can we do better?
plt.subplots(figsize=(20, 5))
args = {'x':""SibSp"", 'y':""Age"", 'hue':""Survived"", 'data':df_imputed}
funcs_to_plot = [sns.boxplot,sns.violinplot,sns.swarmplot]
for i, plot_func in enumerate(funcs_to_plot):
    plt.subplot(1,3,i+1)
    plot_func(**args)
    
plt.show()","def update_line(i, line):
    angle = i * np.pi / 100
    line.set_data([-np.cos(angle), np.cos(angle)], [-np.sin(angle), np.sin(angle)])

fig = plt.figure(figsize=(5,5))
ax = plt.subplot(111, aspect='equal', xlim=(-1, 1), ylim=(-1, 1))
line, = plt.plot([], [], 'b')

animation.FuncAnimation(fig, update_line, fargs=[line], interval=50)

# Example using frames and repeat kewords
#animation.FuncAnimation(fig, update_line, fargs=[line], interval=50, frames=20, repeat=False)","fig = figure(figsize=(25,15))

ss_smooth = observe(ss)

locs = [(15180, 15290), (15145, 15195), (15968, 16035), (16750, 16845)]

F = .992
zpt = np.percentile(ssm,25)

for ix, loc in enumerate(locs):
    subplot(2,2,ix+1)
    #step(ll, ss*angs_to_mic/10, 'k', lw=1)
    step(ll, ss_smooth*angs_to_mic, 'r', lw=2)
    step(llm+16, (ssm-zpt)*13, 'b')

    axhline(0, color='k', lw=.5)
    axhline(600, color='y', lw=.5)
    
    xlim(loc[0]*F, loc[1]/F)
    ylim(-500,7000)
    grid(True)

legend([""Gc"", ""M""])",,03->Simplifying Plotting with Seaborn,Liang 4637887 Nb 14->Animiations,Irmos Sky Background-Checkpoint->Now convolve down the spectrum - Compare MOSFIRE to GIANO
