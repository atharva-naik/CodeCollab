{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b02733c",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset contains information on marketing newsletters/e-mail campaigns (e-mail offers sent to customers) and transaction level data from customers. The transactional data shows which offer customers responded to, and what the customer ended up buying. The data is presented as an Excel workbook containing two worksheets. Each worksheet contains a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df1c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_offers = pd.read_excel(\"./WineKMC.xlsx\", sheet_name=0)\n",
    "df_offers.columns = [\"offer_id\", \"campaign\", \"varietal\", \"min_qty\", \"discount\", \"origin\", \"past_peak\"]\n",
    "df_offers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49fad6e",
   "metadata": {},
   "source": [
    "We see that the first dataset contains information about each offer such as the month it is in effect and several attributes about the wine that the offer refers to: the variety, minimum quantity, discount, country of origin and whether or not it is past peak. The second dataset in the second worksheet contains transactional data -- which offer each customer responded to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transactions = pd.read_excel(\"./WineKMC.xlsx\", sheet_name=1)\n",
    "df_transactions.columns = [\"customer_name\", \"offer_id\"]\n",
    "df_transactions['n'] = 1\n",
    "df_transactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f7879",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4778f44a",
   "metadata": {},
   "source": [
    "We're trying to learn more about how our customers behave, so we can use their behavior (whether or not they purchased something based on an offer) as a way to group similar minded customers together. We can then study those groups to look for patterns and trends which can help us formulate future offers.\n",
    "\n",
    "The first thing we need is a way to compare customers. To do this, we're going to create a matrix that contains each customer and a 0/1 indicator for whether or not they responded to a given offer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26f511",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Checkup Exercise Set I</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Create a data frame where each row has the following columns (Use the pandas [`merge`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) and [`pivot_table`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html) functions for this purpose):\n",
    "<ul>\n",
    "<li> customer_name\n",
    "<li> One column for each offer, with a 1 if the customer responded to the offer\n",
    "</ul>\n",
    "<p>Make sure you also deal with any weird values such as `NaN`. Read the documentation to develop your solution.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your turn\n",
    "matrix=df_transactions.pivot_table(index=['customer_name'], columns=['offer_id'], values='n')\n",
    "matrix=matrix.fillna(0)\n",
    "matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acbeaee",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "Recall that in K-Means Clustering we want to *maximize* the distance between centroids and *minimize* the distance between data points and the respective centroid for the cluster they are in. True evaluation for unsupervised learning would require labeled data; however, we can use a variety of intuitive metrics to try to pick the number of clusters K. We will introduce two methods: the Elbow method, the Silhouette method and the gap statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5595d",
   "metadata": {},
   "source": [
    "### Choosing K: The Elbow Sum-of-Squares Method\n",
    "\n",
    "The first method looks at the sum-of-squares error in each cluster against $K$. We compute the distance from each data point to the center of the cluster (centroid) to which the data point was assigned. \n",
    "\n",
    "$$SS = \\sum_k \\sum_{x_i \\in C_k} \\sum_{x_j \\in C_k} \\left( x_i - x_j \\right)^2 = \\sum_k \\sum_{x_i \\in C_k} \\left( x_i - \\mu_k \\right)^2$$\n",
    "\n",
    "where $x_i$ is a point, $C_k$ represents cluster $k$ and $\\mu_k$ is the centroid for cluster $k$. We can plot SS vs. $K$ and choose the *elbow point* in the plot as the best value for $K$. The elbow point is the point at which the plot starts descending much more slowly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b47781",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Checkup Exercise Set II</h3>\n",
    "\n",
    "<p><b>Exercise:</b></p> \n",
    "<ul>\n",
    "<li> What values of $SS$ do you believe represent better clusterings? Why?\n",
    "<li> Create a numpy matrix `x_cols` with only the columns representing the offers (i.e. the 0/1 columns) \n",
    "<li> Write code that applies the [`KMeans`](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) clustering method from scikit-learn to this matrix. \n",
    "<li> Construct a plot showing $SS$ for each $K$ and pick $K$ using this plot. For simplicity, test $2 \\le K \\le 10$.\n",
    "<li> Make a bar chart showing the number of points in each cluster for k-means under the best $K$.\n",
    "<li> What challenges did you experience using the Elbow method to pick $K$?\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa57f7",
   "metadata": {},
   "source": [
    "Lower values of $SS$ indicated better clustering, as this means that the points are all closer to their centroids.  Individual points being closer to their centroids means that the points are closer to each other and thus better clustered.  However, we do not want to find the $K$ that gives the absolute minimum $SS$.  Taking $K=n$, the number of data points, would yield $SS=0$ because each point would be its own centroid in a cluster consisting of only itself, but that provides no insight whatsoever.  A potential way to solve this problem would be to use PCA to reduce the data to being 2 or 3 dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a59b6",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Checkup Exercise Set III</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Using the documentation for the `silhouette_score` function above, construct a series of silhouette plots like the ones in the article linked above.</p>\n",
    "\n",
    "<p><b>Exercise:</b> Compute the average silhouette score for each $K$ and plot it. What $K$ does the plot suggest we should choose? Does it differ from what we found using the Elbow method?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd7fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn.\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Create a dictionary to store K values and Silhouette scores\n",
    "Sil_scores={}\n",
    "\n",
    "# Create the clusters for each K value, fit them, and score them.\n",
    "for K in range(2,11):\n",
    "    Sil_scores[K]=silhouette_score(matrix,clusters[K-2].predict(matrix))\n",
    "\n",
    "Sil_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8646f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silhouette plots.\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "for K in range(2,11):\n",
    "    plt.plot()\n",
    "\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-.2, 1]\n",
    "    plt.xlim([-.2, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    plt.ylim([0, len(matrix) + (K + 1) * 10])\n",
    "    \n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(matrix,clusters[K-2].predict(matrix))\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(K):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[clusters[K-2].predict(matrix) == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.spectral(float(i) / K)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=Sil_scores[K], color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f3a0e",
   "metadata": {},
   "source": [
    "Based on the guide given above, none of the silhouette scores indicate a cluster that reveals any substantial structure.  However, the best score is dervied from taking $K=7$ with $K=4$ in a close second.\n",
    "\n",
    "Analyzing the plots, we are looking for a plot where each cluster has points that are above the average and each cluster has minimal fluctuation over its points.  Only $K=2$ and $K=5$ are close to satisfying the first condition, but both have large fluctuations within each cluster and points that are likely misclassified.  There is no clear good model based on these graphs.\n",
    "\n",
    "The fact that none of the plots indicate obvious good choices choics for $K$ is consistent with the fact that none of the averages for any $K$ value indicate significant structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9792e",
   "metadata": {},
   "source": [
    "### Choosing $K$: The Gap Statistic\n",
    "\n",
    "There is one last method worth covering for picking $K$, the so-called Gap statistic. The computation for the gap statistic builds on the sum-of-squares established in the Elbow method discussion, and compares it to the sum-of-squares of a \"null distribution,\" that is, a random set of points with no clustering. The estimate for the optimal number of clusters $K$ is the value for which $\\log{SS}$ falls the farthest below that of the reference distribution:\n",
    "\n",
    "$$G_k = E_n^*\\{\\log SS_k\\} - \\log SS_k$$\n",
    "\n",
    "In other words a good clustering yields a much larger difference between the reference distribution and the clustered data. The reference distribution is a Monte Carlo (randomization) procedure that constructs $B$ random distributions of points within the bounding box (limits) of the original data and then applies K-means to this synthetic distribution of data points.. $E_n^*\\{\\log SS_k\\}$ is just the average $SS_k$ over all $B$ replicates. We then compute the standard deviation $\\sigma_{SS}$ of the values of $SS_k$ computed from the $B$ replicates of the reference distribution and compute\n",
    "\n",
    "$$s_k = \\sqrt{1+1/B}\\sigma_{SS}$$\n",
    "\n",
    "Finally, we choose $K=k$ such that $G_k \\geq G_{k+1} - s_{k+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b4c5a",
   "metadata": {},
   "source": [
    "### Aside: Choosing $K$ when we Have Labels\n",
    "\n",
    "Unsupervised learning expects that we do not have the labels. In some situations, we may wish to cluster data that is labeled. Computing the optimal number of clusters is much easier if we have access to labels. There are several methods available. We will not go into the math or details since it is rare to have access to the labels, but we provide the names and references of these measures.\n",
    "\n",
    "* Adjusted Rand Index\n",
    "* Mutual Information\n",
    "* V-Measure\n",
    "* Fowlkesâ€“Mallows index\n",
    "\n",
    "See [this article](http://scikit-learn.org/stable/modules/clustering.html) for more information about these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88d894",
   "metadata": {},
   "source": [
    "## Visualizing Clusters using PCA\n",
    "\n",
    "How do we visualize clusters? If we only had two features, we could likely plot the data as is. But we have 100 data points each containing 32 features (dimensions). Principal Component Analysis (PCA) will help us reduce the dimensionality of our data from 32 to something lower. For a visualization on the coordinate plane, we will use 2 dimensions. In this exercise, we're going to use it to transform our multi-dimensional dataset into a 2 dimensional dataset.\n",
    "\n",
    "This is only one use of PCA for dimension reduction. We can also use PCA when we want to perform regression but we have a set of highly correlated variables. PCA untangles these correlations into a smaller number of features/predictors all of which are orthogonal (not correlated). PCA is also used to reduce a large set of variables into a much smaller one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0080726",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Checkup Exercise Set IV</h3>\n",
    "\n",
    "<p><b>Exercise:</b> Use PCA to plot your clusters:</p>\n",
    "\n",
    "<ul>\n",
    "<li> Use scikit-learn's [`PCA`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) function to reduce the dimensionality of your clustering data to 2 components\n",
    "<li> Create a data frame with the following fields:\n",
    "  <ul>\n",
    "  <li> customer name\n",
    "  <li> cluster id the customer belongs to\n",
    "  <li> the two PCA components (label them `x` and `y`)\n",
    "  </ul>\n",
    "<li> Plot a scatterplot of the `x` vs `y` columns\n",
    "<li> Color-code points differently based on cluster ID\n",
    "<li> How do the clusters look? \n",
    "<li> Based on what you see, what seems to be the best value for $K$? Moreover, which method of choosing $K$ seems to have produced the optimal result visually?\n",
    "</ul>\n",
    "\n",
    "<p><b>Exercise:</b> Now look at both the original raw data about the offers and transactions and look at the fitted clusters. Tell a story about the clusters in context of the original data. For example, do the clusters correspond to wine variants or something else interesting?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a dataframe of customers, the two principal components, and all predictions.\n",
    "pca=PCA(n_components=2)\n",
    "pca_df=pd.DataFrame(index=matrix.index)\n",
    "pca_df['x']=pca.fit_transform(matrix)[:,0]\n",
    "pca_df['y']=pca.fit_transform(matrix)[:,1]\n",
    "for K in predictions.columns.tolist():\n",
    "    pca_df[K]=predictions[K]\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74a4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each of these predictions.\n",
    "for K in range(2,11):\n",
    "    colors = cm.spectral(pca_df.iloc[:,K].astype(float) / K)\n",
    "    plt.scatter(pca_df.x,pca_df.y,c=colors,alpha=.5)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('Clustering for K=%i' %K)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab2d67",
   "metadata": {},
   "source": [
    "Based purely on the clustering graphs of the first two principals components, clusterings with $K\\geq4$ begin to have misclassifications.  Taking $K=3$ appears to be the optimal choice.  It identifies two arms in the data and puts the remaining mass in a single cluster.  This is consistent with the chosen $K$ in the Elbow method, but not with the Silhouette statistic.  Since none of the Silhouette statistics were significant anyway, this is not surprising."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da18e63f",
   "metadata": {},
   "source": [
    "<div class=\"span5 alert alert-info\">\n",
    "<h3>Exercise Set V</h3>\n",
    "\n",
    "<p>As we saw earlier, PCA has a lot of other uses. Since we wanted to visualize our data in 2 dimensions, we restricted the number of dimensions to 2 in PCA. But what is the true optimal number of dimensions?</p>\n",
    "\n",
    "<p><b>Exercise:</b> Using a new PCA object shown in the next cell, plot the `explained_variance_` field and look for the elbow point, the point where the curve's rate of descent seems to slow sharply. This value is one possible value for the optimal number of dimensions. What is it?</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
