{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589053bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd            \n",
    "import numpy as np             \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import MiniBatchKMeans, SpectralClustering\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc0ef97",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ff297",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f22758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "original_data = pd.read_csv('CrowdstormingDataJuly1st.csv')\n",
    "# Drop meaningless columns, we don't need the whole player name nor the birthday\n",
    "original_data.drop(['player','birthday'], 1, inplace=True)\n",
    "# Drop dyads where there are no photoID\n",
    "original_data.dropna(axis=0, subset=['photoID'], inplace=True)\n",
    "original_data.drop('photoID', 1, inplace=True)\n",
    "# Drop dyads where there are no rating\n",
    "original_data.dropna(axis=0, how='all', subset=['rater1', 'rater2'], inplace=True)\n",
    "# Drop dyads where there is no position\n",
    "original_data.dropna(axis=0, how='all', subset=['position'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e34fc6",
   "metadata": {},
   "source": [
    "#### Drop the features we don't want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ea7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = original_data.drop(['refNum', 'refCountry', 'Alpha_3', 'nIAT', 'nExp'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262bb1d0",
   "metadata": {},
   "source": [
    "#### Weight the cards by IAT/Exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e191f",
   "metadata": {},
   "source": [
    "IAT and Exp are linked to the referee and not directly to the player. The cards are what relates them. So let's try to weight the cards by IAT and Exp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09224b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAT \n",
    "df['IATYellows'] = df['yellowCards']*df['meanIAT']+df['seIAT']\n",
    "df['IATYellowReds'] = df['yellowReds']*df['meanIAT']+df['seIAT']\n",
    "df['IATReds'] = df['redCards']*df['meanIAT']+df['seIAT']\n",
    "# Exp\n",
    "df['ExpYellows'] = df['yellowCards']*df['meanExp']+df['seExp']\n",
    "df['ExpYellowReds'] = df['yellowReds']*df['meanExp']+df['seExp']\n",
    "df['ExpReds'] = df['redCards']*df['meanExp']+df['seExp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f12113",
   "metadata": {},
   "source": [
    "#### Aggregate some of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum for aggregation\n",
    "dfSumAgg = df[['playerShort', 'games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards','yellowReds', 'redCards',\n",
    "                'IATYellows', 'IATYellowReds', 'IATReds', 'ExpYellows', 'ExpYellowReds', 'ExpReds']]\n",
    "aggregatedWithSum = dfSumAgg.groupby('playerShort').sum()\n",
    "\n",
    "# No sum for aggregation; also we don't take the IAT and Exp scores from the referees\n",
    "identity = lambda x: x.iloc[0]\n",
    "dfIdentityAgg = df[['playerShort', 'club', 'leagueCountry', 'height', 'weight', 'position', \n",
    "                     'rater1', 'rater2']]\n",
    "aggregatedWithIdentity = dfIdentityAgg.groupby('playerShort').agg(identity)\n",
    "dfd = pd.concat([aggregatedWithIdentity, aggregatedWithSum], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1077ef",
   "metadata": {},
   "source": [
    "#### Normalise/clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd6aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# victories, defeats, ties, goals, cards, etc -> \n",
    "#   for each player x: x._/x.games\n",
    "\n",
    "colsToNorm = ['victories', 'ties', 'defeats', 'goals', 'yellowCards','yellowReds', 'redCards',\n",
    "             'IATYellows', 'IATYellowReds', 'IATReds', 'ExpYellows', 'ExpYellowReds', 'ExpReds']\n",
    "ds = []\n",
    "for c in colsToNorm:\n",
    "    col = []\n",
    "    for i in dfd.index:\n",
    "        col.append(dfd[c].loc[i]/dfd['games'].loc[i])\n",
    "    ds.append(pd.DataFrame(columns=[c], index=dfd.index, data=col))\n",
    "    \n",
    "\n",
    "\n",
    "dfd = pd.concat([pd.concat(ds,axis=1), dfd.drop(colsToNorm, axis=1)] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with NaN in weight and height\n",
    "# There are only few missing data so we can fill them with the mean of the column without\n",
    "# inducing to much bias in the data\n",
    "\n",
    "ws = dfd['weight']\n",
    "meanW = ws.dropna().mean()\n",
    "dfd['weight'].fillna(meanW, inplace=True)\n",
    "\n",
    "hs = dfd['height']\n",
    "meanH = hs.dropna().mean()\n",
    "dfd['height'].fillna(meanH, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80bd204",
   "metadata": {},
   "source": [
    "Since raters have a hard time to agree on a player skin color with the given scale, we decide to divide the observations in two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0549f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meanColor = (dfd['rater1']+dfd['rater2'])/2\n",
    "target = meanColor.apply(lambda x: 0 if x<0.5 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb9ed6",
   "metadata": {},
   "source": [
    "#### Encode the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18418d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "clubEncoded = pd.Series(index=data.index, data=le.fit_transform(data.club))\n",
    "leagueCountreyEncoded = pd.Series(index=data.index, data=le.fit_transform(data.leagueCountry))\n",
    "positionEncoded = pd.Series(index=data.index, data=le.fit_transform(data.position))\n",
    "\n",
    "# Dataframe with categorical data encoded as ints\n",
    "dataEncoded = pd.concat([clubEncoded, positionEncoded, leagueCountreyEncoded], axis=1)\n",
    "dataEncoded.columns=['club', 'position', 'leagueCountry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f24325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoder\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(dataEncoded)\n",
    "oneHotEncodedData = enc.transform(dataEncoded).toarray()\n",
    "#oneHotEncodedData.astype()\n",
    "dfEncoded = pd.DataFrame(index=dataEncoded.index, data=oneHotEncodedData).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data with categorical data encoded\n",
    "dff = pd.concat([data.drop(['club','position','leagueCountry'], axis=1), dfEncoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384b2c3",
   "metadata": {},
   "source": [
    "#### Start the machine larning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a906afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data \n",
    "X_train, X_test, y_train, y_test = train_test_split(dff, target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ffacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier\n",
    "# n_jobs=-1 sets it to the number of cores\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Train accuracy: \", clf.score(X_train, y_train))\n",
    "print(\"Test accuracy: \", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556518f",
   "metadata": {},
   "source": [
    "Let's try to alter the parameters of the random forest to see how we can get overfitting. We see overfitting when the accuracy goes up on the training set and down on the test set. Let's first try with n_estimators, the number of trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96423a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "estimator_range = range(1, 100)\n",
    "for i in estimator_range:\n",
    "    f = RandomForestClassifier(n_estimators=i)\n",
    "    f.fit(X_train, y_train)\n",
    "    train_accuracy.append(f.score(X_train, y_train))\n",
    "    test_accuracy.append(f.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cfa68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "plt.plot(estimator_range, train_accuracy)\n",
    "plt.plot(estimator_range, test_accuracy)\n",
    "plt.legend([\"Train accuracy\", \"Test accuracy\"])\n",
    "plt.ylim([0, 1.5])\n",
    "plt.xlabel(\"Number of estimators\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957597c3",
   "metadata": {},
   "source": [
    "All we can see there is that having too few estimators is bad. Let's try with bigger numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b57fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "estimator_range = range(100, 1000, 50)\n",
    "for i in estimator_range:\n",
    "    f = RandomForestClassifier(n_estimators=i)\n",
    "    f.fit(X_train, y_train)\n",
    "    train_accuracy.append(f.score(X_train, y_train))\n",
    "    test_accuracy.append(f.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108623f5",
   "metadata": {},
   "source": [
    "Right. Doesn't look very concluding. Next, let's try with max_features, which is the number of features to consider when looking for the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cbd91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "feature_range = range(1, X_train.columns.size)\n",
    "for i in feature_range:\n",
    "    f = RandomForestClassifier( max_features=i)\n",
    "    f.fit(X_train, y_train)\n",
    "    train_accuracy.append(f.score(X_train, y_train))\n",
    "    test_accuracy.append(f.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9daa7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "plt.plot(feature_range, train_accuracy)\n",
    "plt.plot(feature_range, test_accuracy)\n",
    "plt.legend([\"Train accuracy\", \"Test accuracy\"])\n",
    "plt.xlim([0, 125])\n",
    "plt.ylim([0, 1.5])\n",
    "plt.xlabel(\"Number of features\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5832de8",
   "metadata": {},
   "source": [
    "Still nothing. Let's take a loot at the max depth of the trees through min_samples_split, _i.e._ the minimum number of samples required to split an internal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ae0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "min_samples_split_range = range(2, 700, 5)\n",
    "for i in min_samples_split_range:\n",
    "    f = RandomForestClassifier(min_samples_split=i)\n",
    "    f.fit(X_train, y_train)\n",
    "    train_accuracy.append(f.score(X_train, y_train))\n",
    "    test_accuracy.append(f.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0685cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "plt.plot(min_samples_split_range, train_accuracy)\n",
    "plt.plot(min_samples_split_range, test_accuracy)\n",
    "plt.legend([\"Train accuracy\", \"Test accuracy\"])\n",
    "plt.xlim([0, 700])\n",
    "plt.ylim([0, 1.5])\n",
    "plt.xlabel(\"Minimum number of samples to split\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd863d5",
   "metadata": {},
   "source": [
    "We see that when the minimum number of samples to split is too small, the test accuracy decreases while the train accuracy increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc616f3",
   "metadata": {},
   "source": [
    "Now let's do cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, min_samples_split=400)\n",
    "cross_val_score(clf, dff, target).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('observations distribution as binary classes')\n",
    "plt.hist(target, bins=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e9cee",
   "metadata": {},
   "source": [
    "We have seen that the distribution of the two classes are not the same. There is way more '0' class than the other this is why we have such accuracy, it is very likely that all samples are classed as 0. Let's inspect the confusion matrix below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5a673",
   "metadata": {},
   "source": [
    "We can see below in the confusion matrix, that all labels are predicted as '0' as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b986ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classifier, using a model that is too regularized (C too low) to see\n",
    "# the impact on the results\n",
    "classifier = RandomForestClassifier(n_estimators=50, min_samples_split=400)\n",
    "y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "class_names =  np.array([0, 1]).astype('str')\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb06f80b",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae40c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = clf\n",
    "X = dff\n",
    "y = target\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"10 most important features\")\n",
    "plt.bar(range(10), importances[indices[0:10]],\n",
    "       color=\"r\", yerr=std[indices[0:10]], align=\"center\")\n",
    "plt.xticks(range(10), X.columns[indices[0:10]])\n",
    "plt.xlim([-1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a022d1c5",
   "metadata": {},
   "source": [
    "We see that almost all the data linked to the cards are there, which seems reasonable. We also see the categorical features 105, 106, 107, and 108, which correspond respectively to the German, French, English and Spanish league. This makes sense; for examples South-American players have a stronger tendency to go in the Spanish League whereas Northern African players are often linked to France. Finally there is height, defeats, victories and games. Those four probably should not have such an impact. So let's try dropping all the features except the ones related to the cards as well as the leagues and positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b04897",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff2 = dff.drop(['victories', 'ties', 'defeats', 'goals', 'height', 'weight', 'games'], axis=1)\n",
    "dff2.drop(list(range(0, 92)), axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039904b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = RandomForestClassifier(n_estimators=50, min_samples_split=400)\n",
    "cross_val_score(clf2, dff2, target).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62305971",
   "metadata": {},
   "source": [
    "Not much change. Let's take a look at the feature importance though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6e8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = clf2\n",
    "X = dff2\n",
    "y = target\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.title(\"10 most important features\")\n",
    "plt.bar(range(10), importances[indices[0:10]],\n",
    "       color=\"r\", yerr=std[indices[0:10]], align=\"center\")\n",
    "plt.xticks(range(10), X.columns[indices[0:10]])\n",
    "plt.xlim([-1, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad797694",
   "metadata": {},
   "source": [
    "We still get the leagues and the cards. However the prediction isn't really improved. Let's take a look at the distributions of the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fa274",
   "metadata": {},
   "source": [
    "So our predictions, which are always in the 70% range, are similar to what we would get by always predicting 0. In other words, with the given data we can't predict the skin tone of a player. This seems pretty reasonable considering the data we have. The only really subjective data are the cards weighted by IAT/Exp. However we only have those values by country and not by referee. And since most of the referees come from the same countries, this is not very helpful.\n",
    "\n",
    "Moreover since the distribution of the skin color is highly concentrated around 0. It is not hard to have a biased classifier on those labels. Looking at the confusion matrix above shows us that it is the case and that all predicted labels are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4640f3bf",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4266d9",
   "metadata": {},
   "source": [
    "Here we compute the model, a mini batch kmeans, for possible combinations of features (and droping the worst feature at each new iteration)\n",
    "\n",
    "We store the features used to calculate the silhouette score and the proportion elements per classe in the first cluster (since there are 2, only one is useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332b04be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name(x): \n",
    "    return dff2.columns[indices[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57eed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the 11th most important features according to ex1 (otherwise there are too many of them)\n",
    "indicesToUse = indices[0:11]\n",
    "# Init clustering model \n",
    "km = MiniBatchKMeans(n_clusters=2)\n",
    "bestICSS = []\n",
    "bestSS = []\n",
    "proportions = []\n",
    "y_train_floated = y_train.map(float)\n",
    "lab, totals = zip(*sorted(Counter(y_train_floated).items()))\n",
    "# Compute the model for possible combinations of features (and droping the worst feature at each new iteration)\n",
    "for i in reversed(range(2,11)):\n",
    "    print(i)\n",
    "    c = itertools.combinations(indicesToUse, i)\n",
    "    maxSS = -1\n",
    "    acc = -1\n",
    "    for ic in c:\n",
    "        X_temp = X_train[dff2.columns[list(ic)]]\n",
    "        km.fit(X_temp)\n",
    "        labels = km.labels_\n",
    "        ss = silhouette_score(X_temp, labels)\n",
    "        \n",
    "        if(ss > maxSS):\n",
    "            maxSS = ss\n",
    "            icBest = list(ic)\n",
    "            clu1 = y_train[labels == 0]\n",
    "            _, values = zip(*sorted(Counter(clu1).items()))\n",
    "            values = [x/y for (x,y) in zip(values,totals)]\n",
    "\n",
    "\n",
    "        \n",
    "    bestICSS.append(icBest)\n",
    "    bestSS.append(maxSS)\n",
    "    proportions.append(values)\n",
    "\n",
    "    ind = -1\n",
    "    for x in indicesToUse:\n",
    "        ind = ind + 1\n",
    "        if x not in icBest:\n",
    "            np.delete(indicesToUse, ind)\n",
    "            break\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5999777",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bestSS,'o', bestSS, '-')\n",
    "plt.title('Silhouette score per feature combinations (printed above)')\n",
    "\n",
    "combSS = [[name(x) for x in arr ] for arr in bestICSS]\n",
    "_ =[print(x) for x in combSS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a53a9",
   "metadata": {},
   "source": [
    "Let us plot the proportional distribution of each class at each feature combination obtained above.\n",
    "(For each class the distribution is the proportion of the class in the cluster)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
