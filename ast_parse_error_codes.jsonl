{"code": "%time grid = \\\n  [graphplot(snodes, hammer_bundle(*star, iterations=5, \\\n                                   decay=decay, initial_bandwidth=bw), \\\n             \"d={:0.2f}, bw={:0.2f}\".format(decay, bw)) \\\n    for decay in [0.1, 0.25, 0.5, 0.9] \\\n    for bw    in [0.1, 0.2, 0.5, 1]]\n    \ntf.Images(*grid).cols(4)", "error": "unexpected indent (<unknown>, line 1)"}
{"code": "SELECT DISTINCT business_id\nFROM category\nWHERE category IN(\n'Acai Bowls',\n'Afghan',\n'African',\n'Alsatian',\n'American (New)',\n'American (Traditional)',\n'Arabian',\n'Argentine',\n'Armenian',\n'Asian Fusion',\n'Bagels',\n'Baguettes',\n'Bakeries',\n'Bangladeshi',\n'Bar Crawl',\n'Barbeque',\n'Bars',\n'Bavarian',\n'Beach Bars',\n'Bed & Breakfast',\n'Beer Bar',\n'Beer Garden',\n'Beer Gardens',\n'Beer Hall',\n'Beer Tours',\n'Belgian',\n'Bistros',\n'Brasseries',\n'Brazilian',\n'Breakfast & Brunch',\n'Breweries',\n'Brewpubs',\n'British',\n'Bubble Tea',\n'Buffets',\n'Bulgarian',\n'Burgers',\n'Burmese',\n'Cafes',\n'Cajun/Creole',\n'Cambodian',\n'Canadian (New)',\n'Cantonese',\n'Caribbean',\n'Caterers',\n'Cheesesteaks',\n'Chicken Shop',\n'Chicken Wings',\n'Chinese',\n'Chocolatiers & Shops',\n'Churros',\n'Cideries',\n'Cigar Bars',\n'Cocktail Bars',\n'Coffee & Tea',\n'Coffee Roasteries',\n'Coffeeshops',\n'Colombian',\n'Comfort Food',\n'Conveyor Belt Sushi',\n'Creperies',\n'Cuban',\n'Cupcakes',\n'Curry Sausage',\n'Custom Cakes',\n'Czech',\n'Czech/Slovakian',\n'Delicatessen',\n'Delis',\n'Desserts',\n'Dim Sum',\n'Diners',\n'Do-It-Yourself Food',\n'Dominican',\n'Donuts',\n'Drive-Thru Bars',\n'Eastern European',\n'Eastern German',\n'Eatertainment',\n'Egyptian',\n'Empanadas',\n'Ethnic Food',\n'Falafel',\n'Farmers Market',\n'Farms',\n'Fast Food',\n'Filipino',\n'Fischbroetchen',\n'Fish & Chips',\n'Flatbread',\n'Fondue',\n'Food',\n'Food Court',\n'Food Trucks',\n'French',\n'Fruits & Veggies',\n'Gastropubs',\n'Gelato',\n'Georgian',\n'German',\n'Gluten-Free',\n'Greek',\n'Guamanian',\n'Haitian',\n'Halal',\n'Hawaiian',\n'Himalayan/Nepalese',\n'Honduran',\n'Honey',\n'Hong Kong Style Cafe',\n'Hookah Bars',\n'Hot Dogs',\n'Hot Pot',\n'Hotel bar',\n'Hungarian',\n'Iberian',\n'Ice Cream & Frozen Yogurt',\n'Imported Food',\n'Indian',\n'Indonesian',\n'International',\n'Irish',\n'Irish Pub',\n'Italian',\n'Izakaya',\n'Japanese',\n'Japanese Curry',\n'Juice Bars & Smoothies',\n'Kebab',\n'Kombucha',\n'Korean',\n'Kosher',\n'Laotian',\n'Latin American',\n'Lebanese',\n'Live/Raw Food',\n'Lounges',\n'Macarons',\n'Malaysian',\n'Mediterranean',\n'Mexican',\n'Middle Eastern',\n'Milkshake Bars',\n'Minho',\n'Modern European',\n'Mongolian',\n'Moroccan',\n'New Mexican Cuisine',\n'Nicaraguan',\n'Nightlife',\n'Noodles',\n'Pakistani',\n'Palatine',\n'Pan Asian',\n'Parent Cafes',\n'Pasta Shops',\n'Patisserie/Cake Shop',\n'Persian/Iranian',\n'Peruvian',\n'Pita',\n'Pizza',\n'Poke',\n'Polish',\n'Pop-Up Restaurants',\n'Popcorn Shops',\n'Portuguese',\n'Poutineries',\n'Pretzels',\n'Pub Food',\n'Pubs',\n'Puerto Rican',\n'Ramen',\n'Rest Stops',\n'Restaurants',\n'Rotisserie Chicken',\n'Russian',\n'Salad',\n'Salvadoran',\n'Sandwiches',\n'Scandinavian',\n'Schnitzel',\n'Scottish',\n'Seafood',\n'Senegalese',\n'Serbo Croatian',\n'Shanghainese',\n'Sicilian',\n'Signature Cuisine',\n'Singaporean',\n'Slovakian',\n'Smokehouse',\n'Soba',\n'Soul Food',\n'Soup',\n'South African',\n'Southern',\n'Spanish',\n'Specialty Food',\n'Sports Bars',\n'Sri Lankan',\n'Steakhouses',\n'Sugar Shacks',\n'Supper Clubs',\n'Sushi Bars',\n'Swabian',\n'Swiss Food',\n'Syrian',\n'Szechuan',\n'Tacos',\n'Taiwanese',\n'Tapas/Small Plates',\n'Tea Rooms',\n'Tempura',\n'Teppanyaki',\n'Tex-Mex',\n'Thai',\n'Tiki Bars',\n'Tonkatsu',\n'Traditional Chinese Medicine',\n'Traditional Norwegian',\n'Trinidadian',\n'Turkish',\n'Tuscan',\n'Udon',\n'Ukrainian',\n'Vegan',\n'Vegetarian',\n'Venezuelan',\n'Vietnamese',\n'Waffles',\n'Whiskey Bars',\n'Wraps'\n) INTO OUTFILE '/tmp/food_businesses.csv'\nFIELDS TERMINATED BY ','\nENCLOSED BY '\"'\nLINES TERMINATED BY '\\n';\n\n\n\n", "error": "invalid syntax (<unknown>, line 1)"}
{"code": "%%html\n<div style=\"width: 100%; text-align: center;\">\n    <h3>Teach a Quadcopter How to Tumble</h3>\n    <video poster=\"images/quadcopter_tumble.png\" width=\"640\" controls muted>\n        <source src=\"images/quadcopter_tumble.mp4\" type=\"video/mp4\" />\n        <p>Video: Quadcopter tumbling, trying to get off the ground</p>\n    </video>\n</div>", "error": "invalid syntax (<unknown>, line 1)"}
{"code": "#Convert gender variable:\ndummy = pd.get_dummies(df_fa['gender'])\ndf_fa = pd.concat([df_fa, dummy], axis=1)\ndf_fa.drop('gender', axis=1, inplace=True)\n\"\"\"Note, we're not concerned about collinearity having both a female and a male category,\nas there are several cases where both values are 0, presumably because the user did not\nsupply the information.  Thus, the two columns, male and female, capture the 3 cases:\nmale, female, and 'not supplied'. \"\"\"\n;", "error": "invalid syntax (<unknown>, line 9)"}
{"code": "%matplotlib inline\nimport pandas as pd\nimport seaborn as sns \nimport matplotlib\nimport numpy as npplt.style.use('seaborn-paper')\nimport matplotlib.pyplot as plt\npd.set_option('max_colwidth',100)\n\n\n# Fixed params\ncol_1,col_2='PetalLengthCm','PetalWidthCm'\ncols=[col_1,col_2]\nalpha=0.03\nfreedomDegrees=2\nplt.figure(figsize=(20,10))\nplt.axis([-0.5,8,-1,4])\n# Iris-Setosa analysis (Green)\nscatterPlot(plt,irisSetosaDF,col_1,col_2,'Green')\nsummaryIrisPD = getProbabilityDensityContour(plt,irisSetosaDF,\\\n                            [col_1,col_2],alpha,freedomDegrees,\\\n                             color='green',name='Iris-setosa')\n# Iris-Versicolor analysis (Blue)\nscatterPlot(plt,irisVersicolorDF,col_1,col_2,'Blue')\nsummaryVersicolorPD = getProbabilityDensityContour(plt,irisVersicolorDF,\\\n                            [col_1,col_2],alpha,freedomDegrees,\\\n                             color='Blue',name='Iris-versicolor')\n# Iris-Virginica analysis (Red)\nscatterPlot(plt,irisVirginicaDF,col_1,col_2,'Red')\nsummaryVirginicaPD = getProbabilityDensityContour(plt,irisVirginicaDF,\\\n                            [col_1,col_2],alpha,freedomDegrees,\\\n                             color='Red',name='Iris-virginica')\nplt.show()\ndisplay(summaryIrisPD)\ndisplay(summaryVersicolorPD)\ndisplay(summaryVirginicaPD)", "error": "invalid syntax (<unknown>, line 4)"}
{"code": "np.add?", "error": "invalid syntax (<unknown>, line 1)"}
{"code": "# search for documentation on a function\nnp.split?", "error": "invalid syntax (<unknown>, line 2)"}
{"code": " imgfiles = import_data(datafolder)\ndf_temp, feature_names = extract_features(imgfiles, feature_funcs, 6, 6)\nscore = run_kmeans_pipeline(df_temp, feature_names, 2, standardize=True, use_pca=True)\nprint(\"With scaling & pca:\", score)\nimgutils.show_large_heatmap(df_temp, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)    \nscore = run_kmeans_pipeline(df_temp, feature_names, 2, standardize=False, use_pca=False)\nprint(\"Without scaling & pca:\", score)\nimgutils.show_large_heatmap(df_temp, 'kmeans', imgfiles, n_rows=n_tiles_y, n_cols=n_tiles_x, fig_size=s)   ", "error": "unexpected indent (<unknown>, line 1)"}
{"code": "%time model = LdaModel(corpus=corpus_vectorized, id2word=id2word, chunksize=chunksize,\\\n                       alpha='auto', eta='auto', \\\n                       iterations=iterations, num_topics=num_topics,\\\n                       passes=passes, eval_every=eval_every)", "error": "unexpected indent (<unknown>, line 1)"}
{"code": "def round_dt(dt): # round time to whole minutes\n    dt = dt + datetime.timedelta(seconds=30, microseconds=999999)\n    dt = dt.replace(second=0, microsecond=0)\n    return dt\n\n\n# process all directories\n\nfor directory in glob('./data/*/'): # search for measurement runs\n    for filename in glob( directory + 'Liulin/*.y*'): # search for Liuline files\n\n        print '----------------------------------------------------------------------'\n        print directory\n        # parse Run number from the directory name\n        run = directory.split('/')\n        run = run[2].split('(')\n        run = run[0].split('.')\n        \n        # extract time from Liulin filename\n        path = filename.split('/')\n        time = path[-1].split('.')\n        date_object = datetime.datetime.strptime(time[0], '%y%m%d%H%M')\n        print date_object\n        print 'RUN ', run[0]\n        \n        # read data from Liulin \n        liulin = pd.read_csv(filename, header = None, sep = ' ', skiprows = 1) # read Liulin data Y\n        liulin = liulin.drop(256, axis=1) # delete last empty column\n        infile = open(filename, 'r')\n        header = infile.readline()\n        print header # print data header\n        exposition = header.split(' ')[4].split('[')[0] # extract exposition time  \n        exposition_val = float(exposition)\n        exposition += 'S'\n        \n        # sum energy\n        energy_scale = np.linspace(0.0407, 20.7977, 256)    # 0.0814 MeV per channel\n        uGy_const = 1.602e-7 / 1.398e-4 * (3600/exposition_val)     # to uGy/h for given exposition\n        liulin_energy = liulin\n        liulin_energy = liulin_energy.multiply(energy_scale)\n        liulin[256] = liulin_energy.sum(axis=1) * uGy_const # DSi\n        liulin[257] = liulin_energy.ix[:,0:11].sum(axis=1) * uGy_const * 1.7 + liulin_energy.ix[:,12:255].sum(axis=1) * uGy_const * 6.1 # H\n       \n        # index liulin data (compute time)\n        liulin_data = pd.DataFrame(index = pd.date_range(date_object, freq=exposition, periods=len(liulin)).tolist(), data = liulin.as_matrix())\n\n\n    # Parse Navigation data\n    nav_data = pd.DataFrame()\n    \n    for file in glob(directory + 'Navigation data/*.xml'): #search for navigation data\n        #print file\n        try:\n            tree = ET.parse(file) # parse XML\n            values = tree.findall('./FlightTrack/Position')\n    \n            data = pd.DataFrame({\n                             #'time' : round_dt(datetime.datetime.strptime(value.find('Time').text, \"%Y-%m-%d %H:%M:%S\")), \n                             #'line' : tree.find('./FlightInfo/FlightLine').text, # !!! IMZ\n                             'from' : tree.findtext('./FlightInfo/Org',default='XXX'),\n                             'to' : tree.findtext('./FlightInfo/Dst',default='XXX'),\n                             'lon' : [value.find('Longtitude').text for value in values],\n                             'lat' : [value.find('Latitude').text for value in values],\n                             'alt' : [value.find('Baralt').text for value in values],\n                             }, index=[round_dt(datetime.datetime.strptime(value.find('Time').text, \"%Y-%m-%d %H:%M:%S\")) for value in values])\n        \n            nav_data = pd.concat([data,nav_data]) # add nav_data from one file\n        except:\n            try:\n                tree = ET.parse(file) # parse XML\n                values = tree.findall('./FlightTrack/Position')\n\n                data = pd.DataFrame({\n                                 #'time' : round_dt(datetime.datetime.strptime(value.find('Time').text, \"%Y-%m-%d %H:%M:%S\")), \n                                 #'line' : tree.find('./FlightInfo/FlightLine').text, # !!! IMZ\n                                 'from' : tree.findtext('./FlightInfo/Org',default='XXX'),\n                                 'to' : tree.findtext('./FlightInfo/Dst',default='XXX'),\n                                 'lon' : [value.find('Longtitude').text for value in values],\n                                 'lat' : [value.find('Latitude').text for value in values],\n                                 'alt' : [value.find('Baralt').text for value in values],\n                                 }, index=[round_dt(datetime.datetime.strptime(value.find('Time').text, \"%Y-%m-%d%H:%M:%S\")) for value in values])\n\n                nav_data = pd.concat([data,nav_data]) # add nav_data from one file\n            except:\n                print 'ERROR parse XML: ' + file\n                print sys.exc_info()\n                continue\n                     \n    liulin_data = liulin_data.resample('5 min', how='first')\n    nav_data = nav_data.resample('5 min', how='first')\n\n    # Merge data\n    data = nav_data.join(liulin_data) # merge liulin data with navigation data\n    data.rename(columns={256: 'DSi', 257: 'H'}, inplace=True)\n    data = data.dropna(subset=[7]) # delele NaN lines\n    data['tdelta'] = exposition_val\n    #data.index.name = 'time'\n    #data = data.sort_index() # sort by time\n    try:\n        data['flight'] = '*' + data['from'] + data['to']\n        data['E'] = 0.0\n    except:\n        print 'ERROR no data:', directory\n        print\n        continue\n    \n    # rearrange culomns    \n    out = data[['flight','lat','lon','alt','DSi','H', 'E']+list(xrange(256))+['tdelta']]\n    # filtering\n    out = out.loc[out.DSi > 1]   # filtering of low radiation (an airplane is possibly on the groung)\n    out.alt = pd.to_numeric(out.alt, errors='coerce')\n    out = out.loc[out.alt > 32000]   # filtering of low altitude (9 753.6 m - transient level - ascent/descent)\n    out.reset_index(inplace=True)\n    # rename and reindex\n    out.columns = ['date','Flight','lat','lon','alt','DSi[uGy/h]','H*(10)[uSv/h]','E(CARI)[uSv/h]']+list(xrange(1,257))+['tdelta']\n    out.set_index('date',inplace=True)\n    out.sort_index(inplace=True) # sort by time\n\n    # save\n    out.to_csv('./output/Run' + run[0] + '.txt', sep='\\t') # save merged data to output directory\n\n    nav_data.alt = pd.to_numeric(nav_data.alt, errors='coerce')\n    nav_data=nav_data.sort_index()\n    plt.figure(figsize=(80, 10))\n    matplotlib.rcParams.update({'font.size': 22})\n    plt.plot(liulin_data[256],c='orange')\n    plt.plot(out['DSi[uGy/h]'],c='red')\n    plt.ylabel('Doserate in Silicon [uGy/h]', color='red')\n    plt.ylim(0, 8)\n    plt.twinx()\n    plt.plot(nav_data['alt'], c='blue', lw=3)\n    plt.plot(out['alt'],c='black', linestyle='', marker='.')\n    plt.ylabel('Altitude [ft]', color='blue')\n    plt.title(directory)  \n\n", "error": "invalid syntax (<unknown>, line 32)"}
{"code": " df.iloc[1:5, 2:5]", "error": "unexpected indent (<unknown>, line 1)"}
{"code": "print(\"n_digits: %d, \\t n_samples %d, \\t n_features %d\"\n      % (n_digits, n_samples, n_features))\nprint(82 * '_')\nprint('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n\ndef bench_k_means(estimator, name, data):\n    t0 = time()\n    estimator.fit(data)\n    print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n          % (name, (time() - t0), estimator.inertia_,\n             metrics.homogeneity_score(labels, estimator.labels_),\n             metrics.completeness_score(labels, estimator.labels_),\n             metrics.v_measure_score(labels, estimator.labels_),\n             metrics.adjusted_rand_score(labels, estimator.labels_),\n             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n             metrics.silhouette_score(data, estimator.labels_,\n                                      metric='euclidean',\n                                      sample_size=sample_size)))\n\nbench_k_means(KMeans(init='k-means++', n_clusters=n_digits, n_init=10000),\n              name=\"k-means++\", data=training)\n\nbench_k_means(KMeans(init='random', n_clusters=n_digits, n_init=10000),\n              name=\"random\", data=training)\n\n# in this case the seeding of the centers is deterministic, hence we run the\n# kmeans algorithm only once with n_init=1\npca = PCA(n_components=n_digits).fit(training)\nbench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n              name=\"PCA-based\",\n              data=training)\nprint(82 * '_')", "error": "invalid syntax (<unknown>, line 2)"}
{"code": "train.loc?", "error": "invalid syntax (<unknown>, line 1)"}
{"code": "from nltk.probability import *\n\ndef word_concat(dsd):\n    \"\"\"\n    concatenate all the words stored in the values of a given dictionary. Each value is a list\n    of tokenized sentences.\n    \"\"\"\n    all_words = []\n    # iterates all patents and store the tokens in a single list.\n    for sent in dsd.values():\n            all_words += sent\n    print \"tokens:\", len(all_words) # total number of tokens \n    print \"types:\", len(set(all_words)) # total number of types\n    return all_words", "error": "invalid syntax (<unknown>, line 13)"}
{"code": "transactions = %contiamo query query:sql:48590597:411:g71GXzJjsx4Uvad11ouKjoYbQUNNPy-qRMKkBNZfyx4\ncustomers = %contiamo query query:sql:48590597:441:MG5W2dMjXzYgsHsgdQYzmhv44dxEQX2Lodu5Uh2Hx_s\napplications = %contiamo query query:sql:48590597:442:-gz3nbw1fdmtSXkD4zGNA-cVa7s6sQtRn8upCSn6uys            ", "error": "invalid syntax (<unknown>, line 1)"}
{"code": "# We need to clean up the following categories:\n#  1 - we standardize every numerical input to floats\n#  2 - weight, standardize [father,mother]_profession\n\n# 2 (pre-processing) - let us check how many unique entries we have on each column\nprint (\"before standardization:\\tfather_profession uniques: %d, mother_profession uniques: %d\"\n       %(qoflDF.father_profession.nunique() , qoflDF.mother_profession.nunique()))\n\n# helper function to check if string ends in suffix list\ndef checkEnds(string, ends):\n    words = string.split()\n    \n    for word in words:\n        if any(word.endswith(end) for end in ends):\n            return True, word\n    \n    return False, string\n\n# 2 - standardize profession\ndef standardize_profession (string):\n    \n    string = string.lower()\n    string = string.strip()\n    \n    endsw, word = checkEnds(string, [\"er\", \"or\", \"ic\", \"st\", \"ct\", \"nt\", \"an\", \"ch\"])\n    \n    if any(x for x in [\"own\", \"self\", \"business\"] if x in string):\n        output = \"BUSINESSMAN\"\n    elif any(x for x in [\"teacher\", \"prof\"] if x in string):\n        output = \"TEACHER\"\n    elif \"home\" in string:\n        output = \"HOMEMAKER\"\n    elif \"construct\" in string:\n        output = \"CONSTRUCTION\"\n    elif \"police\" in string:\n        output = \"POLICEMAN\"\n    elif endsw == True:\n        output = word.upper()\n    elif \"sales\" in string:\n        output = \"SALESMAN\"\n    elif \"retired\" in string:\n        output = \"RETIRED\"\n    elif any(x for x in [\"ceo\", \"cfo\", \"vp\", \"president\", \"cto\"] if x in string):\n        output = \"EXECUTIVE\"\n    else:\n        output = \"OTHER\"\n        \n    return output\n\n# finally drop nan by replacing with empty strings\nqoflDF[\"father_profession\"] = qoflDF[\"father_profession\"].fillna(\"\")\nqoflDF[\"mother_profession\"] = qoflDF[\"mother_profession\"].fillna(\"\")\n\n# apply our filter to standardize the profession columns\nqoflDF[\"father_profession\"] = qoflDF[\"father_profession\"].apply(standardize_profession)\nqoflDF[\"mother_profession\"] = qoflDF[\"mother_profession\"].apply(standardize_profession)\n\n# print new unique counts\nprint (\"after standardization:\\tfather_profession uniques: %d, mother_profession uniques: %d\"\n       %(qoflDF.father_profession.nunique() , qoflDF.mother_profession.nunique()))", "error": "invalid syntax (<unknown>, line 9)"}
{"code": "context_data.append?", "error": "invalid syntax (<unknown>, line 1)"}
{"code": "import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\ntest = pd.read_csv('C:\\Users\\gydoy\\Documents\\EE379K\\KaggleMidterm\\\\test_final.csv')\ntrain = pd.read_csv('C:\\Users\\gydoy\\Documents\\EE379K\\KaggleMidterm\\\\train_final.csv')\n\ntrain_Y = train['Y']\ntrain_X = train.drop(['id', 'Y', 'F25', 'F26', 'F27'], axis=1)\ntest_X = test.drop(['id'], axis=1)\nprint(\"done\")\n\n", "error": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<unknown>, line 8)"}
{"code": "adjvocabtups = (nbdatardd.flatMap(lambda l: l).flatMap(lambda word: word)\n             .map(lambda word: (word, 1))\n             .reduceByKey(lambda a, b: a + b)\n             .map(lambda (x,y): x)\n             .zipWithIndex()\n).cache()\nadjvocab=adjvocabtups.collectAsMap()", "error": "invalid syntax (<unknown>, line 4)"}