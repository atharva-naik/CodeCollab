{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4b65ce",
   "metadata": {},
   "source": [
    "# Non-Parametric Approach (Random Forests)\n",
    "\n",
    "Here we explore a python implementation of random forest for this competition. The reason for the transition to python was a computational one. R was proving to be far too slow to create even trivially sized forests. Python gives an increase in speed by nature, but also has more natural integration of parallel tree creation, allowing for the creation of larger forests. That being said, computation is still an issue and will come into play as we go through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1824d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d7c7c",
   "metadata": {},
   "source": [
    "## Gini Coefficient\n",
    "\n",
    "Code, adapted from the code on collab, to compute the gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalized_gini_index(g, predicted_probabilities):\n",
    "  \n",
    "    if len(g) != len(predicted_probabilities):\n",
    "        print(\"Actual and Predicted need to be equal lengths!\")\n",
    "        return\n",
    "\n",
    "    # arrange data into table with columns of index, predicted values, and actual values\n",
    "    d = {\"truth\": g, \"pred\": predicted_probabilities}\n",
    "    gini_table = pd.DataFrame(data = d, index = range(1,len(g) + 1))\n",
    "\n",
    "    # sort rows in decreasing order of the predicted values, breaking ties according to the index\n",
    "    # gini_table = gini.table[order(-gini.table$predicted.probabilities, gini.table$index), ]\n",
    "    gini_table = gini_table.sort_values(\"pred\", ascending = False)\n",
    "\n",
    "    # get the per-row increment for positives accumulated by the model \n",
    "    num_ground_truth_positivies = sum(gini_table[\"truth\"])\n",
    "    model_percentage_positives_accumulated = gini_table[\"truth\"] / num_ground_truth_positivies\n",
    "\n",
    "    # get the per-row increment for positives accumulated by a random guess\n",
    "    random_guess_percentage_positives_accumulated = 1 / len(gini_table[\"truth\"])\n",
    "\n",
    "    # calculate gini index\n",
    "    gini_sum = np.cumsum(model_percentage_positives_accumulated - random_guess_percentage_positives_accumulated)\n",
    "    gini_index = sum(gini_sum) / len(gini_table[\"truth\"]) \n",
    "    return(gini_index)\n",
    "\n",
    "\n",
    "#' Calculates normalized Gini index from ground truth and predicted probabilities.\n",
    "#' @param ground.truth Ground-truth scalar values (e.g., 0 and 1)\n",
    "#' @param predicted.probabilities Predicted probabilities for the items listed in ground.truth\n",
    "#' @return Normalized Gini index, accounting for theoretical optimal.\n",
    "def normalized_gini_index(g, predicted_probabilities):\n",
    "    model_gini_index = unnormalized_gini_index(g, predicted_probabilities)\n",
    "    optimal_gini_index = unnormalized_gini_index(g, g)\n",
    "    return(model_gini_index / optimal_gini_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba1e0c5",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914b26b",
   "metadata": {},
   "source": [
    "First we are going to just try and fit a random forest to the raw train data and get a baseline for the gini index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a74882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw train set\n",
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a7d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into response and predictors\n",
    "y = train[\"target\"]\n",
    "x = train.drop([\"id\", \"target\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6010b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline model\n",
    "clf = RandomForestClassifier(n_estimators=500, max_depth=2, random_state=0, n_jobs=-1, max_features=1, oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b352b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the test set\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "ids = test[\"id\"]\n",
    "test = test.drop([\"id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cafa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the probabilities\n",
    "probs = clf.predict_proba(test)\n",
    "probs_final = [x[1] for x in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c22121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and write the results\n",
    "result1 = {'id':ids, 'target':probs_final}\n",
    "result1_df = pd.DataFrame(data = result1)\n",
    "\n",
    "result1_df.to_csv(\"predictionsRF10-24-17.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51768c37",
   "metadata": {},
   "source": [
    "Gave a baseline score of  0.232"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedee17",
   "metadata": {},
   "source": [
    "## Number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b78724",
   "metadata": {},
   "source": [
    "We now use OOB to look at the effect of the number of predictors considered for each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a730c",
   "metadata": {},
   "source": [
    "OOB Score for various sizes of features\n",
    "\n",
    "1: 0.96355248214081701\n",
    "\n",
    "10: 0.963542401699\n",
    "\n",
    "20: 0.963552482141\n",
    "\n",
    "30: 0.963545761846\n",
    "\n",
    "Number of features does not seem to make a huge difference, so we will stick with the baseline of log(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4076a0",
   "metadata": {},
   "source": [
    "## Variable Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe36c3",
   "metadata": {},
   "source": [
    "Next we get a sense of some of the important predictors. This is more or less exploratory. We will run a 3 fold cv and find what variables show up in in the top 20 most important predictors for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the x and the y\n",
    "y_kfold = np.array(train[\"target\"])\n",
    "x_kfold = np.array(train.drop([\"id\", \"target\"], axis = 1))\n",
    "\n",
    "# Split into three folds\n",
    "kf = StratifiedKFold(n_splits=3)\n",
    "kf.get_n_splits(x_kfold, y_kfold)\n",
    "\n",
    "# Create a random forest that also store importance\n",
    "forest = ExtraTreesClassifier(n_estimators=500, max_depth=2, random_state=0, n_jobs=-1, max_features=\"log2\")\n",
    "\n",
    "ginis = []\n",
    "\n",
    "# List to store how many times a variable shows up in the top 20 most important variables\n",
    "ratio = [0]*57\n",
    "\n",
    "# Iterate over the 3 folds \n",
    "for train_index, test_index in kf.split(x_kfold, y_kfold):\n",
    "    X_train, X_test = x_kfold[train_index], x_kfold[test_index]\n",
    "    y_train, y_test = y_kfold[train_index], y_kfold[test_index]\n",
    "    \n",
    "    # Fit a model\n",
    "    model = forest.fit(X_train, y_train)\n",
    "    probs = forest.predict_proba(X_test)\n",
    "    probs_final = [x[1] for x in probs]\n",
    "    \n",
    "    # Extract the importances\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # If a variable is in the top 20 importances, increase its value in the ratio list\n",
    "    index = 0\n",
    "    for f in range(X_train.shape[1]):\n",
    "        # print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "        if(index < 20):\n",
    "            ratio[indices[f]] += 1\n",
    "        index += 1\n",
    "\n",
    "    ginis.append(normalized_gini_index(g=y_test, predicted_probabilities=probs_final))\n",
    "    print(len(ginis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9266cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the cross validation Gini\n",
    "print(ginis)\n",
    "\n",
    "# Find which variables top level of important in all folds\n",
    "interest = [x for x in range(0,57) if ratio[x] > 2]\n",
    "interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71661fab",
   "metadata": {},
   "source": [
    "Gives Gini scores of [0.21581968897515907, 0.22396513332529083, 0.22062593969640684]\n",
    "\n",
    "and important columns of [3, 4, 5, 6, 15, 16, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 34]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3902738",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3eb3a",
   "metadata": {},
   "source": [
    "We now dive into our actual cross validation. We will stick to 3-fold cross validation as opposed to OOB error just because this was the approach we took before learning about OOB and the results should be roughly the same. Plus, this will give us an idea of gini as opposed to accuracy. Further, we choose 3 fold as it showed to be the most computationally feasible. We will cross validate on both tree size and max depth of tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015b5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split x and y\n",
    "y_kfold = np.array(train[\"target\"])\n",
    "x_kfold = np.array(train.drop([\"id\", \"target\"], axis = 1))\n",
    "\n",
    "# Split into folds\n",
    "kf = StratifiedKFold(n_splits=3)\n",
    "kf.get_n_splits(x_kfold, y_kfold)\n",
    "forest = RandomForestClassifier(n_estimators=500, max_depth=12, random_state=0, n_jobs=4, max_features=\"log2\")\n",
    "\n",
    "ginis = []\n",
    "index = 1\n",
    "\n",
    "# Iterate over folds to cross validate on various parameters\n",
    "for train_index, test_index in kf.split(x_kfold, y_kfold):\n",
    "    X_train, X_test = x_kfold[train_index], x_kfold[test_index]\n",
    "    y_train, y_test = y_kfold[train_index], y_kfold[test_index]\n",
    "    \n",
    "    model = forest.fit(X_train, y_train)\n",
    "    probs = forest.predict_proba(X_test)\n",
    "    probs_final = [x[1] for x in probs]\n",
    "    ginis.append(normalized_gini_index(g=y_test, predicted_probabilities=probs_final))\n",
    "    print(index)\n",
    "    index += 1\n",
    "    \n",
    "ginis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a2515",
   "metadata": {},
   "source": [
    "Ginis for various tree sizes, 3 fold, max depth 2:\n",
    "\n",
    "n = 100  : [0.22831513135331596, 0.23864099086790785, 0.23103020976139921]\n",
    "\n",
    "n = 500  : [0.23399624333700408, 0.2411106344163601, 0.23516488700519048]\n",
    "\n",
    "n = 750  : [0.23386079491144549, 0.24087016368279662, 0.23520949462470858]\n",
    "\n",
    "n = 1000 : [0.23388456236128888, 0.24129153652280616, 0.23561118600698075]\n",
    "\n",
    "Tree size seems to have little effects, beyond 500, so we will use that and cross validate on the maximum depth of the tree.\n",
    "\n",
    "max depth = 3 : [0.24062733291355087, 0.24570173158111713, 0.24159062096266626]\n",
    "\n",
    "max depth = 4 : [0.2426543130398667, 0.25000749888491419, 0.24464726289094912]\n",
    "\n",
    "max depth = 5 : [0.24782642403706451, 0.25376128306085599, 0.2492086061125024]\n",
    "\n",
    "max depth = 6 : [0.25131812425267502, 0.25640949938610264, 0.25279046253967674]\n",
    "\n",
    "max depth = 7 : [0.2551048402902254, 0.26008715643937819, 0.25576034299523143]\n",
    "\n",
    "max depth = 8 : [0.25678261394094259, 0.2616982944395666, 0.25828227944175075]\n",
    "\n",
    "max depth = 9 : [0.25785094996229607, 0.26305865767777453, 0.25982247787905172]\n",
    "\n",
    "max depth = 10 : [0.26043552198432962, 0.26494932536341631, 0.26163287418190628]\n",
    "\n",
    "max depth = 11 : [0.26024302733938087, 0.266616577059546, 0.26264853956131246]\n",
    "\n",
    "max depth = 12 : [0.25916813682529266, 0.2643633991281692, 0.2615973871654434]\n",
    "\n",
    "Increasing depth does seem to have an effect, but it tapers off after 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3502186",
   "metadata": {},
   "source": [
    "## Actual Model on Raw Data (BEST MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095fa17",
   "metadata": {},
   "source": [
    "Using what we have found, make a prediction tree using the raw, uncleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d0815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in train data\n",
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate predictors and response\n",
    "y = train[\"target\"]\n",
    "x = train.drop([\"id\", \"target\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12680245",
   "metadata": {},
   "source": [
    "Using the best paramaters found using CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the tree using best parameters\n",
    "clf_test = RandomForestClassifier(n_estimators=500, max_depth=11, random_state=0, n_jobs=-1, max_features=\"log2\")\n",
    "clf_test.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b4a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the test set\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "ids = test[\"id\"]\n",
    "test = test.drop([\"id\"], axis = 1)\n",
    "probs = clf_test.predict_proba(test)\n",
    "probs_final = [x[1] for x in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b25964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results\n",
    "result1 = {'id':ids, 'target':probs_final}\n",
    "result1_df = pd.DataFrame(data = result1)\n",
    "\n",
    "result1_df.to_csv(\"predictionsRF11-1-17.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25cf578",
   "metadata": {},
   "source": [
    "** KAGGLE SCORE: 0.259 **"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de191d20",
   "metadata": {},
   "source": [
    "## Rerun on clean dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710ab1ad",
   "metadata": {},
   "source": [
    "Now we try again but use the cleaned dataset with NA's imputed with the median."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
