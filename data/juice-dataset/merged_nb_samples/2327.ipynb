{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7249975",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "##### _Author: Calvin Chi_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ca329",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Viva Slots is a mobile app featuring slot machines with the option of credit purchases. The producer Rocket Games is interested in predicting player churn, defined as not making the next purchase from the previous purchase within a time period. Being able to predict players who are unlikely to pay again within a time frame allows Rocket Games engineers to target the right customers with incentives to encourage them to pay again, potentially improving business.\n",
    "\n",
    "<img src=\"http://i.imgur.com/x4bzboV.jpg\", width=500, height=500> \n",
    "\n",
    "To perform this data analysis we have collected features about individual customers as well as time-series data on in-game features such as number of level-ups within a time frame. Since an individual customer may have multiple transactions, we will define each transaction as a sample in our dataset. \n",
    "\n",
    "Our problem is thus given that a purchase has been made, what is the probability of it occurring again in the next 7, 14, or 30 days. We will choose the exact days depending on our analysis. Our time-series data and user features will be constructed up to the point of purchase. The test set will also similarly have features only up to the point of purchase. Note also that once the modeling is done, the data we will predict on cannot be generated more than `n` days from the last purchase, otherwise their churn labels will already be known.\n",
    "\n",
    "Let's start with exploratory data analysis! First load the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import sklearn.preprocessing as pp\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import Imputer\n",
    "import sys\n",
    "import pickle\n",
    "import copy\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import grid_search\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348bf2ec",
   "metadata": {},
   "source": [
    "Define file locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b01c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data, y, test_size=0.2)\n",
    "clf = DecisionTreeClassifier(max_depth=8, class_weight=\"balanced\")\n",
    "clf.fit(Xtrain, ytrain)\n",
    "pred = clf.predict(Xtest)\n",
    "print(\"Prediction accuracy on test dataset: \")\n",
    "print(clf.score(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b11ae",
   "metadata": {},
   "source": [
    "Determine the recall score based on 50% decision threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab4fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(Xtest)\n",
    "print(recall_score(ytest, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a62ab",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea303e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(ytest, pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d94c48",
   "metadata": {},
   "source": [
    " Area under the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = clf.predict_proba(Xtest)[:, 1]\n",
    "area = average_precision_score(ytest, prob)\n",
    "print(\"Area under PR Curve\")\n",
    "print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55293429",
   "metadata": {},
   "source": [
    "From all the analysis we have done so far, since there appears to be a significant overlap between positive and negative classes, there is going to be a significant tradeoff between recall and precision. Let us save our splitted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ca0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Xtrain, open(dataDir + \"Xtrain.p\", 'wb'))\n",
    "pickle.dump(ytrain, open(dataDir + \"ytrain.p\", 'wb'))\n",
    "pickle.dump(Xtest, open(dataDir + \"Xtest.p\", 'wb'))\n",
    "pickle.dump(ytest, open(dataDir + \"ytest.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b86ed5c",
   "metadata": {},
   "source": [
    "Load our modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c9b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = pickle.load(open(dataDir + \"Xtrain.p\", \"rb\"))\n",
    "ytrain = pickle.load(open(dataDir + \"ytrain.p\", \"rb\"))\n",
    "Xtest = pickle.load(open(dataDir + \"Xtest.p\", \"rb\"))\n",
    "ytest = pickle.load(open(dataDir + \"ytest.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34475e",
   "metadata": {},
   "source": [
    "Import Calvin's decision tree, which will output all the top features used to separate the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966ccb5",
   "metadata": {},
   "source": [
    "Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af91cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data, open(dataDir + \"data.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87d833",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0730c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(dataDir + \"data.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2821a84",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aec4c8",
   "metadata": {},
   "source": [
    "First set the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ff990",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = data['lapse7']\n",
    "y2 = data['lapse14']\n",
    "y3 = data['lapse30']\n",
    "del data['lapse7']\n",
    "del data['lapse14']\n",
    "del data['lapse30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc02ec",
   "metadata": {},
   "source": [
    "Determine the dimensions of the data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da1562",
   "metadata": {},
   "source": [
    "Scale each feature in the data matrix so that each feature has zero mean and unit variance. This is almost always a necessary step for PCA and a step that cannot hurt to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddbbe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataScale = pp.scale(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776bd2c",
   "metadata": {},
   "source": [
    "Let us whiten the data and plot a PCA of the total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=135, whiten=True)\n",
    "pca_transformed = pca.fit_transform(dataScale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1adec14",
   "metadata": {},
   "source": [
    "Plotting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "posIdx = np.where(y1 == 1)[0]\n",
    "negIdx = np.where(y1 == 0)[0]\n",
    "ax.plot(pca_transformed[negIdx,0], pca_transformed[negIdx, 1], pca_transformed[negIdx, 2], '^', markersize=8, \n",
    "        alpha=0.7, color='red', label='Non-Churn')\n",
    "ax.plot(pca_transformed[posIdx, 0], pca_transformed[posIdx, 1], pca_transformed[posIdx, 2], 'o', markersize=8, \n",
    "        color='blue', alpha=0.7, label='Churn')\n",
    "ax.set_xlabel('PC1 (%.2f)' % (pca.explained_variance_ratio_[0]))\n",
    "ax.set_ylabel('PC2 (%.2f)'% (pca.explained_variance_ratio_[1]))\n",
    "ax.set_zlabel('PC3 (%.2f)' % (pca.explained_variance_ratio_[2]))\n",
    "plt.title(\"PCA (Whole Dataset)\")\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350c8cc",
   "metadata": {},
   "source": [
    "Based on the plot alone it looks like there is a great deal of overlap between our classes. However, the overlap is not perfect, so it is possible to separate out the true negatives. However, plenty of false positives and false negatives are expected because plenty of samples from different classes share very similar features. Let us see how well our three components \"capture\" the total structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbdcfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "plt.title(\"Variance Explained vs PCs (Whole Dataset)\")\n",
    "plt.bar(list(range(1, len(pca.explained_variance_ratio_) + 1)), pca.explained_variance_ratio_,\n",
    "       color=\"g\", align=\"center\")\n",
    "plt.xticks(list(range(1, len(pca.explained_variance_ratio_))), list(range(1, len(pca.explained_variance_ratio_))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf283d",
   "metadata": {},
   "source": [
    "It looks like our first three PCs do a reasonable job in capturing the structure of the data compared with the rest of the components. Let us perform PCA again, but this time without the time series data to assess the performance of the non-time series data alone. We need to first subset the data so that only non-time series data are included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04489bc9",
   "metadata": {},
   "source": [
    "To summarize the features we have: \n",
    "\n",
    "1. idfa: player id\n",
    "2. rn: transaction identifier\n",
    "3. rev: purchase amount\n",
    "4. hasemail: boolean as to whether player provided email. Included because providing email provides a player with extra credits.\n",
    "5. fb_friends: number of facebook friends playing. Included because friends can send gift credits.\n",
    "6. e_viptier: vip tier\n",
    "7. event_time: event time\n",
    "8. e_purchaseamount: number of credits purchased\n",
    "9. credits: credit balance prior to purchase\n",
    "10. e_level: player level\n",
    "11. hours_until: hours between this purchase and next\n",
    "12. hours_prior: hours between last purchase and this purchase\n",
    "13. lapse7: boolean as to whether next purchase was made within next 7 days. Lapse14 and lapse30 are defined accordingly\n",
    "14. ooc: out of credit dialogs in unit time\n",
    "15. ss: number of session starts in unit time\n",
    "16. hb: number of heartbeats in unit time\n",
    "17. qw: number of quality wins in unit time\n",
    "18. sp: number of spins in unit time\n",
    "19. lu: number of level ups in unit time\n",
    "20. pv: number of purchases in unit time\n",
    "21. rev: sum of revenue in unit time\n",
    "22. chb: number of hourly bonus collections\n",
    "\n",
    "Note that for the time series data, `ooc_5_4d` means number of out of credit dialogs between day 4 and day 5 prior to current purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d066d07",
   "metadata": {},
   "source": [
    "# Class Distribution\n",
    "Let us view the distribution of classes by label definition:\n",
    "    \n",
    "    1. lapse7\n",
    "    2. lapse14\n",
    "    3. lapse30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbc80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "labels = 'Not Purchased', 'Purchased'\n",
    "positive = sum(data['lapse7'] == 1)\n",
    "negative = sum(data['lapse7'] == 0)\n",
    "sizes = [positive, negative]\n",
    "colors = ['yellowgreen', 'lightcoral']\n",
    "plt.pie(sizes, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5240c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = sum(data['lapse14'] == 1)\n",
    "negative = sum(data['lapse14'] == 0)\n",
    "sizes = [positive, negative]\n",
    "colors = ['yellowgreen', 'lightcoral']\n",
    "plt.pie(sizes, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=30)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c68a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = sum(data['lapse30'] == 1)\n",
    "negative = sum(data['lapse30'] == 0)\n",
    "sizes = [positive, negative]\n",
    "colors = ['yellowgreen', 'lightcoral']\n",
    "plt.pie(sizes, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=0)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da005cd",
   "metadata": {},
   "source": [
    "We see that as we increase our lapse period, the number of transactions that do not churn decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8720ea",
   "metadata": {},
   "source": [
    "# Feature Engineering 1\n",
    " Features that represent unique IDs are not generalizable for classification and should be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87eb577",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['idfa']\n",
    "del data['rn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619c0dc",
   "metadata": {},
   "source": [
    "We may want to convert `event_time` to a time stamp. For example, `event_time` could be converted to a feature that represents the number of minutes from or to midnight, whichever is less. This time stamp may be potentially useful because it is possible more frequent transactions may occur at a different time of the day than less frequent transactions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ca808",
   "metadata": {},
   "source": [
    "### Date Time\n",
    "First check what data structure `event_time` is stored as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c71d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['event_time'][0])\n",
    "print(type(data['event_time'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c1cb02",
   "metadata": {},
   "source": [
    "Looks like it is a string, therefore we need to convert to datetime format, which can in turn be used to calculate the time to/from midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(newData, open(dataDir + \"data.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2128a977",
   "metadata": {},
   "source": [
    "### Change of Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce0311c",
   "metadata": {},
   "source": [
    "Given that time series data seem to be more volatile, we may be interested in the change in time-series over time. For example we may create a feature representing the difference between `chb_2_1d` and `chb_1_0d`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65bcff",
   "metadata": {},
   "source": [
    "Let's subset the data to get only the time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92959f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = data.iloc[:, 8:134]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b095722",
   "metadata": {},
   "source": [
    "Now calculate the difference between the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "originalFeatures = subset.columns.values.tolist()\n",
    "newFeatureName = [x + \"Diff\" for x in originalFeatures]\n",
    "newFeatures = pd.DataFrame()\n",
    "\n",
    "for i in range(0, 126, 14):\n",
    "    sub = subset.iloc[:, i:i+14]\n",
    "    sub = sub.diff(axis=1)\n",
    "    newFeatures = pd.concat([newFeatures, sub], axis=1)\n",
    "\n",
    "newFeatures.columns = newFeatureName\n",
    "newFeatures.dropna(inplace=True, axis=1)\n",
    "newFeatures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9723d",
   "metadata": {},
   "source": [
    "Let us perform PCA on these differences to see how well separated the two classes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = pp.scale(newFeatures)\n",
    "pca = PCA(n_components=113, whiten=True)\n",
    "pca_transformed = pca.fit_transform(diff)\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "posIdx = np.where(y1 == 1)[0]\n",
    "negIdx = np.where(y1 == 0)[0]\n",
    "ax.plot(pca_transformed[negIdx,0], pca_transformed[negIdx, 1], pca_transformed[negIdx, 2], '^', \n",
    "        markersize=7, alpha=0.7, color='red', label='Non-churn')\n",
    "ax.plot(pca_transformed[posIdx, 0], pca_transformed[posIdx, 1], pca_transformed[posIdx, 2], 'o', \n",
    "        markersize=7, color='blue', alpha=0.7, label='Churn')\n",
    "ax.set_xlabel('PC1 (%.2f)' % (pca.explained_variance_ratio_[0]))\n",
    "ax.set_ylabel('PC2 (%.2f)'% (pca.explained_variance_ratio_[1]))\n",
    "ax.set_zlabel('PC3 (%.2f)' % (pca.explained_variance_ratio_[2]))\n",
    "plt.title(\"PCA (Partial Dataset)\")\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5635ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData = pd.concat([data, newFeatures], axis=1)\n",
    "print(data.shape)\n",
    "print(newData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bb9d9d",
   "metadata": {},
   "source": [
    "It looks like these features can well separate the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391d7c7",
   "metadata": {},
   "source": [
    "Add the labels back to the data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a7100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newData['lapse7'] = y1\n",
    "newData['lapse14'] = y2\n",
    "newData['lapse30'] = y3\n",
    "pickle.dump(newData, open(dataDir + \"data.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d53d464",
   "metadata": {},
   "source": [
    "# New Feature Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771269f",
   "metadata": {},
   "source": [
    "To gauge how well the best engineered features perform in separating classes, we will perform PCA. First load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fda450",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7 = pickle.load(open(dataDir + \"data.p\", 'rb'))\n",
    "y7 = data7['lapse7']\n",
    "del data7['lapse7']\n",
    "del data7['lapse14']\n",
    "del data7['lapse30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae80a4c3",
   "metadata": {},
   "source": [
    "Then subset the features to only included the best engineered features, determined to be the `hours_prior` and range features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f778a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSubset = dataScale[:, list(range(8)) + [134]]\n",
    "pca = PCA(n_components=9, whiten=True)\n",
    "pca_transformed = pca.fit_transform(dataSubset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b10dbda",
   "metadata": {},
   "source": [
    "Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d904cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "posIdx = np.where(y1 == 1)[0]\n",
    "negIdx = np.where(y1 == 0)[0]\n",
    "ax.plot(pca_transformed[negIdx,0], pca_transformed[negIdx, 1], pca_transformed[negIdx, 2], '^', \n",
    "        markersize=7, alpha=0.7, color='red', label='Non-Churn')\n",
    "ax.plot(pca_transformed[posIdx, 0], pca_transformed[posIdx, 1], pca_transformed[posIdx, 2], 'o', \n",
    "        markersize=7, color='blue', alpha=0.7, label='Churn')\n",
    "ax.set_xlabel('PC1 (%.2f)' % (pca.explained_variance_ratio_[0]))\n",
    "ax.set_ylabel('PC2 (%.2f)'% (pca.explained_variance_ratio_[1]))\n",
    "ax.set_zlabel('PC3 (%.2f)' % (pca.explained_variance_ratio_[2]))\n",
    "plt.title(\"PCA (Partial Dataset)\")\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f2e8a",
   "metadata": {},
   "source": [
    "Keeping the time series data makes the data look more separable. Let's look at the data two components at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2709e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "PCs = [(0, 1), (0, 2), (1, 2)]\n",
    "i = 1\n",
    "for pc in PCs:\n",
    "    ax = fig.add_subplot(130 + i)\n",
    "    ax.plot(pca_transformed[posIdx, pc[0]], pca_transformed[posIdx, pc[1]], 'o', markersize=7, color='blue', \n",
    "            alpha=0.7, label='Churn')\n",
    "    ax.plot(pca_transformed[negIdx,pc[0]], pca_transformed[negIdx, pc[1]], '^', markersize=7, alpha=0.7, \n",
    "            color='red', label='Non-Churn')\n",
    "    ax.set_xlabel('PC' + str(pc[0] + 1) + ' (%.2f)' % (pca.explained_variance_ratio_[pc[0]]))\n",
    "    ax.set_ylabel('PC' + str(pc[1] + 1) + ' (%.2f)'% (pca.explained_variance_ratio_[pc[1]]))\n",
    "    plt.rcParams['legend.fontsize'] = 7\n",
    "    plt.title(\"PC\" + str(pc[0] + 1) + \" vs \" + \"PC\" + str(pc[1] + 1) +  \"(Partial Dataset)\")\n",
    "    ax.legend(loc='upper right')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd110aa",
   "metadata": {},
   "source": [
    "Indeed, the non-time series features alone will not be very useful in distinguishing the two classes. \n",
    "\n",
    "Let us plot each time-series feature mean against time to see if there are any differences between churn and no churn. In the plots below, increasing time values indicate intervals closer to purchase (ie. `ooc_21_14d` vs `ooc_2_1d`). Keep in mind that for each time-series feature, there are two groups of intervals - intervals over 7 days and intervals over 1 day. Intervals closer to purchase are defined over a day. Bars included represent the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSubset = data.ix[:, 8:134]\n",
    "fig = plt.figure(figsize = (15, 45))\n",
    "titles = [\"Out of Credit\", \"Session Starts\", \"HeartBeats\", \"Quality Wins\", \"Spins\", \"Levelups\", \"Purchases\", \n",
    "         \"Revenue\", \"Number of Hourly Bonus\"]\n",
    "counter = 0\n",
    "for i in range(0, 126, 14):\n",
    "    ax = fig.add_subplot(910 + counter + 1)\n",
    "    pos = dataSubset.ix[posIdx, i:i+14]\n",
    "    neg = dataSubset.ix[negIdx, i:i+14]\n",
    "    posMean = pos.apply(np.mean)\n",
    "    negMean = neg.apply(np.mean)\n",
    "    posStd = pos.apply(np.std)\n",
    "    negStd = neg.apply(np.std)\n",
    "    ax.errorbar(list(range(1, 15)), posMean, yerr=posStd, color='blue', label=\"Churn\")\n",
    "    ax.errorbar(list(range(1, 15)), negMean, yerr=negStd, color='red', label=\"Non-Churn\")\n",
    "    ax.set_xlabel(\"time\")\n",
    "    ax.set_title(titles[counter])\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.rcParams['legend.fontsize'] = 15\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb030b5f",
   "metadata": {},
   "source": [
    "A couple of observations can be made from these plots:\n",
    "\n",
    "1. In general, non-churn players have higher mean feature values than churn players, although the significance of the difference may not be high.\n",
    "2. In general, non-churn players have larger standard deviations than churn players.\n",
    "\n",
    "These observations suggest that churn transactions in general are more diverse than non-churn playes. \n",
    "\n",
    "# Feature Engineering 2\n",
    "### Number of Times Exceeding 1-2 SD\n",
    "Non-churn players seem to have more volatile in-game experiences. We will leverage this to create new features as the number of times a feature value exceeds or goes below 1-2 standard deviations from the mean value over time, and see if it is a good feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "newFeatures = {}\n",
    "features = [\"oocNew\", \"ssNew\", \"hbNew\", \"qwNew\", \"spinsNew\", \"luNew\", \"purchaseNew\", \n",
    "         \"revNew\", \"bonusNew\"]\n",
    "posIdx = np.where(y1 == 1)[0]\n",
    "negIdx = np.where(y1 == 0)[0]\n",
    "counter = 0\n",
    "\n",
    "for i in range(0, 126, 14):\n",
    "    sub = dataSubset.ix[:, i:i+14].as_matrix()\n",
    "    means = np.mean(sub, axis=1).reshape(sub.shape[0], 1)\n",
    "    stds = np.std(sub, axis=1).reshape(sub.shape[0], 1)\n",
    "    truth = (sub > means + 1*stds) | (sub < means - 1*stds)\n",
    "    newFeatures[counter] = np.sum(truth, axis=1)\n",
    "    counter += 1\n",
    "\n",
    "newFeatures = pd.DataFrame(newFeatures)\n",
    "newFeatures.columns = features\n",
    "newFeatures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3f966",
   "metadata": {},
   "source": [
    "Let us see how well our newly created features separate out churn players from non-churn players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c27aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "newFeatures = newFeatures.as_matrix()\n",
    "pos = newFeatures[posIdx, :]\n",
    "neg = newFeatures[negIdx, :]\n",
    "posMeans = np.mean(pos, axis=0)\n",
    "posStds = np.std(pos, axis=0)\n",
    "negMeans = np.mean(neg, axis=0)\n",
    "negStds = np.std(neg, axis=0)\n",
    "plt.figure(figsize = (10, 5))\n",
    "plt.errorbar(list(range(len(features))), posMeans, yerr=posStds, color=\"blue\", label=\"Churn\")\n",
    "plt.errorbar(list(range(len(features))), negMeans, yerr=negStds, color=\"red\", label=\"Non-Churn\")\n",
    "plt.xlim([-0.5, 8.5])\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c521d429",
   "metadata": {},
   "source": [
    "Unfortunately this new feature doesn't seem to capture the differences well. However, there is little harm to adding this new feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e71a6",
   "metadata": {},
   "source": [
    "We should delete `hours_until`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e99907",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['hours_until']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc22ed6f",
   "metadata": {},
   "source": [
    "### Lapse 30\n",
    "Impute the `hours_prior` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a7c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data30 = copy.deepcopy(data)\n",
    "pos = np.squeeze(data30.loc[(data30['hours_prior'] != '(null)') & (data30['lapse30'] == 1), \n",
    "                          ['hours_prior']]).astype('int')\n",
    "print(pos.shape)\n",
    "neg = np.squeeze(data30.loc[(data30['hours_prior'] != '(null)') & (data30['lapse30'] == 0), \n",
    "                              ['hours_prior']]).astype('int')\n",
    "print(neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6011ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "posMean = int(np.mean(pos))\n",
    "negMean = int(np.mean(neg))\n",
    "data30.loc[(data30['lapse30'] == 1) & (data30['hours_prior'] == '(null)'), ['hours_prior']] = posMean\n",
    "data30.loc[(data30['lapse30'] == 0) & (data30['hours_prior'] == '(null)'), ['hours_prior']] = negMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y30 = data30['lapse30']\n",
    "del data30['lapse7']\n",
    "del data30['lapse14']\n",
    "del data30['lapse30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51117c",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782466c8",
   "metadata": {},
   "source": [
    "We will use a decision tree to roughly evaluate the area under the precison-recall curve when the label is defined according to `lapse7`, `lapse14` and `lapse30` respectively. We will pick the definition that allows us to achieve maximum area under the precision-recall curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a324010",
   "metadata": {},
   "source": [
    "#### Lapse 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3629637",
   "metadata": {},
   "source": [
    "Set the label for lapse7 and delete all the other labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d1e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7 = copy.deepcopy(data)\n",
    "y7 = data7['lapse7']\n",
    "del data7['lapse7']\n",
    "del data7['lapse14']\n",
    "del data7['lapse30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b706a",
   "metadata": {},
   "source": [
    "We use the 80-20 split rule for train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438ba603",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data7, y7, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7b7f88",
   "metadata": {},
   "source": [
    "Let us first do hyperparameter tuning to determine the optimal tree depth for our decision tree. We choose the `class_weight` to be \"balanced\" because we have a class imbalance with a majority negative class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f37f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "depths = {'max_depth':list(range(4, 26, 2))}\n",
    "clf = grid_search.GridSearchCV(dt, depths, scoring='average_precision', cv=3, verbose=True)\n",
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a803f255",
   "metadata": {},
   "source": [
    "Look at all the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a263f14c",
   "metadata": {},
   "source": [
    "It looks like the optimal maximum tree depth is 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a773a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=6, class_weight=\"balanced\")\n",
    "clf.fit(Xtrain, ytrain)\n",
    "print(\"Prediction accuracy on test dataset: \")\n",
    "print(clf.score(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12c5f62",
   "metadata": {},
   "source": [
    "Now evaluate area under precision-recall (PR) curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ede176f",
   "metadata": {},
   "source": [
    "Let us finally plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b55d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(ytest, prob)\n",
    "plt.plot(recall, precision, \"o-\", color=\"black\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Lapse7\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a50e5",
   "metadata": {},
   "source": [
    "#### Lapse14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46251afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data14, y14, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8c974",
   "metadata": {},
   "source": [
    "Let us first do hyperparameter tuning to determine the optimal tree depth for our decision tree. We choose the `class_weight` to be \"balanced\" because we have a class imbalance with a majority negative class. Based on the previous run for `lapse7`, it appears that a tall decision tree leads to suboptimal area under the PR curve, so this will decrease the number of hyperparameters we need to tune this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d4e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "depths = {'max_depth':list(range(4, 16, 2))}\n",
    "clf = grid_search.GridSearchCV(dt, depths, scoring='average_precision', cv=3, verbose=True)\n",
    "clf.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9ffb3",
   "metadata": {},
   "source": [
    "It looks like the optimal maximum tree depth is 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8dc123",
   "metadata": {},
   "source": [
    "Look at the scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a086606b",
   "metadata": {},
   "source": [
    "The most optimal depth is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6774210",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=4, class_weight=\"balanced\")\n",
    "clf.fit(Xtrain, ytrain)\n",
    "print(\"Prediction accuracy on test dataset: \")\n",
    "print(clf.score(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0ecbb",
   "metadata": {},
   "source": [
    "Plot the precision-recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4f446",
   "metadata": {},
   "source": [
    "It appears that lapse7 gives us the best area under the PR curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716d9bbe",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8d450",
   "metadata": {},
   "source": [
    "We are interested in which features are the best at discriminating classes. Since a decision tree automatically performs feature selection, we will leverage this property to help us determine that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DecisionTree import DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8571b",
   "metadata": {},
   "source": [
    "Train the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = DecisionTree(stop=0.35, output=True, minSize=1000)\n",
    "clf2.train(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b48f3b",
   "metadata": {},
   "source": [
    "Let's see how well Calvin's decision tree performs on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf28d84",
   "metadata": {},
   "source": [
    "There doesn't appear to be any preference in purchase time between churn and non-churn transactions. However, it is noteworthy that in general, there is a preference for purchasing close to midnight (~3 hours within midnight). A possible explanation for this is that Viva Slots customers have more time to play and pay during the evenings when have less time during the day due to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d160a7",
   "metadata": {},
   "source": [
    "### Email Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8b028",
   "metadata": {},
   "source": [
    "Convert email status to binary so that we can work with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9825e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hasemail'] = data['hasemail'].astype(int)\n",
    "print(data['hasemail'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19616363",
   "metadata": {},
   "source": [
    "The rationale for including whether user associated with transaction has provided email is because providing email awards the user with in-game credits. However, this is a one-time event and may not be useful in separating churn from non-churn transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2bcb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proportion of churn transactions with email: \")\n",
    "print(np.mean(data.loc[(data['lapse7'] == 1), ['hasemail']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proportion of non-churn transactions with email: \")\n",
    "print(np.mean(data.loc[(data['lapse7'] == 0), ['hasemail']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be736850",
   "metadata": {},
   "source": [
    "Indeed, the proportion of transactions providing email is similar in both classes at around 50%, almost like tossing a coin on each transaction to decide whether the user for the transaction provided email! Some additional features will be constructed based on which lapse we choose, and we choose `lapse7` as the default. First save the data so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6b9f6",
   "metadata": {},
   "source": [
    "Load the data back up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad742962",
   "metadata": {},
   "source": [
    "### Hours Prior to Current Purchase\n",
    "\n",
    "We need to throw away `hours_until` because that feature is used to determine our label (i.e. if `hours_until` > 7 then 1 else 0). However, we can keep `hours_prior` to current purchase.\n",
    "\n",
    "We want to see if we should keep `hours_prior`, which contains `(null)` values corresponding to no purchases prior to current purchase. Inclusion of `hours_prior` requires a strategy for treating `(null)` values. First see how many `(null)` values there are as a percentage of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage null: \")\n",
    "np.mean(data['hours_prior'] == '(null)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79adbb86",
   "metadata": {},
   "source": [
    "A sizeable number of samples have `(null)`, hence we continue on to see if `hours_prior` is worth including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = np.squeeze(data.loc[(data['hours_prior'] != '(null)') & (data['lapse7'] == 1), ['hours_prior']]).astype('int')\n",
    "print(pos.shape)\n",
    "neg = np.squeeze(data.loc[(data['hours_prior'] != '(null)') & (data['lapse7'] == 0), ['hours_prior']]).astype('int')\n",
    "print(neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64515eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos.describe())\n",
    "print(neg.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38232dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([pos])\n",
    "plt.title(\"Churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7352b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([neg])\n",
    "plt.title(\"Non-Churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c813c1",
   "metadata": {},
   "source": [
    "It appears that churn transactions have a larger within-class proportion of transactions who have hours prior to current payment greater than 250 hours. A possible explanation is that churn transactions in general have longer time span between successive purchases. Thus, `hours_prior` may be useful and should be included.\n",
    "\n",
    "To treat the `(null)` values, we are going to impute them with the mean `hours_prior` values of transactions within the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2153db",
   "metadata": {},
   "outputs": [],
   "source": [
    "posMean = int(np.mean(pos))\n",
    "negMean = int(np.mean(neg))\n",
    "data.loc[(data['lapse7'] == 1) & (data['hours_prior'] == '(null)'), ['hours_prior']] = posMean\n",
    "data.loc[(data['lapse7'] == 0) & (data['hours_prior'] == '(null)'), ['hours_prior']] = negMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebaf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileDir = \"../data/payerChurnData_20160722.csv\"\n",
    "dataDir = \"data/\"\n",
    "outputDir = \"output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0d5ff",
   "metadata": {},
   "source": [
    "Read data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ca8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(fileDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07134b57",
   "metadata": {},
   "source": [
    "Determine the data structure holding the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d44c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c085603",
   "metadata": {},
   "source": [
    "Take a peek at the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e442e6",
   "metadata": {},
   "source": [
    "Assess the dimensions and print total list of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adbd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions\")\n",
    "print(data.shape)\n",
    "print(\"\\n\")\n",
    "print(data.columns.values[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bef06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf2.predict(Xtest)\n",
    "print(\"Test set accuracy:\")\n",
    "print(np.mean(pred == ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f43f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = clf2.attributes\n",
    "print(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1415ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(attributes, open(outputDir + \"attributes.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04fafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = pickle.load(open(outputDir + \"attributes.p\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7240b9",
   "metadata": {},
   "source": [
    "It looks like the best features are a combination of newly created range features and time series features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colNames = Xtrain.columns.values\n",
    "idx = np.where(np.in1d(colNames, attributes))[0]\n",
    "print(len(idx))\n",
    "dataSubset = Xtrain.iloc[:, idx]\n",
    "print(dataSubset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d315a0",
   "metadata": {},
   "source": [
    "Let us see how separable the two classes are with these selected 34 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(dataDir + \"data.p\", 'rb'))\n",
    "y = data['lapse7']\n",
    "del data['lapse7']\n",
    "del data['lapse14']\n",
    "del data['lapse30']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40c1aa",
   "metadata": {},
   "source": [
    "Let us first split the dataset and train on a simple decision tree to gauge its performance again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301dbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.squeeze(data.as_matrix(['event_time']))\n",
    "# Transform datetime.strptime function so that it can vectorize\n",
    "func = np.vectorize(datetime.strptime)\n",
    "dtVec = func(temp, \"%Y-%m-%d %H:%M:%S\")\n",
    "print(dtVec[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10a30f",
   "metadata": {},
   "source": [
    "Let us define a function that converts our event times to closest time from/to midnight in minutes. For example, 23:56 pm would result in 4 minutes, 11:55 am would result in 715 minutes, and 13:00 pm would result in 660 minutes. There are 1440 minutes in a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9412ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TimeToFromMidnight(dt):\n",
    "    h = dt.hour\n",
    "    m = dt.minute\n",
    "    mTotal = h*60 + m\n",
    "    if (1440 - mTotal > mTotal):\n",
    "        return mTotal\n",
    "    else:\n",
    "        return (1440 - mTotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9724153b",
   "metadata": {},
   "source": [
    "Perform transformation from event times to minutes to/from midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03acc773",
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeToFromMidnightVec = np.vectorize(TimeToFromMidnight)\n",
    "times = TimeToFromMidnightVec(dtVec)\n",
    "print(times[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd8b558",
   "metadata": {},
   "source": [
    "Add this new timestamp feature to our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['event_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f2839",
   "metadata": {},
   "source": [
    "Let's visualize the distribution of time stamp for churn and non-churn transactions to see if certain transactions have any time preferences between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41446363",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "posTime = np.squeeze(data.loc[(data['lapse7'] == 1), ['timeMN']]).astype('int')\n",
    "negTime = np.squeeze(data.loc[(data['lapse7'] == 0), ['timeMN']]).astype('int')\n",
    "plt.hist([posTime])\n",
    "plt.title(\"Churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([negTime])\n",
    "plt.title(\"Non-Churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adeefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['timeMN'] = times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607b1e4",
   "metadata": {},
   "source": [
    "Delete original `event_time` data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a0044a",
   "metadata": {},
   "source": [
    "Impute the `hours_prior` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16c9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "data14 = copy.deepcopy(data)\n",
    "pos = np.squeeze(data14.loc[(data14['hours_prior'] != '(null)') & (data14['lapse14'] == 1), \n",
    "                          ['hours_prior']]).astype('int')\n",
    "print(pos.shape)\n",
    "neg = np.squeeze(data14.loc[(data14['hours_prior'] != '(null)') & (data14['lapse14'] == 0), \n",
    "                              ['hours_prior']]).astype('int')\n",
    "print(neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posMean = int(np.mean(pos))\n",
    "negMean = int(np.mean(neg))\n",
    "data14.loc[(data14['lapse14'] == 1) & (data14['hours_prior'] == '(null)'), ['hours_prior']] = posMean\n",
    "data14.loc[(data14['lapse14'] == 0) & (data14['hours_prior'] == '(null)'), ['hours_prior']] = negMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "y14 = data14['lapse14']\n",
    "del data14['lapse7']\n",
    "del data14['lapse30']\n",
    "del data14['lapse14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699623d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(dataDir + \"data.p\", 'rb'))\n",
    "newFeatures = pd.DataFrame(newFeatures)\n",
    "newFeatures.columns = features\n",
    "data = pd.concat([data, newFeatures], axis=1)\n",
    "pickle.dump(data, open(dataDir + \"data.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2514a202",
   "metadata": {},
   "source": [
    "### Range\n",
    "Given that churn transactions are more volatile, let us construct a new feature using range (measure of dispersion) over time for each of the 9 time-series features as new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7259ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "newFeatures = {}\n",
    "features = [\"oocRange\", \"ssRange\", \"hbRange\", \"qwRange\", \"spinsRange\", \"luRange\", \"purchaseRange\", \n",
    "         \"revRange\", \"bonusRange\"]\n",
    "counter = 0\n",
    "\n",
    "for i in range(0, 126, 14):\n",
    "    sub = dataSubset.ix[:, i:i+14].as_matrix()\n",
    "    maxVal = np.max(sub, axis=1)\n",
    "    minVal = np.min(sub, axis=1)\n",
    "    r = maxVal - minVal\n",
    "    newFeatures[counter] = r\n",
    "    counter += 1\n",
    "\n",
    "newFeatures = pd.DataFrame(newFeatures)\n",
    "newFeatures.columns = features\n",
    "newFeatures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ed8ea",
   "metadata": {},
   "source": [
    "Let's visualize how well this new feature does at separating classes using histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b20762",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15, 30))\n",
    "posIdx = np.where(y1 == 1)[0]\n",
    "negIdx = np.where(y1 == 0)[0]\n",
    "pos = newFeatures.iloc[posIdx, :]\n",
    "neg = newFeatures.iloc[negIdx, :]\n",
    "counter = 1\n",
    "n = 20\n",
    "for feature in features:\n",
    "    ax = fig.add_subplot(910 + counter)\n",
    "    posF = pos[feature]\n",
    "    negF = neg[feature]\n",
    "    ax.hist([posF, negF], color=['blue', 'red'], label=['Churn', 'Non-churn'])\n",
    "    ax.set_title(feature)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb93545",
   "metadata": {},
   "source": [
    "It looks like this feature may capture the class differences, as non-churn trasactions generally have lower maximum values for each of the 9 features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc4870",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3daba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "posIdx = np.where(y7 == 1)[0]\n",
    "negIdx = np.where(y7 == 0)[0]\n",
    "ax.plot(pca_transformed[negIdx,0], pca_transformed[negIdx, 1], pca_transformed[negIdx, 2], '^', \n",
    "        markersize=7, alpha=0.7, color='red', label='Non-churn')\n",
    "ax.plot(pca_transformed[posIdx, 0], pca_transformed[posIdx, 1], pca_transformed[posIdx, 2], 'o', \n",
    "        markersize=7, color='blue', alpha=0.7, label='Churn')\n",
    "ax.set_xlabel('PC1 (%.2f)' % (pca.explained_variance_ratio_[0]))\n",
    "ax.set_ylabel('PC2 (%.2f)'% (pca.explained_variance_ratio_[1]))\n",
    "ax.set_zlabel('PC3 (%.2f)' % (pca.explained_variance_ratio_[2]))\n",
    "plt.title(\"PCA (Partial Dataset)\")\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb43cc",
   "metadata": {},
   "source": [
    "The separation is a little better, which suggests we also need our existing time-series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23803429",
   "metadata": {},
   "source": [
    "# Which Lapse is Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b853de",
   "metadata": {},
   "source": [
    "Rocket Games is interested in high recall of churn transactions, so the objective is to maximize the area under the precision-recall curve. However, this depends on how we define churn transactions. We have the options of 7 days, 14 days, or even 30 days. Which definition is chosen will depend on the tradeoff between AUCs and how frequent Rocket Games would like the purchases to be. Let us first contruct the `hours_prior` feature for `lapse14` and `lapse30` respectively, and compare all the AUCs to see which lapse will allow both maximal recall and precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a60640",
   "metadata": {},
   "source": [
    "### Lapse14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a886c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7 = data7.ix[:, ['hours_prior', \"oocRange\", \"ssRange\", \"hbRange\", \"qwRange\", \"spinsRange\", \"luRange\", \n",
    "                     \"purchaseRange\", \"revRange\", \"bonusRange\"]]\n",
    "print(data7.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1230870",
   "metadata": {},
   "source": [
    "Scale and fit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf50bfc",
   "metadata": {},
   "source": [
    "It looks like these new features can provide some degree of class separation. Let us visualize the PCA plot with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7 = pp.scale(data7)\n",
    "pca = PCA(n_components=153, whiten=True)\n",
    "pca_transformed = pca.fit_transform(data7)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
