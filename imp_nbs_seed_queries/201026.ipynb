{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f8b869",
   "metadata": {},
   "source": [
    "## Combine pre and post pairing data with the pairing data before PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4e76a",
   "metadata": {},
   "source": [
    "When doing PCA, it is useful to combine the pre and post pairing data along with the pairing data to get a large matrix in the temporal sequence pre-pairing, during pairing and post-pairing. This way, one can evaluate the change in stimulation amplitude through the whole session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e908712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pre_during_post_data():\n",
    "    if not os.path.isfile(os.path.join(working_directory, 'pawstimpairingresults','mean_data_for_condition_ALL.hdf5')):\n",
    "        \n",
    "        prepostdata = h5py.File(os.path.join(working_directory, 'pawstimresults','mean_data_for_condition.hdf5'),\n",
    "                                'r')\n",
    "        duringdata = h5py.File(os.path.join(working_directory, 'pawstimpairingresults','mean_data_for_condition.hdf5'),\n",
    "                                'r')\n",
    "\n",
    "        alldata = h5py.File(os.path.join(working_directory, 'pawstimpairingresults','mean_data_for_condition_ALL.hdf5'),\n",
    "                                'a')\n",
    "        for condition in duringdata.keys():\n",
    "            temp = np.concatenate((np.nan_to_num(prepostdata[condition]['pre']),\n",
    "                                   np.nan_to_num(duringdata[condition]),\n",
    "                                   np.nan_to_num(prepostdata[condition]['post'])),axis=3)\n",
    "            print temp.shape\n",
    "            alldata.create_dataset(condition, data=temp)\n",
    "        alldata.close()\n",
    "    alldata = h5py.File(os.path.join(working_directory, 'pawstimpairingresults','mean_data_for_condition_ALL.hdf5'),\n",
    "                                'r')\n",
    "    return alldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = combine_pre_during_post_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_traces_for_each_voxel(alldata, indices_for_windows, numvoxelstoshow=50, periodofinterest='poststim')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f19142",
   "metadata": {},
   "source": [
    "At this point, you can save the figure if you want, to the path you set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "numvoxelstoshow = 50\n",
    "sortby = 'Stimresponse_stimperiod'\n",
    "path_to_save = os.path.join(results_directory,'Top%dvoxels_pawstim_sortedby%s_ordering'%(numvoxelstoshow,\n",
    "                                                                                                sortby))\n",
    "fig.savefig(path_to_save + '.png', format='png', dpi=300)\n",
    "#fig.savefig(path_to_save + '.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_selective fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eaa24d",
   "metadata": {},
   "source": [
    "## Convert data into PCA space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d70bf8e",
   "metadata": {},
   "source": [
    "Now, we will do PCA on the data to get an unbiased factorization of the data. The PCA is done to compress the dimensionality of the voxels. These first set of functions are useful for performing the PCA and visualizing their raw results. Additional functions for advanced visualization will be defined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c84afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # This function flattens the data along the spatial dimensions. So size of data will\n",
    "    # change from (numx, numy, numz, numt) to (numx*numy*numz, numt)\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. The data to be flattened\n",
    "    \n",
    "    # Returns: Flattened data\n",
    "    flattened_data = np.reshape(data,(np.prod(data.shape[:-1]), data.shape[3]))\n",
    "    return flattened_data\n",
    "\n",
    "def PCA_decomp(data, \n",
    "               pca_results_path, \n",
    "               indices_for_windows,\n",
    "               min_variance_explained=0.8):\n",
    "    # This function is called to do PCA decomposition. This checks if the PCA has already\n",
    "    # been done by checking if a pickled file exists in pca_results_path. If this file\n",
    "    # exists, it just loads those results. Otherwise, it performs the PCA decomposition.\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)\n",
    "    #   2. pca_results_path: Path to where the PCA result would have been stored\n",
    "    #                        if PCA has already been performed for this data.\n",
    "    #   3. The minimum amount of variance that should be explained. The number of \n",
    "    #      PCs stored will be determined by this number.\n",
    "    \n",
    "    # Returns: Data in PCA space\n",
    "    \n",
    "    if os.path.isfile(pca_results_path): #has PCA already been done?\n",
    "        return transform_from_loadedpca(data, pca_results_path)\n",
    "    else:\n",
    "        return perform_PCA_decomp(data, pca_results_path, indices_for_windows, min_variance_explained)\n",
    "\n",
    "def transform_from_loadedpca(data, \n",
    "                             pca_results_path):\n",
    "    # This function transforms inputted data based on stored PCA results for that data.\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. The data on which PCA was performed.\n",
    "    #   2. pca_results_path: Path to where the PCA result would have been stored\n",
    "    #                        if PCA has already been performed for this data.\n",
    "    \n",
    "    # Returns: Data in PCA space\n",
    "    transformed_data_path = os.path.join(results_directory, 'transformed_data.h5')\n",
    "    if not os.path.isfile(transformed_data_path):\n",
    "        transformed_data_handle = h5py.File(transformed_data_path,'x')\n",
    "    transformed_data_handle = h5py.File(transformed_data_path,'r')\n",
    "    if pca_results_path in transformed_data_handle:\n",
    "        return transformed_data_handle[pca_results_path]\n",
    "    else:\n",
    "        transformed_data_handle.close()\n",
    "        transformed_data_handle = h5py.File(transformed_data_path,'a')\n",
    "        return perform_transformation(data, pca_results_path, transformed_data_handle)    \n",
    "    \n",
    "def perform_transformation(data,\n",
    "                           pca_results_path,\n",
    "                           transformed_data_handle):\n",
    "    flattened_data = preprocess_data(data)\n",
    "    pca = load_calculated_pca(pca_results_path)\n",
    "    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints\n",
    "    transformed_data_handle.create_dataset(pca_results_path, data=compressed_data)\n",
    "    transformed_data_handle.close()\n",
    "    return compressed_data\n",
    "\n",
    "def perform_PCA_decomp(data, \n",
    "                       pca_results_path, \n",
    "                       indices_for_windows,\n",
    "                       min_variance_explained=0.8):\n",
    "    # This function performs the PCA decomposition. This is called only if \n",
    "    # there aren't any results from a previous run stored in pca_results_path.\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. The data to perform PCA on. Shape: (numx, numy, numz, numt)\n",
    "    #   2. pca_results_path: Path to where the PCA result should be stored.\n",
    "    #   3. The minimum amount of variance that should be explained. The number of \n",
    "    #      PCs stored will be determined by this number.\n",
    "    \n",
    "    # Returns: Data in PCA space\n",
    "    \n",
    "    flattened_data = preprocess_data(data)\n",
    "    pca = PCA(n_components=min_variance_explained)\n",
    "    pca.fit(flattened_data.T) \n",
    "    compressed_data = pca.transform(flattened_data.T).T #transform back to shape n_components x n_timepoints\n",
    "    pca, compressed_data = standardize_pca_sign(pca, compressed_data, indices_for_windows)\n",
    "    joblib.dump(pca, pca_results_path)\n",
    "    return compressed_data\n",
    "\n",
    "def load_calculated_pca(pca_results_path):\n",
    "    # This function loads the calculated PCA object.\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. pca_results_path: Path to where the PCA result would have been stored\n",
    "    #                        if PCA has already been performed for this data.\n",
    "    \n",
    "    # Returns: sklearn PCA object\n",
    "    pca = joblib.load(pca_results_path)\n",
    "    return pca\n",
    "\n",
    "def standardize_pca_sign(pca, \n",
    "                         compressed_data,\n",
    "                         indices_for_windows,\n",
    "                         criterion='positivestimresponse'):\n",
    "    # The PCs are only defined upto a negative sign, i.e. a 180 degree rotated PC vector\n",
    "    # is equivalently a PC vector. This function prevents this ambiguity by enforcing\n",
    "    # the sign of each PC to be such that the derivative of the trace is positive at the \n",
    "    # onset of the first stimulation. Other forms of standardization could also be used.  \n",
    "    # Obviously, this works only for this current experiment with stimulation. In case\n",
    "    # there is no stimulation, you could set the criterion to \"positiveslope\" in which \n",
    "    # case the function will ensure that the PC's trace has a positive linear trend \n",
    "    # through the recording.\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. The sklearn PCA object.\n",
    "    #   2. compressed_data: Data in PCA space\n",
    "    #   3. indices for windows. Explained above\n",
    "    #   4. criterion for standardization. Set to positivestimresponse to ensure that\n",
    "    #      the PCs have a positive stimulation response.\n",
    "    \n",
    "    # Returns: Sign standardized input parameters\n",
    "    for pc in range(compressed_data.shape[0]):\n",
    "        trace = compressed_data[pc,:].T\n",
    "        if criterion=='positivestimresponse':\n",
    "            if np.diff(trace)[indices_for_windows[1]]<0:\n",
    "                compressed_data[pc,:] = -compressed_data[pc,:]\n",
    "                pca.components_[pc,:] = -pca.components_[pc,:]  \n",
    "        elif criterion=='positiveslope':\n",
    "            time = np.arange(trace.shape[0]).T\n",
    "            time = sm.add_constant(time)\n",
    "            lm = sm.OLS(trace, time).fit()\n",
    "            if lm.params[1] < 0: #if slope < 0, flip the PC vector and the trace\n",
    "                compressed_data[pc,:] = -compressed_data[pc,:]\n",
    "                pca.components_[pc,:] = -pca.components_[pc,:]\n",
    "    return pca, compressed_data\n",
    "\n",
    "def extract_pc_vectors(pca_results_path, \n",
    "                       (numx, numy, numz)):\n",
    "    # This function is used to extract the PC vectors from the stored results \n",
    "    # in pca_results_path and reshapes them to the original voxel tiling.\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. PCA results path\n",
    "    \n",
    "    # Returns: pca_vectors\n",
    "    \n",
    "    pca = load_calculated_pca(pca_results_path)\n",
    "    pca_vectors = np.reshape(pca.components_.T, (numx,numy,numz,pca.components_.shape[0]))\n",
    "    return pca_vectors\n",
    "\n",
    "def plot_variance_explained_per_pc(pca_results_path,\n",
    "                                   fig=None,\n",
    "                                   ax=None,\n",
    "                                   label=''):\n",
    "    # This function plots the % of variance explained by each PC.\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. PCA results path\n",
    "    #   2. Figure handle. Optional. Useful if you want to layer plots\n",
    "    #      across all conditions\n",
    "    #   3. Axis handle. Optional. Same as above.\n",
    "    #   4. Label for the plot. Will be set to a condition when called later.\n",
    "    \n",
    "    # Returns: Figure and axis handle to the plot   \n",
    "    pca = load_calculated_pca(pca_results_path)\n",
    "    if fig is None or ax is None:\n",
    "        fig,ax = plt.subplots()\n",
    "    ax.plot(100*pca.explained_variance_ratio_, '.-', label=label)\n",
    "    ax.set_ylabel('% of variance explained')\n",
    "    ax.set_xlabel('PC number')\n",
    "    return fig, ax\n",
    "\n",
    "def plot_pc_vectors(pca_results_path,\n",
    "                    (numx, numy, numz),\n",
    "                    pc_of_interest):\n",
    "    # This function plots the PC vector for pc_of_interest based on\n",
    "    # the results stored in pca_results_path.\n",
    "    # It returns a figure handle for this plot. One could then iterate over all PCs\n",
    "    # to save the figures to your path of choice.\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. PCA results path\n",
    "    #   2. Spatial shape of the data\n",
    "    #   3. pc_of_interest\n",
    "    \n",
    "    # Returns: Figure handle to the plot.\n",
    "    \n",
    "    pca_vectors = extract_pc_vectors(pca_results_path, (numx, numy, numz))\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 4)\n",
    "    vmax = np.amax(pca_vectors[:,:,:,pc_of_interest])\n",
    "    vmin = np.amin(pca_vectors[:,:,:,pc_of_interest])\n",
    "    vmaxsymmetric = np.maximum(np.abs(vmax),np.abs(vmin))\n",
    "    vminsymmetric = -np.maximum(np.abs(vmax),np.abs(vmin))\n",
    "\n",
    "    temp = np.swapaxes(pca_vectors,0,1) #For making the plot\n",
    "    for ax, zplane in zip(axs.flat, range(0,numz)):\n",
    "        ax.matshow(temp[:,:,zplane,pc_of_interest],\n",
    "                   vmin=vminsymmetric,\n",
    "                   vmax=vmaxsymmetric,\n",
    "                   cmap=plt.get_cmap('seismic'))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    fig.tight_layout()\n",
    "        \n",
    "    return fig\n",
    "\n",
    "        \n",
    "def plot_pc_traces(data_in_pcaspace, pc_of_interest):  \n",
    "    # This function plots the PC trace associated with pc_of_interest\n",
    "    # given the data in pca space.\n",
    "    # Calling PCA_decomp() returns the data in pca space. \n",
    "    # This function calculates a z-score of all the traces.\n",
    "    # So each trace is normalized within itself.\n",
    "    # Hence, note that this function wouldn't be appropriate to compare two traces\n",
    "    # since their magnitudes are normalized within themselves, rather than between\n",
    "    # them.\n",
    "    \n",
    "    #Parameters:\n",
    "    #   1. Data in PCA space\n",
    "    \n",
    "    # Returns: figure handle to the plot.\n",
    "    \n",
    "    temp = (data_in_pcaspace[pc_of_interest,:])\n",
    "    baseline = np.mean(temp[:indices_for_windows[1]])\n",
    "    ztrace = (temp-np.mean(temp))/np.std(temp)#-np.log(temp/baseline) #=R2* multiplied by TE\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.tsplot(ztrace, ax=ax)\n",
    "    ax.set_ylabel('z-score PC signal (score)')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    fig.tight_layout()\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec567934",
   "metadata": {},
   "source": [
    "Calculate the data for the application of PCA. In our case, we will perform PCA on the mean data across all animals and runs for both conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "(numx, numy, numz, numt) = calculate_shape_of_data(f)\n",
    "path_to_meandata_ALL = os.path.join(results_directory,'mean_data_for_condition_ALL.hdf5')\n",
    "mean_data_for_condition = calculate_meansignal_across_animals(f, (numx,numy,numz,numt), path_to_meandata_ALL,\n",
    "                                                             MIONdata, indices_for_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf6ec5",
   "metadata": {},
   "source": [
    "Now specify the paths where you would like to store the results of the PCA. This set of paths will also be useful if you just want to load the calculated PCA results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c5ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_results_paths = {}\n",
    "for condition in f.keys():\n",
    "    pca_results_paths[condition] = os.path.join(results_directory,condition,'pca_results.pkl')\n",
    "    mkdir_p(os.path.dirname(pca_results_paths[condition]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385ca77b",
   "metadata": {},
   "source": [
    "Now we will perform the PCA for all conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21482973",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = pca_results_paths.keys()\n",
    "conditions.sort()\n",
    "for condition in conditions:\n",
    "    PCA_decomp(mean_data_for_condition[condition], pca_results_paths[condition], indices_for_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6d24d",
   "metadata": {},
   "source": [
    "Since the PCA has been performed for all conditions, the most important thing to check how much variance the different number of consecutive principal components explain. If there are a few PCs that explain a much higher fraction of the variance than the rest of them, these are going to be the important PCs. It's likely that the last PCs explaining roughly equal fraction of the variance are noise-related.\n",
    "\n",
    "We will now plot the percentage of explained variance per PC for all conditions and layer them on top of each other. This can be saved if you choose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f78185",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for condition in conditions:\n",
    "    plot_variance_explained_per_pc(pca_results_paths[condition],fig,ax, label=condition)\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5f8d8",
   "metadata": {},
   "source": [
    "Save the above figure if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "figfile = os.path.join(results_directory,'Percent_variance_explained_by_pcs')\n",
    "fig.savefig(figfile+'.png',format='png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d97409",
   "metadata": {},
   "source": [
    "The next important thing to check is how the traces of activation in the PCA space look. If none of the PCs contain a strong stimulation evoked response, it is likely that there is no stimulation effect. Further, from the above plot of the fraction of variance explained, one can see that only the first 3 or so PCs are likely going to be representing anything interesting. Whether or not this representation is stimulation related is something that we will check by plotting the traces below.\n",
    "\n",
    "It is a good idea to not save all the figure handles separately when plotting them. This will use a lot of memory and likely crash your computer. So save the figures immediately if you wish to save them into a directory so that you can scroll through the figures. Also, once the figures are saved, it's a good idea to clear all those handles so as to clear memory.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
