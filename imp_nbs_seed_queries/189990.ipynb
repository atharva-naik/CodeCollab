{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b1567e",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840b56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from types import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "promo_dataset = pd.read_csv(\"promotion_dataset_subject.csv\", encoding='utf-8')\n",
    "iit_subject_dataset = pd.read_csv('iit_email_subject.csv',encoding='latin1') \n",
    "iit_subject_dataset = iit_subject_dataset.drop('Unnamed: 0',1)\n",
    "promo_dataset = promo_dataset.drop('Unnamed: 0',1)\n",
    "New_dataset = iit_subject_dataset.append(promo_dataset,ignore_index=True)\n",
    "New_dataset['Email'] = New_dataset[['From', 'Subject']].apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "New_dataset=New_dataset.fillna(9)\n",
    "\n",
    "drop_list=[]\n",
    "for i in range(0,len(New_dataset['Label'])):\n",
    "    if New_dataset.iloc[i]['Label']==9:\n",
    "        drop_list.append(i)\n",
    "\n",
    "New_dataset = New_dataset.drop(drop_list).reset_index(drop=True)\n",
    "New_dataset.Label = New_dataset.Label.astype(int)\n",
    "Y = New_dataset.iloc[:,4:5]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21baf0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(New_dataset.groupby('Label').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#vectorizer = CountVectorizer(max_df=0.5,stop_words='english')#shape=(500, 20928) for Email\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (wnl.lemmatize(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=stemmed_words,binary=True)#shape=(500, 20936) for Email\n",
    "X_sparse = vectorizer.fit_transform(New_dataset['Email'])#shape=(561, 1159)\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "X = pd.DataFrame(X_sparse.todense()) \n",
    "\n",
    "print(\"feature total number :\",len(feature_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb6ef4",
   "metadata": {},
   "source": [
    "## Set Classifier and parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c312e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Sampling_lwoR():\n",
    "    RS_Lwo_R=[]\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.99, random_state = seed)\n",
    "    for i in range(0,Budget): \n",
    "        length_trainset.append(len(X_train))\n",
    "        nb.fit(X_train, Y_train)\n",
    "        predicted = nb.predict(X_test)\n",
    "        RS_Lwo_R.append(nb.score(X_test,Y_test))\n",
    "        X_train = X_train.append(X_test.iloc[0:5])\n",
    "        X_test = X_test.drop(X_test.iloc[0:5].index)\n",
    "        Y_train = Y_train.append(Y_test.iloc[0:5])\n",
    "        Y_test = Y_test.drop(Y_test.iloc[0:5].index)\n",
    "Random_Sampling_lwoR()\n",
    "print(\"Accuracy: \",RS_Lwo_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87678e73",
   "metadata": {},
   "source": [
    "## Finding_Rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03021ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Social_rationale_list=[\"linkedin\",\"indeed\",\"datacamp\",\"ita\"]\n",
    "Primary_rationale_list=[\"iit\",\"piazza\",\"submission\",\"handshake\",\"collaborate\",\"confirm\",\"confirmation\"\n",
    ",\"edu\",\"application\",\"association\",\"apics\",\"hawklink\",\"comsubmission\"]\n",
    "promo_rationale_list=[\"purchase\",\"special\",\"off\",\"gift\",\"sample\",\"shipping\",\"promotion\",\"left\"\n",
    ",\"tonight\",\"sale\",\"new\",\"miss\",\"save\",\"discount\",\"free\",\"last\",\"only\"\n",
    ",\"soon\",\"deal\",\"exclusive\",\"today\",\"ends\",\"best\",\"prices\",\"weekend\",\"holiday\",\"now\"\n",
    ",\"reward\",\"final\",\"release\",\"clearance\",\"collection\",\"minute\"]\n",
    "\n",
    "Rationale_list = [Primary_rationale_list,promo_rationale_list,Social_rationale_list]\n",
    "                \n",
    "def findind_rationale(uncertain_index):\n",
    "    print(\"Finding_Rationale: \")\n",
    "    for i, value in enumerate(uncertain_index):\n",
    "        label = Y_test.loc[value,'Label']\n",
    "        for j in range(0,len(feature_name)):\n",
    "            if X_test.loc[value, j] >0:\n",
    "                for k,rationale in enumerate(Rationale_list[label]):       \n",
    "                    if rationale == feature_name[j]:\n",
    "                        print(New_dataset.iloc[value]['Email'])\n",
    "                        print(\"Rationale:\",rationale)\n",
    "                        X_test.set_value(value,j,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90cc792",
   "metadata": {},
   "source": [
    "## Finding_uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b61404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_uncertain(X_test):\n",
    "    print(\"Uncertain_Email\")\n",
    "    predict_proba = nb.predict_proba(X_test)\n",
    "    ratio = []\n",
    "    neural_index = []\n",
    "    for x in range(0,len(X_test)):\n",
    "        ratio.append(predict_proba[x,1]/0.3+predict_proba[x,0]/0.3+predict_proba[x,2]/0.3)\n",
    "    ratio = np.array(ratio)\n",
    "    neural_row = np.argsort(ratio)[::1][:5]\n",
    "    for i,value in enumerate(neural_row):\n",
    "        print(X_test.index[value],New_dataset.iloc[X_test.index[value]]['Email'])\n",
    "        neural_index.append(X_test.index[value])\n",
    "    return(neural_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bfc75",
   "metadata": {},
   "source": [
    "## Random Sampling- Lw/R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fba65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Sampling_lwR():\n",
    "    RS_Lw_R=[]\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.99, random_state = seed)\n",
    "    for i in range(0,Budget):\n",
    "        print(i)\n",
    "        nb.fit(X_train, Y_train)\n",
    "        predicted = nb.predict(X_test)\n",
    "        RS_Lw_R.append(nb.score(X_test,Y_test))\n",
    "        random_list = X_test.iloc[0:5].index\n",
    "        print(random_list)\n",
    "        findind_rationale(random_list)\n",
    "        X_train = X_train.append(X_test.iloc[0:5])\n",
    "        X_test = X_test.drop(X_test.iloc[0:5].index)\n",
    "        Y_train = Y_train.append(Y_test.iloc[0:5])\n",
    "        Y_test = Y_test.drop(Y_test.iloc[0:5].index)\n",
    "\n",
    "Random_Sampling_lwR()\n",
    "print(\"Accuracy: \",RS_Lw_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9600390c",
   "metadata": {},
   "source": [
    "## Active Learning- Lw/oR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Active_Learning_lwoR():\n",
    "    AL_Lwo_R=[]\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.99, random_state = seed)\n",
    "    for i in range(0,Budget): \n",
    "        print(i)\n",
    "        nb.fit(X_train, Y_train)\n",
    "        predicted = nb.predict(X_test)\n",
    "        AL_Lwo_R.append(nb.score(X_test,Y_test))\n",
    "        uncertain_list = finding_uncertain(X_test)\n",
    "        for i, value in enumerate(uncertain_list):\n",
    "            X_train = X_train.append(X_test.loc[value])\n",
    "            Y_train = Y_train.append(Y_test.loc[value])\n",
    "            X_test = X_test.drop(value)\n",
    "            Y_test = Y_test.drop(value)\n",
    "Active_Learning_lwoR()\n",
    "print(\"Accuracy: \",AL_Lwo_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaef2a5",
   "metadata": {},
   "source": [
    "## Active Learning- Lw/R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1408a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Active_Learning_lwR():\n",
    "    AL_Lw_R=[]\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.99, random_state = seed)\n",
    "    for i in range(0,Budget):\n",
    "        print(i)\n",
    "        nb.fit(X_train, Y_train)\n",
    "        predicted = nb.predict(X_test)\n",
    "        AL_Lw_R.append(nb.score(X_test,Y_test))\n",
    "        uncertain_list = finding_uncertain(X_test)\n",
    "        findind_rationale(uncertain_list)\n",
    "        for i, value in enumerate(uncertain_list): \n",
    "            X_train = X_train.append(X_test.loc[value])\n",
    "            Y_train = Y_train.append(Y_test.loc[value])\n",
    "            X_test = X_test.drop(value)\n",
    "            Y_test = Y_test.drop(value)\n",
    "Active_Learning_lwR()\n",
    "print(\"Accuracy: \",AL_Lw_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813405cc",
   "metadata": {},
   "source": [
    "## Compare the result Lw/R & Lw/oR -CountVectorizer"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
