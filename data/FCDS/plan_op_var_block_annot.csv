,id,blockid,block,plan operator,variable,answer,answer with test
0,2665,1,"    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction",TODO,diction,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
1,2665,2,"        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length",TODO,value,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
2,2665,3,"        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length",TODO,length,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
3,1072,1,"    full_movie = pd.merge(movies, ratings, left_index=True, right_on=""item_id"", how=""left"")
    full_movie[genres] = full_movie[genres].multiply(full_movie[""rating""], axis=""index"")
    full_movie_with_nans = full_movie.replace(0, np.NaN)",TODO,full_movie,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    full_movie = pd.merge(movies, ratings, left_index=True, right_on=""item_id"", how=""left"")
    full_movie[genres] = full_movie[genres].multiply(full_movie[""rating""], axis=""index"")
    full_movie_with_nans = full_movie.replace(0, np.NaN)
    genre_rating = {}
    for genre in genres:
        genre_rating[genre] = full_movie_with_nans[genre].mean()
    return genre_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    full_movie = pd.merge(movies, ratings, left_index=True, right_on=""item_id"", how=""left"")
    full_movie[genres] = full_movie[genres].multiply(full_movie[""rating""], axis=""index"")
    full_movie_with_nans = full_movie.replace(0, np.NaN)
    genre_rating = {}
    for genre in genres:
        genre_rating[genre] = full_movie_with_nans[genre].mean()
    return genre_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
4,1072,2,"    full_movie_with_nans = full_movie.replace(0, np.NaN)
    genre_rating = {}
    for genre in genres:
        genre_rating[genre] = full_movie_with_nans[genre].mean()",TODO,full_movie_with_nans,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    full_movie = pd.merge(movies, ratings, left_index=True, right_on=""item_id"", how=""left"")
    full_movie[genres] = full_movie[genres].multiply(full_movie[""rating""], axis=""index"")
    full_movie_with_nans = full_movie.replace(0, np.NaN)
    genre_rating = {}
    for genre in genres:
        genre_rating[genre] = full_movie_with_nans[genre].mean()
    return genre_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    full_movie = pd.merge(movies, ratings, left_index=True, right_on=""item_id"", how=""left"")
    full_movie[genres] = full_movie[genres].multiply(full_movie[""rating""], axis=""index"")
    full_movie_with_nans = full_movie.replace(0, np.NaN)
    genre_rating = {}
    for genre in genres:
        genre_rating[genre] = full_movie_with_nans[genre].mean()
    return genre_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
5,1072,3,"    genre_rating = {}
    for genre in genres:
        genre_rating[genre] = full_movie_with_nans[genre].mean()
    return genre_rating",TODO,genre_rating,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    full_movie = pd.merge(movies, ratings, left_index=True, right_on=""item_id"", how=""left"")
    full_movie[genres] = full_movie[genres].multiply(full_movie[""rating""], axis=""index"")
    full_movie_with_nans = full_movie.replace(0, np.NaN)
    genre_rating = {}
    for genre in genres:
        genre_rating[genre] = full_movie_with_nans[genre].mean()
    return genre_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    full_movie = pd.merge(movies, ratings, left_index=True, right_on=""item_id"", how=""left"")
    full_movie[genres] = full_movie[genres].multiply(full_movie[""rating""], axis=""index"")
    full_movie_with_nans = full_movie.replace(0, np.NaN)
    genre_rating = {}
    for genre in genres:
        genre_rating[genre] = full_movie_with_nans[genre].mean()
    return genre_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
6,21310,1,"    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]",TODO,df,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
7,21310,2,"    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict",TODO,dict,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
8,21310,3,"        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()",TODO,df2,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
9,21310,4,"        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean",TODO,mean,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
10,15440,1,"    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y",TODO,rating_x,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
11,15440,2,"    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y",TODO,count_x,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
12,15440,3,"    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y",TODO,y,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
13,15440,4,"    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)",TODO,rating_z,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
14,15440,5,"    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)",TODO,count_z,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
15,15440,6,"    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count",TODO,total_rating,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
16,15440,7,"    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count",TODO,total_count,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
17,15440,8,"    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}",TODO,avg_rating,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
18,15440,9,"    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating",TODO,genre_avg_rating,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_x = ratings.groupby('item_id').agg({'rating':'sum'})
    count_x = ratings.groupby('item_id').agg({'rating':'count'})
    y = movies.loc[rating_x.index.values,genres].values
    rating_z = rating_x.values * y
    count_z = count_x.values * y
    total_rating = np.sum(rating_z, axis=0)
    total_count = np.sum(count_z, axis=0)
    avg_rating = total_rating / total_count
    genre_avg_rating = {genres[i]: avg_rating[i] for i in range(len(genres))}
    return genre_avg_rating
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
19,17143,1,"    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]",TODO,df,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
20,17143,2,"    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict",TODO,dict,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
21,17143,3,"        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()",TODO,df2,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
22,17143,4,"        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean",TODO,mean,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    dict = {}
    for x in genres:
        df2 = df[df.loc[:,x] >0]
        mean = df2.loc[:,'rating'].mean()
        dict[x] = mean
    return dict
   
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
23,17899,1,"    by_genre = {}
    for genre in genres:
        df = ratings.merge(movies[movies[genre]==1], left_on=""item_id"", right_on=""movie_id"", how=""inner"")
        by_genre[genre]=df[""rating""].sum()/len(df)
    return by_genre",TODO,by_genre,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    by_genre = {}
    for genre in genres:
        df = ratings.merge(movies[movies[genre]==1], left_on=""item_id"", right_on=""movie_id"", how=""inner"")
        by_genre[genre]=df[""rating""].sum()/len(df)
    return by_genre
    pass","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    by_genre = {}
    for genre in genres:
        df = ratings.merge(movies[movies[genre]==1], left_on=""item_id"", right_on=""movie_id"", how=""inner"")
        by_genre[genre]=df[""rating""].sum()/len(df)
    return by_genre
    pass
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
24,17899,2,"        df = ratings.merge(movies[movies[genre]==1], left_on=""item_id"", right_on=""movie_id"", how=""inner"")
        by_genre[genre]=df[""rating""].sum()/len(df)",TODO,df,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    by_genre = {}
    for genre in genres:
        df = ratings.merge(movies[movies[genre]==1], left_on=""item_id"", right_on=""movie_id"", how=""inner"")
        by_genre[genre]=df[""rating""].sum()/len(df)
    return by_genre
    pass","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    by_genre = {}
    for genre in genres:
        df = ratings.merge(movies[movies[genre]==1], left_on=""item_id"", right_on=""movie_id"", how=""inner"")
        by_genre[genre]=df[""rating""].sum()/len(df)
    return by_genre
    pass
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
25,23219,1,"    genre_avg = dict()
    
    for genre in genres:
        genre_avg[genre] = np.mean(ratings[ratings[""item_id""].isin(movies[movies[genre]>0].index)]['rating'])
        
    return genre_avg",TODO,genre_avg,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    genre_avg = dict()
    
    for genre in genres:
        genre_avg[genre] = np.mean(ratings[ratings[""item_id""].isin(movies[movies[genre]>0].index)]['rating'])
        
    return genre_avg","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    genre_avg = dict()
    
    for genre in genres:
        genre_avg[genre] = np.mean(ratings[ratings[""item_id""].isin(movies[movies[genre]>0].index)]['rating'])
        
    return genre_avg
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
26,20194,1,"    ratings_genre={}

    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg
    
    return ratings_genre",TODO,ratings_genre,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ratings_genre={}

    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg
    
    return ratings_genre","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ratings_genre={}

    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg
    
    return ratings_genre
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
27,20194,2,"    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        ",TODO,merge_data,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ratings_genre={}

    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg
    
    return ratings_genre","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ratings_genre={}

    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg
    
    return ratings_genre
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
28,20194,3,"        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        ",TODO,movies_of_genre,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ratings_genre={}

    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg
    
    return ratings_genre","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ratings_genre={}

    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg
    
    return ratings_genre
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
29,20194,4,"        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg",TODO,avg,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ratings_genre={}

    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg
    
    return ratings_genre","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ratings_genre={}

    merge_data = pd.merge(movies,ratings,left_on = 'movie_id', right_on='item_id')

    for i in genres:
        movies_of_genre = merge_data[merge_data[i] == 1].copy()
        avg = movies_of_genre['rating'].mean()
        movies_of_genre.loc[:, 'rating'] = avg
        merge_data.loc[movies_of_genre.index, 'rating'] = movies_of_genre['rating']        
        ratings_genre[i] = avg
    
    return ratings_genre
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
30,21554,1,"    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
#     joined_df = joined_df.sort_values(by=['item_id'])
    
    mp = {}
    for genre in genres: 
        genre_df = joined_df.loc[joined_df[genre]==1]",TODO,joined_df,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
#     joined_df = joined_df.sort_values(by=['item_id'])
    
    mp = {}
    for genre in genres: 
        genre_df = joined_df.loc[joined_df[genre]==1]
        mp[genre] = genre_df['rating'].mean()
    return mp","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
#     joined_df = joined_df.sort_values(by=['item_id'])
    
    mp = {}
    for genre in genres: 
        genre_df = joined_df.loc[joined_df[genre]==1]
        mp[genre] = genre_df['rating'].mean()
    return mp
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
31,21554,2,"    mp = {}
    for genre in genres: 
        genre_df = joined_df.loc[joined_df[genre]==1]
        mp[genre] = genre_df['rating'].mean()
    return mp",TODO,mp,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
#     joined_df = joined_df.sort_values(by=['item_id'])
    
    mp = {}
    for genre in genres: 
        genre_df = joined_df.loc[joined_df[genre]==1]
        mp[genre] = genre_df['rating'].mean()
    return mp","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
#     joined_df = joined_df.sort_values(by=['item_id'])
    
    mp = {}
    for genre in genres: 
        genre_df = joined_df.loc[joined_df[genre]==1]
        mp[genre] = genre_df['rating'].mean()
    return mp
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
32,21554,3,"        genre_df = joined_df.loc[joined_df[genre]==1]
        mp[genre] = genre_df['rating'].mean()",TODO,genre_df,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
#     joined_df = joined_df.sort_values(by=['item_id'])
    
    mp = {}
    for genre in genres: 
        genre_df = joined_df.loc[joined_df[genre]==1]
        mp[genre] = genre_df['rating'].mean()
    return mp","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
#     joined_df = joined_df.sort_values(by=['item_id'])
    
    mp = {}
    for genre in genres: 
        genre_df = joined_df.loc[joined_df[genre]==1]
        mp[genre] = genre_df['rating'].mean()
    return mp
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
33,20005,1,"    rating_movie = ratings.merge(movies, left_on='item_id', right_on='movie_id', how='left')
    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']",TODO,rating_movie,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_movie = ratings.merge(movies, left_on='item_id', right_on='movie_id', how='left')
    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()
        result.update({genre: average_rating})
        
    return result","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_movie = ratings.merge(movies, left_on='item_id', right_on='movie_id', how='left')
    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()
        result.update({genre: average_rating})
        
    return result
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
34,20005,2,"    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()
        result.update({genre: average_rating})
        
    return result",TODO,result,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_movie = ratings.merge(movies, left_on='item_id', right_on='movie_id', how='left')
    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()
        result.update({genre: average_rating})
        
    return result","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_movie = ratings.merge(movies, left_on='item_id', right_on='movie_id', how='left')
    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()
        result.update({genre: average_rating})
        
    return result
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
35,20005,3,"        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()",TODO,filtered,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_movie = ratings.merge(movies, left_on='item_id', right_on='movie_id', how='left')
    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()
        result.update({genre: average_rating})
        
    return result","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_movie = ratings.merge(movies, left_on='item_id', right_on='movie_id', how='left')
    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()
        result.update({genre: average_rating})
        
    return result
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
36,20005,4,"        average_rating = filtered.mean()
        result.update({genre: average_rating})",TODO,average_rating,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_movie = ratings.merge(movies, left_on='item_id', right_on='movie_id', how='left')
    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()
        result.update({genre: average_rating})
        
    return result","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    rating_movie = ratings.merge(movies, left_on='item_id', right_on='movie_id', how='left')
    result = dict()
    for genre in genres:
        filtered = rating_movie.loc[rating_movie[genre] == 1]['rating']
        average_rating = filtered.mean()
        result.update({genre: average_rating})
        
    return result
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
37,16594,1,"    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')",TODO,s1,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
38,16594,2,"    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")",TODO,year,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
39,16594,3,"    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]",TODO,df,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
40,16594,4,"    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict",TODO,dict,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
41,16594,5,"            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()",TODO,df2,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
42,16594,6,"            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()",TODO,Count,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
43,16594,7,"            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0",TODO,dict0,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    s1 = movies.loc[:,""release_date""].str[-4:]

    year = s1.to_frame(name='Year')
    
    year = year.dropna()
    year['Year'] = year['Year'].astype(int)


    df = movies.merge(year, left_on = ""movie_id"", right_on = ""movie_id"", how = ""left"")
   

    dict={}
    for x in genres:
            df2 = df[df.loc[:,x] >0]
            Count = df2.groupby(""Year"").count()
            dict0 = Count.loc[:,x].to_dict()
            dict[x] = dict0
            
    return dict
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
44,19079,1,"    count_ratings = {}
    movies['Year'] = pd.DatetimeIndex(movies['release_date']).year
    for genre in genres:
        movie_genre = movies[movies[genre]==1]
        count_ratings[genre] = movie_genre.groupby(""Year"")[""Year""].count().to_dict()
    return count_ratings",TODO,count_ratings,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    count_ratings = {}
    movies['Year'] = pd.DatetimeIndex(movies['release_date']).year
    for genre in genres:
        movie_genre = movies[movies[genre]==1]
        count_ratings[genre] = movie_genre.groupby(""Year"")[""Year""].count().to_dict()
    return count_ratings
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    count_ratings = {}
    movies['Year'] = pd.DatetimeIndex(movies['release_date']).year
    for genre in genres:
        movie_genre = movies[movies[genre]==1]
        count_ratings[genre] = movie_genre.groupby(""Year"")[""Year""].count().to_dict()
    return count_ratings
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
45,19079,2,"        movie_genre = movies[movies[genre]==1]
        count_ratings[genre] = movie_genre.groupby(""Year"")[""Year""].count().to_dict()",TODO,movie_genre,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    count_ratings = {}
    movies['Year'] = pd.DatetimeIndex(movies['release_date']).year
    for genre in genres:
        movie_genre = movies[movies[genre]==1]
        count_ratings[genre] = movie_genre.groupby(""Year"")[""Year""].count().to_dict()
    return count_ratings
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    count_ratings = {}
    movies['Year'] = pd.DatetimeIndex(movies['release_date']).year
    for genre in genres:
        movie_genre = movies[movies[genre]==1]
        count_ratings[genre] = movie_genre.groupby(""Year"")[""Year""].count().to_dict()
    return count_ratings
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
46,19349,1,"    genre_count = {genre: defaultdict(int) for genre in genres}
    for genre in genres:
        for index, movie in movies.iterrows():
            if movie[genre] == 1:
                try:
                    year = int(movie['release_date'][-4:])
                    genre_count[genre][year] += 1
                except:
                    continue

    return genre_count",TODO,genre_count,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genre_count = {genre: defaultdict(int) for genre in genres}
    for genre in genres:
        for index, movie in movies.iterrows():
            if movie[genre] == 1:
                try:
                    year = int(movie['release_date'][-4:])
                    genre_count[genre][year] += 1
                except:
                    continue

    return genre_count
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genre_count = {genre: defaultdict(int) for genre in genres}
    for genre in genres:
        for index, movie in movies.iterrows():
            if movie[genre] == 1:
                try:
                    year = int(movie['release_date'][-4:])
                    genre_count[genre][year] += 1
                except:
                    continue

    return genre_count
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
47,19349,2,"                    year = int(movie['release_date'][-4:])
                    genre_count[genre][year] += 1",TODO,year,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genre_count = {genre: defaultdict(int) for genre in genres}
    for genre in genres:
        for index, movie in movies.iterrows():
            if movie[genre] == 1:
                try:
                    year = int(movie['release_date'][-4:])
                    genre_count[genre][year] += 1
                except:
                    continue

    return genre_count
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genre_count = {genre: defaultdict(int) for genre in genres}
    for genre in genres:
        for index, movie in movies.iterrows():
            if movie[genre] == 1:
                try:
                    year = int(movie['release_date'][-4:])
                    genre_count[genre][year] += 1
                except:
                    continue

    return genre_count
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
48,17531,1,"    ans = {}
    for genre in genres:
        in_genre = movies.loc[movies[genre]==1,[genre,""release_date""]].copy()
        in_genre.loc[:,""year""]=in_genre[""release_date""].apply(lambda x : int(x[-4:]))
        ans[genre] = in_genre.groupby(""year"").sum(numeric_only=True).squeeze().to_dict()
    return ans",TODO,ans,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    for genre in genres:
        in_genre = movies.loc[movies[genre]==1,[genre,""release_date""]].copy()
        in_genre.loc[:,""year""]=in_genre[""release_date""].apply(lambda x : int(x[-4:]))
        ans[genre] = in_genre.groupby(""year"").sum(numeric_only=True).squeeze().to_dict()
    return ans
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    for genre in genres:
        in_genre = movies.loc[movies[genre]==1,[genre,""release_date""]].copy()
        in_genre.loc[:,""year""]=in_genre[""release_date""].apply(lambda x : int(x[-4:]))
        ans[genre] = in_genre.groupby(""year"").sum(numeric_only=True).squeeze().to_dict()
    return ans
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
49,17531,2,"        in_genre = movies.loc[movies[genre]==1,[genre,""release_date""]].copy()
        in_genre.loc[:,""year""]=in_genre[""release_date""].apply(lambda x : int(x[-4:]))
        ans[genre] = in_genre.groupby(""year"").sum(numeric_only=True).squeeze().to_dict()",TODO,in_genre,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    for genre in genres:
        in_genre = movies.loc[movies[genre]==1,[genre,""release_date""]].copy()
        in_genre.loc[:,""year""]=in_genre[""release_date""].apply(lambda x : int(x[-4:]))
        ans[genre] = in_genre.groupby(""year"").sum(numeric_only=True).squeeze().to_dict()
    return ans
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    for genre in genres:
        in_genre = movies.loc[movies[genre]==1,[genre,""release_date""]].copy()
        in_genre.loc[:,""year""]=in_genre[""release_date""].apply(lambda x : int(x[-4:]))
        ans[genre] = in_genre.groupby(""year"").sum(numeric_only=True).squeeze().to_dict()
    return ans
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
50,2279,1,"    genres_dict = {}
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)
    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count

    
    return genres_dict",TODO,genres_dict,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genres_dict = {}
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)
    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count

    
    return genres_dict
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genres_dict = {}
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)
    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count

    
    return genres_dict
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
51,2279,2,"    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        ",TODO,movies_year,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genres_dict = {}
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)
    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count

    
    return genres_dict
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genres_dict = {}
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)
    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count

    
    return genres_dict
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
52,2279,3,"        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}",TODO,pivot_table1,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genres_dict = {}
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)
    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count

    
    return genres_dict
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genres_dict = {}
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)
    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count

    
    return genres_dict
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
53,2279,4,"        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count",TODO,year_count,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genres_dict = {}
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)
    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count

    
    return genres_dict
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    genres_dict = {}
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)
    movies_year = movies.copy()
    movies_year['release_date'] = pd.to_datetime(movies_year['release_date'])
    movies_year['year'] = movies_year['release_date'].dt.year
    #dropping rows where year = NA
    movies_year=movies_year.dropna(subset=['year'])
    #display(movies_year[movies_year['year'].isna()])
    movies_year['year'] = movies_year['year'].astype(int)
    #display(movies_year.head())
    #movies.loc[movies_ratings['Action'] == 1]
    for key in genres_dict.keys():
        pivot_table1 = movies_year.pivot_table(index='year', columns=key, aggfunc='size')        
        year_count = {key: value.sum() for key, value in pivot_table1.groupby(""year"")[1]}
        year_count = {key: int(value) for key, value in year_count.items() if value != 0.0}
        genres_dict[key] = year_count

    
    return genres_dict
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
54,20510,1,"    mov = movies[['release_date','Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]",TODO,mov,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # pass
    mov = movies[['release_date','Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()
    # print(ans)
    # print(subs['year'])
    # for i in subs['year']:
    #   print(type(i))
    #   break
    return ans
# movie_count_by_genre(movies, genres)","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # pass
    mov = movies[['release_date','Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()
    # print(ans)
    # print(subs['year'])
    # for i in subs['year']:
    #   print(type(i))
    #   break
    return ans
# movie_count_by_genre(movies, genres)
# ans= movie_count_by_genre(movies, genres)
# print(ans['Crime'])
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
55,20510,2,"    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()
    # print(ans)
    # print(subs['year'])
    # for i in subs['year']:
    #   print(type(i))
    #   break
    return ans",TODO,ans,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # pass
    mov = movies[['release_date','Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()
    # print(ans)
    # print(subs['year'])
    # for i in subs['year']:
    #   print(type(i))
    #   break
    return ans
# movie_count_by_genre(movies, genres)","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # pass
    mov = movies[['release_date','Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()
    # print(ans)
    # print(subs['year'])
    # for i in subs['year']:
    #   print(type(i))
    #   break
    return ans
# movie_count_by_genre(movies, genres)
# ans= movie_count_by_genre(movies, genres)
# print(ans['Crime'])
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
56,20510,3,"      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()",TODO,subs,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # pass
    mov = movies[['release_date','Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()
    # print(ans)
    # print(subs['year'])
    # for i in subs['year']:
    #   print(type(i))
    #   break
    return ans
# movie_count_by_genre(movies, genres)","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # pass
    mov = movies[['release_date','Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()
    # print(ans)
    # print(subs['year'])
    # for i in subs['year']:
    #   print(type(i))
    #   break
    return ans
# movie_count_by_genre(movies, genres)
# ans= movie_count_by_genre(movies, genres)
# print(ans['Crime'])
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
57,20510,4,"        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp",TODO,temp,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # pass
    mov = movies[['release_date','Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()
    # print(ans)
    # print(subs['year'])
    # for i in subs['year']:
    #   print(type(i))
    #   break
    return ans
# movie_count_by_genre(movies, genres)","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # pass
    mov = movies[['release_date','Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    ans = {}
    # print(type(mov['release_date']))
    # temp = mov['release_date'].str.split('-')
    mov['year'] = 0
    # print(len(mov))
    for i in range(1, len(mov)+1):
      # print(i)
      if type(mov['release_date'][i]) == str:
        # print(mov['release_date'][i])
        temp = mov['release_date'][i][-4:]
        mov['year'][i] = temp
    # print(mov.head())
    # print(len(mov))
    mov = mov[(mov.year != 0)]
    # print(len(mov))
    mov['year'] = pd.to_numeric(mov['year'])
    # print(genres)
    for i in genres:
      subs = mov[mov[i]==1]
      subs = subs[[""year""]]      
      # print(type(subs))
      # print(subs['year'].value_counts().to_dict())
      ans[i] = subs['year'].value_counts().to_dict()
    # print(ans)
    # print(subs['year'])
    # for i in subs['year']:
    #   print(type(i))
    #   break
    return ans
# movie_count_by_genre(movies, genres)
# ans= movie_count_by_genre(movies, genres)
# print(ans['Crime'])
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
58,18152,1,"    ans = {}
    movies = movies.dropna(subset=['release_date'])
    movies['release_year'] = pd.DatetimeIndex(movies['release_date']).year.astype(int)
    for genre in genres:
        x = movies.groupby('release_year').agg({genre:'sum'})
        x = x.loc[(x!=0).any(axis=1)]
        ans.update(x.to_dict())
    return ans",TODO,ans,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    movies = movies.dropna(subset=['release_date'])
    movies['release_year'] = pd.DatetimeIndex(movies['release_date']).year.astype(int)
    for genre in genres:
        x = movies.groupby('release_year').agg({genre:'sum'})
        x = x.loc[(x!=0).any(axis=1)]
        ans.update(x.to_dict())
    return ans","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    movies = movies.dropna(subset=['release_date'])
    movies['release_year'] = pd.DatetimeIndex(movies['release_date']).year.astype(int)
    for genre in genres:
        x = movies.groupby('release_year').agg({genre:'sum'})
        x = x.loc[(x!=0).any(axis=1)]
        ans.update(x.to_dict())
    return ans
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
59,18152,2,"    movies = movies.dropna(subset=['release_date'])
    movies['release_year'] = pd.DatetimeIndex(movies['release_date']).year.astype(int)
    for genre in genres:
        x = movies.groupby('release_year').agg({genre:'sum'})",TODO,movies,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    movies = movies.dropna(subset=['release_date'])
    movies['release_year'] = pd.DatetimeIndex(movies['release_date']).year.astype(int)
    for genre in genres:
        x = movies.groupby('release_year').agg({genre:'sum'})
        x = x.loc[(x!=0).any(axis=1)]
        ans.update(x.to_dict())
    return ans","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    movies = movies.dropna(subset=['release_date'])
    movies['release_year'] = pd.DatetimeIndex(movies['release_date']).year.astype(int)
    for genre in genres:
        x = movies.groupby('release_year').agg({genre:'sum'})
        x = x.loc[(x!=0).any(axis=1)]
        ans.update(x.to_dict())
    return ans
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
60,18152,3,"        x = movies.groupby('release_year').agg({genre:'sum'})
        x = x.loc[(x!=0).any(axis=1)]
        ans.update(x.to_dict())",TODO,x,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    movies = movies.dropna(subset=['release_date'])
    movies['release_year'] = pd.DatetimeIndex(movies['release_date']).year.astype(int)
    for genre in genres:
        x = movies.groupby('release_year').agg({genre:'sum'})
        x = x.loc[(x!=0).any(axis=1)]
        ans.update(x.to_dict())
    return ans","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    ans = {}
    movies = movies.dropna(subset=['release_date'])
    movies['release_year'] = pd.DatetimeIndex(movies['release_date']).year.astype(int)
    for genre in genres:
        x = movies.groupby('release_year').agg({genre:'sum'})
        x = x.loc[(x!=0).any(axis=1)]
        ans.update(x.to_dict())
    return ans
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
61,17810,1,"    Dict={key:'' for key in genres}
    for key in genres:
      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict
    return Dict",TODO,Dict,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    pd.options.mode.chained_assignment = None
    movies['release_date'] = pd.to_datetime(movies['release_date'], format='%d-%b-%Y')
    Dict={key:'' for key in genres}
    for key in genres:
      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict
    return Dict
    pd.options.mode.chained_assignment = 'warn'
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    pd.options.mode.chained_assignment = None
    movies['release_date'] = pd.to_datetime(movies['release_date'], format='%d-%b-%Y')
    Dict={key:'' for key in genres}
    for key in genres:
      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict
    return Dict
    pd.options.mode.chained_assignment = 'warn'
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
62,17810,2,"      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()",TODO,filtered_movies,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    pd.options.mode.chained_assignment = None
    movies['release_date'] = pd.to_datetime(movies['release_date'], format='%d-%b-%Y')
    Dict={key:'' for key in genres}
    for key in genres:
      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict
    return Dict
    pd.options.mode.chained_assignment = 'warn'
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    pd.options.mode.chained_assignment = None
    movies['release_date'] = pd.to_datetime(movies['release_date'], format='%d-%b-%Y')
    Dict={key:'' for key in genres}
    for key in genres:
      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict
    return Dict
    pd.options.mode.chained_assignment = 'warn'
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
63,17810,3,"      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()",TODO,year_no_movie,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    pd.options.mode.chained_assignment = None
    movies['release_date'] = pd.to_datetime(movies['release_date'], format='%d-%b-%Y')
    Dict={key:'' for key in genres}
    for key in genres:
      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict
    return Dict
    pd.options.mode.chained_assignment = 'warn'
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    pd.options.mode.chained_assignment = None
    movies['release_date'] = pd.to_datetime(movies['release_date'], format='%d-%b-%Y')
    Dict={key:'' for key in genres}
    for key in genres:
      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict
    return Dict
    pd.options.mode.chained_assignment = 'warn'
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
64,17810,4,"      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict",TODO,year_dict,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    pd.options.mode.chained_assignment = None
    movies['release_date'] = pd.to_datetime(movies['release_date'], format='%d-%b-%Y')
    Dict={key:'' for key in genres}
    for key in genres:
      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict
    return Dict
    pd.options.mode.chained_assignment = 'warn'
    pass","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    pd.options.mode.chained_assignment = None
    movies['release_date'] = pd.to_datetime(movies['release_date'], format='%d-%b-%Y')
    Dict={key:'' for key in genres}
    for key in genres:
      filtered_movies = movies[movies[key]==1]
      filtered_movies.loc[:,'year']=filtered_movies['release_date'].dt.year
      #using series to calculate then turn it into dict
      year_no_movie=filtered_movies['year'].value_counts().sort_index()
      year_dict=year_no_movie.to_dict()
      Dict[key]=year_dict
    return Dict
    pd.options.mode.chained_assignment = 'warn'
    pass
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
65,18521,1,"    result = {}
    for genre in genres:
        group = list(movies.groupby(genre))[1][1]
        result[genre] = group[""release_date""].apply(lambda x: int(x[-4:])).value_counts().to_dict()
    return result",TODO,result,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = {}
    for genre in genres:
        group = list(movies.groupby(genre))[1][1]
        result[genre] = group[""release_date""].apply(lambda x: int(x[-4:])).value_counts().to_dict()
    return result","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = {}
    for genre in genres:
        group = list(movies.groupby(genre))[1][1]
        result[genre] = group[""release_date""].apply(lambda x: int(x[-4:])).value_counts().to_dict()
    return result
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
66,18521,2,"        group = list(movies.groupby(genre))[1][1]
        result[genre] = group[""release_date""].apply(lambda x: int(x[-4:])).value_counts().to_dict()",TODO,group,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = {}
    for genre in genres:
        group = list(movies.groupby(genre))[1][1]
        result[genre] = group[""release_date""].apply(lambda x: int(x[-4:])).value_counts().to_dict()
    return result","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = {}
    for genre in genres:
        group = list(movies.groupby(genre))[1][1]
        result[genre] = group[""release_date""].apply(lambda x: int(x[-4:])).value_counts().to_dict()
    return result
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
67,2334,1,"    valid_movies = movies[(movies['release_year'].notna()) & (movies['release_year'] >= starting_year)]
    
    # Merge movies with ratings to get rating for each movie by each user
    merged = pd.merge(valid_movies, ratings, left_on='movie_id', right_on='item_id')",TODO,valid_movies,"# def movie_rating_distribution(movies, ratings, starting_year = 1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     pass

# def movie_rating_distribution(movies, ratings, starting_year=1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     # Filter out movies released before the starting year
#     movies = movies[movies['release_date'].str[-4:].astype(float) >= starting_year]

#     # Get a list of all movie IDs that have at least one rating
#     rated_movie_ids = ratings['item_id'].unique()

#     # Filter out unrated movies
#     movies = movies[movies['movie_id'].isin(rated_movie_ids)]

#     # Merge movies and ratings dataframes
#     # merged = pd.merge(movies, ratings, on='itemId')
#     merged = ratings.join(movies, on='item_id')

#     # Extract the release year from the release date column
#     merged['release_year'] = merged['release_date'].str[-4:].astype(int)

#     # Keep only the columns we want
#     merged = merged[['release_year', 'rating']]

#     return merged


def movie_rating_distribution(movies, ratings, starting_year=1990):
    # Extract year from the release date column and convert to int
    movies['release_year'] = pd.to_datetime(movies['release_date'], errors='coerce').dt.year.astype('Int64')
    
    # Filter out movies with invalid release years and movies released before starting_year
    valid_movies = movies[(movies['release_year'].notna()) & (movies['release_year'] >= starting_year)]
    
    # Merge movies with ratings to get rating for each movie by each user
    merged = pd.merge(valid_movies, ratings, left_on='movie_id', right_on='item_id')
    
    # Extract release_year and rating columns and convert rating to int
    result = merged[['release_year', 'rating']].astype({'rating': int})
    
    return result","# def movie_rating_distribution(movies, ratings, starting_year = 1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     pass

# def movie_rating_distribution(movies, ratings, starting_year=1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     # Filter out movies released before the starting year
#     movies = movies[movies['release_date'].str[-4:].astype(float) >= starting_year]

#     # Get a list of all movie IDs that have at least one rating
#     rated_movie_ids = ratings['item_id'].unique()

#     # Filter out unrated movies
#     movies = movies[movies['movie_id'].isin(rated_movie_ids)]

#     # Merge movies and ratings dataframes
#     # merged = pd.merge(movies, ratings, on='itemId')
#     merged = ratings.join(movies, on='item_id')

#     # Extract the release year from the release date column
#     merged['release_year'] = merged['release_date'].str[-4:].astype(int)

#     # Keep only the columns we want
#     merged = merged[['release_year', 'rating']]

#     return merged


def movie_rating_distribution(movies, ratings, starting_year=1990):
    # Extract year from the release date column and convert to int
    movies['release_year'] = pd.to_datetime(movies['release_date'], errors='coerce').dt.year.astype('Int64')
    
    # Filter out movies with invalid release years and movies released before starting_year
    valid_movies = movies[(movies['release_year'].notna()) & (movies['release_year'] >= starting_year)]
    
    # Merge movies with ratings to get rating for each movie by each user
    merged = pd.merge(valid_movies, ratings, left_on='movie_id', right_on='item_id')
    
    # Extract release_year and rating columns and convert rating to int
    result = merged[['release_year', 'rating']].astype({'rating': int})
    
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
68,2334,2,"    merged = pd.merge(valid_movies, ratings, left_on='movie_id', right_on='item_id')
    
    # Extract release_year and rating columns and convert rating to int
    result = merged[['release_year', 'rating']].astype({'rating': int})",TODO,merged,"# def movie_rating_distribution(movies, ratings, starting_year = 1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     pass

# def movie_rating_distribution(movies, ratings, starting_year=1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     # Filter out movies released before the starting year
#     movies = movies[movies['release_date'].str[-4:].astype(float) >= starting_year]

#     # Get a list of all movie IDs that have at least one rating
#     rated_movie_ids = ratings['item_id'].unique()

#     # Filter out unrated movies
#     movies = movies[movies['movie_id'].isin(rated_movie_ids)]

#     # Merge movies and ratings dataframes
#     # merged = pd.merge(movies, ratings, on='itemId')
#     merged = ratings.join(movies, on='item_id')

#     # Extract the release year from the release date column
#     merged['release_year'] = merged['release_date'].str[-4:].astype(int)

#     # Keep only the columns we want
#     merged = merged[['release_year', 'rating']]

#     return merged


def movie_rating_distribution(movies, ratings, starting_year=1990):
    # Extract year from the release date column and convert to int
    movies['release_year'] = pd.to_datetime(movies['release_date'], errors='coerce').dt.year.astype('Int64')
    
    # Filter out movies with invalid release years and movies released before starting_year
    valid_movies = movies[(movies['release_year'].notna()) & (movies['release_year'] >= starting_year)]
    
    # Merge movies with ratings to get rating for each movie by each user
    merged = pd.merge(valid_movies, ratings, left_on='movie_id', right_on='item_id')
    
    # Extract release_year and rating columns and convert rating to int
    result = merged[['release_year', 'rating']].astype({'rating': int})
    
    return result","# def movie_rating_distribution(movies, ratings, starting_year = 1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     pass

# def movie_rating_distribution(movies, ratings, starting_year=1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     # Filter out movies released before the starting year
#     movies = movies[movies['release_date'].str[-4:].astype(float) >= starting_year]

#     # Get a list of all movie IDs that have at least one rating
#     rated_movie_ids = ratings['item_id'].unique()

#     # Filter out unrated movies
#     movies = movies[movies['movie_id'].isin(rated_movie_ids)]

#     # Merge movies and ratings dataframes
#     # merged = pd.merge(movies, ratings, on='itemId')
#     merged = ratings.join(movies, on='item_id')

#     # Extract the release year from the release date column
#     merged['release_year'] = merged['release_date'].str[-4:].astype(int)

#     # Keep only the columns we want
#     merged = merged[['release_year', 'rating']]

#     return merged


def movie_rating_distribution(movies, ratings, starting_year=1990):
    # Extract year from the release date column and convert to int
    movies['release_year'] = pd.to_datetime(movies['release_date'], errors='coerce').dt.year.astype('Int64')
    
    # Filter out movies with invalid release years and movies released before starting_year
    valid_movies = movies[(movies['release_year'].notna()) & (movies['release_year'] >= starting_year)]
    
    # Merge movies with ratings to get rating for each movie by each user
    merged = pd.merge(valid_movies, ratings, left_on='movie_id', right_on='item_id')
    
    # Extract release_year and rating columns and convert rating to int
    result = merged[['release_year', 'rating']].astype({'rating': int})
    
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
69,2334,3,"    result = merged[['release_year', 'rating']].astype({'rating': int})
    
    return result",TODO,result,"# def movie_rating_distribution(movies, ratings, starting_year = 1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     pass

# def movie_rating_distribution(movies, ratings, starting_year=1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     # Filter out movies released before the starting year
#     movies = movies[movies['release_date'].str[-4:].astype(float) >= starting_year]

#     # Get a list of all movie IDs that have at least one rating
#     rated_movie_ids = ratings['item_id'].unique()

#     # Filter out unrated movies
#     movies = movies[movies['movie_id'].isin(rated_movie_ids)]

#     # Merge movies and ratings dataframes
#     # merged = pd.merge(movies, ratings, on='itemId')
#     merged = ratings.join(movies, on='item_id')

#     # Extract the release year from the release date column
#     merged['release_year'] = merged['release_date'].str[-4:].astype(int)

#     # Keep only the columns we want
#     merged = merged[['release_year', 'rating']]

#     return merged


def movie_rating_distribution(movies, ratings, starting_year=1990):
    # Extract year from the release date column and convert to int
    movies['release_year'] = pd.to_datetime(movies['release_date'], errors='coerce').dt.year.astype('Int64')
    
    # Filter out movies with invalid release years and movies released before starting_year
    valid_movies = movies[(movies['release_year'].notna()) & (movies['release_year'] >= starting_year)]
    
    # Merge movies with ratings to get rating for each movie by each user
    merged = pd.merge(valid_movies, ratings, left_on='movie_id', right_on='item_id')
    
    # Extract release_year and rating columns and convert rating to int
    result = merged[['release_year', 'rating']].astype({'rating': int})
    
    return result","# def movie_rating_distribution(movies, ratings, starting_year = 1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     pass

# def movie_rating_distribution(movies, ratings, starting_year=1990):
#     """"""
#     Record the release year and rating of every movie released on or after a starting year in the dataset.

#     args:
#         movies (pd.DataFrame)   : Dataframe containing movie attributes
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         starting_year (int) : the earliest release year for a movie to be considered

#     return:
#         pd.DataFrame  : a DataFrame of the movie release years and ratings
#     """"""
#     # Filter out movies released before the starting year
#     movies = movies[movies['release_date'].str[-4:].astype(float) >= starting_year]

#     # Get a list of all movie IDs that have at least one rating
#     rated_movie_ids = ratings['item_id'].unique()

#     # Filter out unrated movies
#     movies = movies[movies['movie_id'].isin(rated_movie_ids)]

#     # Merge movies and ratings dataframes
#     # merged = pd.merge(movies, ratings, on='itemId')
#     merged = ratings.join(movies, on='item_id')

#     # Extract the release year from the release date column
#     merged['release_year'] = merged['release_date'].str[-4:].astype(int)

#     # Keep only the columns we want
#     merged = merged[['release_year', 'rating']]

#     return merged


def movie_rating_distribution(movies, ratings, starting_year=1990):
    # Extract year from the release date column and convert to int
    movies['release_year'] = pd.to_datetime(movies['release_date'], errors='coerce').dt.year.astype('Int64')
    
    # Filter out movies with invalid release years and movies released before starting_year
    valid_movies = movies[(movies['release_year'].notna()) & (movies['release_year'] >= starting_year)]
    
    # Merge movies with ratings to get rating for each movie by each user
    merged = pd.merge(valid_movies, ratings, left_on='movie_id', right_on='item_id')
    
    # Extract release_year and rating columns and convert rating to int
    result = merged[['release_year', 'rating']].astype({'rating': int})
    
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
70,1389,1,"    ratings_movies = ratings.merge(movies, how=""inner"", left_on=""item_id"", right_on=""movie_id"")
    temp = ratings_movies[ratings_movies[""release_year""] >= starting_year][[""release_year"",""rating""]]",TODO,ratings_movies,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies[""release_year""] = movies[""release_date""].apply(lambda year: float(str(year)[-4:]))
    ratings_movies = ratings.merge(movies, how=""inner"", left_on=""item_id"", right_on=""movie_id"")
    temp = ratings_movies[ratings_movies[""release_year""] >= starting_year][[""release_year"",""rating""]]
    temp[""release_year""] = temp[""release_year""].astype(int)
    temp[""rating""] = temp[""rating""].astype(int)
    return temp","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies[""release_year""] = movies[""release_date""].apply(lambda year: float(str(year)[-4:]))
    ratings_movies = ratings.merge(movies, how=""inner"", left_on=""item_id"", right_on=""movie_id"")
    temp = ratings_movies[ratings_movies[""release_year""] >= starting_year][[""release_year"",""rating""]]
    temp[""release_year""] = temp[""release_year""].astype(int)
    temp[""rating""] = temp[""rating""].astype(int)
    return temp
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
71,1389,2,"    temp = ratings_movies[ratings_movies[""release_year""] >= starting_year][[""release_year"",""rating""]]
    temp[""release_year""] = temp[""release_year""].astype(int)
    temp[""rating""] = temp[""rating""].astype(int)
    return temp",TODO,temp,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies[""release_year""] = movies[""release_date""].apply(lambda year: float(str(year)[-4:]))
    ratings_movies = ratings.merge(movies, how=""inner"", left_on=""item_id"", right_on=""movie_id"")
    temp = ratings_movies[ratings_movies[""release_year""] >= starting_year][[""release_year"",""rating""]]
    temp[""release_year""] = temp[""release_year""].astype(int)
    temp[""rating""] = temp[""rating""].astype(int)
    return temp","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies[""release_year""] = movies[""release_date""].apply(lambda year: float(str(year)[-4:]))
    ratings_movies = ratings.merge(movies, how=""inner"", left_on=""item_id"", right_on=""movie_id"")
    temp = ratings_movies[ratings_movies[""release_year""] >= starting_year][[""release_year"",""rating""]]
    temp[""release_year""] = temp[""release_year""].astype(int)
    temp[""rating""] = temp[""rating""].astype(int)
    return temp
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
72,20736,1,"    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year",TODO,release_date,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
73,20736,2,"    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])",TODO,release_year,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
74,20736,3,"    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)",TODO,year_df,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
75,20736,4,"    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,",TODO,rating_df,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
76,20736,5,"    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})",TODO,merged_df,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
77,21898,1,"    df = movies.copy()
    df = df[df['release_date'].notna()]
    df['release_year'] = df['release_date'].str[-4:].astype(int)
    df = df[df['release_year'] >= starting_year]
    df = df.merge(ratings, left_on='movie_id', right_on='item_id', how='inner')
    return df[['release_year', 'rating']]",TODO,df,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.copy()
    df = df[df['release_date'].notna()]
    df['release_year'] = df['release_date'].str[-4:].astype(int)
    df = df[df['release_year'] >= starting_year]
    df = df.merge(ratings, left_on='movie_id', right_on='item_id', how='inner')
    return df[['release_year', 'rating']]","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.copy()
    df = df[df['release_date'].notna()]
    df['release_year'] = df['release_date'].str[-4:].astype(int)
    df = df[df['release_year'] >= starting_year]
    df = df.merge(ratings, left_on='movie_id', right_on='item_id', how='inner')
    return df[['release_year', 'rating']]
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
78,15001,1,"    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df",TODO,result_df,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
79,15001,2,"    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]",TODO,id_to_rating,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
80,15001,3,"        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]",TODO,item_id,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
81,15001,4,"        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)",TODO,rating,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
82,15001,5,"        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])",TODO,release_date,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
83,15001,6,"        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)",TODO,release_year,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
84,15001,7,"        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]",TODO,movie_id,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
85,15001,8,"            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:",TODO,movie_ratings,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    result_df = pd.DataFrame(columns=[""release_year"", ""rating""])

    id_to_rating = {}
    for index, row in ratings.iterrows():
        item_id = row['item_id']
        rating = row['rating']
        if item_id in id_to_rating:
            id_to_rating[item_id][0] += rating
            id_to_rating[item_id][1] += 1
        else:
            id_to_rating[item_id] = [rating, 1]

    for index, movie in movies.iterrows():
        release_date = movie[""release_date""]

        # Ignore movies with invalid year values
        if pd.isnull(release_date):
            continue

        release_year = int(release_date[-4:])

        # Ignore movies released before the starting year
        if release_year < starting_year:
            continue

        movie_id = index

        # Check if movie has ratings
        if movie_id in ratings[""item_id""].unique():
            movie_ratings = ratings.loc[ratings[""item_id""] == movie_id, ""rating""]

            for rating in movie_ratings:
                result_df = result_df.append({""release_year"": release_year, ""rating"": int(rating)}, ignore_index=True)

    return result_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
86,20538,1,"    ratings_with_year = ratings.merge(movies[['release_year']], left_on='item_id', right_index=True, how='left')
    ans = ratings_with_year.loc[ratings_with_year['release_year'] >= 1990, :].loc[:, ['release_year', 'rating']]",TODO,ratings_with_year,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""

    movies['release_year'] = (movies['release_date'].str[-4:]).astype('Int64')
    ratings_with_year = ratings.merge(movies[['release_year']], left_on='item_id', right_index=True, how='left')
    ans = ratings_with_year.loc[ratings_with_year['release_year'] >= 1990, :].loc[:, ['release_year', 'rating']]

    return ans","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""

    movies['release_year'] = (movies['release_date'].str[-4:]).astype('Int64')
    ratings_with_year = ratings.merge(movies[['release_year']], left_on='item_id', right_index=True, how='left')
    ans = ratings_with_year.loc[ratings_with_year['release_year'] >= 1990, :].loc[:, ['release_year', 'rating']]

    return ans
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
87,20538,2,"    ans = ratings_with_year.loc[ratings_with_year['release_year'] >= 1990, :].loc[:, ['release_year', 'rating']]

    return ans",TODO,ans,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""

    movies['release_year'] = (movies['release_date'].str[-4:]).astype('Int64')
    ratings_with_year = ratings.merge(movies[['release_year']], left_on='item_id', right_index=True, how='left')
    ans = ratings_with_year.loc[ratings_with_year['release_year'] >= 1990, :].loc[:, ['release_year', 'rating']]

    return ans","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""

    movies['release_year'] = (movies['release_date'].str[-4:]).astype('Int64')
    ratings_with_year = ratings.merge(movies[['release_year']], left_on='item_id', right_index=True, how='left')
    ans = ratings_with_year.loc[ratings_with_year['release_year'] >= 1990, :].loc[:, ['release_year', 'rating']]

    return ans
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
88,1974,1,"    movies_with_ratings = pd.merge(ratings, movies, how=""left"", left_on=""item_id"", right_on=""movie_id"", suffixes=(""_r"", ""_m""))

    # perform release_date conversion and filter out invalid release years and ones past starting year
    def parse_year(release_date):
        year = str(release_date)[-4:]
        try:
            return int(year)
        except:
            return pd.NA

    movies_with_ratings[""release_year""] = movies_with_ratings[""release_date""].apply(parse_year)

    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""].notnull()]
    movies_with_ratings[""release_year""] =  movies_with_ratings[""release_year""].astype(""int"")
    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""] >= starting_year]

    movies_with_ratings[""rating""] = movies_with_ratings[""rating""].astype(""int"")
    # return dataframe with just release year and rating and make sure data types match
    return movies_with_ratings[[""release_year"", ""rating""]]",TODO,movies_with_ratings,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    # merge ratings with movies
    movies_with_ratings = pd.merge(ratings, movies, how=""left"", left_on=""item_id"", right_on=""movie_id"", suffixes=(""_r"", ""_m""))

    # perform release_date conversion and filter out invalid release years and ones past starting year
    def parse_year(release_date):
        year = str(release_date)[-4:]
        try:
            return int(year)
        except:
            return pd.NA

    movies_with_ratings[""release_year""] = movies_with_ratings[""release_date""].apply(parse_year)

    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""].notnull()]
    movies_with_ratings[""release_year""] =  movies_with_ratings[""release_year""].astype(""int"")
    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""] >= starting_year]

    movies_with_ratings[""rating""] = movies_with_ratings[""rating""].astype(""int"")
    # return dataframe with just release year and rating and make sure data types match
    return movies_with_ratings[[""release_year"", ""rating""]]","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    # merge ratings with movies
    movies_with_ratings = pd.merge(ratings, movies, how=""left"", left_on=""item_id"", right_on=""movie_id"", suffixes=(""_r"", ""_m""))

    # perform release_date conversion and filter out invalid release years and ones past starting year
    def parse_year(release_date):
        year = str(release_date)[-4:]
        try:
            return int(year)
        except:
            return pd.NA

    movies_with_ratings[""release_year""] = movies_with_ratings[""release_date""].apply(parse_year)

    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""].notnull()]
    movies_with_ratings[""release_year""] =  movies_with_ratings[""release_year""].astype(""int"")
    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""] >= starting_year]

    movies_with_ratings[""rating""] = movies_with_ratings[""rating""].astype(""int"")
    # return dataframe with just release year and rating and make sure data types match
    return movies_with_ratings[[""release_year"", ""rating""]]
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
89,1974,2,"        year = str(release_date)[-4:]
        try:
            return int(year)",TODO,year,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    # merge ratings with movies
    movies_with_ratings = pd.merge(ratings, movies, how=""left"", left_on=""item_id"", right_on=""movie_id"", suffixes=(""_r"", ""_m""))

    # perform release_date conversion and filter out invalid release years and ones past starting year
    def parse_year(release_date):
        year = str(release_date)[-4:]
        try:
            return int(year)
        except:
            return pd.NA

    movies_with_ratings[""release_year""] = movies_with_ratings[""release_date""].apply(parse_year)

    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""].notnull()]
    movies_with_ratings[""release_year""] =  movies_with_ratings[""release_year""].astype(""int"")
    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""] >= starting_year]

    movies_with_ratings[""rating""] = movies_with_ratings[""rating""].astype(""int"")
    # return dataframe with just release year and rating and make sure data types match
    return movies_with_ratings[[""release_year"", ""rating""]]","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    # merge ratings with movies
    movies_with_ratings = pd.merge(ratings, movies, how=""left"", left_on=""item_id"", right_on=""movie_id"", suffixes=(""_r"", ""_m""))

    # perform release_date conversion and filter out invalid release years and ones past starting year
    def parse_year(release_date):
        year = str(release_date)[-4:]
        try:
            return int(year)
        except:
            return pd.NA

    movies_with_ratings[""release_year""] = movies_with_ratings[""release_date""].apply(parse_year)

    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""].notnull()]
    movies_with_ratings[""release_year""] =  movies_with_ratings[""release_year""].astype(""int"")
    movies_with_ratings = movies_with_ratings[movies_with_ratings[""release_year""] >= starting_year]

    movies_with_ratings[""rating""] = movies_with_ratings[""rating""].astype(""int"")
    # return dataframe with just release year and rating and make sure data types match
    return movies_with_ratings[[""release_year"", ""rating""]]
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
90,21222,1,"    new_movies = movies[movies['release_year'] >= starting_year]
    merged_data = ratings.merge(new_movies, left_on='item_id', right_on = 'movie_id')",TODO,new_movies,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies[['day','month','release_year']] = movies['release_date'].str.split('-',expand=True)
    movies.dropna(subset=['release_year'], inplace=True)
    movies['release_year'] = movies['release_year'].astype(int)
    new_movies = movies[movies['release_year'] >= starting_year]
    merged_data = ratings.merge(new_movies, left_on='item_id', right_on = 'movie_id')

    return merged_data[[""release_year"", ""rating""]]","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies[['day','month','release_year']] = movies['release_date'].str.split('-',expand=True)
    movies.dropna(subset=['release_year'], inplace=True)
    movies['release_year'] = movies['release_year'].astype(int)
    new_movies = movies[movies['release_year'] >= starting_year]
    merged_data = ratings.merge(new_movies, left_on='item_id', right_on = 'movie_id')

    return merged_data[[""release_year"", ""rating""]]
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
91,21222,2,"    merged_data = ratings.merge(new_movies, left_on='item_id', right_on = 'movie_id')

    return merged_data[[""release_year"", ""rating""]]",TODO,merged_data,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies[['day','month','release_year']] = movies['release_date'].str.split('-',expand=True)
    movies.dropna(subset=['release_year'], inplace=True)
    movies['release_year'] = movies['release_year'].astype(int)
    new_movies = movies[movies['release_year'] >= starting_year]
    merged_data = ratings.merge(new_movies, left_on='item_id', right_on = 'movie_id')

    return merged_data[[""release_year"", ""rating""]]","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies[['day','month','release_year']] = movies['release_date'].str.split('-',expand=True)
    movies.dropna(subset=['release_year'], inplace=True)
    movies['release_year'] = movies['release_year'].astype(int)
    new_movies = movies[movies['release_year'] >= starting_year]
    merged_data = ratings.merge(new_movies, left_on='item_id', right_on = 'movie_id')

    return merged_data[[""release_year"", ""rating""]]
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
92,19764,1,"    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year",TODO,release_date,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
93,19764,2,"    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])",TODO,release_year,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
94,19764,3,"    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)",TODO,year_df,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
95,19764,4,"    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,",TODO,rating_df,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
96,19764,5,"    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})",TODO,merged_df,"def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})","def movie_rating_distribution(movies: pd.DataFrame,
                              ratings: pd.DataFrame,
                              starting_year=1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    release_date = pd.to_datetime(movies.release_date.dropna())
    release_year = release_date.dt.year
    year_df = pd.DataFrame(release_year[release_year >= starting_year])
    rating_df = ratings[ratings.item_id.isin(year_df.index)]
    merged_df = rating_df.merge(year_df,
                                left_on='item_id', right_on=year_df.index)
    return pd.DataFrame({'release_year': merged_df.release_date.values,
                        'rating': merged_df.rating.values})
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")


test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x=""release_year"", y=""rating"", data=ratings_dist, kind=""violin"", height=5, aspect=2)


plot_movie_rating_dist(movie_rating_distribution(movies, ratings))"
97,20611,1,"    df_rating_timestamp = ratings.loc[:, [""timestamp""]]
    
    #now we convert the timestamps to datetime format
    df_rating_timestamp[""date""] = pd.to_datetime(df_rating_timestamp[""timestamp""], unit = ""s"")
    
    #we filter out ratings from other years
    df_rating_timestamp = df_rating_timestamp[df_rating_timestamp[""date""].dt.year == year]
    
    
    df_rating_timestamp[""month""] = df_rating_timestamp[""date""].dt.month
    
    output = df_rating_timestamp.groupby(""month"").count().reset_index()",TODO,df_rating_timestamp,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #no need for user_id, item_id or rating
    df_rating_timestamp = ratings.loc[:, [""timestamp""]]
    
    #now we convert the timestamps to datetime format
    df_rating_timestamp[""date""] = pd.to_datetime(df_rating_timestamp[""timestamp""], unit = ""s"")
    
    #we filter out ratings from other years
    df_rating_timestamp = df_rating_timestamp[df_rating_timestamp[""date""].dt.year == year]
    
    
    df_rating_timestamp[""month""] = df_rating_timestamp[""date""].dt.month
    
    output = df_rating_timestamp.groupby(""month"").count().reset_index()
    
    
    output.rename(columns = {'date':'rating_count'}, inplace = True)
    output = output[[""month"", ""rating_count""]]
    return output","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #no need for user_id, item_id or rating
    df_rating_timestamp = ratings.loc[:, [""timestamp""]]
    
    #now we convert the timestamps to datetime format
    df_rating_timestamp[""date""] = pd.to_datetime(df_rating_timestamp[""timestamp""], unit = ""s"")
    
    #we filter out ratings from other years
    df_rating_timestamp = df_rating_timestamp[df_rating_timestamp[""date""].dt.year == year]
    
    
    df_rating_timestamp[""month""] = df_rating_timestamp[""date""].dt.month
    
    output = df_rating_timestamp.groupby(""month"").count().reset_index()
    
    
    output.rename(columns = {'date':'rating_count'}, inplace = True)
    output = output[[""month"", ""rating_count""]]
    return output
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
98,20611,2,"    output = df_rating_timestamp.groupby(""month"").count().reset_index()
    
    
    output.rename(columns = {'date':'rating_count'}, inplace = True)
    output = output[[""month"", ""rating_count""]]
    return output",TODO,output,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #no need for user_id, item_id or rating
    df_rating_timestamp = ratings.loc[:, [""timestamp""]]
    
    #now we convert the timestamps to datetime format
    df_rating_timestamp[""date""] = pd.to_datetime(df_rating_timestamp[""timestamp""], unit = ""s"")
    
    #we filter out ratings from other years
    df_rating_timestamp = df_rating_timestamp[df_rating_timestamp[""date""].dt.year == year]
    
    
    df_rating_timestamp[""month""] = df_rating_timestamp[""date""].dt.month
    
    output = df_rating_timestamp.groupby(""month"").count().reset_index()
    
    
    output.rename(columns = {'date':'rating_count'}, inplace = True)
    output = output[[""month"", ""rating_count""]]
    return output","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #no need for user_id, item_id or rating
    df_rating_timestamp = ratings.loc[:, [""timestamp""]]
    
    #now we convert the timestamps to datetime format
    df_rating_timestamp[""date""] = pd.to_datetime(df_rating_timestamp[""timestamp""], unit = ""s"")
    
    #we filter out ratings from other years
    df_rating_timestamp = df_rating_timestamp[df_rating_timestamp[""date""].dt.year == year]
    
    
    df_rating_timestamp[""month""] = df_rating_timestamp[""date""].dt.month
    
    output = df_rating_timestamp.groupby(""month"").count().reset_index()
    
    
    output.rename(columns = {'date':'rating_count'}, inplace = True)
    output = output[[""month"", ""rating_count""]]
    return output
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
99,15957,1,"    rating_count = ratings.groupby('month')['rating'].count().reset_index(name='rating_count')
    return rating_count",TODO,rating_count,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
    ratings['month'] = ratings['timestamp'].dt.month
    rating_count = ratings.groupby('month')['rating'].count().reset_index(name='rating_count')
    return rating_count
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
    ratings['month'] = ratings['timestamp'].dt.month
    rating_count = ratings.groupby('month')['rating'].count().reset_index(name='rating_count')
    return rating_count
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
100,17407,1,"    ratings_copy = ratings.copy()
    ratings_copy['datetime'] = pd.to_datetime(ratings_copy['timestamp'], unit='s')
    
    grouped = ratings_copy[ratings_copy['datetime'].dt.year == year].groupby(
        ratings_copy['datetime'].dt.month).count()[['rating']].reset_index()",TODO,ratings_copy,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings_copy = ratings.copy()
    ratings_copy['datetime'] = pd.to_datetime(ratings_copy['timestamp'], unit='s')
    
    grouped = ratings_copy[ratings_copy['datetime'].dt.year == year].groupby(
        ratings_copy['datetime'].dt.month).count()[['rating']].reset_index()
    
    grouped.columns=['month', 'rating_count']
    
    return grouped","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings_copy = ratings.copy()
    ratings_copy['datetime'] = pd.to_datetime(ratings_copy['timestamp'], unit='s')
    
    grouped = ratings_copy[ratings_copy['datetime'].dt.year == year].groupby(
        ratings_copy['datetime'].dt.month).count()[['rating']].reset_index()
    
    grouped.columns=['month', 'rating_count']
    
    return grouped
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
101,17407,2,"    grouped = ratings_copy[ratings_copy['datetime'].dt.year == year].groupby(
        ratings_copy['datetime'].dt.month).count()[['rating']].reset_index()
    
    grouped.columns=['month', 'rating_count']
    
    return grouped",TODO,grouped,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings_copy = ratings.copy()
    ratings_copy['datetime'] = pd.to_datetime(ratings_copy['timestamp'], unit='s')
    
    grouped = ratings_copy[ratings_copy['datetime'].dt.year == year].groupby(
        ratings_copy['datetime'].dt.month).count()[['rating']].reset_index()
    
    grouped.columns=['month', 'rating_count']
    
    return grouped","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings_copy = ratings.copy()
    ratings_copy['datetime'] = pd.to_datetime(ratings_copy['timestamp'], unit='s')
    
    grouped = ratings_copy[ratings_copy['datetime'].dt.year == year].groupby(
        ratings_copy['datetime'].dt.month).count()[['rating']].reset_index()
    
    grouped.columns=['month', 'rating_count']
    
    return grouped
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
102,18073,1,"    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)",TODO,year_month_vec,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
103,18073,2,"    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)",TODO,ser,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
104,18073,3,"    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})",TODO,yr,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
105,18073,4,"    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})",TODO,month,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
106,18073,5,"    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()",TODO,df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # function to return date and year from datetime string
    def year_month(a:str):
        return a[:4], int(a[5:7])

    year_month_vec = np.vectorize(year_month)
    
    # year and months for all ratings
    ser = pd.to_datetime(ratings[""timestamp""], unit='s').astype(str).apply(lambda x:x[:7]).values
    yr, month = year_month_vec(ser)

    # filterng ratings according to given year and combining by month
    df = pd.DataFrame(data={""year"":yr, ""month"":month})
    df = df[df[""year""]==str(year)].groupby(""month"").count()
    return df.rename(columns={""year"":""rating_count""}).reset_index()
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
107,21791,1,"    datetime_series = ratings[""timestamp""].apply(lambda x: datetime.datetime.utcfromtimestamp(x))
    df = pd.DataFrame({""month"": datetime_series.dt.month, ""year"": datetime_series.dt.year})",TODO,datetime_series,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    datetime_series = ratings[""timestamp""].apply(lambda x: datetime.datetime.utcfromtimestamp(x))
    df = pd.DataFrame({""month"": datetime_series.dt.month, ""year"": datetime_series.dt.year})
    df = df[df[""year""] == year]
    grouped_df = df.groupby(""month"").agg(rating_count = (""year"", ""count""))
    grouped_df[""month""] = grouped_df.index
    grouped_df.reset_index(drop=True, inplace=True)
    return grouped_df","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    datetime_series = ratings[""timestamp""].apply(lambda x: datetime.datetime.utcfromtimestamp(x))
    df = pd.DataFrame({""month"": datetime_series.dt.month, ""year"": datetime_series.dt.year})
    df = df[df[""year""] == year]
    grouped_df = df.groupby(""month"").agg(rating_count = (""year"", ""count""))
    grouped_df[""month""] = grouped_df.index
    grouped_df.reset_index(drop=True, inplace=True)
    return grouped_df
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
108,21791,2,"    df = pd.DataFrame({""month"": datetime_series.dt.month, ""year"": datetime_series.dt.year})
    df = df[df[""year""] == year]
    grouped_df = df.groupby(""month"").agg(rating_count = (""year"", ""count""))",TODO,df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    datetime_series = ratings[""timestamp""].apply(lambda x: datetime.datetime.utcfromtimestamp(x))
    df = pd.DataFrame({""month"": datetime_series.dt.month, ""year"": datetime_series.dt.year})
    df = df[df[""year""] == year]
    grouped_df = df.groupby(""month"").agg(rating_count = (""year"", ""count""))
    grouped_df[""month""] = grouped_df.index
    grouped_df.reset_index(drop=True, inplace=True)
    return grouped_df","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    datetime_series = ratings[""timestamp""].apply(lambda x: datetime.datetime.utcfromtimestamp(x))
    df = pd.DataFrame({""month"": datetime_series.dt.month, ""year"": datetime_series.dt.year})
    df = df[df[""year""] == year]
    grouped_df = df.groupby(""month"").agg(rating_count = (""year"", ""count""))
    grouped_df[""month""] = grouped_df.index
    grouped_df.reset_index(drop=True, inplace=True)
    return grouped_df
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
109,21791,3,"    grouped_df = df.groupby(""month"").agg(rating_count = (""year"", ""count""))
    grouped_df[""month""] = grouped_df.index
    grouped_df.reset_index(drop=True, inplace=True)
    return grouped_df",TODO,grouped_df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    datetime_series = ratings[""timestamp""].apply(lambda x: datetime.datetime.utcfromtimestamp(x))
    df = pd.DataFrame({""month"": datetime_series.dt.month, ""year"": datetime_series.dt.year})
    df = df[df[""year""] == year]
    grouped_df = df.groupby(""month"").agg(rating_count = (""year"", ""count""))
    grouped_df[""month""] = grouped_df.index
    grouped_df.reset_index(drop=True, inplace=True)
    return grouped_df","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    datetime_series = ratings[""timestamp""].apply(lambda x: datetime.datetime.utcfromtimestamp(x))
    df = pd.DataFrame({""month"": datetime_series.dt.month, ""year"": datetime_series.dt.year})
    df = df[df[""year""] == year]
    grouped_df = df.groupby(""month"").agg(rating_count = (""year"", ""count""))
    grouped_df[""month""] = grouped_df.index
    grouped_df.reset_index(drop=True, inplace=True)
    return grouped_df
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
110,23629,1,"    cleaned_ratings = ratings[ratings['year'] == year]
    grouped = cleaned_ratings.groupby('month').count().reset_index()",TODO,cleaned_ratings,"def movies_reviewed_by_month(ratings, year):
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
    ratings['year'] = ratings['timestamp'].dt.year
    ratings['month'] = ratings['timestamp'].dt.month
    cleaned_ratings = ratings[ratings['year'] == year]
    grouped = cleaned_ratings.groupby('month').count().reset_index()
    grouped['rating_count']=grouped['user_id']
    result_df = grouped[['month','rating_count']]
    result_df
        
    return result_df
    pass","def movies_reviewed_by_month(ratings, year):
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
    ratings['year'] = ratings['timestamp'].dt.year
    ratings['month'] = ratings['timestamp'].dt.month
    cleaned_ratings = ratings[ratings['year'] == year]
    grouped = cleaned_ratings.groupby('month').count().reset_index()
    grouped['rating_count']=grouped['user_id']
    result_df = grouped[['month','rating_count']]
    result_df
        
    return result_df
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
111,23629,2,"    grouped = cleaned_ratings.groupby('month').count().reset_index()
    grouped['rating_count']=grouped['user_id']
    result_df = grouped[['month','rating_count']]",TODO,grouped,"def movies_reviewed_by_month(ratings, year):
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
    ratings['year'] = ratings['timestamp'].dt.year
    ratings['month'] = ratings['timestamp'].dt.month
    cleaned_ratings = ratings[ratings['year'] == year]
    grouped = cleaned_ratings.groupby('month').count().reset_index()
    grouped['rating_count']=grouped['user_id']
    result_df = grouped[['month','rating_count']]
    result_df
        
    return result_df
    pass","def movies_reviewed_by_month(ratings, year):
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
    ratings['year'] = ratings['timestamp'].dt.year
    ratings['month'] = ratings['timestamp'].dt.month
    cleaned_ratings = ratings[ratings['year'] == year]
    grouped = cleaned_ratings.groupby('month').count().reset_index()
    grouped['rating_count']=grouped['user_id']
    result_df = grouped[['month','rating_count']]
    result_df
        
    return result_df
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
112,23629,3,"    result_df = grouped[['month','rating_count']]
    result_df
        
    return result_df",TODO,result_df,"def movies_reviewed_by_month(ratings, year):
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
    ratings['year'] = ratings['timestamp'].dt.year
    ratings['month'] = ratings['timestamp'].dt.month
    cleaned_ratings = ratings[ratings['year'] == year]
    grouped = cleaned_ratings.groupby('month').count().reset_index()
    grouped['rating_count']=grouped['user_id']
    result_df = grouped[['month','rating_count']]
    result_df
        
    return result_df
    pass","def movies_reviewed_by_month(ratings, year):
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')
    ratings['year'] = ratings['timestamp'].dt.year
    ratings['month'] = ratings['timestamp'].dt.month
    cleaned_ratings = ratings[ratings['year'] == year]
    grouped = cleaned_ratings.groupby('month').count().reset_index()
    grouped['rating_count']=grouped['user_id']
    result_df = grouped[['month','rating_count']]
    result_df
        
    return result_df
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
113,16930,1,"    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()",TODO,year_df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
114,16930,2,"    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})",TODO,count_df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
115,16930,3,"    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df",TODO,result_df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
116,2398,1,"    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')",TODO,month_freq,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
117,2398,2,"    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:",TODO,temp,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
118,2398,3,"    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')",TODO,dat,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
119,2398,4,"    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)",TODO,month_result,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
120,2398,5,"    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating",TODO,month_rating,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""    
    templ_df = pd.DataFrame(
        data={
            'month': [1,2,3,4,5,6,7,8,9,10,11,12],
            'rating_count': [0,0,0,0,0,0,0,0,0,0,0,0]
        }
    )
    ratings['date'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['date'] = ratings['date'].apply(lambda x: pd.Timestamp(x).strftime('%m-%Y'))

    ratings['month'] = ratings['date'].str[:2]
    ratings['month'] = pd.to_numeric(ratings['month'])
    ratings['month'] = ratings['month'].astype('int')
    ratings['year'] = ratings['date'].str[-4:]
    ratings['year'] = pd.to_numeric(ratings['year'])
    ratings['year'] = ratings['year'].astype('int')

    month_freq = ratings[ratings['year'] == year].groupby(['month']).count()[['rating']].reset_index().rename(columns={'rating':'rating_count'})
    temp = list(month_freq['month'].unique())
    dat = month_freq.to_dict('list')
    for i in range(0,12):
        if i not in temp:
            dat['month'].append(i)
            dat['rating_count'].append(0)
    
    
    month_result = pd.DataFrame(data=dat).sort_values(by='month')
    month_rating = month_result.drop(month_result[(month_result['rating_count'] == 0)].index)
    #display(month_rating)
    return month_rating
    
    
    
    pass
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
121,24304,1,"    ratings_copy = ratings.copy()
    datetime_index = pd.DatetimeIndex(pd.to_datetime(ratings_copy['timestamp'], unit='s'))
    ratings_copy['year'] = datetime_index.year
    ratings_copy['month'] = datetime_index.month
    return ratings_copy[ratings_copy['year'] == year].groupby('month')['rating'].count().to_frame().reset_index().rename(columns = {""rating"": ""rating_count""})",TODO,ratings_copy,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings_copy = ratings.copy()
    datetime_index = pd.DatetimeIndex(pd.to_datetime(ratings_copy['timestamp'], unit='s'))
    ratings_copy['year'] = datetime_index.year
    ratings_copy['month'] = datetime_index.month
    return ratings_copy[ratings_copy['year'] == year].groupby('month')['rating'].count().to_frame().reset_index().rename(columns = {""rating"": ""rating_count""})","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings_copy = ratings.copy()
    datetime_index = pd.DatetimeIndex(pd.to_datetime(ratings_copy['timestamp'], unit='s'))
    ratings_copy['year'] = datetime_index.year
    ratings_copy['month'] = datetime_index.month
    return ratings_copy[ratings_copy['year'] == year].groupby('month')['rating'].count().to_frame().reset_index().rename(columns = {""rating"": ""rating_count""})
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
122,24304,2,"    datetime_index = pd.DatetimeIndex(pd.to_datetime(ratings_copy['timestamp'], unit='s'))
    ratings_copy['year'] = datetime_index.year
    ratings_copy['month'] = datetime_index.month",TODO,datetime_index,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings_copy = ratings.copy()
    datetime_index = pd.DatetimeIndex(pd.to_datetime(ratings_copy['timestamp'], unit='s'))
    ratings_copy['year'] = datetime_index.year
    ratings_copy['month'] = datetime_index.month
    return ratings_copy[ratings_copy['year'] == year].groupby('month')['rating'].count().to_frame().reset_index().rename(columns = {""rating"": ""rating_count""})","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings_copy = ratings.copy()
    datetime_index = pd.DatetimeIndex(pd.to_datetime(ratings_copy['timestamp'], unit='s'))
    ratings_copy['year'] = datetime_index.year
    ratings_copy['month'] = datetime_index.month
    return ratings_copy[ratings_copy['year'] == year].groupby('month')['rating'].count().to_frame().reset_index().rename(columns = {""rating"": ""rating_count""})
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
123,2695,1,"    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]",TODO,ratings,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
124,2695,2,"    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')",TODO,rating_year,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
125,2695,3,"    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])",TODO,month_count,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
126,2695,4,"    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])",TODO,month,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
127,2695,5,"    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount",TODO,df_ratecount,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
128,2695,6,"        h = len(rating_year[rating_year.month == i])
        month_count.append(h)",TODO,h,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    # change the unix to datatime 
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')

    # extract the month and year from datetime
    ratings['month'] = pd.DatetimeIndex(ratings['timestamp']).month
    ratings['year'] = pd.DatetimeIndex(ratings['timestamp']).year
    
    # drop the nan and change the year and month to int
    ratings = ratings[ratings['year'].notna()]
    ratings = ratings[ratings['month'].notna()]
    ratings['year'] = ratings['year'].astype(int)
    ratings['month'] = ratings['month'].astype(int)

    # find the year we are looking for
    rating_year = ratings.loc[ratings['year'] == year]
    rating_year = rating_year[ratings['rating'].notna()]

    # create new data frame to store the month and year for this year
    rating_year = rating_year[['month', 'year']]

    # count the number of each month and store them in list
    month_count = []
    for i in range(1,13):
        h = len(rating_year[rating_year.month == i])
        month_count.append(h)
    
    # sort the data frame in month ascending, from 1 to 12
    rating_year.sort_values('month')

    # make a list of month from 1 to 12
    month = list(range(1,13))

    # create a new dataframe to store the months and the number of rating for each month
    df_ratecount = pd.DataFrame(list(zip(month, month_count)), columns = ['month', 'rating_count'])

    # drop the rows with no ratings
    df_ratecount = df_ratecount[df_ratecount['rating_count'] > 0]
    
    return df_ratecount
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
129,15660,1,"    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")",TODO,df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
130,15660,2,"    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")",TODO,df1,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
131,15660,3,"    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]",TODO,df2,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
132,15660,4,"    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")",TODO,df3,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
133,15660,5,"    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()",TODO,df4,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
134,15660,6,"    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")",TODO,df5,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
135,15660,7,"    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()",TODO,df6,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
136,15660,8,"    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]",TODO,list0,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
137,15660,9,"    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1",TODO,list1,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
138,15660,10,"    list2 = list0[-size:]
    list2.reverse()
    return list2, list1",TODO,list2,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = pd.DataFrame()
    df['rating_counts'] = ratings[[""item_id"",""rating""]].groupby(""item_id"").count().copy()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating""]] = ratings[[""item_id"",""rating""]].copy()
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating_counts'] > threshold]
    df4 = df3.groupby(""item_id"").mean(""rating"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
139,23115,1,"    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()",TODO,df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
140,23115,2,"    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()",TODO,idx,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
141,23115,3,"    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)",TODO,ans,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
142,23115,4,"    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)",TODO,best,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
143,23115,5,"    worst = list(ans[:size].index)
    
    return (best, worst)",TODO,worst,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.copy()
    # get above threshold
    idx = df.groupby('item_id').rating.count() > threshold
    df = df.set_index('item_id')
    df = df.loc[idx].reset_index()
    # get mean and sort
    ans = df.groupby('item_id').rating.mean().sort_values()
    best = list(ans[-size:].sort_values(ascending=False).index)
    worst = list(ans[:size].index)
    
    return (best, worst)
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
144,18353,1,"    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]",TODO,avg_ratings,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
145,18353,2,"    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]",TODO,rating_counts,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
146,18353,3,"    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()",TODO,valid_ratings,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
147,18353,4,"    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids",TODO,best_movie_ids,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
148,18353,5,"    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids",TODO,worst_movie_ids,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    avg_ratings = ratings.groupby('item_id')['rating'].mean()
    rating_counts = ratings.groupby('item_id')['rating'].count()
    valid_ratings = avg_ratings[rating_counts > threshold]
    best_movie_ids = valid_ratings.sort_values(ascending=False).head(size).index.tolist()
    worst_movie_ids = valid_ratings.sort_values().head(size).index.tolist()
    return best_movie_ids, worst_movie_ids
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
149,19946,1,"    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")",TODO,df_avg_rating,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
150,19946,2,"    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")",TODO,df_rating_counts,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
151,19946,3,"    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]",TODO,df_merged,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
152,19946,4,"    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)",TODO,df_filtered,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
153,19946,5,"    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] ",TODO,df_sorted,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
154,19946,6,"    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids",TODO,best_movie_ids,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
155,19946,7,"    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids",TODO,worst_movie_ids,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    
    # calculate the rating by mean
    df_avg_rating = ratings.groupby(""item_id"")[""rating""].mean().reset_index()    
    
    # calculate the rating count by item_id
    df_rating_counts = ratings.groupby(""item_id"").size().reset_index(name=""rating_counts"")
    
    # merge the means and counts based on item_id
    df_merged = pd.merge(df_avg_rating, df_rating_counts, on=""item_id"")
    
    # filter by threshold
    df_filtered = df_merged[df_merged[""rating_counts""] > threshold]
    
    # sort by rating
    df_sorted = df_filtered.sort_values(by=""rating"", ascending=False)

    # get the top and bottom movies
    best_movie_ids = df_sorted.head(size)[""item_id""].tolist()
    worst_movie_ids = df_sorted.tail(size)[""item_id""].tolist()[::-1] 

    return best_movie_ids, worst_movie_ids
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
156,17363,1,"    movie_rating_count = ratings.groupby('item_id')['item_id'].count()
    movies_of_interest = movie_rating_count.loc[movie_rating_count > threshold].index",TODO,movie_rating_count,"def best_worst_movies(ratings: pd.DataFrame, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    movie_rating_count = ratings.groupby('item_id')['item_id'].count()
    movies_of_interest = movie_rating_count.loc[movie_rating_count > threshold].index

    movie_mean_ratings = ratings.groupby('item_id')['rating'].mean()[movies_of_interest]

    # sort from highest to lowest
    movie_mean_ratings = movie_mean_ratings.sort_values().index.to_list()  # sorted in ascending order; convert Series to list
    return movie_mean_ratings[-size:][::-1], movie_mean_ratings[:size]   # reverse the order so that best movie ranking is from highest to lowest","def best_worst_movies(ratings: pd.DataFrame, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    movie_rating_count = ratings.groupby('item_id')['item_id'].count()
    movies_of_interest = movie_rating_count.loc[movie_rating_count > threshold].index

    movie_mean_ratings = ratings.groupby('item_id')['rating'].mean()[movies_of_interest]

    # sort from highest to lowest
    movie_mean_ratings = movie_mean_ratings.sort_values().index.to_list()  # sorted in ascending order; convert Series to list
    return movie_mean_ratings[-size:][::-1], movie_mean_ratings[:size]   # reverse the order so that best movie ranking is from highest to lowest
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
157,17363,2,"    movies_of_interest = movie_rating_count.loc[movie_rating_count > threshold].index

    movie_mean_ratings = ratings.groupby('item_id')['rating'].mean()[movies_of_interest]",TODO,movies_of_interest,"def best_worst_movies(ratings: pd.DataFrame, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    movie_rating_count = ratings.groupby('item_id')['item_id'].count()
    movies_of_interest = movie_rating_count.loc[movie_rating_count > threshold].index

    movie_mean_ratings = ratings.groupby('item_id')['rating'].mean()[movies_of_interest]

    # sort from highest to lowest
    movie_mean_ratings = movie_mean_ratings.sort_values().index.to_list()  # sorted in ascending order; convert Series to list
    return movie_mean_ratings[-size:][::-1], movie_mean_ratings[:size]   # reverse the order so that best movie ranking is from highest to lowest","def best_worst_movies(ratings: pd.DataFrame, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    movie_rating_count = ratings.groupby('item_id')['item_id'].count()
    movies_of_interest = movie_rating_count.loc[movie_rating_count > threshold].index

    movie_mean_ratings = ratings.groupby('item_id')['rating'].mean()[movies_of_interest]

    # sort from highest to lowest
    movie_mean_ratings = movie_mean_ratings.sort_values().index.to_list()  # sorted in ascending order; convert Series to list
    return movie_mean_ratings[-size:][::-1], movie_mean_ratings[:size]   # reverse the order so that best movie ranking is from highest to lowest
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
158,17363,3,"    movie_mean_ratings = ratings.groupby('item_id')['rating'].mean()[movies_of_interest]

    # sort from highest to lowest
    movie_mean_ratings = movie_mean_ratings.sort_values().index.to_list()  # sorted in ascending order; convert Series to list
    return movie_mean_ratings[-size:][::-1], movie_mean_ratings[:size]   # reverse the order so that best movie ranking is from highest to lowest",TODO,movie_mean_ratings,"def best_worst_movies(ratings: pd.DataFrame, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    movie_rating_count = ratings.groupby('item_id')['item_id'].count()
    movies_of_interest = movie_rating_count.loc[movie_rating_count > threshold].index

    movie_mean_ratings = ratings.groupby('item_id')['rating'].mean()[movies_of_interest]

    # sort from highest to lowest
    movie_mean_ratings = movie_mean_ratings.sort_values().index.to_list()  # sorted in ascending order; convert Series to list
    return movie_mean_ratings[-size:][::-1], movie_mean_ratings[:size]   # reverse the order so that best movie ranking is from highest to lowest","def best_worst_movies(ratings: pd.DataFrame, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    movie_rating_count = ratings.groupby('item_id')['item_id'].count()
    movies_of_interest = movie_rating_count.loc[movie_rating_count > threshold].index

    movie_mean_ratings = ratings.groupby('item_id')['rating'].mean()[movies_of_interest]

    # sort from highest to lowest
    movie_mean_ratings = movie_mean_ratings.sort_values().index.to_list()  # sorted in ascending order; convert Series to list
    return movie_mean_ratings[-size:][::-1], movie_mean_ratings[:size]   # reverse the order so that best movie ranking is from highest to lowest
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
159,14985,1,"    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)",TODO,df_useful,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
160,14985,2,"    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()",TODO,df_count,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
161,14985,3,"    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")",TODO,df_average_ratings,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
162,14985,4,"    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()",TODO,df_sorted_average_ratings,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
163,14985,5,"    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]",TODO,list_sorted_ids_by_avg_rating,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    #we only take useful columns
    df_useful = ratings[[""item_id"", ""rating""]]
    
    df_count = df_useful.groupby(""item_id"").filter(lambda x: len(x) > threshold)
    #df_count = df_useful.groupby(""item_id"").count().rename(columns = {'rating':'num_ratings'})
    
    
    #df_movie_ids_satisfy_threshold = df_count[df_count[""num_ratings""] > threshold]
    #print(df_movie_ids_satisfy_threshold.head(20))
    
    df_average_ratings = df_count.groupby(""item_id"").mean().rename(columns = {'rating':'avg_ratings'}).reset_index()
    
    
    #now we sort
    df_sorted_average_ratings = df_average_ratings.sort_values(""avg_ratings"")
    
    list_sorted_ids_by_avg_rating = df_sorted_average_ratings[""item_id""].to_list()
    
    return list_sorted_ids_by_avg_rating[-size:][::-1], list_sorted_ids_by_avg_rating[:size]
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
164,21711,1,"    df = ratings.groupby(""item_id"").agg(count = (""user_id"", ""count""), rating=(""rating"", ""mean""))
    df = df[df[""count""]>threshold].sort_values(""rating"")
    worst = list(df[:size].index.values)
    best = list(df[-size:].index.values)",TODO,df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.groupby(""item_id"").agg(count = (""user_id"", ""count""), rating=(""rating"", ""mean""))
    df = df[df[""count""]>threshold].sort_values(""rating"")
    worst = list(df[:size].index.values)
    best = list(df[-size:].index.values)
    best.reverse()
    return best, worst
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.groupby(""item_id"").agg(count = (""user_id"", ""count""), rating=(""rating"", ""mean""))
    df = df[df[""count""]>threshold].sort_values(""rating"")
    worst = list(df[:size].index.values)
    best = list(df[-size:].index.values)
    best.reverse()
    return best, worst
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
165,21711,2,"    worst = list(df[:size].index.values)
    best = list(df[-size:].index.values)
    best.reverse()
    return best, worst",TODO,worst,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.groupby(""item_id"").agg(count = (""user_id"", ""count""), rating=(""rating"", ""mean""))
    df = df[df[""count""]>threshold].sort_values(""rating"")
    worst = list(df[:size].index.values)
    best = list(df[-size:].index.values)
    best.reverse()
    return best, worst
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.groupby(""item_id"").agg(count = (""user_id"", ""count""), rating=(""rating"", ""mean""))
    df = df[df[""count""]>threshold].sort_values(""rating"")
    worst = list(df[:size].index.values)
    best = list(df[-size:].index.values)
    best.reverse()
    return best, worst
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
166,21711,3,"    best = list(df[-size:].index.values)
    best.reverse()
    return best, worst",TODO,best,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.groupby(""item_id"").agg(count = (""user_id"", ""count""), rating=(""rating"", ""mean""))
    df = df[df[""count""]>threshold].sort_values(""rating"")
    worst = list(df[:size].index.values)
    best = list(df[-size:].index.values)
    best.reverse()
    return best, worst
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings.groupby(""item_id"").agg(count = (""user_id"", ""count""), rating=(""rating"", ""mean""))
    df = df[df[""count""]>threshold].sort_values(""rating"")
    worst = list(df[:size].index.values)
    best = list(df[-size:].index.values)
    best.reverse()
    return best, worst
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
167,2372,1,"    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index",TODO,rating_counts,"# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)","# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
168,2372,2,"    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()",TODO,popular_movies,"# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)","# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
169,2372,3,"    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index",TODO,movie_ratings,"# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)","# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
170,2372,4,"    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)",TODO,best_movies,"# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)","# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
171,2372,5,"    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)",TODO,worst_movies,"# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)","# def best_worst_movies(ratings, threshold = 50, size = 10):
#     """"""
#     Get the top movies with highest average ratings and top movies with lowest average ratings

#     args:
#         ratings (pd.DataFrame)  : Dataframe containing user ratings

#     kwargs:
#         threshold (int) : movies that are considered should have more ratings than this threshold
#         size (int) : the number of movies with lowest / highest average ratings to get

#     return: Tuple (best_movie_ids, worst_movie_ids)
#             best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
#             worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
#     """"""
#     pass

def best_worst_movies(ratings, threshold=50, size=10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    # Select movies that have more than threshold ratings
    rating_counts = ratings['item_id'].value_counts()
    popular_movies = rating_counts[rating_counts > threshold].index

    # Calculate the average rating for each popular movie
    movie_ratings = ratings[ratings['item_id'].isin(popular_movies)].groupby('item_id')['rating'].mean()

    # Get the indices of the top and bottom rated movies
    best_movies = movie_ratings.sort_values(ascending=False).head(size).index
    worst_movies = movie_ratings.sort_values().head(size).index

    return list(best_movies), list(worst_movies)
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
172,20387,1,"    groups = ratings.groupby([""item_id""])[""rating""].agg([""count"",""mean""]).query(""count>@threshold"")\
    .loc[:,[""mean""]]#
    largest = groups.nlargest(columns = ""mean"",n = size).index.to_list()
    smallest = groups.nsmallest(columns = ""mean"",n = size).index.to_list()",TODO,groups,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    groups = ratings.groupby([""item_id""])[""rating""].agg([""count"",""mean""]).query(""count>@threshold"")\
    .loc[:,[""mean""]]#
    largest = groups.nlargest(columns = ""mean"",n = size).index.to_list()
    smallest = groups.nsmallest(columns = ""mean"",n = size).index.to_list()
    return largest , smallest
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    groups = ratings.groupby([""item_id""])[""rating""].agg([""count"",""mean""]).query(""count>@threshold"")\
    .loc[:,[""mean""]]#
    largest = groups.nlargest(columns = ""mean"",n = size).index.to_list()
    smallest = groups.nsmallest(columns = ""mean"",n = size).index.to_list()
    return largest , smallest
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
173,20387,2,"    largest = groups.nlargest(columns = ""mean"",n = size).index.to_list()
    smallest = groups.nsmallest(columns = ""mean"",n = size).index.to_list()
    return largest , smallest",TODO,largest,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    groups = ratings.groupby([""item_id""])[""rating""].agg([""count"",""mean""]).query(""count>@threshold"")\
    .loc[:,[""mean""]]#
    largest = groups.nlargest(columns = ""mean"",n = size).index.to_list()
    smallest = groups.nsmallest(columns = ""mean"",n = size).index.to_list()
    return largest , smallest
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    groups = ratings.groupby([""item_id""])[""rating""].agg([""count"",""mean""]).query(""count>@threshold"")\
    .loc[:,[""mean""]]#
    largest = groups.nlargest(columns = ""mean"",n = size).index.to_list()
    smallest = groups.nsmallest(columns = ""mean"",n = size).index.to_list()
    return largest , smallest
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
174,20387,3,"    smallest = groups.nsmallest(columns = ""mean"",n = size).index.to_list()
    return largest , smallest",TODO,smallest,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    groups = ratings.groupby([""item_id""])[""rating""].agg([""count"",""mean""]).query(""count>@threshold"")\
    .loc[:,[""mean""]]#
    largest = groups.nlargest(columns = ""mean"",n = size).index.to_list()
    smallest = groups.nsmallest(columns = ""mean"",n = size).index.to_list()
    return largest , smallest
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    groups = ratings.groupby([""item_id""])[""rating""].agg([""count"",""mean""]).query(""count>@threshold"")\
    .loc[:,[""mean""]]#
    largest = groups.nlargest(columns = ""mean"",n = size).index.to_list()
    smallest = groups.nsmallest(columns = ""mean"",n = size).index.to_list()
    return largest , smallest
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
175,23919,1,"    X_with_na = ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max')
    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values",TODO,X_with_na,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X_with_na = ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max')
    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X_with_na = ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max')
    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)
copied_ratings = ratings.copy()
dff = pd.DataFrame({'user_id': 1, 'item_id': 10, 'rating': 5.0, 'timestamp': 1000000000}, index=[100000])
copied_ratings.loc[100] = [1, 10, 5.0, 1000000000]

display(ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max').iloc[:10, :10])
display(copied_ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max').iloc[:10, :10])
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
176,23919,2,"    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)",TODO,X,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X_with_na = ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max')
    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X_with_na = ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max')
    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)
copied_ratings = ratings.copy()
dff = pd.DataFrame({'user_id': 1, 'item_id': 10, 'rating': 5.0, 'timestamp': 1000000000}, index=[100000])
copied_ratings.loc[100] = [1, 10, 5.0, 1000000000]

display(ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max').iloc[:10, :10])
display(copied_ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max').iloc[:10, :10])
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
177,23919,3,"    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)",TODO,user_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X_with_na = ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max')
    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X_with_na = ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max')
    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)
copied_ratings = ratings.copy()
dff = pd.DataFrame({'user_id': 1, 'item_id': 10, 'rating': 5.0, 'timestamp': 1000000000}, index=[100000])
copied_ratings.loc[100] = [1, 10, 5.0, 1000000000]

display(ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max').iloc[:10, :10])
display(copied_ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max').iloc[:10, :10])
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
178,23919,4,"    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)",TODO,movie_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X_with_na = ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max')
    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X_with_na = ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max')
    X = X_with_na.fillna(0).astype('int64').values
    user_means = X_with_na.mean(axis=1).values
    movie_means = X_with_na.mean(axis=0).values
    return (X, user_means, movie_means)
copied_ratings = ratings.copy()
dff = pd.DataFrame({'user_id': 1, 'item_id': 10, 'rating': 5.0, 'timestamp': 1000000000}, index=[100000])
copied_ratings.loc[100] = [1, 10, 5.0, 1000000000]

display(ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max').iloc[:10, :10])
display(copied_ratings.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='max').iloc[:10, :10])
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
179,19335,1,"X = ratings.pivot_table(index='user_id', columns='item_id', values='rating')
display(X)
movie_means = X.mean(axis=0)
user_means = X.mean(axis=1)
X.fillna(0, inplace=True)

X.astype('int64').values.dtype",TODO,X,"# date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')

X = ratings.pivot_table(index='user_id', columns='item_id', values='rating')
display(X)
movie_means = X.mean(axis=0)
user_means = X.mean(axis=1)
X.fillna(0, inplace=True)

X.astype('int64').values.dtype","# date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')

X = ratings.pivot_table(index='user_id', columns='item_id', values='rating')
display(X)
movie_means = X.mean(axis=0)
user_means = X.mean(axis=1)
X.fillna(0, inplace=True)

X.astype('int64').values.dtype
def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X = ratings.pivot_table(index='user_id', columns='item_id', values='rating')
    movie_means = X.mean(axis=0)
    user_means = X.mean(axis=1)
    X.fillna(0, inplace=True)
    
    return (X.astype('int64').values, user_means.values, movie_means.values)
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
180,18912,1,"    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)",TODO,func_vec,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
181,18912,2,"    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))",TODO,m,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
182,18912,3,"    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))",TODO,n,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
183,18912,4,"    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means",TODO,X,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
184,18912,5,"    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values",TODO,df,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
185,18912,6,"    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:",TODO,rows,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
186,18912,7,"    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:",TODO,cols,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
187,18912,8,"    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values",TODO,a,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
188,18912,9,"    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values",TODO,b,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
189,18912,10,"    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values",TODO,R,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
190,18912,11,"    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values",TODO,C,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
191,18912,12,"    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1",TODO,i,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
192,18912,13,"    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c",TODO,p,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
193,18912,14,"    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values",TODO,par_vec,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
194,18912,15,"    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means",TODO,user_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
195,18912,16,"    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means",TODO,movies_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    # function to return separate lists of [a1,a2,...] and [b1,b2,...]
    # from a single list of [(a1,b1), (a2,b2), .......],
    # (-1 for indexing purposes)
    def func(x):
        return x[0], x[1]

    func_vec = np.vectorize(func)

    #dimensions of matrix based on number of unique user and item ids
    m = ratings[""user_id""].nunique()
    n = ratings[""item_id""].nunique()

    #Initializing matrix with zeros as most ratings will be 0
    X = np.zeros((m, n))

    # grouping ratings df by user_id and item_id and aggregating by
    # max of rating(if multiple entries for same user, item pair) and storing a sa series
    df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
    rows = sorted(ratings[""user_id""].unique())
    cols = sorted(ratings[""item_id""].unique())

    # applying vectorized function on indices of the series which are in a [(r,c)] form
    a,b = func_vec(df.index.values)
    # print(list(b))
    R = {}
    C = {}
    i=0
    p=-1
    for r in rows:
        if p!=r:
            R[r]=i
            i+=1
            p=r
    i=0
    p=-1
    for c in cols:
        if p!=c:
            C[c]=i
            i+=1
            p=c

    def par(a, L:dict):
        return L[a]
    
    par_vec = np.vectorize(par)

    # Filling the matrix with ratings at appropriate positions
    X[par_vec(a, R), par_vec(b, C)] = df.values

    # Taking sum accross appropriate axis, and dividing by total non zero entries in that axis
    user_means = np.sum(X, axis=1)/np.count_nonzero(X, axis=1)
    movies_means = np.sum(X, axis=0)/np.count_nonzero(X, axis=0)
    
    return X.astype(np.int64), user_means, movies_means
    
    pass
# ratings.groupby([""user_id"", ""item_id""])[""rating""].max().index
# matrix_data(ratings)
# r = ratings.iloc[[0, -1]]
# r
# matrix_data(ratings)
# # X, user_means, movie_means = matrix_data(ratings)
# def func(x):
#     return x[0]-1, x[1]-1

# func_vec = np.vectorize(func)

# m = ratings[""user_id""].nunique()
# n = ratings[""item_id""].nunique()

# X = np.zeros((m, n))
# Y = X.copy()
# df = ratings.groupby([""user_id"", ""item_id""])[""rating""].max()
# r, c = func_vec(df.index.values)

# X[r, c] = df.values
# mat = coo_matrix((df.values, (r, c)), shape=(m, n))
# Y = Y+mat
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
196,14986,1,"    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}",TODO,unique_users,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
197,14986,2,"    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()",TODO,M,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
198,14986,3,"    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}",TODO,unique_movies,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
199,14986,4,"    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()",TODO,N,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
200,14986,5,"    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])",TODO,user_id_to_index,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
201,14986,6,"    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])",TODO,movie_id_to_index,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
202,14986,7,"    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()",TODO,np_row_indices,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
203,14986,8,"    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()",TODO,np_col_indices,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
204,14986,9,"    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()",TODO,np_ratings,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
205,14986,10,"    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means",TODO,X,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
206,14986,11,"    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means",TODO,user_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
207,14986,12,"    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means",TODO,movie_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""    
    unique_users = ratings[""user_id""].unique()
    M = len(unique_users)
    unique_movies = ratings[""item_id""].unique()
    N = len(unique_movies)
    unique_users.sort()
    unique_movies.sort()
    
    #Now we want to crate a mapping between (users and movies) and row/col indexes
    user_id_to_index = {user_id : i for i, user_id in enumerate(unique_users)}
    movie_id_to_index = {movie_id : i for i, movie_id in enumerate(unique_movies)}
    
    #we now build the three lists for the COO
    np_row_indices = np.array([user_id_to_index[user_id] for user_id in ratings['user_id']])
    np_col_indices = np.array([movie_id_to_index[movie_id] for movie_id in ratings['item_id']])
    np_ratings = ratings['rating'].to_numpy()
    
    #apparently its a numpy array that we need to return. I dont feel like making the changes
    X =  coo_matrix((np_ratings, (np_row_indices, np_col_indices)), shape=(M, N)).tocsr()

    user_means = np.asarray(X.sum(axis = 1)/X.getnnz(axis = 1).reshape((-1,1))).reshape(-1)
    movie_means = np.asarray(X.sum(axis = 0)/X.getnnz(axis = 0).reshape((1,-1))).reshape(-1)
    
    return X.toarray(), user_means, movie_means
    
    
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
208,636,1,"    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))",TODO,mr_dict,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
209,636,2,"    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))",TODO,ur_dict,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
210,636,3,"    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:",TODO,sorted_mr,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
211,636,4,"    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:",TODO,sorted_ur,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
212,636,5,"    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]",TODO,sorted_md,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
213,636,6,"    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1",TODO,count,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
214,636,7,"    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]",TODO,sorted_ud,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
215,636,8,"    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg",TODO,mat,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
216,636,9,"    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg",TODO,user_avg,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
217,636,10,"    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg",TODO,movie_avg,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
218,636,11,"        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)",TODO,md,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
219,636,12,"        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)",TODO,ud,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    mr_dict = {}
    ur_dict = {}
    
    for m, r in zip(ratings['item_id'], ratings['rating']):
        if m not in mr_dict:
            mr_dict[m] = []
        mr_dict[m].append(r)
    
    for u, r in zip(ratings['user_id'], ratings['rating']):
        if u not in ur_dict:
            ur_dict[u] = []
        ur_dict[u].append(r)
    
    sorted_mr = sorted(list(mr_dict.keys()))
    sorted_ur = sorted(list(ur_dict.keys()))
    
    sorted_md = {}
    count = 0
    for m in sorted_mr:
        if m not in sorted_md:
            sorted_md[m] = count
            count += 1
    
    sorted_ud = {}
    count = 0
    for u in sorted_ur:
        if u not in sorted_ud:
            sorted_ud[u] = count
            count += 1
    
    mat = np.zeros((943, 1682), dtype=np.int64)
    for m, u, r in zip(ratings['item_id'], ratings['user_id'], ratings['rating']):
        md = sorted_md[m]
        ud = sorted_ud[u]
        if r > mat[ud][md]:
            mat[ud][md] = int(r)
    
    user_avg = np.mean(mat, axis = 1, where= mat!=0)
    movie_avg = np.mean(mat, axis = 0, where= mat!=0)
    
    return mat, user_avg, movie_avg
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
220,1113,1,"    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)",TODO,user_movie_array,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
221,1113,2,"    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)",TODO,u_list,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
222,1113,3,"    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)",TODO,m_list,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
223,1113,4,"    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)",TODO,user_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
224,1113,5,"    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)",TODO,movie_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_array = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").fillna(0).astype(int).values
    user_movie_array = user_movie_array.astype('int64')

    u_list = []
    m_list = []

    for i in range(user_movie_array.shape[0]):
        u_list.append(user_movie_array[i][user_movie_array[i]!=0].mean())


    for j in range(user_movie_array.shape[1]):
        m_list.append(user_movie_array[:,j][user_movie_array[:,j]!=0].mean())

    user_means = np.array(u_list)
    movie_means = np.array(m_list)
    
    return(user_movie_array,user_means,movie_means)
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
225,23180,1,"    X_norm = X / np.linalg.norm(X, axis=-1)[:, np.newaxis]
    cosine = np.dot(X_norm, X_norm.T)",TODO,X_norm,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    X_norm = X / np.linalg.norm(X, axis=-1)[:, np.newaxis]
    cosine = np.dot(X_norm, X_norm.T)
    return cosine","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    X_norm = X / np.linalg.norm(X, axis=-1)[:, np.newaxis]
    cosine = np.dot(X_norm, X_norm.T)
    return cosine
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
226,23180,2,"    cosine = np.dot(X_norm, X_norm.T)
    return cosine",TODO,cosine,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    X_norm = X / np.linalg.norm(X, axis=-1)[:, np.newaxis]
    cosine = np.dot(X_norm, X_norm.T)
    return cosine","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    X_norm = X / np.linalg.norm(X, axis=-1)[:, np.newaxis]
    cosine = np.dot(X_norm, X_norm.T)
    return cosine
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
227,17419,1,"    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)",TODO,norm_X,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)
    return W
    ","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)
    return W
    
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
228,17419,2,"    W = X @ X.T / np.outer(norm_X, norm_X)
    return W",TODO,W,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)
    return W
    ","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)
    return W
    
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
229,15554,1,"    norm_val = norm(X, axis = 1)
    return np.dot(X, X.T) / np.outer(norm_val, norm_val)",TODO,norm_val,"from numpy.linalg import norm
def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_val = norm(X, axis = 1)
    return np.dot(X, X.T) / np.outer(norm_val, norm_val)
    ","from numpy.linalg import norm
def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_val = norm(X, axis = 1)
    return np.dot(X, X.T) / np.outer(norm_val, norm_val)
    
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
230,16924,1,"    dot_product = np.dot(X, X.T)
    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product",TODO,dot_product,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    dot_product = np.dot(X, X.T)
    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product
    return similarity","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    dot_product = np.dot(X, X.T)
    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product
    return similarity
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
231,16924,2,"    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)",TODO,norms,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    dot_product = np.dot(X, X.T)
    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product
    return similarity","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    dot_product = np.dot(X, X.T)
    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product
    return similarity
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
232,16924,3,"    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product",TODO,norms_product,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    dot_product = np.dot(X, X.T)
    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product
    return similarity","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    dot_product = np.dot(X, X.T)
    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product
    return similarity
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
233,16924,4,"    similarity = dot_product / norms_product
    return similarity",TODO,similarity,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    dot_product = np.dot(X, X.T)
    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product
    return similarity","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    dot_product = np.dot(X, X.T)
    norms = np.linalg.norm(X, axis=1)
    norms_product = np.outer(norms, norms.T)
    similarity = dot_product / norms_product
    return similarity
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
234,23081,1,"    dot_product = np.matmul(X, X.T)

    # Compute the norms of the rows of the rating matrix
    norms = np.linalg.norm(X, axis=1, keepdims=True)

    # Compute the cosine similarity matrix
    similarity_matrix = dot_product / (norms * norms.T)",TODO,dot_product,"def cosine_similarity(X):
   # Compute the dot product of the rating matrix with its transpose
    dot_product = np.matmul(X, X.T)

    # Compute the norms of the rows of the rating matrix
    norms = np.linalg.norm(X, axis=1, keepdims=True)

    # Compute the cosine similarity matrix
    similarity_matrix = dot_product / (norms * norms.T)
    
    return similarity_matrix
    pass ","def cosine_similarity(X):
   # Compute the dot product of the rating matrix with its transpose
    dot_product = np.matmul(X, X.T)

    # Compute the norms of the rows of the rating matrix
    norms = np.linalg.norm(X, axis=1, keepdims=True)

    # Compute the cosine similarity matrix
    similarity_matrix = dot_product / (norms * norms.T)
    
    return similarity_matrix
    pass 
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
235,23081,2,"    norms = np.linalg.norm(X, axis=1, keepdims=True)

    # Compute the cosine similarity matrix
    similarity_matrix = dot_product / (norms * norms.T)",TODO,norms,"def cosine_similarity(X):
   # Compute the dot product of the rating matrix with its transpose
    dot_product = np.matmul(X, X.T)

    # Compute the norms of the rows of the rating matrix
    norms = np.linalg.norm(X, axis=1, keepdims=True)

    # Compute the cosine similarity matrix
    similarity_matrix = dot_product / (norms * norms.T)
    
    return similarity_matrix
    pass ","def cosine_similarity(X):
   # Compute the dot product of the rating matrix with its transpose
    dot_product = np.matmul(X, X.T)

    # Compute the norms of the rows of the rating matrix
    norms = np.linalg.norm(X, axis=1, keepdims=True)

    # Compute the cosine similarity matrix
    similarity_matrix = dot_product / (norms * norms.T)
    
    return similarity_matrix
    pass 
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
236,23081,3,"    similarity_matrix = dot_product / (norms * norms.T)
    
    return similarity_matrix",TODO,similarity_matrix,"def cosine_similarity(X):
   # Compute the dot product of the rating matrix with its transpose
    dot_product = np.matmul(X, X.T)

    # Compute the norms of the rows of the rating matrix
    norms = np.linalg.norm(X, axis=1, keepdims=True)

    # Compute the cosine similarity matrix
    similarity_matrix = dot_product / (norms * norms.T)
    
    return similarity_matrix
    pass ","def cosine_similarity(X):
   # Compute the dot product of the rating matrix with its transpose
    dot_product = np.matmul(X, X.T)

    # Compute the norms of the rows of the rating matrix
    norms = np.linalg.norm(X, axis=1, keepdims=True)

    # Compute the cosine similarity matrix
    similarity_matrix = dot_product / (norms * norms.T)
    
    return similarity_matrix
    pass 
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
237,18886,1,"    Xt = np.transpose(X) 

    DenomX = np.sqrt(np.sum(np.square(X), 1))
    DenomX = np.reshape(DenomX, (DenomX.shape[0],1))
    DenomXt = np.reshape(DenomX, (1,DenomX.shape[0]))
    return X.dot(Xt) / DenomX.dot(DenomXt)",TODO,Xt,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    Xt = np.transpose(X) 

    DenomX = np.sqrt(np.sum(np.square(X), 1))
    DenomX = np.reshape(DenomX, (DenomX.shape[0],1))
    DenomXt = np.reshape(DenomX, (1,DenomX.shape[0]))
    return X.dot(Xt) / DenomX.dot(DenomXt)","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    Xt = np.transpose(X) 

    DenomX = np.sqrt(np.sum(np.square(X), 1))
    DenomX = np.reshape(DenomX, (DenomX.shape[0],1))
    DenomXt = np.reshape(DenomX, (1,DenomX.shape[0]))
    return X.dot(Xt) / DenomX.dot(DenomXt)
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
238,18886,2,"    DenomX = np.sqrt(np.sum(np.square(X), 1))
    DenomX = np.reshape(DenomX, (DenomX.shape[0],1))
    DenomXt = np.reshape(DenomX, (1,DenomX.shape[0]))
    return X.dot(Xt) / DenomX.dot(DenomXt)",TODO,DenomX,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    Xt = np.transpose(X) 

    DenomX = np.sqrt(np.sum(np.square(X), 1))
    DenomX = np.reshape(DenomX, (DenomX.shape[0],1))
    DenomXt = np.reshape(DenomX, (1,DenomX.shape[0]))
    return X.dot(Xt) / DenomX.dot(DenomXt)","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    Xt = np.transpose(X) 

    DenomX = np.sqrt(np.sum(np.square(X), 1))
    DenomX = np.reshape(DenomX, (DenomX.shape[0],1))
    DenomXt = np.reshape(DenomX, (1,DenomX.shape[0]))
    return X.dot(Xt) / DenomX.dot(DenomXt)
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
239,18886,3,"    DenomXt = np.reshape(DenomX, (1,DenomX.shape[0]))
    return X.dot(Xt) / DenomX.dot(DenomXt)",TODO,DenomXt,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    Xt = np.transpose(X) 

    DenomX = np.sqrt(np.sum(np.square(X), 1))
    DenomX = np.reshape(DenomX, (DenomX.shape[0],1))
    DenomXt = np.reshape(DenomX, (1,DenomX.shape[0]))
    return X.dot(Xt) / DenomX.dot(DenomXt)","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    Xt = np.transpose(X) 

    DenomX = np.sqrt(np.sum(np.square(X), 1))
    DenomX = np.reshape(DenomX, (DenomX.shape[0],1))
    DenomXt = np.reshape(DenomX, (1,DenomX.shape[0]))
    return X.dot(Xt) / DenomX.dot(DenomXt)
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
240,21614,1,"    norms = np.sqrt((X**2).sum(axis=1))

    # Compute the dot product for each pair of user vectors
    dot_product = X @ X.T

    # Use np.outer to compute the product of the norms for each pair of users
    # and divide the dot product by this to get the cosine similarity
    W = dot_product / np.outer(norms, norms)",TODO,norms,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args:
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # Compute the norms for each user vector
    norms = np.sqrt((X**2).sum(axis=1))

    # Compute the dot product for each pair of user vectors
    dot_product = X @ X.T

    # Use np.outer to compute the product of the norms for each pair of users
    # and divide the dot product by this to get the cosine similarity
    W = dot_product / np.outer(norms, norms)

    return W
    # pass","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args:
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # Compute the norms for each user vector
    norms = np.sqrt((X**2).sum(axis=1))

    # Compute the dot product for each pair of user vectors
    dot_product = X @ X.T

    # Use np.outer to compute the product of the norms for each pair of users
    # and divide the dot product by this to get the cosine similarity
    W = dot_product / np.outer(norms, norms)

    return W
    # pass
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
241,21614,2,"    dot_product = X @ X.T

    # Use np.outer to compute the product of the norms for each pair of users
    # and divide the dot product by this to get the cosine similarity
    W = dot_product / np.outer(norms, norms)",TODO,dot_product,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args:
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # Compute the norms for each user vector
    norms = np.sqrt((X**2).sum(axis=1))

    # Compute the dot product for each pair of user vectors
    dot_product = X @ X.T

    # Use np.outer to compute the product of the norms for each pair of users
    # and divide the dot product by this to get the cosine similarity
    W = dot_product / np.outer(norms, norms)

    return W
    # pass","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args:
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # Compute the norms for each user vector
    norms = np.sqrt((X**2).sum(axis=1))

    # Compute the dot product for each pair of user vectors
    dot_product = X @ X.T

    # Use np.outer to compute the product of the norms for each pair of users
    # and divide the dot product by this to get the cosine similarity
    W = dot_product / np.outer(norms, norms)

    return W
    # pass
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
242,21614,3,"    W = dot_product / np.outer(norms, norms)

    return W",TODO,W,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args:
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # Compute the norms for each user vector
    norms = np.sqrt((X**2).sum(axis=1))

    # Compute the dot product for each pair of user vectors
    dot_product = X @ X.T

    # Use np.outer to compute the product of the norms for each pair of users
    # and divide the dot product by this to get the cosine similarity
    W = dot_product / np.outer(norms, norms)

    return W
    # pass","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args:
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # Compute the norms for each user vector
    norms = np.sqrt((X**2).sum(axis=1))

    # Compute the dot product for each pair of user vectors
    dot_product = X @ X.T

    # Use np.outer to compute the product of the norms for each pair of users
    # and divide the dot product by this to get the cosine similarity
    W = dot_product / np.outer(norms, norms)

    return W
    # pass
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
243,2699,1,"    dif = X_true[X_true != 0] - X_pred[X_true != 0]
    mean_di = np.mean(dif ** 2, dtype=np.float64)",TODO,dif,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    dif = X_true[X_true != 0] - X_pred[X_true != 0]
    mean_di = np.mean(dif ** 2, dtype=np.float64)
    return mean_di","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    dif = X_true[X_true != 0] - X_pred[X_true != 0]
    mean_di = np.mean(dif ** 2, dtype=np.float64)
    return mean_di
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
244,2699,2,"    mean_di = np.mean(dif ** 2, dtype=np.float64)
    return mean_di",TODO,mean_di,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    dif = X_true[X_true != 0] - X_pred[X_true != 0]
    mean_di = np.mean(dif ** 2, dtype=np.float64)
    return mean_di","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    dif = X_true[X_true != 0] - X_pred[X_true != 0]
    mean_di = np.mean(dif ** 2, dtype=np.float64)
    return mean_di
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
245,15168,1,"    mask = np.where(X_true!=0, 1, 0) #(3,2)
    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)",TODO,mask,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = np.where(X_true!=0, 1, 0) #(3,2)
    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)
    ans = np.float64(mean_diff)
    return ans","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = np.where(X_true!=0, 1, 0) #(3,2)
    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)
    ans = np.float64(mean_diff)
    return ans
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    print(diff)
    assert np.allclose(diff, 0.7992514466097069)
    print(""All tests passed!"")

test_mean_rating_diff()"
246,15168,2,"    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)",TODO,up,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = np.where(X_true!=0, 1, 0) #(3,2)
    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)
    ans = np.float64(mean_diff)
    return ans","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = np.where(X_true!=0, 1, 0) #(3,2)
    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)
    ans = np.float64(mean_diff)
    return ans
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    print(diff)
    assert np.allclose(diff, 0.7992514466097069)
    print(""All tests passed!"")

test_mean_rating_diff()"
247,15168,3,"    mean_diff = np.sum(up) / np.sum(mask)
    ans = np.float64(mean_diff)",TODO,mean_diff,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = np.where(X_true!=0, 1, 0) #(3,2)
    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)
    ans = np.float64(mean_diff)
    return ans","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = np.where(X_true!=0, 1, 0) #(3,2)
    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)
    ans = np.float64(mean_diff)
    return ans
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    print(diff)
    assert np.allclose(diff, 0.7992514466097069)
    print(""All tests passed!"")

test_mean_rating_diff()"
248,15168,4,"    ans = np.float64(mean_diff)
    return ans",TODO,ans,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = np.where(X_true!=0, 1, 0) #(3,2)
    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)
    ans = np.float64(mean_diff)
    return ans","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = np.where(X_true!=0, 1, 0) #(3,2)
    up = mask*(X_true-X_pred)**2
    mean_diff = np.sum(up) / np.sum(mask)
    ans = np.float64(mean_diff)
    return ans
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    print(diff)
    assert np.allclose(diff, 0.7992514466097069)
    print(""All tests passed!"")

test_mean_rating_diff()"
249,16790,1,"    X_bool = (X_true!=0).astype(int)
    return np.sum(X_bool*((X_true - X_pred)**2)) / np.sum(X_bool)",TODO,X_bool,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    X_bool = (X_true!=0).astype(int)
    return np.sum(X_bool*((X_true - X_pred)**2)) / np.sum(X_bool)","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    X_bool = (X_true!=0).astype(int)
    return np.sum(X_bool*((X_true - X_pred)**2)) / np.sum(X_bool)
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
250,17501,1,"    indicator_matrix = (X_true != 0).astype(int)
    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)",TODO,indicator_matrix,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    indicator_matrix = (X_true != 0).astype(int)
    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            
    return diff
    pass","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    indicator_matrix = (X_true != 0).astype(int)
    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            
    return diff
    pass
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
251,17501,2,"    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            ",TODO,norm,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    indicator_matrix = (X_true != 0).astype(int)
    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            
    return diff
    pass","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    indicator_matrix = (X_true != 0).astype(int)
    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            
    return diff
    pass
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
252,17501,3,"    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            ",TODO,denorm,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    indicator_matrix = (X_true != 0).astype(int)
    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            
    return diff
    pass","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    indicator_matrix = (X_true != 0).astype(int)
    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            
    return diff
    pass
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
253,17501,4,"    diff = np.float64(norm/denorm).mean()            
    return diff",TODO,diff,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    indicator_matrix = (X_true != 0).astype(int)
    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            
    return diff
    pass","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    indicator_matrix = (X_true != 0).astype(int)
    norm = np.sum(indicator_matrix * ((X_true - X_pred)**2))
    denorm = np.sum(indicator_matrix)
    diff = np.float64(norm/denorm).mean()            
    return diff
    pass
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
254,1529,1,"    binary_nonzero = (X_true > 0).astype(int)

    numerator = np.sum(((X_true - X_pred)**2)*binary_nonzero)
    denominator = np.sum(binary_nonzero)",TODO,binary_nonzero,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    binary_nonzero = (X_true > 0).astype(int)

    numerator = np.sum(((X_true - X_pred)**2)*binary_nonzero)
    denominator = np.sum(binary_nonzero)

    return numerator/denominator
    pass","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    binary_nonzero = (X_true > 0).astype(int)

    numerator = np.sum(((X_true - X_pred)**2)*binary_nonzero)
    denominator = np.sum(binary_nonzero)

    return numerator/denominator
    pass
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
255,1529,2,"    numerator = np.sum(((X_true - X_pred)**2)*binary_nonzero)
    denominator = np.sum(binary_nonzero)

    return numerator/denominator",TODO,numerator,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    binary_nonzero = (X_true > 0).astype(int)

    numerator = np.sum(((X_true - X_pred)**2)*binary_nonzero)
    denominator = np.sum(binary_nonzero)

    return numerator/denominator
    pass","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    binary_nonzero = (X_true > 0).astype(int)

    numerator = np.sum(((X_true - X_pred)**2)*binary_nonzero)
    denominator = np.sum(binary_nonzero)

    return numerator/denominator
    pass
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
256,1529,3,"    denominator = np.sum(binary_nonzero)

    return numerator/denominator",TODO,denominator,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    binary_nonzero = (X_true > 0).astype(int)

    numerator = np.sum(((X_true - X_pred)**2)*binary_nonzero)
    denominator = np.sum(binary_nonzero)

    return numerator/denominator
    pass","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    binary_nonzero = (X_true > 0).astype(int)

    numerator = np.sum(((X_true - X_pred)**2)*binary_nonzero)
    denominator = np.sum(binary_nonzero)

    return numerator/denominator
    pass
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
257,18518,1,"display(X)
display(X_predicted)

X = X.astype('float')
X[X==0] = np.nan
diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')",TODO,X,"
display(X)
display(X_predicted)

X = X.astype('float')
X[X==0] = np.nan
diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)","
display(X)
display(X_predicted)

X = X.astype('float')
X[X==0] = np.nan
diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)
def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    X = X_true.astype('float')

    # change zero entries in X to nan to be ignored when subtract 
    X[X==0] = np.nan
    

    diff = X - X_pred
    diff[np.isnan(diff)] = 0
    nume = np.sum(diff**2)

    ft = (~np.isnan(X)).astype('int')

    return nume / np.sum(ft)
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
258,18518,2,"diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)",TODO,diff,"
display(X)
display(X_predicted)

X = X.astype('float')
X[X==0] = np.nan
diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)","
display(X)
display(X_predicted)

X = X.astype('float')
X[X==0] = np.nan
diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)
def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    X = X_true.astype('float')

    # change zero entries in X to nan to be ignored when subtract 
    X[X==0] = np.nan
    

    diff = X - X_pred
    diff[np.isnan(diff)] = 0
    nume = np.sum(diff**2)

    ft = (~np.isnan(X)).astype('int')

    return nume / np.sum(ft)
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
259,18518,3,"nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)",TODO,nume,"
display(X)
display(X_predicted)

X = X.astype('float')
X[X==0] = np.nan
diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)","
display(X)
display(X_predicted)

X = X.astype('float')
X[X==0] = np.nan
diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)
def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    X = X_true.astype('float')

    # change zero entries in X to nan to be ignored when subtract 
    X[X==0] = np.nan
    

    diff = X - X_pred
    diff[np.isnan(diff)] = 0
    nume = np.sum(diff**2)

    ft = (~np.isnan(X)).astype('int')

    return nume / np.sum(ft)
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
260,18518,4,"ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)",TODO,ft,"
display(X)
display(X_predicted)

X = X.astype('float')
X[X==0] = np.nan
diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)","
display(X)
display(X_predicted)

X = X.astype('float')
X[X==0] = np.nan
diff = X - X_predicted
diff[np.isnan(diff)] = 0
nume = np.sum(diff**2)

ft = (~np.isnan(X)).astype('int')

nume / np.sum(ft)
def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    X = X_true.astype('float')

    # change zero entries in X to nan to be ignored when subtract 
    X[X==0] = np.nan
    

    diff = X - X_pred
    diff[np.isnan(diff)] = 0
    nume = np.sum(diff**2)

    ft = (~np.isnan(X)).astype('int')

    return nume / np.sum(ft)
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
261,16682,1,"    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):",TODO,n_users,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
262,16682,2,"    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):",TODO,n_movies,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
263,16682,3,"    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)",TODO,U,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
264,16682,4,"    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)",TODO,V,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
265,16682,5,"            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]",TODO,non_zero_indices_movie,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
266,16682,6,"            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)",TODO,U_i,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
267,16682,7,"            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)",TODO,X_i,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
268,16682,8,"            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)",TODO,A,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
269,16682,9,"            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)",TODO,B,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
270,16682,10,"            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V",TODO,inverse_prod_V,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
271,16682,11,"            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]",TODO,non_zero_indices_user,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
272,16682,12,"            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)",TODO,V_j,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
273,16682,13,"            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)",TODO,X_j,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
274,16682,14,"            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)",TODO,C,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
275,16682,15,"            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)",TODO,D,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
276,16682,16,"            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U",TODO,inverse_prod_U,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    n_users,n_movies = X_sparse.shape

    U = np.random.normal(loc=0,scale=0.1,size=(n_users, k))
    V = np.random.normal(loc=0,scale=0.1,size=(k, n_movies))

    #binmat = np.where(X_check!=0,True,False)
    #print(mask)

    for _ in range(niters):
        for movie in range(n_movies):

            non_zero_indices_movie = np.nonzero(X_sparse.A[:,movie])[0]
            U_i = U[non_zero_indices_movie,:]
            #print(X_check[non_zero_indices])
            X_i = X_sparse.A[non_zero_indices_movie][:,movie]
            #print(X_i)
            #print(X_check[binmat[:,j],j])
            #print(U_i)
            #print(U[binmat[:,j]])
            A = lam*np.eye(k) + np.dot(U_i.T,U_i)
            #print(A)

            B = np.matmul(X_i,U_i)
            #print(B)
            inverse_prod_V = la.solve(A,B)
            #print(inverse_prod_V)
            V[:,movie] = inverse_prod_V
            #print(V)

            #print(""end of iteration"")

        #print(""U iterations"")    
        for user in range(n_users):
            non_zero_indices_user = np.nonzero(X_sparse.A[user,:])[0]
            V_j = V[:,non_zero_indices_user]
            #print(V_j)

            X_j = X_sparse.A[:,non_zero_indices_user][user,:]
            #print(X_j)


            C = lam*np.eye(k) + np.dot(V_j,V_j.T)
            #print(C)

            D = np.matmul(X_j,V_j.T)
            #print(D)
            inverse_prod_U = la.solve(C,D)
            #print(inverse_prod_U)
            U[user:,] = inverse_prod_U
            #print(U)
            #print(""end of iteration"",str(user+1))

        #print(""MAIN ITER"")    

#     print(""final"")        
#     print(U)
#     print(V)
            
            

    return (U, V)
    #pass
# X_check = np.array([[0,0,1], [3,1,0], [0,0,3], [4,0,0]])
# np.random.seed(0)
    
#     # your implementation here
# n_users,n_movies = X_check.shape
# k = 2
# lam = 0.1
# U = np.array([[0.1,0.5],[0.2,0.6],[0.3,0.7],[0.4,0.8]])
# V = np.array([[0.15,0.25,0.35],[0.45,0.55,0.65]])

# binmat = np.where(X_check!=0,True,False)
# print(binmat)

# for _ in range(5):
#     for movie in range(n_movies):
#         non_zero_indices_movie = binmat[:,movie]
#         #print(""non zero indices "",non_zero_indices_movie)
#         #U_i = U[non_zero_indices_movie,:]
#         U_i = U[non_zero_indices_movie,:]
#         #print(X_check[non_zero_indices])
#         X_i = X_check[non_zero_indices_movie,movie]
#         print(""xi shape"",X_i.shape)
#         #print(X_check[binmat[:,j],j])
#         print(""ui shape"",U_i.shape)
#         #print(U[binmat[:,j]])
#         A = lam*np.eye(k) + np.dot(U_i.T,U_i)
#         print(A.shape)

#         B = np.matmul(X_i,U_i)
#         print(B.shape)
#         inverse_prod_V = la.solve(A,B)
#         #print(inverse_prod_V)
#         print(""V col shape"",V[:,movie].shape)
#         print(""inverse prod shape"",inverse_prod_V.shape)
#         V[:,movie] = inverse_prod_V
#         print(V)

#         print(""end of iteration"")

#     print(""U iterations"")    
#     for user in range(n_users):
#         non_zero_indices_user = binmat[user,:]
#         V_j = V[:,non_zero_indices_user]
#         print(V_j)

#         X_j = X_check[:,non_zero_indices_user][user,:]
#         print(X_j)


#         C = lam*np.eye(k) + np.dot(V_j,V_j.T)
#         print(C)

#         D = np.matmul(X_j,V[:,binmat[user,:]].T)
#         print(D)
#         inverse_prod_U = la.solve(C,D)
#         print(inverse_prod_U)
#         U[user:,] = inverse_prod_U
#         print(U)
#         print(""end of iteration"",str(user+1))
        
#     print(""MAIN ITER"")    
        
# print(""final"")        
# print(U)
# print(V)
      
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
277,18528,1,"    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)",TODO,U,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here
    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here
    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
278,18528,2,"    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)",TODO,V,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here
    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here
    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
279,18528,3,"            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)",TODO,A,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here
    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here
    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
280,18528,4,"            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)",TODO,B,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here
    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here
    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))
    while(niters):
        for col in np.arange(V.shape[1], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for ui in (U.T * (X_sparse.A[:, col] != 0)).T:
                A += np.outer(ui, ui)
            B = (U.T * (X_sparse.A[:, col] != 0)) @ (X_sparse.A[:, col] * (X_sparse.A[:, col] != 0))
            V[:, col] = la.solve(A + lam * np.eye(k, k), B)
        for row in np.arange(U.shape[0], dtype=np.int32):
            A, B = np.zeros((k, k)), np.zeros((k,))
            for vj in (V * (X_sparse.A[row, :] != 0)).T:
                A += np.outer(vj, vj)
            B = (V * (X_sparse.A[row, :] != 0)) @ (X_sparse.A[row, :] * (X_sparse.A[row, :] != 0))
            U[row, :] = la.solve(A + lam * np.eye(k, k), B)
        niters -= 1
    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
281,18402,1,"    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):",TODO,m,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
282,18402,2,"    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)",TODO,n,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
283,18402,3,"    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)",TODO,indicator,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
284,18402,4,"    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)",TODO,U,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
285,18402,5,"    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)",TODO,V,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
286,18402,6,"      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T",TODO,V_temp,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
287,18402,7,"        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T",TODO,rows,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
288,18402,8,"        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)",TODO,A,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
289,18402,9,"        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)",TODO,B,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
290,18402,10,"        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T",TODO,cols,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m = X_sparse.A.shape[0]
    n = X_sparse.A.shape[1]
    indicator = X_sparse.A != 0
    U = np.random.normal(loc = 0, scale = 0.1, size = (m,k) ) 
    V = np.random.normal(loc = 0, scale = 0.1, size = (k,n) )

    for iter in range(niters):
      #update v rows
      V_temp = V.T
      for i in range(n):
        rows = (indicator.T[i].reshape((m,1))*U).T
        A = rows@rows.T
        B = rows@X_sparse.A.T[i].T
        V_temp[i] = la.solve(A + lam * np.eye(k), B)
        
      V = V_temp.T

      #update u cols
      for i in range(m):
        cols = (indicator[i].reshape((n,1)).T*V)
        A = cols@cols.T
        B = cols@X_sparse.A[i].T
        U[i] = la.solve(A + lam * np.eye(k), B)

    return (U,V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
291,16619,1,"    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] ",TODO,X,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
292,16619,2,"    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):",TODO,i,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
293,16619,3,"    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):",TODO,j,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
294,16619,4,"    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V",TODO,U,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
295,16619,5,"    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V",TODO,V,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
296,16619,6,"    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] ",TODO,indicator_matrix,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
297,16619,7,"            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)",TODO,U_filtered,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
298,16619,8,"            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)",TODO,X_filtered,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
299,16619,9,"            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)",TODO,A,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
300,16619,10,"            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol",TODO,vcol,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
301,16619,11,"            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)",TODO,V_filtered,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
302,16619,12,"            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow",TODO,urow,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
# do not modify this line
    np.random.seed(seed)

    X = X_sparse.toarray()
    i, j = X_sparse.shape
    U = np.random.normal(loc=0, scale=0.1, size=(i, k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, j))

    indicator_matrix = (X != 0).astype(int)
    for _ in range(niters):
        for column_index in range(j):
            U_filtered = U[indicator_matrix[:, column_index] != 0]
            X_filtered = X[:, column_index][indicator_matrix[:, column_index] != 0]
            A = lam * np.eye(k) + U_filtered.T @ U_filtered
            vcol = la.inv(A) @ np.dot(X_filtered, U_filtered)
            V[:, column_index] = vcol

        for row_index in range(i):
            V_filtered = V[:, indicator_matrix[row_index, :] != 0]
            X_filtered = X[row_index,:][indicator_matrix[row_index,:]!=0] 
            A = lam * np.eye(k) + V_filtered @ V_filtered.T
            urow = V_filtered @ X_filtered @ la.inv(A)
            U[row_index,:] = urow

    return U, V
    pass
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
303,23481,1,"X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)",TODO,X_check,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
304,23481,2,"    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):",TODO,num_users,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
305,23481,3,"    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):",TODO,num_movies,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
306,23481,4,"    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v",TODO,u,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
307,23481,5,"    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v",TODO,v,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
308,23481,6,"            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]",TODO,X_col,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
309,23481,7,"            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]",TODO,binary_matrix,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
310,23481,8,"            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:",TODO,filtered_u,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
311,23481,9,"            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]",TODO,left_side,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
312,23481,10,"            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]",TODO,right_side,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
313,23481,11,"            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T",TODO,new_col,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
314,23481,12,"            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]",TODO,X_row,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
315,23481,13,"            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:",TODO,filtered_v,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    num_users, num_movies = X_sparse.shape
    u = np.random.normal(loc=0, scale=0.1, size=(num_users, k))
    v = np.random.normal(loc=0, scale=0.1, size=(k, num_movies))
    for _ in range(niters):
        # update v
        for j in range(num_movies):
            # filter the u for nonzero values of X
            X_col = X_sparse.toarray()[:, j]
            binary_matrix = np.ones(shape=X_col.shape)
            binary_matrix[X_col == 0] = 0
            filtered_u = u * binary_matrix[:, np.newaxis]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_u:
                left_side += (row[:, np.newaxis] @ row[np.newaxis, :])
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = u * X_col[:, np.newaxis]
            right_side = right_side.sum(axis=0)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            v[:, j] = new_col.T
        # update u:
        for i in range(num_users):
            # filter the u for nonzero values of X
            X_row = X_sparse.toarray()[i, :]
            binary_matrix = np.ones(shape=X_row.shape)
            binary_matrix[X_row == 0] = 0
            filtered_v = v * binary_matrix[np.newaxis, :]
            # get left side
            left_side = lam*np.eye(k)
            for row in filtered_v.T:
                left_side += row[:, np.newaxis] @ row[np.newaxis, :]
            left_side = np.linalg.inv(left_side)
            # get right side
            right_side = v * X_row[np.newaxis, :]
            right_side = right_side.sum(axis=1)
            # combine
            new_col = left_side @ right_side[:, np.newaxis]
            u[i] = new_col.T
    # your implementation here
    return u, v
X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
U_check, V_check = low_rank_matrix_factorization(X_check, 2, lam=0.1, niters=1)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
316,13444,1,"    if a == 0:
        return b
    if b == 0:
        return a

    # return if one of the number is the divosor of another
    if a % b == 0:
        return b;
    if b % a == 0:
        return a

    # calculate the greatest common divisor if none of the above cases exist
    while a >= 0 & b >= 0:
      # use the larger number to minus the smaller number
      if a > b:
        a = a - b
      elif a < b:
        b = b - a
      else:
        # return the greatest common divisor if a == b
        return a",TODO,a,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""

    # return the other integer if one of them is 0
    if a == 0:
        return b
    if b == 0:
        return a

    # return if one of the number is the divosor of another
    if a % b == 0:
        return b;
    if b % a == 0:
        return a

    # calculate the greatest common divisor if none of the above cases exist
    while a >= 0 & b >= 0:
      # use the larger number to minus the smaller number
      if a > b:
        a = a - b
      elif a < b:
        b = b - a
      else:
        # return the greatest common divisor if a == b
        return a
    
    ","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""

    # return the other integer if one of them is 0
    if a == 0:
        return b
    if b == 0:
        return a

    # return if one of the number is the divosor of another
    if a % b == 0:
        return b;
    if b % a == 0:
        return a

    # calculate the greatest common divisor if none of the above cases exist
    while a >= 0 & b >= 0:
      # use the larger number to minus the smaller number
      if a > b:
        a = a - b
      elif a < b:
        b = b - a
      else:
        # return the greatest common divisor if a == b
        return a
    
    "
317,13444,2,"        return b
    if b == 0:
        return a

    # return if one of the number is the divosor of another
    if a % b == 0:
        return b;
    if b % a == 0:
        return a

    # calculate the greatest common divisor if none of the above cases exist
    while a >= 0 & b >= 0:
      # use the larger number to minus the smaller number
      if a > b:
        a = a - b
      elif a < b:
        b = b - a",TODO,b,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""

    # return the other integer if one of them is 0
    if a == 0:
        return b
    if b == 0:
        return a

    # return if one of the number is the divosor of another
    if a % b == 0:
        return b;
    if b % a == 0:
        return a

    # calculate the greatest common divisor if none of the above cases exist
    while a >= 0 & b >= 0:
      # use the larger number to minus the smaller number
      if a > b:
        a = a - b
      elif a < b:
        b = b - a
      else:
        # return the greatest common divisor if a == b
        return a
    
    ","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""

    # return the other integer if one of them is 0
    if a == 0:
        return b
    if b == 0:
        return a

    # return if one of the number is the divosor of another
    if a % b == 0:
        return b;
    if b % a == 0:
        return a

    # calculate the greatest common divisor if none of the above cases exist
    while a >= 0 & b >= 0:
      # use the larger number to minus the smaller number
      if a > b:
        a = a - b
      elif a < b:
        b = b - a
      else:
        # return the greatest common divisor if a == b
        return a
    
    "
318,2711,1,"      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini",TODO,mini,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)"
319,2711,2,"      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi",TODO,maxi,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)"
320,2711,3,"    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)",TODO,a,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)"
321,2711,4,"    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)",TODO,b,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while min(a, b) > 1:
      mini = min(a, b)
      maxi = max(a, b)
      maxi = maxi %  mini
      a = mini
      b = maxi
    
    if a == 0: return b
    if b == 0: return a
    return min(a, b)"
322,2715,1,"        temp = a
        a = b
        b = temp % b",TODO,temp,"def gcd(a, b):
    while(b > 0):
        temp = a
        a = b
        b = temp % b
    return a
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    pass","def gcd(a, b):
    while(b > 0):
        temp = a
        a = b
        b = temp % b
    return a
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    pass"
323,2715,2,"        temp = a
        a = b
        b = temp % b
    return a",TODO,a,"def gcd(a, b):
    while(b > 0):
        temp = a
        a = b
        b = temp % b
    return a
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    pass","def gcd(a, b):
    while(b > 0):
        temp = a
        a = b
        b = temp % b
    return a
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    pass"
324,2715,3,"    while(b > 0):
        temp = a
        a = b
        b = temp % b",TODO,b,"def gcd(a, b):
    while(b > 0):
        temp = a
        a = b
        b = temp % b
    return a
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    pass","def gcd(a, b):
    while(b > 0):
        temp = a
        a = b
        b = temp % b
    return a
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    pass"
325,13515,1,"        a, b = b, a % b
    return a",TODO,a,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b != 0:
        a, b = b, a % b
    return a","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b != 0:
        a, b = b, a % b
    return a"
326,13515,2,"    while b != 0:
        a, b = b, a % b",TODO,b,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b != 0:
        a, b = b, a % b
    return a","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b != 0:
        a, b = b, a % b
    return a"
327,13487,1,"    if a < b:
        a, b = b, a
    if b == 0:
        return a

    return gcd(b, a%b)",TODO,a,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a < b:
        a, b = b, a
    if b == 0:
        return a

    return gcd(b, a%b)","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a < b:
        a, b = b, a
    if b == 0:
        return a

    return gcd(b, a%b)"
328,13487,2,"    if a < b:
        a, b = b, a
    if b == 0:
        return a

    return gcd(b, a%b)",TODO,b,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a < b:
        a, b = b, a
    if b == 0:
        return a

    return gcd(b, a%b)","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a < b:
        a, b = b, a
    if b == 0:
        return a

    return gcd(b, a%b)"
329,13486,1,"    if a == 0:
      return b
    if b == 0:
      return a
    a, b = max(a, b), min(a, b)
    if a % b == 0:
      return b
    return gcd(b, a%b)",TODO,a,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
      return b
    if b == 0:
      return a
    a, b = max(a, b), min(a, b)
    if a % b == 0:
      return b
    return gcd(b, a%b)","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
      return b
    if b == 0:
      return a
    a, b = max(a, b), min(a, b)
    if a % b == 0:
      return b
    return gcd(b, a%b)"
330,13486,2,"      return b
    if b == 0:
      return a
    a, b = max(a, b), min(a, b)
    if a % b == 0:
      return b
    return gcd(b, a%b)",TODO,b,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
      return b
    if b == 0:
      return a
    a, b = max(a, b), min(a, b)
    if a % b == 0:
      return b
    return gcd(b, a%b)","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
      return b
    if b == 0:
      return a
    a, b = max(a, b), min(a, b)
    if a % b == 0:
      return b
    return gcd(b, a%b)"
331,13465,1,"    if a > b:
        a, b = b, a
    if a == 0:
        return b
    for i in range(1, a+1):
        if a % i == 0 and b % i == 0:",TODO,a,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a > b:
        a, b = b, a
    if a == 0:
        return b
    for i in range(1, a+1):
        if a % i == 0 and b % i == 0:
            result = i
    return result","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a > b:
        a, b = b, a
    if a == 0:
        return b
    for i in range(1, a+1):
        if a % i == 0 and b % i == 0:
            result = i
    return result"
332,13465,2,"    if a > b:
        a, b = b, a
    if a == 0:
        return b
    for i in range(1, a+1):
        if a % i == 0 and b % i == 0:",TODO,b,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a > b:
        a, b = b, a
    if a == 0:
        return b
    for i in range(1, a+1):
        if a % i == 0 and b % i == 0:
            result = i
    return result","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a > b:
        a, b = b, a
    if a == 0:
        return b
    for i in range(1, a+1):
        if a % i == 0 and b % i == 0:
            result = i
    return result"
333,13465,3,"            result = i
    return result",TODO,result,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a > b:
        a, b = b, a
    if a == 0:
        return b
    for i in range(1, a+1):
        if a % i == 0 and b % i == 0:
            result = i
    return result","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a > b:
        a, b = b, a
    if a == 0:
        return b
    for i in range(1, a+1):
        if a % i == 0 and b % i == 0:
            result = i
    return result"
334,12209,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
335,12209,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
336,12209,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
337,12209,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
338,12209,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
339,12209,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
340,12209,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
341,12209,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
342,12209,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
343,12209,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
344,12209,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
345,12209,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
346,12209,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
347,12209,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
348,12209,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
349,12209,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
350,5371,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
351,5371,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
352,5371,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
353,5371,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
354,5371,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
355,5371,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
356,5371,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
357,5371,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
358,5371,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
359,5371,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
360,5371,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
361,5371,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
362,5371,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
363,5371,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
364,5371,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
365,5371,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
366,12062,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
367,12062,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
368,12062,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
369,12062,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
370,12062,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
371,12062,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
372,12062,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
373,12062,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
374,12062,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
375,12062,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
376,12062,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
377,12062,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
378,12062,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
379,12062,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
380,12062,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
381,12062,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
382,9707,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
383,9707,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
384,9707,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
385,9707,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
386,9707,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
387,9707,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
388,9707,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
389,9707,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
390,9707,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
391,9707,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
392,9707,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
393,9707,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
394,9707,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
395,9707,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
396,9707,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
397,9707,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
398,11418,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
399,11418,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
400,11418,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
401,11418,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
402,11418,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
403,11418,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
404,11418,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
405,11418,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
406,11418,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
407,11418,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
408,11418,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
409,11418,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
410,11418,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
411,11418,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
412,11418,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
413,11418,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
414,3768,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
415,3768,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
416,3768,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
417,3768,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
418,3768,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
419,3768,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
420,3768,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
421,3768,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
422,3768,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
423,3768,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
424,3768,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
425,3768,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
426,3768,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
427,3768,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
428,3768,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
429,3768,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
430,10092,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
431,10092,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
432,10092,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
433,10092,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
434,10092,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
435,10092,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
436,10092,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
437,10092,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
438,10092,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
439,10092,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
440,10092,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
441,10092,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
442,10092,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
443,10092,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
444,10092,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
445,10092,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
446,11530,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
447,11530,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
448,11530,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
449,11530,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
450,11530,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
451,11530,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
452,11530,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
453,11530,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
454,11530,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
455,11530,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
456,11530,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
457,11530,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
458,11530,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
459,11530,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
460,11530,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
461,11530,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
462,9322,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
463,9322,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
464,9322,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
465,9322,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
466,9322,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
467,9322,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
468,9322,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
469,9322,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
470,9322,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
471,9322,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
472,9322,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
473,9322,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
474,9322,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
475,9322,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
476,9322,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
477,9322,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
478,12657,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
479,12657,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
480,12657,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
481,12657,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
482,12657,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
483,12657,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
484,12657,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
485,12657,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
486,12657,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
487,12657,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
488,12657,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
489,12657,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
490,12657,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
491,12657,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
492,12657,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
493,12657,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
494,6247,1,"        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist"
495,6247,2,"        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist"
496,9099,1,"        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))

    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))

    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures

    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id

    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))

    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))

    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures

    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id

    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))

    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))

    return blacklist, whitelist"
497,9099,2,"        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))

    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures

    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id

    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))

    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))

    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures

    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id

    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))

    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))

    return blacklist, whitelist"
498,8938,1,"        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)"
499,8938,2,"        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)"
500,4420,1,"        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist"
501,4420,2,"        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist"
502,13071,1,"        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
    
    return blacklist, whitelist"
503,13071,2,"        whitelist = set([int(x.strip()) for x in f.readlines()]) 
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
    
    return blacklist, whitelist"
504,12518,1,"        blacklist = {int(x.strip()) for x in f.readlines()}
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 

    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()}
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 

    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()}
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 

    return blacklist, whitelist"
505,12518,2,"        whitelist = {int(x.strip()) for x in f.readlines()} 

    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()}
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 

    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()}
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 

    return blacklist, whitelist"
506,3454,1,"        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
507,3454,2,"        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
508,2810,1,"        blacklist = [int(x.strip()) for x in f.readlines()]
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()]
    
    return set(blacklist), set(whitelist)",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()]
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()]
    
    return set(blacklist), set(whitelist)","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()]
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()]
    
    return set(blacklist), set(whitelist)"
509,2810,2,"        whitelist = [int(x.strip()) for x in f.readlines()]
    
    return set(blacklist), set(whitelist)",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()]
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()]
    
    return set(blacklist), set(whitelist)","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()]
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()]
    
    return set(blacklist), set(whitelist)"
510,7685,1,"        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist"
511,7685,2,"        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()} 
    
    return blacklist, whitelist"
512,12889,1,"        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
513,12889,2,"        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
514,10508,1,"        id_state = False
    elif id_to_check in blacklist:
        id_state = True
    else:
        id_state = False

    return id_state",TODO,id_state,"def check_single_id(id_to_check, blacklist, whitelist):
    '''
    Checks whether an input ID is banned, based on the stored blacklist and whiteliist
    
    args:
        id_to_check (int) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        id_state (bool) : True if id_to_check belongs to a banned user, and False otherwise
    '''
    
    if id_to_check in whitelist:
        id_state = False
    elif id_to_check in blacklist:
        id_state = True
    else:
        id_state = False

    return id_state","def check_single_id(id_to_check, blacklist, whitelist):
    '''
    Checks whether an input ID is banned, based on the stored blacklist and whiteliist
    
    args:
        id_to_check (int) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        id_state (bool) : True if id_to_check belongs to a banned user, and False otherwise
    '''
    
    if id_to_check in whitelist:
        id_state = False
    elif id_to_check in blacklist:
        id_state = True
    else:
        id_state = False

    return id_state
def test_check_single_id():
    blacklist, whitelist = read_blacklist_and_whitelist(""blacklist.txt"", ""whitelist.txt"")
    assert check_single_id(59735, blacklist, whitelist) == False, ""Check that you handle cases when id is in both blacklist and whitelist!""
    assert check_single_id(5935, blacklist, whitelist) == True, ""Check that you handle cases when id is in blacklist and not in whitelist!""
    print(""All tests passed!"")
    
test_check_single_id()

# let's also see how long it takes to run this function
%timeit check_single_id(59735, blacklist, whitelist)"
515,9324,1,"    in_blacklist = id_to_check in blacklist
    in_whitelist = id_to_check in whitelist 
    return (in_blacklist and not in_whitelist)",TODO,in_blacklist,"def check_single_id(id_to_check, blacklist, whitelist):
    '''
    Checks whether an input ID is banned, based on the stored blacklist and whiteliist
    
    args:
        id_to_check (int) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        id_state (bool) : True if id_to_check belongs to a banned user, and False otherwise
    '''
    in_blacklist = id_to_check in blacklist
    in_whitelist = id_to_check in whitelist 
    return (in_blacklist and not in_whitelist)","def check_single_id(id_to_check, blacklist, whitelist):
    '''
    Checks whether an input ID is banned, based on the stored blacklist and whiteliist
    
    args:
        id_to_check (int) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        id_state (bool) : True if id_to_check belongs to a banned user, and False otherwise
    '''
    in_blacklist = id_to_check in blacklist
    in_whitelist = id_to_check in whitelist 
    return (in_blacklist and not in_whitelist)
def test_check_single_id():
    blacklist, whitelist = read_blacklist_and_whitelist(""blacklist.txt"", ""whitelist.txt"")
    assert check_single_id(59735, blacklist, whitelist) == False, ""Check that you handle cases when id is in both blacklist and whitelist!""
    assert check_single_id(5935, blacklist, whitelist) == True, ""Check that you handle cases when id is in blacklist and not in whitelist!""
    print(""All tests passed!"")
    
test_check_single_id()

# let's also see how long it takes to run this function
%timeit check_single_id(59735, blacklist, whitelist)"
516,9324,2,"    in_whitelist = id_to_check in whitelist 
    return (in_blacklist and not in_whitelist)",TODO,in_whitelist,"def check_single_id(id_to_check, blacklist, whitelist):
    '''
    Checks whether an input ID is banned, based on the stored blacklist and whiteliist
    
    args:
        id_to_check (int) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        id_state (bool) : True if id_to_check belongs to a banned user, and False otherwise
    '''
    in_blacklist = id_to_check in blacklist
    in_whitelist = id_to_check in whitelist 
    return (in_blacklist and not in_whitelist)","def check_single_id(id_to_check, blacklist, whitelist):
    '''
    Checks whether an input ID is banned, based on the stored blacklist and whiteliist
    
    args:
        id_to_check (int) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        id_state (bool) : True if id_to_check belongs to a banned user, and False otherwise
    '''
    in_blacklist = id_to_check in blacklist
    in_whitelist = id_to_check in whitelist 
    return (in_blacklist and not in_whitelist)
def test_check_single_id():
    blacklist, whitelist = read_blacklist_and_whitelist(""blacklist.txt"", ""whitelist.txt"")
    assert check_single_id(59735, blacklist, whitelist) == False, ""Check that you handle cases when id is in both blacklist and whitelist!""
    assert check_single_id(5935, blacklist, whitelist) == True, ""Check that you handle cases when id is in blacklist and not in whitelist!""
    print(""All tests passed!"")
    
test_check_single_id()

# let's also see how long it takes to run this function
%timeit check_single_id(59735, blacklist, whitelist)"
517,13339,1,"    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
#     fifo_storage = deque()
#     for data in initial_data:
#         fifo_storage.append(data)
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
#     fifo_storage = deque()
#     for data in initial_data:
#         fifo_storage.append(data)
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
518,3848,1,"    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
519,11141,1,"    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
520,10137,1,"    q = 0
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            # if len(fifo_storage) > 0:
            #     yield fifo_storage.pop(0)
            if len(fifo_storage)> q:
                yield fifo_storage[q]
                q = q+1",TODO,q,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    q = 0
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            # if len(fifo_storage) > 0:
            #     yield fifo_storage.pop(0)
            if len(fifo_storage)> q:
                yield fifo_storage[q]
                q = q+1

                # if q >= len(fifo_storage)
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    q = 0
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            # if len(fifo_storage) > 0:
            #     yield fifo_storage.pop(0)
            if len(fifo_storage)> q:
                yield fifo_storage[q]
                q = q+1

                # if q >= len(fifo_storage)
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
521,10137,2,"    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            # if len(fifo_storage) > 0:
            #     yield fifo_storage.pop(0)
            if len(fifo_storage)> q:
                yield fifo_storage[q]
                q = q+1

                # if q >= len(fifo_storage)
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    q = 0
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            # if len(fifo_storage) > 0:
            #     yield fifo_storage.pop(0)
            if len(fifo_storage)> q:
                yield fifo_storage[q]
                q = q+1

                # if q >= len(fifo_storage)
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    q = 0
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            # if len(fifo_storage) > 0:
            #     yield fifo_storage.pop(0)
            if len(fifo_storage)> q:
                yield fifo_storage[q]
                q = q+1

                # if q >= len(fifo_storage)
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
522,2791,1,"    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
523,4821,1,"    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
524,10623,1,"    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
525,6508,1,"    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
526,11260,1,"    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
#     fifo_storage = deque()
#     for data in initial_data:
#         fifo_storage.append(data)
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
#     fifo_storage = deque()
#     for data in initial_data:
#         fifo_storage.append(data)
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
527,7569,1,"                chain_of_ancestors = path.copy()
                found = True
                # traverse
                break

            dfs_helper(target, children, path)
            if not found:  # If not found, remove the node from the path
                path.pop()

    chain_of_ancestors = []
    history = []
    found = False
    dfs_helper(target, tree, [])
    return chain_of_ancestors, history",TODO,chain_of_ancestors,"def dfs(target, tree, history=[], chain_of_ancestors=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    def dfs_helper(target, subtree, path):
        # nonlocal
        nonlocal chain_of_ancestors, history, found
        # 
        for key, children in subtree.items():
            if found:  # If target is found, no need to search further
                break

            history.append(key)  # Append node to history
            path.append(key)  # Append node to path

            if key == target:
                chain_of_ancestors = path.copy()
                found = True
                # traverse
                break

            dfs_helper(target, children, path)
            if not found:  # If not found, remove the node from the path
                path.pop()

    chain_of_ancestors = []
    history = []
    found = False
    dfs_helper(target, tree, [])
    return chain_of_ancestors, history","def dfs(target, tree, history=[], chain_of_ancestors=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    def dfs_helper(target, subtree, path):
        # nonlocal
        nonlocal chain_of_ancestors, history, found
        # 
        for key, children in subtree.items():
            if found:  # If target is found, no need to search further
                break

            history.append(key)  # Append node to history
            path.append(key)  # Append node to path

            if key == target:
                chain_of_ancestors = path.copy()
                found = True
                # traverse
                break

            dfs_helper(target, children, path)
            if not found:  # If not found, remove the node from the path
                path.pop()

    chain_of_ancestors = []
    history = []
    found = False
    dfs_helper(target, tree, [])
    return chain_of_ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
528,7569,2,"            history.append(key)  # Append node to history
            path.append(key)  # Append node to path

            if key == target:
                chain_of_ancestors = path.copy()
                found = True
                # traverse
                break

            dfs_helper(target, children, path)
            if not found:  # If not found, remove the node from the path
                path.pop()

    chain_of_ancestors = []
    history = []
    found = False
    dfs_helper(target, tree, [])
    return chain_of_ancestors, history",TODO,history,"def dfs(target, tree, history=[], chain_of_ancestors=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    def dfs_helper(target, subtree, path):
        # nonlocal
        nonlocal chain_of_ancestors, history, found
        # 
        for key, children in subtree.items():
            if found:  # If target is found, no need to search further
                break

            history.append(key)  # Append node to history
            path.append(key)  # Append node to path

            if key == target:
                chain_of_ancestors = path.copy()
                found = True
                # traverse
                break

            dfs_helper(target, children, path)
            if not found:  # If not found, remove the node from the path
                path.pop()

    chain_of_ancestors = []
    history = []
    found = False
    dfs_helper(target, tree, [])
    return chain_of_ancestors, history","def dfs(target, tree, history=[], chain_of_ancestors=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    def dfs_helper(target, subtree, path):
        # nonlocal
        nonlocal chain_of_ancestors, history, found
        # 
        for key, children in subtree.items():
            if found:  # If target is found, no need to search further
                break

            history.append(key)  # Append node to history
            path.append(key)  # Append node to path

            if key == target:
                chain_of_ancestors = path.copy()
                found = True
                # traverse
                break

            dfs_helper(target, children, path)
            if not found:  # If not found, remove the node from the path
                path.pop()

    chain_of_ancestors = []
    history = []
    found = False
    dfs_helper(target, tree, [])
    return chain_of_ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
529,7569,3,"            if found:  # If target is found, no need to search further
                break

            history.append(key)  # Append node to history
            path.append(key)  # Append node to path

            if key == target:
                chain_of_ancestors = path.copy()
                found = True
                # traverse
                break

            dfs_helper(target, children, path)
            if not found:  # If not found, remove the node from the path
                path.pop()

    chain_of_ancestors = []
    history = []
    found = False",TODO,found,"def dfs(target, tree, history=[], chain_of_ancestors=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    def dfs_helper(target, subtree, path):
        # nonlocal
        nonlocal chain_of_ancestors, history, found
        # 
        for key, children in subtree.items():
            if found:  # If target is found, no need to search further
                break

            history.append(key)  # Append node to history
            path.append(key)  # Append node to path

            if key == target:
                chain_of_ancestors = path.copy()
                found = True
                # traverse
                break

            dfs_helper(target, children, path)
            if not found:  # If not found, remove the node from the path
                path.pop()

    chain_of_ancestors = []
    history = []
    found = False
    dfs_helper(target, tree, [])
    return chain_of_ancestors, history","def dfs(target, tree, history=[], chain_of_ancestors=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    def dfs_helper(target, subtree, path):
        # nonlocal
        nonlocal chain_of_ancestors, history, found
        # 
        for key, children in subtree.items():
            if found:  # If target is found, no need to search further
                break

            history.append(key)  # Append node to history
            path.append(key)  # Append node to path

            if key == target:
                chain_of_ancestors = path.copy()
                found = True
                # traverse
                break

            dfs_helper(target, children, path)
            if not found:  # If not found, remove the node from the path
                path.pop()

    chain_of_ancestors = []
    history = []
    found = False
    dfs_helper(target, tree, [])
    return chain_of_ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
530,11618,1,"        history_ref = history_ref + [root]
        if root == target:
            return parents_ref + [root], history_ref
        elif isinstance(child, dict):
            parents_ref, history_ref = dfs_recursion(parents_ref + [root], history_ref, target, child)
            if parents_ref:
                return parents_ref, history_ref    
    return [], history_ref",TODO,history_ref,"def dfs_recursion(parents_ref, history_ref, target, tree):
    for root, child in tree.items():
        history_ref = history_ref + [root]
        if root == target:
            return parents_ref + [root], history_ref
        elif isinstance(child, dict):
            parents_ref, history_ref = dfs_recursion(parents_ref + [root], history_ref, target, child)
            if parents_ref:
                return parents_ref, history_ref    
    return [], history_ref

        
def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    return dfs_recursion([], [], target, tree)","def dfs_recursion(parents_ref, history_ref, target, tree):
    for root, child in tree.items():
        history_ref = history_ref + [root]
        if root == target:
            return parents_ref + [root], history_ref
        elif isinstance(child, dict):
            parents_ref, history_ref = dfs_recursion(parents_ref + [root], history_ref, target, child)
            if parents_ref:
                return parents_ref, history_ref    
    return [], history_ref

        
def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    return dfs_recursion([], [], target, tree)
tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
target = '9'
dfs_recursion([], [], target, tree)
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()
# tree = {
#   '1' : {'2':{}},
#   '3' : {'4':{}, '5':{}},
#   '6' : {'7': {'8': {'9':{}, '10': {}}}},
#   '11' : {} 
# }
# root = next(iter(tree))
# print(""root"")
# print(root)
# root2 = next(iter(root))
# print(""next(root)"")
# print(root2)
# level1 = tree.get(root, {})
# print(""get root"")
# print(level1)
# print(""get root 0"")
# print(list(level1.keys()))
# level2 = level1.get(next(iter(level1)), {})
# tree.items()
# target = '9'
# for root, child in tree.items():
#     print(root)
#     print(next(iter(child)))



# def test_1(tree):
#     for key, value in tree.items():
#         print(key)
#         if isinstance(value, dict):
#             test_1(value)
#         else:
#             print(value)
            
# for key, value in tree.items():
#         print(key)
#         if isinstance(value, dict):
#             print(value)
#             test_1(value)
#         else:
#             print(value)
#         print(""hi"")
# test_1(tree)
"
531,11618,2,"            return parents_ref + [root], history_ref
        elif isinstance(child, dict):
            parents_ref, history_ref = dfs_recursion(parents_ref + [root], history_ref, target, child)
            if parents_ref:
                return parents_ref, history_ref    ",TODO,parents_ref,"def dfs_recursion(parents_ref, history_ref, target, tree):
    for root, child in tree.items():
        history_ref = history_ref + [root]
        if root == target:
            return parents_ref + [root], history_ref
        elif isinstance(child, dict):
            parents_ref, history_ref = dfs_recursion(parents_ref + [root], history_ref, target, child)
            if parents_ref:
                return parents_ref, history_ref    
    return [], history_ref

        
def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    return dfs_recursion([], [], target, tree)","def dfs_recursion(parents_ref, history_ref, target, tree):
    for root, child in tree.items():
        history_ref = history_ref + [root]
        if root == target:
            return parents_ref + [root], history_ref
        elif isinstance(child, dict):
            parents_ref, history_ref = dfs_recursion(parents_ref + [root], history_ref, target, child)
            if parents_ref:
                return parents_ref, history_ref    
    return [], history_ref

        
def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    return dfs_recursion([], [], target, tree)
tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
target = '9'
dfs_recursion([], [], target, tree)
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()
# tree = {
#   '1' : {'2':{}},
#   '3' : {'4':{}, '5':{}},
#   '6' : {'7': {'8': {'9':{}, '10': {}}}},
#   '11' : {} 
# }
# root = next(iter(tree))
# print(""root"")
# print(root)
# root2 = next(iter(root))
# print(""next(root)"")
# print(root2)
# level1 = tree.get(root, {})
# print(""get root"")
# print(level1)
# print(""get root 0"")
# print(list(level1.keys()))
# level2 = level1.get(next(iter(level1)), {})
# tree.items()
# target = '9'
# for root, child in tree.items():
#     print(root)
#     print(next(iter(child)))



# def test_1(tree):
#     for key, value in tree.items():
#         print(key)
#         if isinstance(value, dict):
#             test_1(value)
#         else:
#             print(value)
            
# for key, value in tree.items():
#         print(key)
#         if isinstance(value, dict):
#             print(value)
#             test_1(value)
#         else:
#             print(value)
#         print(""hi"")
# test_1(tree)
"
532,6559,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
533,6559,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
534,7155,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
535,7155,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
536,8403,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
537,8403,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
538,4354,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
539,4354,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
540,5572,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
541,5572,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
542,5929,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
543,5929,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
544,5082,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
545,5082,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
546,9950,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
547,9950,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
548,10413,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
549,10413,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
550,10286,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
551,10286,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
552,12474,1,"        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache",TODO,ram_cache,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
553,12474,2,"        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache",TODO,disk_cache,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
554,12474,3,"        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value",TODO,computed_value,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
555,12474,4,"            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)",TODO,ram_least_recent_text,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
556,12474,5,"            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)",TODO,ram_least_recent_value,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
557,12474,6,"                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])",TODO,lines,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        ram_cache = self.get_from_ram_cache(new_text)
        if ram_cache is not None:
            return ram_cache
        
        disk_cache = self.get_from_disk_cache(new_text)
        if disk_cache is not None:
            return disk_cache
        
        computed_value = predict(new_text)
        self.add_to_cache(new_text, computed_value)
        return computed_value
    
    def add_to_cache(self, text, value):
        # if ram is full, push the least frequent value to disk
        if self.ram_size() == self.ram_max:
            # check if disk cache is full. If so, remove the least recent cache on disk to make room for the least recent cache on RAM.
            if self.disk_size() == self.disk_max:
                # pop the least recent value from disk cache (remove the last line in the cache file)
                with open(self.cache_disk, 'r') as f:
                    lines = f.readlines()
                with open(self.cache_disk, 'w') as f:
                    f.writelines(lines[:-1])

            # pop the least recent cache from RAM, to make room for the new cache
            ram_least_recent_text = next(iter(self.cache_ram))
            ram_least_recent_value = self.cache_ram.pop(ram_least_recent_text)

            # add as most recent cache on disk
            self.add_to_disk_cache(ram_least_recent_text, ram_least_recent_value)
        
        # add the new value as most recent cache on RAM
        self.add_to_ram_cache(text, value)
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
558,9363,1,"        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram",TODO,value_from_ram,"import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent","import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
559,9363,2,"        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk",TODO,value_from_disk,"import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent","import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
560,9363,3,"        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted",TODO,value_from_predicted,"import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent","import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
561,9363,4,"          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)",TODO,least_recent_ram_string,"import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent","import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
562,9363,5,"        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent",TODO,least_recent,"import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent","import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
563,9363,6,"            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)",TODO,least_recent_disk_string,"import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent","import time

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_count = 0
        self.disk_count = 0
        self.ram_cache = {}
        self.disk_cache = {}
        
        # initialize additional fields here
        
    def get(self, new_text):
        # implement the caching algorithm
        # raise NotImplementedError(""Complete this function!"")
        # print(""input is"", new_text)
        # time.sleep(0.1)
        value_from_ram = self.get_from_ram_cache(new_text)
        if (value_from_ram is not None):
          self.update_hit(new_text, True)
          # print(""output from ram: "", value_from_ram)
          # time.sleep(1)
          return value_from_ram
        
        value_from_disk = self.get_from_disk_cache(new_text)
        if (value_from_disk is not None):
          self.update_hit(new_text, False)
          # print(""output from disk: "", value_from_disk)
          # time.sleep(1)
          return value_from_disk
        
        value_from_predicted = super().get(new_text)
        if (self.ram_size() == self.ram_max):
          if (self.disk_size() == self.disk_max):
            least_recent_disk_string = self.update_miss_remove(False)
            # print(""Removing from disk "", least_recent_disk_string)
            self.remove_from_disk_cache(least_recent_disk_string)
            # time.sleep(0.1)
          least_recent_ram_string = self.update_miss_remove(True)
          # time.sleep(0.1)
          self.update_hit(least_recent_ram_string, False)
          # print(""Writing to Disk, "", least_recent_ram_string)
          self.add_to_disk_cache(least_recent_ram_string, self.get_from_ram_cache(least_recent_ram_string))
          # time.sleep(0.1)
          # print(""Removing from RAM, "", least_recent_ram_string)
          self.remove_from_ram_cache(least_recent_ram_string)
        self.update_hit(new_text, True)
        # print(""Writing to RAM, "", new_text)
        self.add_to_ram_cache(new_text, value_from_predicted)
        # time.sleep(0.1)

        return value_from_predicted


    def update_hit(self, new_str, is_ram):
      if is_ram == True:
        self.ram_count += 1
        self.ram_cache[new_str] = self.ram_count
      else:
        self.disk_count += 1
        self.disk_cache[new_str] = self.disk_count
    
    def update_miss_remove(self, is_ram):
      if is_ram == True:
        least_recent = min(value for value in self.ram_cache.values())
        for k, v in self.ram_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.ram_cache.pop(least_recent)
        return least_recent
      else:
        least_recent = min(value for value in self.disk_cache.values())
        for k, v in self.disk_cache.items():
          if v == least_recent:
            least_recent = k
            break
        self.disk_cache.pop(least_recent)
        return least_recent
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
564,8278,1,"        rating = self.get_from_ram_cache(new_text)
        if not rating == None:
            self.ram_seq.remove(new_text)
            self.ram_seq.append(new_text)
            return rating
        
        rating = self.get_from_disk_cache(new_text)
        if not rating == None:  
            self.disk_seq.remove(new_text)
            self.disk_seq.append(new_text)
            return rating
        
        rating = predict(new_text)
        if self.ram_size() == self.ram_max:
            # print(""------------------"")
            # If the disk cache's size is equal to disk_max, remove the least recent element from the disk cache.
            if self.disk_size() == self.disk_max: 
                self.remove_from_disk_cache(self.disk_seq.popleft())
                # remove it from RAM cache and add it to disk cache
            least_ram = self.ram_seq.popleft()
            self.add_to_disk_cache(least_ram, self.get_from_ram_cache(least_ram))
            self.disk_seq.append(least_ram)
            self.remove_from_ram_cache(least_ram)
        self.add_to_ram_cache(new_text, rating)
        self.ram_seq.append(new_text)
        return rating",TODO,rating,"from collections import deque

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_seq = deque()
        self.disk_seq = deque()
        
        # initialize additional fields here
        
    def get(self, new_text):
        '''
        print(f""ram size{self.ram_size()}"")
        print(f""ram_seq size{len(self.ram_seq)}"")
        print(f""disk size{self.disk_size()}"")
        print(f""disk_seq size{len(self.disk_seq)}"")
        '''
        # implement the caching algorithm
        rating = self.get_from_ram_cache(new_text)
        if not rating == None:
            self.ram_seq.remove(new_text)
            self.ram_seq.append(new_text)
            return rating
        
        rating = self.get_from_disk_cache(new_text)
        if not rating == None:  
            self.disk_seq.remove(new_text)
            self.disk_seq.append(new_text)
            return rating
        
        rating = predict(new_text)
        if self.ram_size() == self.ram_max:
            # print(""------------------"")
            # If the disk cache's size is equal to disk_max, remove the least recent element from the disk cache.
            if self.disk_size() == self.disk_max: 
                self.remove_from_disk_cache(self.disk_seq.popleft())
                # remove it from RAM cache and add it to disk cache
            least_ram = self.ram_seq.popleft()
            self.add_to_disk_cache(least_ram, self.get_from_ram_cache(least_ram))
            self.disk_seq.append(least_ram)
            self.remove_from_ram_cache(least_ram)
        self.add_to_ram_cache(new_text, rating)
        self.ram_seq.append(new_text)
        return rating

        # raise NotImplementedError(""Complete this function!"")","from collections import deque

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_seq = deque()
        self.disk_seq = deque()
        
        # initialize additional fields here
        
    def get(self, new_text):
        '''
        print(f""ram size{self.ram_size()}"")
        print(f""ram_seq size{len(self.ram_seq)}"")
        print(f""disk size{self.disk_size()}"")
        print(f""disk_seq size{len(self.disk_seq)}"")
        '''
        # implement the caching algorithm
        rating = self.get_from_ram_cache(new_text)
        if not rating == None:
            self.ram_seq.remove(new_text)
            self.ram_seq.append(new_text)
            return rating
        
        rating = self.get_from_disk_cache(new_text)
        if not rating == None:  
            self.disk_seq.remove(new_text)
            self.disk_seq.append(new_text)
            return rating
        
        rating = predict(new_text)
        if self.ram_size() == self.ram_max:
            # print(""------------------"")
            # If the disk cache's size is equal to disk_max, remove the least recent element from the disk cache.
            if self.disk_size() == self.disk_max: 
                self.remove_from_disk_cache(self.disk_seq.popleft())
                # remove it from RAM cache and add it to disk cache
            least_ram = self.ram_seq.popleft()
            self.add_to_disk_cache(least_ram, self.get_from_ram_cache(least_ram))
            self.disk_seq.append(least_ram)
            self.remove_from_ram_cache(least_ram)
        self.add_to_ram_cache(new_text, rating)
        self.ram_seq.append(new_text)
        return rating

        # raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
565,8278,2,"            least_ram = self.ram_seq.popleft()
            self.add_to_disk_cache(least_ram, self.get_from_ram_cache(least_ram))
            self.disk_seq.append(least_ram)
            self.remove_from_ram_cache(least_ram)",TODO,least_ram,"from collections import deque

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_seq = deque()
        self.disk_seq = deque()
        
        # initialize additional fields here
        
    def get(self, new_text):
        '''
        print(f""ram size{self.ram_size()}"")
        print(f""ram_seq size{len(self.ram_seq)}"")
        print(f""disk size{self.disk_size()}"")
        print(f""disk_seq size{len(self.disk_seq)}"")
        '''
        # implement the caching algorithm
        rating = self.get_from_ram_cache(new_text)
        if not rating == None:
            self.ram_seq.remove(new_text)
            self.ram_seq.append(new_text)
            return rating
        
        rating = self.get_from_disk_cache(new_text)
        if not rating == None:  
            self.disk_seq.remove(new_text)
            self.disk_seq.append(new_text)
            return rating
        
        rating = predict(new_text)
        if self.ram_size() == self.ram_max:
            # print(""------------------"")
            # If the disk cache's size is equal to disk_max, remove the least recent element from the disk cache.
            if self.disk_size() == self.disk_max: 
                self.remove_from_disk_cache(self.disk_seq.popleft())
                # remove it from RAM cache and add it to disk cache
            least_ram = self.ram_seq.popleft()
            self.add_to_disk_cache(least_ram, self.get_from_ram_cache(least_ram))
            self.disk_seq.append(least_ram)
            self.remove_from_ram_cache(least_ram)
        self.add_to_ram_cache(new_text, rating)
        self.ram_seq.append(new_text)
        return rating

        # raise NotImplementedError(""Complete this function!"")","from collections import deque

class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_seq = deque()
        self.disk_seq = deque()
        
        # initialize additional fields here
        
    def get(self, new_text):
        '''
        print(f""ram size{self.ram_size()}"")
        print(f""ram_seq size{len(self.ram_seq)}"")
        print(f""disk size{self.disk_size()}"")
        print(f""disk_seq size{len(self.disk_seq)}"")
        '''
        # implement the caching algorithm
        rating = self.get_from_ram_cache(new_text)
        if not rating == None:
            self.ram_seq.remove(new_text)
            self.ram_seq.append(new_text)
            return rating
        
        rating = self.get_from_disk_cache(new_text)
        if not rating == None:  
            self.disk_seq.remove(new_text)
            self.disk_seq.append(new_text)
            return rating
        
        rating = predict(new_text)
        if self.ram_size() == self.ram_max:
            # print(""------------------"")
            # If the disk cache's size is equal to disk_max, remove the least recent element from the disk cache.
            if self.disk_size() == self.disk_max: 
                self.remove_from_disk_cache(self.disk_seq.popleft())
                # remove it from RAM cache and add it to disk cache
            least_ram = self.ram_seq.popleft()
            self.add_to_disk_cache(least_ram, self.get_from_ram_cache(least_ram))
            self.disk_seq.append(least_ram)
            self.remove_from_ram_cache(least_ram)
        self.add_to_ram_cache(new_text, rating)
        self.ram_seq.append(new_text)
        return rating

        # raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
566,6785,1,"    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)",TODO,name,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
567,6785,2,"    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab",TODO,vocab,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
568,6785,3,"    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()",TODO,description,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
569,6785,4,"    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])",TODO,descrpition_words,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
570,6785,5,"    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list",TODO,spans_list,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
571,6785,6,"        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end",TODO,node,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
572,6785,7,"        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)",TODO,processedName,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
573,6785,8,"            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))",TODO,neighborhood_name,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
574,13367,1,"    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val",TODO,ret_val,"def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val","def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val
def test_check_list_ids():
    test_input_ids = [15795, 860, 76820, 54886, 6265, 82386, 37194, 87498, 44131, 60263]
    ref = [False, True, True, False, True, False, False, False, False, False]
    assert check_list_ids(test_input_ids, blacklist, whitelist) == ref
    print(""All tests passed!"")

test_check_list_ids()

# let's also see how long it takes to run this function
random.seed(42)
test_input_ids = [random.randint(0, 100000) for x in range(1000000)]
%timeit check_list_ids(test_input_ids, blacklist, whitelist)"
575,10523,1,"    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val",TODO,ret_val,"def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val","def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val
def test_check_list_ids():
    test_input_ids = [15795, 860, 76820, 54886, 6265, 82386, 37194, 87498, 44131, 60263]
    ref = [False, True, True, False, True, False, False, False, False, False]
    assert check_list_ids(test_input_ids, blacklist, whitelist) == ref
    print(""All tests passed!"")

test_check_list_ids()

# let's also see how long it takes to run this function
random.seed(42)
test_input_ids = [random.randint(0, 100000) for x in range(1000000)]
%timeit check_list_ids(test_input_ids, blacklist, whitelist)"
576,10410,1,"    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val",TODO,ret_val,"def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val","def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val
def test_check_list_ids():
    test_input_ids = [15795, 860, 76820, 54886, 6265, 82386, 37194, 87498, 44131, 60263]
    ref = [False, True, True, False, True, False, False, False, False, False]
    assert check_list_ids(test_input_ids, blacklist, whitelist) == ref
    print(""All tests passed!"")

test_check_list_ids()

# let's also see how long it takes to run this function
random.seed(42)
test_input_ids = [random.randint(0, 100000) for x in range(1000000)]
%timeit check_list_ids(test_input_ids, blacklist, whitelist)"
577,7545,1,"    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val",TODO,ret_val,"def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val","def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val
def test_check_list_ids():
    test_input_ids = [15795, 860, 76820, 54886, 6265, 82386, 37194, 87498, 44131, 60263]
    ref = [False, True, True, False, True, False, False, False, False, False]
    assert check_list_ids(test_input_ids, blacklist, whitelist) == ref
    print(""All tests passed!"")

test_check_list_ids()

# let's also see how long it takes to run this function
random.seed(42)
test_input_ids = [random.randint(0, 100000) for x in range(1000000)]
%timeit check_list_ids(test_input_ids, blacklist, whitelist)"
578,13963,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def Clean_names(country_name):
        if re.search('.*\(.*\)', country_name):
            name_1 = country_name.split(' (')[0]
            return name_1
        else:
            return country_name
          
    
    
    country_data.Country = country_data.Country.apply(Clean_names)
    country_data.drop(country_data[country_data.Country.isin(countries_to_remove)].index, inplace=True)",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def Clean_names(country_name):
        if re.search('.*\(.*\)', country_name):
            name_1 = country_name.split(' (')[0]
            return name_1
        else:
            return country_name
          
    
    
    country_data.Country = country_data.Country.apply(Clean_names)
    country_data.drop(country_data[country_data.Country.isin(countries_to_remove)].index, inplace=True)
    return country_data



# df_country_cleaned = preprocess_countries(df_country.copy())
# # print(len(df_country_cleaned))
# # print(df_country_cleaned[df_country_cleaned.AverageTemperature == 'Nan'])
# # print(set(df_country_cleaned.Country))
# print(df_country_cleaned.head())
# print(df_country_cleaned.dtypes)
# print(len(df_country_cleaned))
# display(df_country_cleaned[""Country""].unique())
# print(len(df_country_cleaned[""Country""].unique()))","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def Clean_names(country_name):
        if re.search('.*\(.*\)', country_name):
            name_1 = country_name.split(' (')[0]
            return name_1
        else:
            return country_name
          
    
    
    country_data.Country = country_data.Country.apply(Clean_names)
    country_data.drop(country_data[country_data.Country.isin(countries_to_remove)].index, inplace=True)
    return country_data



# df_country_cleaned = preprocess_countries(df_country.copy())
# # print(len(df_country_cleaned))
# # print(df_country_cleaned[df_country_cleaned.AverageTemperature == 'Nan'])
# # print(set(df_country_cleaned.Country))
# print(df_country_cleaned.head())
# print(df_country_cleaned.dtypes)
# print(len(df_country_cleaned))
# display(df_country_cleaned[""Country""].unique())
# print(len(df_country_cleaned[""Country""].unique()))
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
579,13963,2,"            name_1 = country_name.split(' (')[0]
            return name_1",TODO,name_1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def Clean_names(country_name):
        if re.search('.*\(.*\)', country_name):
            name_1 = country_name.split(' (')[0]
            return name_1
        else:
            return country_name
          
    
    
    country_data.Country = country_data.Country.apply(Clean_names)
    country_data.drop(country_data[country_data.Country.isin(countries_to_remove)].index, inplace=True)
    return country_data



# df_country_cleaned = preprocess_countries(df_country.copy())
# # print(len(df_country_cleaned))
# # print(df_country_cleaned[df_country_cleaned.AverageTemperature == 'Nan'])
# # print(set(df_country_cleaned.Country))
# print(df_country_cleaned.head())
# print(df_country_cleaned.dtypes)
# print(len(df_country_cleaned))
# display(df_country_cleaned[""Country""].unique())
# print(len(df_country_cleaned[""Country""].unique()))","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def Clean_names(country_name):
        if re.search('.*\(.*\)', country_name):
            name_1 = country_name.split(' (')[0]
            return name_1
        else:
            return country_name
          
    
    
    country_data.Country = country_data.Country.apply(Clean_names)
    country_data.drop(country_data[country_data.Country.isin(countries_to_remove)].index, inplace=True)
    return country_data



# df_country_cleaned = preprocess_countries(df_country.copy())
# # print(len(df_country_cleaned))
# # print(df_country_cleaned[df_country_cleaned.AverageTemperature == 'Nan'])
# # print(set(df_country_cleaned.Country))
# print(df_country_cleaned.head())
# print(df_country_cleaned.dtypes)
# print(len(df_country_cleaned))
# display(df_country_cleaned[""Country""].unique())
# print(len(df_country_cleaned[""Country""].unique()))
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
580,26915,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    # remove affiliations
    country_data['Country'] = country_data['Country'].str.replace("" \(.*\)"", """", regex=True)
    # remove countries without information
    valid_only = country_data[~country_data['Country'].isin(countries_to_remove)]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    # remove affiliations
    country_data['Country'] = country_data['Country'].str.replace("" \(.*\)"", """", regex=True)
    # remove countries without information
    valid_only = country_data[~country_data['Country'].isin(countries_to_remove)]
    return valid_only","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    # remove affiliations
    country_data['Country'] = country_data['Country'].str.replace("" \(.*\)"", """", regex=True)
    # remove countries without information
    valid_only = country_data[~country_data['Country'].isin(countries_to_remove)]
    return valid_only
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
581,26915,2,"    valid_only = country_data[~country_data['Country'].isin(countries_to_remove)]
    return valid_only",TODO,valid_only,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    # remove affiliations
    country_data['Country'] = country_data['Country'].str.replace("" \(.*\)"", """", regex=True)
    # remove countries without information
    valid_only = country_data[~country_data['Country'].isin(countries_to_remove)]
    return valid_only","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    # remove affiliations
    country_data['Country'] = country_data['Country'].str.replace("" \(.*\)"", """", regex=True)
    # remove countries without information
    valid_only = country_data[~country_data['Country'].isin(countries_to_remove)]
    return valid_only
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
582,26960,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
583,26960,2,"    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy",TODO,df_copy,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
584,24445,1,"regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())",TODO,regex_exp,"regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]","regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    d1 = country_data.copy()
    
    regex_exp = re.compile(""([\w\s]*)"")
    
    d1[""Country""] = d1[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
    d1 = d1[d1[""Country""].apply(lambda x: x not in countries_to_remove)]
    
    # print(d1)
    return d1
    
    pass
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
585,24445,2,"s = ""name1 (name2)""
match_example = regex_exp.search(s)",TODO,s,"regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]","regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    d1 = country_data.copy()
    
    regex_exp = re.compile(""([\w\s]*)"")
    
    d1[""Country""] = d1[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
    d1 = d1[d1[""Country""].apply(lambda x: x not in countries_to_remove)]
    
    # print(d1)
    return d1
    
    pass
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
586,24445,3,"match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())",TODO,match_example,"regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]","regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    d1 = country_data.copy()
    
    regex_exp = re.compile(""([\w\s]*)"")
    
    d1[""Country""] = d1[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
    d1 = d1[d1[""Country""].apply(lambda x: x not in countries_to_remove)]
    
    # print(d1)
    return d1
    
    pass
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
587,24445,4,"d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]",TODO,d,"regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]","regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    d1 = country_data.copy()
    
    regex_exp = re.compile(""([\w\s]*)"")
    
    d1[""Country""] = d1[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
    d1 = d1[d1[""Country""].apply(lambda x: x not in countries_to_remove)]
    
    # print(d1)
    return d1
    
    pass
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
588,24445,5,"s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)",TODO,s1,"regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]","regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    d1 = country_data.copy()
    
    regex_exp = re.compile(""([\w\s]*)"")
    
    d1[""Country""] = d1[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
    d1 = d1[d1[""Country""].apply(lambda x: x not in countries_to_remove)]
    
    # print(d1)
    return d1
    
    pass
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
589,24445,6,"new_example = regex_exp.search(s1)
new_example.group()",TODO,new_example,"regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]","regex_exp = re.compile(""([\w\s]*)"")
s = ""name1 (name2)""
match_example = regex_exp.search(s)
print("".match example\n"", match_example)
print(match_example.group())

d = df_country.copy()

s1 = ""Falkland Islands (Islas Malvinas)""
new_example = regex_exp.search(s1)
new_example.group()
d[""Country1""] = d[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
d[""Country1""]
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    d1 = country_data.copy()
    
    regex_exp = re.compile(""([\w\s]*)"")
    
    d1[""Country""] = d1[""Country""].apply(lambda x: regex_exp.search(x).group().strip())
    d1 = d1[d1[""Country""].apply(lambda x: x not in countries_to_remove)]
    
    # print(d1)
    return d1
    
    pass
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
590,24375,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
591,24375,2,"    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy",TODO,df_copy,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    df_copy = country_data.copy()

    df_copy['Country'] = df_copy['Country'].str.replace(r'\s+\(.*\)', '', regex=True)
    df_copy = df_copy.loc[df_copy['Country'].isin(countries_to_remove) == False]

    return df_copy
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
592,14098,1,"    mask = country_data[""Country""].str.contains('\(')
    temp = country_data[mask][""Country""].apply(lambda i: i[0:i.index('(')-1])",TODO,mask,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    mask = country_data[""Country""].str.contains('\(')
    temp = country_data[mask][""Country""].apply(lambda i: i[0:i.index('(')-1])
    country_data.loc[temp.index, ""Country""] = temp
    return country_data.query('Country not in @countries_to_remove')","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    mask = country_data[""Country""].str.contains('\(')
    temp = country_data[mask][""Country""].apply(lambda i: i[0:i.index('(')-1])
    country_data.loc[temp.index, ""Country""] = temp
    return country_data.query('Country not in @countries_to_remove')
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
593,14098,2,"    temp = country_data[mask][""Country""].apply(lambda i: i[0:i.index('(')-1])
    country_data.loc[temp.index, ""Country""] = temp",TODO,temp,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    mask = country_data[""Country""].str.contains('\(')
    temp = country_data[mask][""Country""].apply(lambda i: i[0:i.index('(')-1])
    country_data.loc[temp.index, ""Country""] = temp
    return country_data.query('Country not in @countries_to_remove')","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    mask = country_data[""Country""].str.contains('\(')
    temp = country_data[mask][""Country""].apply(lambda i: i[0:i.index('(')-1])
    country_data.loc[temp.index, ""Country""] = temp
    return country_data.query('Country not in @countries_to_remove')
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
594,25530,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    pattern=r""\s\((\w+|\s)+\)""
    country_data[""Country""]=country_data[""Country""].apply(lambda x: re.sub(pattern,"""",str(x)))
    country_data.drop(country_data.index[country_data.Country.isin(countries_to_remove)==True],inplace=True)",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    pattern=r""\s\((\w+|\s)+\)""
    country_data[""Country""]=country_data[""Country""].apply(lambda x: re.sub(pattern,"""",str(x)))
    country_data.drop(country_data.index[country_data.Country.isin(countries_to_remove)==True],inplace=True)
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    pattern=r""\s\((\w+|\s)+\)""
    country_data[""Country""]=country_data[""Country""].apply(lambda x: re.sub(pattern,"""",str(x)))
    country_data.drop(country_data.index[country_data.Country.isin(countries_to_remove)==True],inplace=True)
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
595,25530,2,"    pattern=r""\s\((\w+|\s)+\)""
    country_data[""Country""]=country_data[""Country""].apply(lambda x: re.sub(pattern,"""",str(x)))",TODO,pattern,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    pattern=r""\s\((\w+|\s)+\)""
    country_data[""Country""]=country_data[""Country""].apply(lambda x: re.sub(pattern,"""",str(x)))
    country_data.drop(country_data.index[country_data.Country.isin(countries_to_remove)==True],inplace=True)
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    pattern=r""\s\((\w+|\s)+\)""
    country_data[""Country""]=country_data[""Country""].apply(lambda x: re.sub(pattern,"""",str(x)))
    country_data.drop(country_data.index[country_data.Country.isin(countries_to_remove)==True],inplace=True)
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
596,26380,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = country_data['Country'].apply(lambda x: x.split('(')[0].strip())
    country_data.drop(country_data[country_data['Country'].isin(countries_to_remove)].index, inplace=True)",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = country_data['Country'].apply(lambda x: x.split('(')[0].strip())
    country_data.drop(country_data[country_data['Country'].isin(countries_to_remove)].index, inplace=True)
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = country_data['Country'].apply(lambda x: x.split('(')[0].strip())
    country_data.drop(country_data[country_data['Country'].isin(countries_to_remove)].index, inplace=True)
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
597,24770,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    
    
    def extract_str(s):
    
        if '(' in s:
            regex_exp = re.compile(""([ \w]+) \\("")
            return str(regex_exp.findall(s)[0])
        else:
            return str(s)
    

    
    country_data['Country'] = country_data['Country'].apply(extract_str)
    
    country_data = country_data[(1-country_data['Country'].isin(countries_to_remove)).astype('boolean')]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    
    
    def extract_str(s):
    
        if '(' in s:
            regex_exp = re.compile(""([ \w]+) \\("")
            return str(regex_exp.findall(s)[0])
        else:
            return str(s)
    

    
    country_data['Country'] = country_data['Country'].apply(extract_str)
    
    country_data = country_data[(1-country_data['Country'].isin(countries_to_remove)).astype('boolean')]
    
    return country_data
    
    ","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    
    
    def extract_str(s):
    
        if '(' in s:
            regex_exp = re.compile(""([ \w]+) \\("")
            return str(regex_exp.findall(s)[0])
        else:
            return str(s)
    

    
    country_data['Country'] = country_data['Country'].apply(extract_str)
    
    country_data = country_data[(1-country_data['Country'].isin(countries_to_remove)).astype('boolean')]
    
    return country_data
    
    
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
598,24770,2,"    country_data['Country'] = country_data['Country'].apply(extract_str)
    
    country_data = country_data[(1-country_data['Country'].isin(countries_to_remove)).astype('boolean')]
    
    return country_data",TODO,country_data,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    
    
    def extract_str(s):
    
        if '(' in s:
            regex_exp = re.compile(""([ \w]+) \\("")
            return str(regex_exp.findall(s)[0])
        else:
            return str(s)
    

    
    country_data['Country'] = country_data['Country'].apply(extract_str)
    
    country_data = country_data[(1-country_data['Country'].isin(countries_to_remove)).astype('boolean')]
    
    return country_data
    
    ","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    
    
    def extract_str(s):
    
        if '(' in s:
            regex_exp = re.compile(""([ \w]+) \\("")
            return str(regex_exp.findall(s)[0])
        else:
            return str(s)
    

    
    country_data['Country'] = country_data['Country'].apply(extract_str)
    
    country_data = country_data[(1-country_data['Country'].isin(countries_to_remove)).astype('boolean')]
    
    return country_data
    
    
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
599,24770,3,"            regex_exp = re.compile(""([ \w]+) \\("")
            return str(regex_exp.findall(s)[0])",TODO,regex_exp,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    
    
    def extract_str(s):
    
        if '(' in s:
            regex_exp = re.compile(""([ \w]+) \\("")
            return str(regex_exp.findall(s)[0])
        else:
            return str(s)
    

    
    country_data['Country'] = country_data['Country'].apply(extract_str)
    
    country_data = country_data[(1-country_data['Country'].isin(countries_to_remove)).astype('boolean')]
    
    return country_data
    
    ","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    
    
    
    def extract_str(s):
    
        if '(' in s:
            regex_exp = re.compile(""([ \w]+) \\("")
            return str(regex_exp.findall(s)[0])
        else:
            return str(s)
    

    
    country_data['Country'] = country_data['Country'].apply(extract_str)
    
    country_data = country_data[(1-country_data['Country'].isin(countries_to_remove)).astype('boolean')]
    
    return country_data
    
    
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
600,13929,1,"    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    processed_data = pd.concat([country_average, global_average], axis=1)
    processed_data.boxplot()
    plt.title('Plot Temperature')
    plt.ylabel('Average Temperature (C)')
    plt.show()

    return country_average, global_average",TODO,country_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    processed_data = pd.concat([country_average, global_average], axis=1)
    processed_data.boxplot()
    plt.title('Plot Temperature')
    plt.ylabel('Average Temperature (C)')
    plt.show()

    return country_average, global_average
    #pass","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    processed_data = pd.concat([country_average, global_average], axis=1)
    processed_data.boxplot()
    plt.title('Plot Temperature')
    plt.ylabel('Average Temperature (C)')
    plt.show()

    return country_average, global_average
    #pass
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
601,13929,2,"    global_average = df_global_filtered[""LandAverageTemperature""]

    processed_data = pd.concat([country_average, global_average], axis=1)
    processed_data.boxplot()
    plt.title('Plot Temperature')
    plt.ylabel('Average Temperature (C)')
    plt.show()

    return country_average, global_average",TODO,global_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    processed_data = pd.concat([country_average, global_average], axis=1)
    processed_data.boxplot()
    plt.title('Plot Temperature')
    plt.ylabel('Average Temperature (C)')
    plt.show()

    return country_average, global_average
    #pass","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    processed_data = pd.concat([country_average, global_average], axis=1)
    processed_data.boxplot()
    plt.title('Plot Temperature')
    plt.ylabel('Average Temperature (C)')
    plt.show()

    return country_average, global_average
    #pass
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
602,13929,3,"    processed_data = pd.concat([country_average, global_average], axis=1)
    processed_data.boxplot()",TODO,processed_data,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    processed_data = pd.concat([country_average, global_average], axis=1)
    processed_data.boxplot()
    plt.title('Plot Temperature')
    plt.ylabel('Average Temperature (C)')
    plt.show()

    return country_average, global_average
    #pass","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    processed_data = pd.concat([country_average, global_average], axis=1)
    processed_data.boxplot()
    plt.title('Plot Temperature')
    plt.ylabel('Average Temperature (C)')
    plt.show()

    return country_average, global_average
    #pass
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
603,26791,1,"    dfc = df_country_filtered[['AverageTemperature']]
    dfc['type'] = ""Average Country Temperature""
    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg",TODO,dfc,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    # sns.catplot(kind=""box"", data=dfc)
    # print(dfc.max())
    dfc = df_country_filtered[['AverageTemperature']]
    dfc['type'] = ""Average Country Temperature""
    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    # sns.catplot(kind=""box"", data=dfc)
    # print(dfc.max())
    dfc = df_country_filtered[['AverageTemperature']]
    dfc['type'] = ""Average Country Temperature""
    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
604,26791,2,"    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg",TODO,dfg,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    # sns.catplot(kind=""box"", data=dfc)
    # print(dfc.max())
    dfc = df_country_filtered[['AverageTemperature']]
    dfc['type'] = ""Average Country Temperature""
    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    # sns.catplot(kind=""box"", data=dfc)
    # print(dfc.max())
    dfc = df_country_filtered[['AverageTemperature']]
    dfc['type'] = ""Average Country Temperature""
    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
605,26791,3,"    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)",TODO,df,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    # sns.catplot(kind=""box"", data=dfc)
    # print(dfc.max())
    dfc = df_country_filtered[['AverageTemperature']]
    dfc['type'] = ""Average Country Temperature""
    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    # sns.catplot(kind=""box"", data=dfc)
    # print(dfc.max())
    dfc = df_country_filtered[['AverageTemperature']]
    dfc['type'] = ""Average Country Temperature""
    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
606,26791,4,"    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    # sns.catplot(kind=""box"", data=dfc)
    # print(dfc.max())
    dfc = df_country_filtered[['AverageTemperature']]
    dfc['type'] = ""Average Country Temperature""
    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    # sns.catplot(kind=""box"", data=dfc)
    # print(dfc.max())
    dfc = df_country_filtered[['AverageTemperature']]
    dfc['type'] = ""Average Country Temperature""
    dfg = df_global_filtered[['LandAverageTemperature']]
    dfg['type'] = ""Average Global Temperature""
    dfg.columns = ['AverageTemperature', 'type']
    df = dfc.append(dfg, ignore_index=True)
    # print(df.head())
    dfc = pd.Series(df_country_filtered.AverageTemperature)
    dfg = pd.Series(df_global_filtered.LandAverageTemperature)
    sns.set_style(""whitegrid"")
    ax = sns.catplot(x=""type"", y=""AverageTemperature"", kind=""box"", data=df)
    ax.set(ylabel='Average Temperature (C)')
    return dfc,dfg
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
607,13744,1,"    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average",TODO,country_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
608,13744,2,"    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])",TODO,df1,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
609,13744,3,"    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average",TODO,global_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
610,13744,4,"    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])",TODO,df2,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
611,13744,5,"    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)",TODO,df,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    df1 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df1[""Average Temperature (C)""] = country_average
    df1["" ""] = ""Average Country Temperature""

    global_average = df_global_filtered[""LandAverageTemperature""]
    df2 = pd.DataFrame(columns=["" "", ""Average Temperature (C)""])
    df2[""Average Temperature (C)""] = global_average
    df2["" ""] = ""Average Global Temperature""

    df = pd.concat([df1, df2])
    
    sns.catplot(x="" "", y=""Average Temperature (C)"", kind=""box"", data=df)
    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
612,25811,1,"    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']

    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    temp_data = pd.DataFrame({
        'Temperature': pd.concat([country_average, global_average]),
        'Type': ['Average Country Temperature'] * len(country_average) + ['Average Global Temperature'] * len(global_average)
    })

    sns.boxplot(x='Type', y='Temperature', data=temp_data)

    plt.ylabel('Average Temperature (C)')
    plt.xlabel('')

    plt.show()

    return country_average, global_average",TODO,country_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']

    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    temp_data = pd.DataFrame({
        'Temperature': pd.concat([country_average, global_average]),
        'Type': ['Average Country Temperature'] * len(country_average) + ['Average Global Temperature'] * len(global_average)
    })

    sns.boxplot(x='Type', y='Temperature', data=temp_data)

    plt.ylabel('Average Temperature (C)')
    plt.xlabel('')

    plt.show()

    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']

    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    temp_data = pd.DataFrame({
        'Temperature': pd.concat([country_average, global_average]),
        'Type': ['Average Country Temperature'] * len(country_average) + ['Average Global Temperature'] * len(global_average)
    })

    sns.boxplot(x='Type', y='Temperature', data=temp_data)

    plt.ylabel('Average Temperature (C)')
    plt.xlabel('')

    plt.show()

    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
613,25811,2,"    global_average = df_global_filtered['LandAverageTemperature']

    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    temp_data = pd.DataFrame({
        'Temperature': pd.concat([country_average, global_average]),
        'Type': ['Average Country Temperature'] * len(country_average) + ['Average Global Temperature'] * len(global_average)
    })

    sns.boxplot(x='Type', y='Temperature', data=temp_data)

    plt.ylabel('Average Temperature (C)')
    plt.xlabel('')

    plt.show()

    return country_average, global_average",TODO,global_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']

    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    temp_data = pd.DataFrame({
        'Temperature': pd.concat([country_average, global_average]),
        'Type': ['Average Country Temperature'] * len(country_average) + ['Average Global Temperature'] * len(global_average)
    })

    sns.boxplot(x='Type', y='Temperature', data=temp_data)

    plt.ylabel('Average Temperature (C)')
    plt.xlabel('')

    plt.show()

    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']

    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    temp_data = pd.DataFrame({
        'Temperature': pd.concat([country_average, global_average]),
        'Type': ['Average Country Temperature'] * len(country_average) + ['Average Global Temperature'] * len(global_average)
    })

    sns.boxplot(x='Type', y='Temperature', data=temp_data)

    plt.ylabel('Average Temperature (C)')
    plt.xlabel('')

    plt.show()

    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
614,25811,3,"    temp_data = pd.DataFrame({
        'Temperature': pd.concat([country_average, global_average]),
        'Type': ['Average Country Temperature'] * len(country_average) + ['Average Global Temperature'] * len(global_average)
    })

    sns.boxplot(x='Type', y='Temperature', data=temp_data)",TODO,temp_data,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']

    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    temp_data = pd.DataFrame({
        'Temperature': pd.concat([country_average, global_average]),
        'Type': ['Average Country Temperature'] * len(country_average) + ['Average Global Temperature'] * len(global_average)
    })

    sns.boxplot(x='Type', y='Temperature', data=temp_data)

    plt.ylabel('Average Temperature (C)')
    plt.xlabel('')

    plt.show()

    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    # pass
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']

    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    temp_data = pd.DataFrame({
        'Temperature': pd.concat([country_average, global_average]),
        'Type': ['Average Country Temperature'] * len(country_average) + ['Average Global Temperature'] * len(global_average)
    })

    sns.boxplot(x='Type', y='Temperature', data=temp_data)

    plt.ylabel('Average Temperature (C)')
    plt.xlabel('')

    plt.show()

    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
615,14044,1,"    combined_dfs = [df_country_filtered['AverageTemperature'].values, df_global_filtered['LandAverageTemperature'].values]

    fig = plt.figure()
    fig.suptitle('Temparture Plots')

    ax = fig.add_subplot(111)

    ax.boxplot(combined_dfs,labels=['Average Country Temperature','Average Global Temperature'])",TODO,combined_dfs,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    # combined_dfs = pd.DataFrame({'Average Country Temperature': df_country_filtered['AverageTemperature'],
    #                          'Average Global Temperature': df_global_filtered['LandAverageTemperature']})

    # ax = sns.boxplot(data=combined_dfs)
    # ax.set(ylabel='Average Temperature (C)')
    # plt.show()

    combined_dfs = [df_country_filtered['AverageTemperature'].values, df_global_filtered['LandAverageTemperature'].values]

    fig = plt.figure()
    fig.suptitle('Temparture Plots')

    ax = fig.add_subplot(111)

    ax.boxplot(combined_dfs,labels=['Average Country Temperature','Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.grid()
    plt.show()
    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    # combined_dfs = pd.DataFrame({'Average Country Temperature': df_country_filtered['AverageTemperature'],
    #                          'Average Global Temperature': df_global_filtered['LandAverageTemperature']})

    # ax = sns.boxplot(data=combined_dfs)
    # ax.set(ylabel='Average Temperature (C)')
    # plt.show()

    combined_dfs = [df_country_filtered['AverageTemperature'].values, df_global_filtered['LandAverageTemperature'].values]

    fig = plt.figure()
    fig.suptitle('Temparture Plots')

    ax = fig.add_subplot(111)

    ax.boxplot(combined_dfs,labels=['Average Country Temperature','Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.grid()
    plt.show()
    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
616,14044,2,"    fig = plt.figure()
    fig.suptitle('Temparture Plots')

    ax = fig.add_subplot(111)",TODO,fig,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    # combined_dfs = pd.DataFrame({'Average Country Temperature': df_country_filtered['AverageTemperature'],
    #                          'Average Global Temperature': df_global_filtered['LandAverageTemperature']})

    # ax = sns.boxplot(data=combined_dfs)
    # ax.set(ylabel='Average Temperature (C)')
    # plt.show()

    combined_dfs = [df_country_filtered['AverageTemperature'].values, df_global_filtered['LandAverageTemperature'].values]

    fig = plt.figure()
    fig.suptitle('Temparture Plots')

    ax = fig.add_subplot(111)

    ax.boxplot(combined_dfs,labels=['Average Country Temperature','Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.grid()
    plt.show()
    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    # combined_dfs = pd.DataFrame({'Average Country Temperature': df_country_filtered['AverageTemperature'],
    #                          'Average Global Temperature': df_global_filtered['LandAverageTemperature']})

    # ax = sns.boxplot(data=combined_dfs)
    # ax.set(ylabel='Average Temperature (C)')
    # plt.show()

    combined_dfs = [df_country_filtered['AverageTemperature'].values, df_global_filtered['LandAverageTemperature'].values]

    fig = plt.figure()
    fig.suptitle('Temparture Plots')

    ax = fig.add_subplot(111)

    ax.boxplot(combined_dfs,labels=['Average Country Temperature','Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.grid()
    plt.show()
    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
617,14044,3,"    ax = fig.add_subplot(111)

    ax.boxplot(combined_dfs,labels=['Average Country Temperature','Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    # combined_dfs = pd.DataFrame({'Average Country Temperature': df_country_filtered['AverageTemperature'],
    #                          'Average Global Temperature': df_global_filtered['LandAverageTemperature']})

    # ax = sns.boxplot(data=combined_dfs)
    # ax.set(ylabel='Average Temperature (C)')
    # plt.show()

    combined_dfs = [df_country_filtered['AverageTemperature'].values, df_global_filtered['LandAverageTemperature'].values]

    fig = plt.figure()
    fig.suptitle('Temparture Plots')

    ax = fig.add_subplot(111)

    ax.boxplot(combined_dfs,labels=['Average Country Temperature','Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.grid()
    plt.show()
    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    # combined_dfs = pd.DataFrame({'Average Country Temperature': df_country_filtered['AverageTemperature'],
    #                          'Average Global Temperature': df_global_filtered['LandAverageTemperature']})

    # ax = sns.boxplot(data=combined_dfs)
    # ax.set(ylabel='Average Temperature (C)')
    # plt.show()

    combined_dfs = [df_country_filtered['AverageTemperature'].values, df_global_filtered['LandAverageTemperature'].values]

    fig = plt.figure()
    fig.suptitle('Temparture Plots')

    ax = fig.add_subplot(111)

    ax.boxplot(combined_dfs,labels=['Average Country Temperature','Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.grid()
    plt.show()
    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
618,26091,1,"    fig, ax = plt.subplots()
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots()
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered ","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots()
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered 
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
619,26091,2,"    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered",TODO,df_country_filtered,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots()
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered ","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots()
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered 
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
620,26091,3,"    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered",TODO,df_global_filtered,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots()
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered ","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots()
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered 
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
621,26091,4,"    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)",TODO,data,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots()
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered ","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots()
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))['AverageTemperature']
    df_global_filtered = drop_missing_values(df_global)['LandAverageTemperature']
    data = [df_country_filtered, df_global_filtered]
    box_plot = ax.boxplot(data)

    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_title('Box Plot for Country and Global Temperature')
    ax.set_ylabel('Average Temperature (C)')

    plt.show()

    return df_country_filtered, df_global_filtered 
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
622,25576,1,"    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average",TODO,country_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
623,25576,2,"    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average",TODO,global_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
624,25576,3,"    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())",TODO,data,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
625,25576,4,"    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]
    data = {""Average Country Temperature"": country_average, ""Average Global Temperature"": global_average}
    fig, ax = plt.subplots()
    ax.boxplot(data.values())
    ax.set_xticklabels(data.keys())
    ax.set_ylabel('Average Temperature (C)')
    plt.show()
    return country_average, global_average
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
626,25681,1,"    fig, ax = plt.subplots(figsize=(7, 5))
    ax.set_ylim(ymin=min(df_country_filtered[""AverageTemperature""].min(), df_global_filtered[""LandAverageTemperature""].min()) - 5, \
        ymax=max(df_country_filtered[""AverageTemperature""].max(), df_global_filtered[""LandAverageTemperature""].max()) + 5)
    ax.set_ylabel(""Average Temperature (C)"")
    ax.boxplot([df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""]], \",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    sns.set_style(""whitegrid"")
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.set_ylim(ymin=min(df_country_filtered[""AverageTemperature""].min(), df_global_filtered[""LandAverageTemperature""].min()) - 5, \
        ymax=max(df_country_filtered[""AverageTemperature""].max(), df_global_filtered[""LandAverageTemperature""].max()) + 5)
    ax.set_ylabel(""Average Temperature (C)"")
    ax.boxplot([df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""]], \
        labels=[""Average Country Temperature"", ""Avg Global Temperature""])
    plt.show()
    return (df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    sns.set_style(""whitegrid"")
    fig, ax = plt.subplots(figsize=(7, 5))
    ax.set_ylim(ymin=min(df_country_filtered[""AverageTemperature""].min(), df_global_filtered[""LandAverageTemperature""].min()) - 5, \
        ymax=max(df_country_filtered[""AverageTemperature""].max(), df_global_filtered[""LandAverageTemperature""].max()) + 5)
    ax.set_ylabel(""Average Temperature (C)"")
    ax.boxplot([df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""]], \
        labels=[""Average Country Temperature"", ""Avg Global Temperature""])
    plt.show()
    return (df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
627,26066,1,"    df1 = pd.DataFrame(df_country_filtered['AverageTemperature'])

    df1.rename(columns = {'AverageTemperature':'Average Country Temperature'}, inplace = True)

    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])",TODO,df1,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    df1 = pd.DataFrame(df_country_filtered['AverageTemperature'])

    df1.rename(columns = {'AverageTemperature':'Average Country Temperature'}, inplace = True)

    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])
    df = df[['Average Country Temperature', 'Average Global Temperature']]
    ax = df.plot.box(grid = True)
    ax.set_ylabel('Average Temperature (C)')

    return df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    df1 = pd.DataFrame(df_country_filtered['AverageTemperature'])

    df1.rename(columns = {'AverageTemperature':'Average Country Temperature'}, inplace = True)

    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])
    df = df[['Average Country Temperature', 'Average Global Temperature']]
    ax = df.plot.box(grid = True)
    ax.set_ylabel('Average Temperature (C)')

    return df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
628,26066,2,"    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])",TODO,df2,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    df1 = pd.DataFrame(df_country_filtered['AverageTemperature'])

    df1.rename(columns = {'AverageTemperature':'Average Country Temperature'}, inplace = True)

    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])
    df = df[['Average Country Temperature', 'Average Global Temperature']]
    ax = df.plot.box(grid = True)
    ax.set_ylabel('Average Temperature (C)')

    return df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    df1 = pd.DataFrame(df_country_filtered['AverageTemperature'])

    df1.rename(columns = {'AverageTemperature':'Average Country Temperature'}, inplace = True)

    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])
    df = df[['Average Country Temperature', 'Average Global Temperature']]
    ax = df.plot.box(grid = True)
    ax.set_ylabel('Average Temperature (C)')

    return df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
629,26066,3,"    df = pd.concat([df1,df2])
    df = df[['Average Country Temperature', 'Average Global Temperature']]
    ax = df.plot.box(grid = True)",TODO,df,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    df1 = pd.DataFrame(df_country_filtered['AverageTemperature'])

    df1.rename(columns = {'AverageTemperature':'Average Country Temperature'}, inplace = True)

    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])
    df = df[['Average Country Temperature', 'Average Global Temperature']]
    ax = df.plot.box(grid = True)
    ax.set_ylabel('Average Temperature (C)')

    return df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    df1 = pd.DataFrame(df_country_filtered['AverageTemperature'])

    df1.rename(columns = {'AverageTemperature':'Average Country Temperature'}, inplace = True)

    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])
    df = df[['Average Country Temperature', 'Average Global Temperature']]
    ax = df.plot.box(grid = True)
    ax.set_ylabel('Average Temperature (C)')

    return df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
630,26066,4,"    ax = df.plot.box(grid = True)
    ax.set_ylabel('Average Temperature (C)')",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    df1 = pd.DataFrame(df_country_filtered['AverageTemperature'])

    df1.rename(columns = {'AverageTemperature':'Average Country Temperature'}, inplace = True)

    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])
    df = df[['Average Country Temperature', 'Average Global Temperature']]
    ax = df.plot.box(grid = True)
    ax.set_ylabel('Average Temperature (C)')

    return df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    df1 = pd.DataFrame(df_country_filtered['AverageTemperature'])

    df1.rename(columns = {'AverageTemperature':'Average Country Temperature'}, inplace = True)

    df2 = pd.DataFrame(df_global_filtered['LandAverageTemperature'])
    df2.rename(columns = {'LandAverageTemperature':'Average Global Temperature'}, inplace = True)
    df = pd.concat([df1,df2])
    df = df[['Average Country Temperature', 'Average Global Temperature']]
    ax = df.plot.box(grid = True)
    ax.set_ylabel('Average Temperature (C)')

    return df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
631,13779,1,"    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    newdf = pd.DataFrame({'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']})
    # print(newdf.head())

    data = {'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']}",TODO,data,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    newdf = pd.DataFrame({'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']})
    # print(newdf.head())

    data = {'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']}


    boxplot =  newdf.boxplot()
    boxplot.set_ylabel('Average Temperature (C)')

    return (df_country_filtered['AverageTemperature'],df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    newdf = pd.DataFrame({'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']})
    # print(newdf.head())

    data = {'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']}


    boxplot =  newdf.boxplot()
    boxplot.set_ylabel('Average Temperature (C)')

    return (df_country_filtered['AverageTemperature'],df_global_filtered['LandAverageTemperature'])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")
    # print(df_country_filtered.head())
    # print (df_global_filtered['LandAverageTemperature'].head(20))

test_plot_temperature()"
632,13779,2,"    newdf = pd.DataFrame({'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']})
    # print(newdf.head())

    data = {'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']}


    boxplot =  newdf.boxplot()",TODO,newdf,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    newdf = pd.DataFrame({'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']})
    # print(newdf.head())

    data = {'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']}


    boxplot =  newdf.boxplot()
    boxplot.set_ylabel('Average Temperature (C)')

    return (df_country_filtered['AverageTemperature'],df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    newdf = pd.DataFrame({'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']})
    # print(newdf.head())

    data = {'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']}


    boxplot =  newdf.boxplot()
    boxplot.set_ylabel('Average Temperature (C)')

    return (df_country_filtered['AverageTemperature'],df_global_filtered['LandAverageTemperature'])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")
    # print(df_country_filtered.head())
    # print (df_global_filtered['LandAverageTemperature'].head(20))

test_plot_temperature()"
633,13779,3,"    boxplot =  newdf.boxplot()
    boxplot.set_ylabel('Average Temperature (C)')",TODO,boxplot,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    newdf = pd.DataFrame({'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']})
    # print(newdf.head())

    data = {'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']}


    boxplot =  newdf.boxplot()
    boxplot.set_ylabel('Average Temperature (C)')

    return (df_country_filtered['AverageTemperature'],df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    newdf = pd.DataFrame({'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']})
    # print(newdf.head())

    data = {'AverageTemperature':df_country_filtered['AverageTemperature'],'LandAverageTemperature': df_global_filtered['LandAverageTemperature']}


    boxplot =  newdf.boxplot()
    boxplot.set_ylabel('Average Temperature (C)')

    return (df_country_filtered['AverageTemperature'],df_global_filtered['LandAverageTemperature'])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")
    # print(df_country_filtered.head())
    # print (df_global_filtered['LandAverageTemperature'].head(20))

test_plot_temperature()"
634,25367,1,"dfc = get_preprocessed_data(global_data = False)
average_temps = avg_temp_by_country(dfc)",TODO,dfc,"def avg_temp_by_country(df_country):
  """"""
  Compute the average temperature in each country and order the countries by average temperatures from high to low.

  args:
      df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries

  return:
      pd.Series : a descending series of average temperature values, indexed by country
  """"""
  df_country = df_country.groupby(""Country"").agg({""AverageTemperature"":""mean""}).loc[:,""AverageTemperature""].sort_values(ascending = False)
  return df_country

dfc = get_preprocessed_data(global_data = False)
average_temps = avg_temp_by_country(dfc)","def avg_temp_by_country(df_country):
  """"""
  Compute the average temperature in each country and order the countries by average temperatures from high to low.

  args:
      df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries

  return:
      pd.Series : a descending series of average temperature values, indexed by country
  """"""
  df_country = df_country.groupby(""Country"").agg({""AverageTemperature"":""mean""}).loc[:,""AverageTemperature""].sort_values(ascending = False)
  return df_country

dfc = get_preprocessed_data(global_data = False)
average_temps = avg_temp_by_country(dfc)
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64

    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
635,25367,2,"  df_country = df_country.groupby(""Country"").agg({""AverageTemperature"":""mean""}).loc[:,""AverageTemperature""].sort_values(ascending = False)
  return df_country",TODO,df_country,"def avg_temp_by_country(df_country):
  """"""
  Compute the average temperature in each country and order the countries by average temperatures from high to low.

  args:
      df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries

  return:
      pd.Series : a descending series of average temperature values, indexed by country
  """"""
  df_country = df_country.groupby(""Country"").agg({""AverageTemperature"":""mean""}).loc[:,""AverageTemperature""].sort_values(ascending = False)
  return df_country

dfc = get_preprocessed_data(global_data = False)
average_temps = avg_temp_by_country(dfc)","def avg_temp_by_country(df_country):
  """"""
  Compute the average temperature in each country and order the countries by average temperatures from high to low.

  args:
      df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries

  return:
      pd.Series : a descending series of average temperature values, indexed by country
  """"""
  df_country = df_country.groupby(""Country"").agg({""AverageTemperature"":""mean""}).loc[:,""AverageTemperature""].sort_values(ascending = False)
  return df_country

dfc = get_preprocessed_data(global_data = False)
average_temps = avg_temp_by_country(dfc)
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64

    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
636,26607,1,"    mean_temp = df_country.groupby('Country').mean()[['AverageTemperature']].sort_values(by='AverageTemperature', ascending=False)

    return mean_temp.squeeze()",TODO,mean_temp,"def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    
    mean_temp = df_country.groupby('Country').mean()[['AverageTemperature']].sort_values(by='AverageTemperature', ascending=False)

    return mean_temp.squeeze()","def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    
    mean_temp = df_country.groupby('Country').mean()[['AverageTemperature']].sort_values(by='AverageTemperature', ascending=False)

    return mean_temp.squeeze()
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64
    
    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
637,24957,1,"    df = df_country[['Country','AverageTemperature']]\
        .groupby('Country',as_index=True)\
        .agg('mean')\
        .sort_values(by='AverageTemperature', ascending=False)

    result = df.squeeze()",TODO,df,"def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.

    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries

    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    df = df_country[['Country','AverageTemperature']]\
        .groupby('Country',as_index=True)\
        .agg('mean')\
        .sort_values(by='AverageTemperature', ascending=False)

    result = df.squeeze()
    return result","def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.

    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries

    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    df = df_country[['Country','AverageTemperature']]\
        .groupby('Country',as_index=True)\
        .agg('mean')\
        .sort_values(by='AverageTemperature', ascending=False)

    result = df.squeeze()
    return result
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64

    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
638,24957,2,"    result = df.squeeze()
    return result",TODO,result,"def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.

    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries

    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    df = df_country[['Country','AverageTemperature']]\
        .groupby('Country',as_index=True)\
        .agg('mean')\
        .sort_values(by='AverageTemperature', ascending=False)

    result = df.squeeze()
    return result","def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.

    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries

    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    df = df_country[['Country','AverageTemperature']]\
        .groupby('Country',as_index=True)\
        .agg('mean')\
        .sort_values(by='AverageTemperature', ascending=False)

    result = df.squeeze()
    return result
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64

    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
639,24548,1,"    df = df.drop_duplicates()
    year_cols = [col for col in df.columns if col.startswith('Y')]
    df[year_cols] = df[year_cols].where(df[year_cols] > 0, 0)
    df = df.fillna(0)
    df[year_cols] = df[year_cols].astype(np.int64)
    return df",TODO,df,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    year_cols = [col for col in df.columns if col.startswith('Y')]
    df[year_cols] = df[year_cols].where(df[year_cols] > 0, 0)
    df = df.fillna(0)
    df[year_cols] = df[year_cols].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    year_cols = [col for col in df.columns if col.startswith('Y')]
    df[year_cols] = df[year_cols].where(df[year_cols] > 0, 0)
    df = df.fillna(0)
    df[year_cols] = df[year_cols].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)

    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")

test_get_cleaned_food_data()"
640,24548,2,"    year_cols = [col for col in df.columns if col.startswith('Y')]
    df[year_cols] = df[year_cols].where(df[year_cols] > 0, 0)
    df = df.fillna(0)
    df[year_cols] = df[year_cols].astype(np.int64)",TODO,year_cols,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    year_cols = [col for col in df.columns if col.startswith('Y')]
    df[year_cols] = df[year_cols].where(df[year_cols] > 0, 0)
    df = df.fillna(0)
    df[year_cols] = df[year_cols].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    year_cols = [col for col in df.columns if col.startswith('Y')]
    df[year_cols] = df[year_cols].where(df[year_cols] > 0, 0)
    df = df.fillna(0)
    df[year_cols] = df[year_cols].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)

    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")

test_get_cleaned_food_data()"
641,13761,1,"    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)",TODO,year_columns,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
642,13761,2,"    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df",TODO,df,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
643,25873,1,"    newdf = df.iloc[:,10:]
    newdf[newdf<0]=0
    newdf.fillna(0,inplace=True)
    newdf = newdf.astype(np.int64)
    df.iloc[:,10:]=newdf",TODO,newdf,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(keep='first',inplace=True)
    newdf = df.iloc[:,10:]
    newdf[newdf<0]=0
    newdf.fillna(0,inplace=True)
    newdf = newdf.astype(np.int64)
    df.iloc[:,10:]=newdf

    return df
    pass","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(keep='first',inplace=True)
    newdf = df.iloc[:,10:]
    newdf[newdf<0]=0
    newdf.fillna(0,inplace=True)
    newdf = newdf.astype(np.int64)
    df.iloc[:,10:]=newdf

    return df
    pass
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
644,13991,1,"    df_new = df.drop_duplicates()
    df_new = df_new.fillna(0)
    
    for col in df_new.columns:
        if col[0] != 'Y':
            continue
        df_new[col] = df_new[col].clip(0).astype(np.int64)
    
    return df_new",TODO,df_new,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df_new = df.drop_duplicates()
    df_new = df_new.fillna(0)
    
    for col in df_new.columns:
        if col[0] != 'Y':
            continue
        df_new[col] = df_new[col].clip(0).astype(np.int64)
    
    return df_new","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df_new = df.drop_duplicates()
    df_new = df_new.fillna(0)
    
    for col in df_new.columns:
        if col[0] != 'Y':
            continue
        df_new[col] = df_new[col].clip(0).astype(np.int64)
    
    return df_new
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
645,26738,1,"    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result",TODO,result,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)

    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")

test_get_cleaned_food_data()"
646,26738,2,"    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:",TODO,years,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)

    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")

test_get_cleaned_food_data()"
647,26708,1,"    df=df.drop_duplicates().copy()
    p = re.compile(""Y\d{4}"")
    cols=[x for x in df.columns if p.match(x)]
    df[cols]=df[cols].mask(df[cols]<0,0)
    df.fillna(0,inplace=True)
    df[cols]=df[cols].astype(np.int64)
    return df",TODO,df,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df=df.drop_duplicates().copy()
    p = re.compile(""Y\d{4}"")
    cols=[x for x in df.columns if p.match(x)]
    df[cols]=df[cols].mask(df[cols]<0,0)
    df.fillna(0,inplace=True)
    df[cols]=df[cols].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df=df.drop_duplicates().copy()
    p = re.compile(""Y\d{4}"")
    cols=[x for x in df.columns if p.match(x)]
    df[cols]=df[cols].mask(df[cols]<0,0)
    df.fillna(0,inplace=True)
    df[cols]=df[cols].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
648,26708,2,"    p = re.compile(""Y\d{4}"")
    cols=[x for x in df.columns if p.match(x)]",TODO,p,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df=df.drop_duplicates().copy()
    p = re.compile(""Y\d{4}"")
    cols=[x for x in df.columns if p.match(x)]
    df[cols]=df[cols].mask(df[cols]<0,0)
    df.fillna(0,inplace=True)
    df[cols]=df[cols].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df=df.drop_duplicates().copy()
    p = re.compile(""Y\d{4}"")
    cols=[x for x in df.columns if p.match(x)]
    df[cols]=df[cols].mask(df[cols]<0,0)
    df.fillna(0,inplace=True)
    df[cols]=df[cols].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
649,26708,3,"    cols=[x for x in df.columns if p.match(x)]
    df[cols]=df[cols].mask(df[cols]<0,0)
    df.fillna(0,inplace=True)
    df[cols]=df[cols].astype(np.int64)",TODO,cols,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df=df.drop_duplicates().copy()
    p = re.compile(""Y\d{4}"")
    cols=[x for x in df.columns if p.match(x)]
    df[cols]=df[cols].mask(df[cols]<0,0)
    df.fillna(0,inplace=True)
    df[cols]=df[cols].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df=df.drop_duplicates().copy()
    p = re.compile(""Y\d{4}"")
    cols=[x for x in df.columns if p.match(x)]
    df[cols]=df[cols].mask(df[cols]<0,0)
    df.fillna(0,inplace=True)
    df[cols]=df[cols].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
650,25693,1,"    df = df.drop_duplicates(keep = 'first')
    df = df.fillna(0)
    year_col = list(set(df.columns) - set(['Area Abbreviation', 'Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit', 'latitude', 'longitude']))
    for year in year_col:
        df.loc[df[year] < 0, year] = 0
    df[year_col] = df[year_col].astype(np.int64)
    return df",TODO,df,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates(keep = 'first')
    df = df.fillna(0)
    year_col = list(set(df.columns) - set(['Area Abbreviation', 'Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit', 'latitude', 'longitude']))
    for year in year_col:
        df.loc[df[year] < 0, year] = 0
    df[year_col] = df[year_col].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates(keep = 'first')
    df = df.fillna(0)
    year_col = list(set(df.columns) - set(['Area Abbreviation', 'Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit', 'latitude', 'longitude']))
    for year in year_col:
        df.loc[df[year] < 0, year] = 0
    df[year_col] = df[year_col].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
651,25693,2,"    year_col = list(set(df.columns) - set(['Area Abbreviation', 'Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit', 'latitude', 'longitude']))
    for year in year_col:
        df.loc[df[year] < 0, year] = 0
    df[year_col] = df[year_col].astype(np.int64)",TODO,year_col,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates(keep = 'first')
    df = df.fillna(0)
    year_col = list(set(df.columns) - set(['Area Abbreviation', 'Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit', 'latitude', 'longitude']))
    for year in year_col:
        df.loc[df[year] < 0, year] = 0
    df[year_col] = df[year_col].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates(keep = 'first')
    df = df.fillna(0)
    year_col = list(set(df.columns) - set(['Area Abbreviation', 'Area Code', 'Area', 'Item Code', 'Item', 'Element Code', 'Element', 'Unit', 'latitude', 'longitude']))
    for year in year_col:
        df.loc[df[year] < 0, year] = 0
    df[year_col] = df[year_col].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
652,25253,1,"    year_cols = df.columns[df.columns.str.startswith('Y')]
    df[year_cols] = df[year_cols].clip(lower=0).fillna(0).astype(np.int64)",TODO,year_cols,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    # pass
    df.drop_duplicates(inplace=True)
    year_cols = df.columns[df.columns.str.startswith('Y')]
    df[year_cols] = df[year_cols].clip(lower=0).fillna(0).astype(np.int64)

    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    # pass
    df.drop_duplicates(inplace=True)
    year_cols = df.columns[df.columns.str.startswith('Y')]
    df[year_cols] = df[year_cols].clip(lower=0).fillna(0).astype(np.int64)

    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
653,14137,1,"    year_cols = [col for col in df_food_cleaned.columns if col.startswith('Y')]  # select columns starting with 'Y'
    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]",TODO,year_cols,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col.startswith('Y')]  # select columns starting with 'Y'
    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)
    table.index.name = 'Year'
    table.columns.name = 'Country'
    table.fillna(0).astype(np.int64)
    return (table)
    pass","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col.startswith('Y')]  # select columns starting with 'Y'
    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)
    table.index.name = 'Year'
    table.columns.name = 'Country'
    table.fillna(0).astype(np.int64)
    return (table)
    pass
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
654,14137,2,"    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)",TODO,year_rename_dict,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col.startswith('Y')]  # select columns starting with 'Y'
    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)
    table.index.name = 'Year'
    table.columns.name = 'Country'
    table.fillna(0).astype(np.int64)
    return (table)
    pass","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col.startswith('Y')]  # select columns starting with 'Y'
    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)
    table.index.name = 'Year'
    table.columns.name = 'Country'
    table.fillna(0).astype(np.int64)
    return (table)
    pass
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
655,14137,3,"    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)",TODO,df,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col.startswith('Y')]  # select columns starting with 'Y'
    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)
    table.index.name = 'Year'
    table.columns.name = 'Country'
    table.fillna(0).astype(np.int64)
    return (table)
    pass","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col.startswith('Y')]  # select columns starting with 'Y'
    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)
    table.index.name = 'Year'
    table.columns.name = 'Country'
    table.fillna(0).astype(np.int64)
    return (table)
    pass
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
656,14137,4,"    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)
    table.index.name = 'Year'
    table.columns.name = 'Country'
    table.fillna(0).astype(np.int64)
    return (table)",TODO,table,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col.startswith('Y')]  # select columns starting with 'Y'
    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)
    table.index.name = 'Year'
    table.columns.name = 'Country'
    table.fillna(0).astype(np.int64)
    return (table)
    pass","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col.startswith('Y')]  # select columns starting with 'Y'
    year_rename_dict={i:int(i[1:]) for i in year_cols}
    year_cols.append('Area')
    df=df_food_cleaned[year_cols]
    table = pd.pivot_table(df, columns='Area', aggfunc=np.sum)
    table = table.rename(index=year_rename_dict)
    table.index.name = 'Year'
    table.columns.name = 'Country'
    table.fillna(0).astype(np.int64)
    return (table)
    pass
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
657,25149,1,"    food_by_year = df_food_cleaned.copy()

    year_column = food_by_year.columns[food_by_year.columns.str.startswith('Y')].tolist()
    year_column.append('Area')
    #print(year_column)

    production = food_by_year[year_column].groupby(by = ['Area']).sum()",TODO,food_by_year,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #pass

    food_by_year = df_food_cleaned.copy()

    year_column = food_by_year.columns[food_by_year.columns.str.startswith('Y')].tolist()
    year_column.append('Area')
    #print(year_column)

    production = food_by_year[year_column].groupby(by = ['Area']).sum()
    production.columns = production.columns.str.replace('Y','')
    production.columns = production.columns.astype(int)

    production = production.T
    production.index.name='Year'
    production.columns.name='Country'

    return production","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #pass

    food_by_year = df_food_cleaned.copy()

    year_column = food_by_year.columns[food_by_year.columns.str.startswith('Y')].tolist()
    year_column.append('Area')
    #print(year_column)

    production = food_by_year[year_column].groupby(by = ['Area']).sum()
    production.columns = production.columns.str.replace('Y','')
    production.columns = production.columns.astype(int)

    production = production.T
    production.index.name='Year'
    production.columns.name='Country'

    return production
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
658,25149,2,"    year_column = food_by_year.columns[food_by_year.columns.str.startswith('Y')].tolist()
    year_column.append('Area')
    #print(year_column)

    production = food_by_year[year_column].groupby(by = ['Area']).sum()",TODO,year_column,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #pass

    food_by_year = df_food_cleaned.copy()

    year_column = food_by_year.columns[food_by_year.columns.str.startswith('Y')].tolist()
    year_column.append('Area')
    #print(year_column)

    production = food_by_year[year_column].groupby(by = ['Area']).sum()
    production.columns = production.columns.str.replace('Y','')
    production.columns = production.columns.astype(int)

    production = production.T
    production.index.name='Year'
    production.columns.name='Country'

    return production","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #pass

    food_by_year = df_food_cleaned.copy()

    year_column = food_by_year.columns[food_by_year.columns.str.startswith('Y')].tolist()
    year_column.append('Area')
    #print(year_column)

    production = food_by_year[year_column].groupby(by = ['Area']).sum()
    production.columns = production.columns.str.replace('Y','')
    production.columns = production.columns.astype(int)

    production = production.T
    production.index.name='Year'
    production.columns.name='Country'

    return production
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
659,25149,3,"    production = food_by_year[year_column].groupby(by = ['Area']).sum()
    production.columns = production.columns.str.replace('Y','')
    production.columns = production.columns.astype(int)

    production = production.T
    production.index.name='Year'
    production.columns.name='Country'

    return production",TODO,production,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #pass

    food_by_year = df_food_cleaned.copy()

    year_column = food_by_year.columns[food_by_year.columns.str.startswith('Y')].tolist()
    year_column.append('Area')
    #print(year_column)

    production = food_by_year[year_column].groupby(by = ['Area']).sum()
    production.columns = production.columns.str.replace('Y','')
    production.columns = production.columns.astype(int)

    production = production.T
    production.index.name='Year'
    production.columns.name='Country'

    return production","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #pass

    food_by_year = df_food_cleaned.copy()

    year_column = food_by_year.columns[food_by_year.columns.str.startswith('Y')].tolist()
    year_column.append('Area')
    #print(year_column)

    production = food_by_year[year_column].groupby(by = ['Area']).sum()
    production.columns = production.columns.str.replace('Y','')
    production.columns = production.columns.astype(int)

    production = production.T
    production.index.name='Year'
    production.columns.name='Country'

    return production
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
660,13807,1,"    df_food_grouped = df_food_cleaned.groupby(['Area', 'Year'])['Value'].sum().reset_index()

    # Pivot the data to get the country names as columns and years as indices
    df_food_pivot = df_food_grouped.pivot(index='Year', columns='Area', values='Value')",TODO,df_food_grouped,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    # pass
        # Group data by year and country, then sum up the food and feed production
    df_food_grouped = df_food_cleaned.groupby(['Area', 'Year'])['Value'].sum().reset_index()

    # Pivot the data to get the country names as columns and years as indices
    df_food_pivot = df_food_grouped.pivot(index='Year', columns='Area', values='Value')

    # Rename the index column to 'Year'
    df_food_pivot.index.name = 'Year'

    return df_food_pivot","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    # pass
        # Group data by year and country, then sum up the food and feed production
    df_food_grouped = df_food_cleaned.groupby(['Area', 'Year'])['Value'].sum().reset_index()

    # Pivot the data to get the country names as columns and years as indices
    df_food_pivot = df_food_grouped.pivot(index='Year', columns='Area', values='Value')

    # Rename the index column to 'Year'
    df_food_pivot.index.name = 'Year'

    return df_food_pivot
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
661,13807,2,"    df_food_pivot = df_food_grouped.pivot(index='Year', columns='Area', values='Value')

    # Rename the index column to 'Year'
    df_food_pivot.index.name = 'Year'

    return df_food_pivot",TODO,df_food_pivot,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    # pass
        # Group data by year and country, then sum up the food and feed production
    df_food_grouped = df_food_cleaned.groupby(['Area', 'Year'])['Value'].sum().reset_index()

    # Pivot the data to get the country names as columns and years as indices
    df_food_pivot = df_food_grouped.pivot(index='Year', columns='Area', values='Value')

    # Rename the index column to 'Year'
    df_food_pivot.index.name = 'Year'

    return df_food_pivot","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    # pass
        # Group data by year and country, then sum up the food and feed production
    df_food_grouped = df_food_cleaned.groupby(['Area', 'Year'])['Value'].sum().reset_index()

    # Pivot the data to get the country names as columns and years as indices
    df_food_pivot = df_food_grouped.pivot(index='Year', columns='Area', values='Value')

    # Rename the index column to 'Year'
    df_food_pivot.index.name = 'Year'

    return df_food_pivot
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
662,25564,1,"    df = df_food_cleaned.melt(id_vars='Area', value_vars=[col for col in df_food_cleaned.columns if col.startswith('Y')], var_name='Year', value_name='prod')
    df['Year'] = df['Year'].replace('Y', '', regex=True).astype(np.int64)
    df = df.pivot_table(values='prod', index='Year', columns='Area', aggfunc='sum')
    df.columns = df.columns.rename('Country')
    return df",TODO,df,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df = df_food_cleaned.melt(id_vars='Area', value_vars=[col for col in df_food_cleaned.columns if col.startswith('Y')], var_name='Year', value_name='prod')
    df['Year'] = df['Year'].replace('Y', '', regex=True).astype(np.int64)
    df = df.pivot_table(values='prod', index='Year', columns='Area', aggfunc='sum')
    df.columns = df.columns.rename('Country')
    return df","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df = df_food_cleaned.melt(id_vars='Area', value_vars=[col for col in df_food_cleaned.columns if col.startswith('Y')], var_name='Year', value_name='prod')
    df['Year'] = df['Year'].replace('Y', '', regex=True).astype(np.int64)
    df = df.pivot_table(values='prod', index='Year', columns='Area', aggfunc='sum')
    df.columns = df.columns.rename('Country')
    return df
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
663,26944,1,"    aggregate = df_food_cleaned.groupby(['Area']).sum()
    year_cols = aggregate.iloc[:,5:]",TODO,aggregate,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    aggregate = df_food_cleaned.groupby(['Area']).sum()
    year_cols = aggregate.iloc[:,5:]
    df = year_cols.T
    df.columns.name = ""Country""
    df.index.name = ""Year""
    df.index = df.index.str[1:].astype(int)
    return df","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    aggregate = df_food_cleaned.groupby(['Area']).sum()
    year_cols = aggregate.iloc[:,5:]
    df = year_cols.T
    df.columns.name = ""Country""
    df.index.name = ""Year""
    df.index = df.index.str[1:].astype(int)
    return df
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
664,26944,2,"    year_cols = aggregate.iloc[:,5:]
    df = year_cols.T",TODO,year_cols,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    aggregate = df_food_cleaned.groupby(['Area']).sum()
    year_cols = aggregate.iloc[:,5:]
    df = year_cols.T
    df.columns.name = ""Country""
    df.index.name = ""Year""
    df.index = df.index.str[1:].astype(int)
    return df","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    aggregate = df_food_cleaned.groupby(['Area']).sum()
    year_cols = aggregate.iloc[:,5:]
    df = year_cols.T
    df.columns.name = ""Country""
    df.index.name = ""Year""
    df.index = df.index.str[1:].astype(int)
    return df
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
665,26944,3,"    df = year_cols.T
    df.columns.name = ""Country""
    df.index.name = ""Year""
    df.index = df.index.str[1:].astype(int)
    return df",TODO,df,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    aggregate = df_food_cleaned.groupby(['Area']).sum()
    year_cols = aggregate.iloc[:,5:]
    df = year_cols.T
    df.columns.name = ""Country""
    df.index.name = ""Year""
    df.index = df.index.str[1:].astype(int)
    return df","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    aggregate = df_food_cleaned.groupby(['Area']).sum()
    year_cols = aggregate.iloc[:,5:]
    df = year_cols.T
    df.columns.name = ""Country""
    df.index.name = ""Year""
    df.index = df.index.str[1:].astype(int)
    return df
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
666,26409,1,"    df_n = df_food_cleaned
    df_n = df_n.groupby([""Area""]).sum()
    cols = {}
    for col in df_n.columns:
        if col[0] == 'Y':
            cols[col] = int(col[1:])
    df_n.rename(columns=cols, inplace=True)
    df_n.drop(columns=[col for col in df_n.columns if type(col) != int], inplace=True)
    df_n.index.names=['Country']
    df_n = df_n.T
    df_n.index.names=['Year']
    df_n.index = df_n.index.astype(int)
    # display(df_n)
    return df_n",TODO,df_n,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df_n = df_food_cleaned
    df_n = df_n.groupby([""Area""]).sum()
    cols = {}
    for col in df_n.columns:
        if col[0] == 'Y':
            cols[col] = int(col[1:])
    df_n.rename(columns=cols, inplace=True)
    df_n.drop(columns=[col for col in df_n.columns if type(col) != int], inplace=True)
    df_n.index.names=['Country']
    df_n = df_n.T
    df_n.index.names=['Year']
    df_n.index = df_n.index.astype(int)
    # display(df_n)
    return df_n","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df_n = df_food_cleaned
    df_n = df_n.groupby([""Area""]).sum()
    cols = {}
    for col in df_n.columns:
        if col[0] == 'Y':
            cols[col] = int(col[1:])
    df_n.rename(columns=cols, inplace=True)
    df_n.drop(columns=[col for col in df_n.columns if type(col) != int], inplace=True)
    df_n.index.names=['Country']
    df_n = df_n.T
    df_n.index.names=['Year']
    df_n.index = df_n.index.astype(int)
    # display(df_n)
    return df_n
# df_n = get_cleaned_food_data()
# display(df_n)
# df_n = df_n.groupby([""Area""]).sum()
# .drop(columns=['Element'])
# df_n
# cols = {}
# for col in df_n.columns:
#     if col[0] == 'Y':
#         cols[col] = int(col[1:])
# df_n.rename(columns=cols, inplace=True)
# # print([col for col in df_n.columns if type(col) != int])
# df_n.drop(columns=[col for col in df_n.columns if type(col) != int], inplace=True)
# df_n.index.names=['Country']
# df_n = df_n.T
# df_n.index.names=['Year']
# df_n.index = df_n.index.astype(int)
# display(df_n)
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
667,26409,2,"    cols = {}
    for col in df_n.columns:
        if col[0] == 'Y':
            cols[col] = int(col[1:])
    df_n.rename(columns=cols, inplace=True)",TODO,cols,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df_n = df_food_cleaned
    df_n = df_n.groupby([""Area""]).sum()
    cols = {}
    for col in df_n.columns:
        if col[0] == 'Y':
            cols[col] = int(col[1:])
    df_n.rename(columns=cols, inplace=True)
    df_n.drop(columns=[col for col in df_n.columns if type(col) != int], inplace=True)
    df_n.index.names=['Country']
    df_n = df_n.T
    df_n.index.names=['Year']
    df_n.index = df_n.index.astype(int)
    # display(df_n)
    return df_n","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df_n = df_food_cleaned
    df_n = df_n.groupby([""Area""]).sum()
    cols = {}
    for col in df_n.columns:
        if col[0] == 'Y':
            cols[col] = int(col[1:])
    df_n.rename(columns=cols, inplace=True)
    df_n.drop(columns=[col for col in df_n.columns if type(col) != int], inplace=True)
    df_n.index.names=['Country']
    df_n = df_n.T
    df_n.index.names=['Year']
    df_n.index = df_n.index.astype(int)
    # display(df_n)
    return df_n
# df_n = get_cleaned_food_data()
# display(df_n)
# df_n = df_n.groupby([""Area""]).sum()
# .drop(columns=['Element'])
# df_n
# cols = {}
# for col in df_n.columns:
#     if col[0] == 'Y':
#         cols[col] = int(col[1:])
# df_n.rename(columns=cols, inplace=True)
# # print([col for col in df_n.columns if type(col) != int])
# df_n.drop(columns=[col for col in df_n.columns if type(col) != int], inplace=True)
# df_n.index.names=['Country']
# df_n = df_n.T
# df_n.index.names=['Year']
# df_n.index = df_n.index.astype(int)
# display(df_n)
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
668,26259,1,"    year_cols = [col for col in df_food_cleaned.columns if col[0]=='Y']
    df_agg = df_food_cleaned.groupby(['Area']).sum()[year_cols].transpose()",TODO,year_cols,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col[0]=='Y']
    df_agg = df_food_cleaned.groupby(['Area']).sum()[year_cols].transpose()
    df_agg.index = df_agg.index.str[1:]
    df_agg.index.name = ""Year""
    df_agg.columns.name = ""Country""
    df_agg.index = df_agg.index.astype(np.int64)
    return df_agg
    #pass","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col[0]=='Y']
    df_agg = df_food_cleaned.groupby(['Area']).sum()[year_cols].transpose()
    df_agg.index = df_agg.index.str[1:]
    df_agg.index.name = ""Year""
    df_agg.columns.name = ""Country""
    df_agg.index = df_agg.index.astype(np.int64)
    return df_agg
    #pass
# year_cols = [col for col in df_food_production.columns if col[0]=='Y']
# year_cols
# df_agg = df_food_production.groupby(['Area']).sum()[year_cols].transpose()
# df_agg.index = df_agg.index.str[1:]
# df_agg.index.name = ""Year""
# df_agg.columns.name = ""Country""
# df_agg.index = df_agg.index.astype(np.int64)
# df_agg
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
669,26259,2,"    df_agg = df_food_cleaned.groupby(['Area']).sum()[year_cols].transpose()
    df_agg.index = df_agg.index.str[1:]
    df_agg.index.name = ""Year""
    df_agg.columns.name = ""Country""
    df_agg.index = df_agg.index.astype(np.int64)
    return df_agg",TODO,df_agg,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col[0]=='Y']
    df_agg = df_food_cleaned.groupby(['Area']).sum()[year_cols].transpose()
    df_agg.index = df_agg.index.str[1:]
    df_agg.index.name = ""Year""
    df_agg.columns.name = ""Country""
    df_agg.index = df_agg.index.astype(np.int64)
    return df_agg
    #pass","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_cols = [col for col in df_food_cleaned.columns if col[0]=='Y']
    df_agg = df_food_cleaned.groupby(['Area']).sum()[year_cols].transpose()
    df_agg.index = df_agg.index.str[1:]
    df_agg.index.name = ""Year""
    df_agg.columns.name = ""Country""
    df_agg.index = df_agg.index.astype(np.int64)
    return df_agg
    #pass
# year_cols = [col for col in df_food_production.columns if col[0]=='Y']
# year_cols
# df_agg = df_food_production.groupby(['Area']).sum()[year_cols].transpose()
# df_agg.index = df_agg.index.str[1:]
# df_agg.index.name = ""Year""
# df_agg.columns.name = ""Country""
# df_agg.index = df_agg.index.astype(np.int64)
# df_agg
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
670,23363,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
671,23363,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
672,23363,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
673,21435,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
674,21435,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
675,21435,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
676,15781,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
677,15781,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
678,15781,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
679,16754,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
680,16754,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
681,16754,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
682,14790,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
683,14790,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
684,14790,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
685,23245,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
686,23245,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
687,23245,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
