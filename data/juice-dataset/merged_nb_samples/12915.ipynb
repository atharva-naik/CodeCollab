{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0096341c",
   "metadata": {},
   "source": [
    "# \"You *rarely* want to use DataFrame.apply\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474af7c",
   "metadata": {},
   "source": [
    "Tom Augspurger, one of the maintainers of Python's `Pandas` library for data analysis, has an awesome series of blog posts on [writing idiomatic Pandas code](https://tomaugspurger.github.io/modern-1.html). In fact you should probably leave this site now and go read one of those blog posts, they're really good. His [post on Performance](https://tomaugspurger.github.io/modern-4-performance.html) has an especially interesting tip: \n",
    "\n",
    "\"You rarely want to use DataFrame.apply and almost never should use it with axis=1 [which processes the DataFrame row-by-row, \"across columns\"]. Better to write functions that take arrays and pass those in directly...\" \n",
    "\n",
    "In Tom's example, he has a function with `numpy` math function calls, and he shows that his function works dramatically faster when those numpy functions are passed entire columns as arguments, which can be processed as vectors. Using `.apply()`, on the other and, calls those functions on one number at a time through a loop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f4e89e",
   "metadata": {},
   "source": [
    "## Trying out this advice on a simple text-processing function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7eb07",
   "metadata": {},
   "source": [
    "It certainly makes sense that the vectorized approach --passing whole DataFrame columns to a function which accepts array(s) as input-- should provide significant speedup for functions with `numpy` math function calls. We expect those can operate on arrays/vectors directly. But what about text processing?\n",
    "\n",
    "Here, I'll take a simple text-processing function I've used with `.apply()` before, and compare its performance with a slightly modified version meant to accept whole DataFrame columns instead of single strings.\n",
    "\n",
    "First, let's load our dataset - the Quora Duplicate Questions dataset released earlier this year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b71c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/quora_kaggle.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3697f4",
   "metadata": {},
   "source": [
    "### A simple text-processing function, and baseline speed test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828d577f",
   "metadata": {},
   "source": [
    "The function I'll be testing is a simple text-processing function for `tokenizing` a string - returning the string as a list of words, after doing a bit of preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize(text):\n",
    "    ''' Accept a string, return list of words (lowercased) without punctuation or stopwords'''\n",
    "\n",
    "    # lowercase everything\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation (\\W matches any non-alphanumeric character)\n",
    "    text = re.sub(\"\\W\", \" \", text)\n",
    "    \n",
    "    # return list of words, without stopwords (stopwords are very common words which may not convey much info)\n",
    "    droplist = stopwords.words('english')\n",
    "    \n",
    "    return [word for word in text.split() if word not in droplist]\n",
    "    \n",
    "tokenize('This is a sentence. And another one with punctuation and special characters to strip!?*&^%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d5f85",
   "metadata": {},
   "source": [
    "It takes about 49 seconds to apply this function to all of our 'question1' questions using `.apply()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bd0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start = datetime.now()\n",
    "df['q1_tokenized'] = df['question1'].apply(tokenize) \n",
    "\n",
    "print('Time elapsed: ', datetime.now() - start, '\\n')\n",
    "print(df[['question1', 'q1_tokenized']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51e2dad",
   "metadata": {},
   "source": [
    "..............................................\n",
    "\n",
    "NOTE: replace 53 seconds above with 0:00:49.371585, the middle time of three runs from my shell. times were more consistent when running from my shell, think the gc provides more comparable times to compare this with next run.\n",
    "\n",
    "................................................"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec26f78",
   "metadata": {},
   "source": [
    "### The Vectorized Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b998b8",
   "metadata": {},
   "source": [
    "Let's see if we can speed this up by modifying our `tokenize` function to accept a `Pandas Series` of strings, instead of a single string. That way we won't have to use `.apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96814e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize2(text_series):\n",
    "    ''' Accept a series of strings, returns list of words (lowercased) without punctuation or stopwords'''\n",
    "\n",
    "    # lowercase everything\n",
    "    text_series = text_series.str.lower()\n",
    "    \n",
    "    # remove punctuation (r'\\W' is regex, matches any non-alphanumeric character)\n",
    "    text_series = text_series.str.replace(r'\\W', ' ')\n",
    "    \n",
    "    # return list of words, without stopwords\n",
    "    sw = stopwords.words('english')\n",
    "    \n",
    "    return text_series.apply(lambda row: [word for word in row.split() if word not in sw])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db987167",
   "metadata": {},
   "source": [
    "And to measure performance of the (mostly) vectorized approach:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
