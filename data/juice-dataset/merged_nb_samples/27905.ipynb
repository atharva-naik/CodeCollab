{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9269f666",
   "metadata": {},
   "source": [
    "#  The normal model with pymc\n",
    "\n",
    "##### Keywords: bayesian, normal-normal model, conjugate prior, MCMC engineering, pymc3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67909ba1",
   "metadata": {},
   "source": [
    "## Example of the normal model for fixed $\\sigma$\n",
    "\n",
    "We have data on the wing length in millimeters of a nine members of a particular species of moth. We wish to make inferences from those measurements on the population mean $\\mu$. Other studies show the wing length to be around 19 mm. We also know that the length must be positive. We can choose a prior that is normal and most of the density is above zero ($\\mu=19.5,\\tau=10$). This is only a **marginally informative** prior.\n",
    "\n",
    "Many bayesians would prefer you choose relatively uninformative (and thus weakly regularizing) priors. This keeps the posterior in-line (it really does help a sampler remain in important regions), but does not add too much information into the problem.\n",
    "\n",
    "The measurements were: 16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8 giving $\\bar{y}=18.14$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f27e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [16.4, 17.0, 17.2, 17.4, 18.2, 18.2, 18.2, 19.9, 20.8]\n",
    "#Data Quantities\n",
    "sig = np.std(Y) # assume that is the value of KNOWN sigma (in the likelihood)\n",
    "mu_data = np.mean(Y)\n",
    "n = len(Y)\n",
    "print(\"sigma\", sig, \"mu\", mu_data, \"n\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior mean\n",
    "mu_prior = 19.5\n",
    "# prior std\n",
    "tau = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ef266",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = sig**2 / tau**2\n",
    "sig_post =np.sqrt(1./( 1./tau**2 + n/sig**2));\n",
    "# posterior mean\n",
    "mu_post = kappa / (kappa + n) *mu_prior + n/(kappa+n)* mu_data\n",
    "print(\"mu post\", mu_post, \"sig_post\", sig_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples\n",
    "N = 18000\n",
    "theta_prior = np.random.normal(loc=mu_prior, scale=tau, size=N);\n",
    "theta_post = np.random.normal(loc=mu_post, scale=sig_post, size=N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a4df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(theta_post, bins=30, alpha=0.9, label=\"posterior\");\n",
    "plt.hist(theta_prior, bins=30, alpha=0.2, label=\"prior\");\n",
    "#plt.xlim([10, 30])\n",
    "plt.xlabel(\"wing length (mm)\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616733b6",
   "metadata": {},
   "source": [
    "## Sampling by  code\n",
    "\n",
    "We now set up code to do metropolis using logs of distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372cdc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis(logp, qdraw, stepsize, nsamp, xinit):\n",
    "    samples=np.empty(nsamp)\n",
    "    x_prev = xinit\n",
    "    accepted = 0\n",
    "    for i in range(nsamp):\n",
    "        x_star = qdraw(x_prev, stepsize)\n",
    "        logp_star = logp(x_star)\n",
    "        logp_prev = logp(x_prev)\n",
    "        logpdfratio = logp_star -logp_prev\n",
    "        u = np.random.uniform()\n",
    "        if np.log(u) <= logpdfratio:\n",
    "            samples[i] = x_star\n",
    "            x_prev = x_star\n",
    "            accepted += 1\n",
    "        else:#we always get a sample\n",
    "            samples[i]= x_prev\n",
    "            \n",
    "    return samples, accepted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9a354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop(x, step):\n",
    "    return np.random.normal(x, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9ebbd",
   "metadata": {},
   "source": [
    "Remember, that up to normalization, the posterior is the likelihood times the prior. Thus the log of the posterior is the sum of the logs of the likelihood and the prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior = lambda mu: norm.logpdf(mu, loc=19.5, scale=10)\n",
    "loglike = lambda mu: np.sum(norm.logpdf(Y, loc=mu, scale=np.std(Y)))\n",
    "logpost = lambda mu: loglike(mu) + logprior(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d1828",
   "metadata": {},
   "source": [
    "Now we sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98734bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.random.uniform()\n",
    "nsamps=100000\n",
    "samps, acc = metropolis(logpost, prop, 1, nsamps, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe97c2",
   "metadata": {},
   "source": [
    "The acceptance rate is reasonable. You should shoot for somewhere between 20 and 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5690c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc/nsamps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b5f28",
   "metadata": {},
   "source": [
    "appropriately thinned, we lose any correlation.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f633ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrplot(trace, maxlags=50):\n",
    "    plt.acorr(trace-np.mean(trace),  normed=True, maxlags=maxlags);\n",
    "    plt.xlim([0, maxlags])\n",
    "corrplot(samps[10000::5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb712d4",
   "metadata": {},
   "source": [
    "We compare kdeplots of the exact sampling to our MCMC result and find that we do well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(theta_post);\n",
    "sns.kdeplot(samps[10000::5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03582b42",
   "metadata": {},
   "source": [
    "## Sampling with pymc\n",
    "\n",
    "We'll use this simple example to show how to sample with pymc. To install pymc3, do\n",
    "\n",
    "`conda install pymc3`.\n",
    "\n",
    "For me this installed  `pymc3.0 rc4`.\n",
    "\n",
    "Pymc3 is basically a sampler which uses NUTS for continuous variables and Metropolis for discrete ones, but we can force it to use Metropolis for all, which is what we shall do for now.\n",
    "\n",
    "pymc3 docs are available [here](https://pymc-devs.github.io/pymc3/). \n",
    "\n",
    "The structure is that we define a model within a context manager, and optionally do the sampling there. The model name (`model1` below) and trace name (`model1trace` below) are both important names you should keep track of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf69a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "with pm.Model() as model1:\n",
    "    mu = pm.Normal('mu', mu=19.5, sd=10)#parameter's prior\n",
    "    wingspan = pm.Normal('wingspan', mu=mu, sd=np.std(Y), observed=Y)#likelihood\n",
    "    stepper=pm.Metropolis()\n",
    "    tracemodel1=pm.sample(100000, step=stepper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73083ea",
   "metadata": {},
   "source": [
    "Notice that `wingspan`, which is the  data, is defined using the same exact notation as the  prior abovem with the addition of the `observed` argument. This is because Bayesian notation does not distinguish between data d=and parameter nodes..everything is treated equally, and all the action is in taking conditionals and marginals of distributions.\n",
    "\n",
    "Pymc3 gives us a nice summary of our trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03bfb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(tracemodel1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68bc5e",
   "metadata": {},
   "source": [
    "The highest-posterior-density  is the  smallest width interval containing a pre-specified density amount. Here the default is the smallest width containing 95% of the density. Such an interval is called a **Bayesian Credible Interval**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2bd243",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.hpd(tracemodel1)#pm.hpd(tracemodel1, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed324e",
   "metadata": {},
   "source": [
    "You can also get quantiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035eb2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.quantiles(tracemodel1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137c587",
   "metadata": {},
   "source": [
    "`pm.traceplot` will give you marginal posteriors and traces for all the \"stochastics\" in your model (ie non-data). It can even give you traces for some deterministic functions of stochastics..we shall see an example of this soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c97e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.traceplot(tracemodel1[10000::5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7eef1",
   "metadata": {},
   "source": [
    "Autocorrelation is easily accessible as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67936d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.autocorrplot(tracemodel1[10000::5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe868d9",
   "metadata": {},
   "source": [
    "Here we plot the results of our sampling against the exact solution and out manual sampler and see that all three match well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cffb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(theta_post);\n",
    "sns.kdeplot(samps[10000::5]);\n",
    "sns.kdeplot(tracemodel1[10000::5]['mu']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effdaf7e",
   "metadata": {},
   "source": [
    "The **posterior predictive** is accessed via the `sample_ppc` function, which takes the trace, the number of samples wanted, and the model as arguments. The sampler will use the posterior traces and the defined likelihood to return samples from the posterior predictive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750e7981",
   "metadata": {},
   "source": [
    "## Normal Model for fixed $\\sigma$\n",
    "\n",
    "Now we wish to condition on a known $\\sigma^2$. The prior probability distribution for it can then be written as:\n",
    "\n",
    "$$p(\\sigma^2) = \\delta(\\sigma^2 -\\sigma_0^2)$$\n",
    "\n",
    "(which does integrate to 1).\n",
    "\n",
    "Now, keeping in mind that $p(\\mu, \\sigma^2) = p(\\mu \\vert \\sigma^2) p(\\sigma^2)$ and carrying out the integral over $\\sigma^2$ which because of the delta distribution means that we must just substitute $\\sigma_0^2$ in, we get:\n",
    "\n",
    "$$ p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2 = \\sigma_0^2)  \\propto p(\\mu \\vert \\sigma^2=\\sigma_0^2) \\,e^{ - \\frac{1}{2\\sigma_0^2} \\sum (y_i - \\mu)^2 }$$\n",
    "\n",
    "where I have dropped the $\\frac{1}{\\sqrt{2\\pi\\sigma_0^2}}$ factor as there is no stochasticity in it (its fixed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20c27d7",
   "metadata": {},
   "source": [
    "\n",
    "Say we have the prior\n",
    "\n",
    "$$ p(\\mu \\vert \\sigma^2) = \\exp \\left\\{ -\\frac{1}{2 \\tau^2} (\\hat{\\mu}-\\mu)^2 \\right\\} $$\n",
    "\n",
    "then it can be shown that the posterior is \n",
    "\n",
    "$$  p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2) \\propto \\exp \\left\\{ -\\frac{a}{2} (\\mu-b/a)^2 \\right\\} $$\n",
    "where \n",
    "$$ a = \\frac{1}{\\tau^2} + \\frac{n}{\\sigma_0^2} , \\;\\;\\;\\;\\; b = \\frac{\\hat{\\mu}}{\\tau^2} + \\frac{\\sum y_i}{\\sigma_0^2} $$\n",
    "This is a normal density curve with $1/\\sqrt{a}$ playing the role of the \n",
    "standard deviation and $b/a$ playing the role of the mean. Re-writing this, \n",
    "\n",
    "$$ p( \\mu \\vert  y_1, \\ldots, y_n, \\sigma^2)  \\propto \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{\\mu-b/a}{1/\\sqrt(a)}\\right)^2 \\right\\} $$\n",
    "\n",
    "**The conjugate of the normal is the normal itself**. \n",
    "\n",
    "Define $\\kappa = \\sigma^2 / \\tau^2 $ to be the variance of the sample model  in units of variance\n",
    "of our prior belief (prior distribution) then the *posterior mean* is \n",
    "\n",
    "$$\\mu_p = \\frac{b}{a} = \\frac{ \\kappa}{\\kappa + n }  \\hat{\\mu} + \\frac{n}{\\kappa + n} \\bar{y} $$\n",
    "\n",
    "which is a weighted average of prior mean and sampling mean.\n",
    "The variance is \n",
    "\n",
    "$$ \\sigma_p^2 = \\frac{1}{1/\\tau^2+n/\\sigma^2} $$\n",
    "or better \n",
    "\n",
    "$$ \\frac{1}{\\sigma_p^2} = \\frac{1}{\\tau^2} + \\frac{n}{\\sigma^2}. $$\n",
    "\n",
    "You can see that as $n$ increases, the data dominates the prior and the posterior mean approaches the data mean, with the posterior distribution narrowing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1 = tracemodel1[10000::5]\n",
    "postpred = pm.sample_ppc(tr1, 1000, model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50883c33",
   "metadata": {},
   "source": [
    "The posterior predictive will return samples for all data in the model's  `observed_RVs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.observed_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e079e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "postpred['wingspan'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd382f1",
   "metadata": {},
   "source": [
    "We plot the posterior predictive against the posterior to see how it is spread out! When we compare the posterior predictive to the posterior (unlike in the beta-binomial distribution where one is a rate and one is a count, here both are on the same scale), we find that the posterior predictive is smeared out due to the additional uncertainty from the sampling distribution."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
