{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe355e80",
   "metadata": {},
   "source": [
    "# Predicting stock prices using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e70c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "from pandas_datareader import data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf # This code has been tested with TensorFlow 1.6\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ec268",
   "metadata": {},
   "source": [
    "## 1.- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'kaggle' # alphavantage or kaggle\n",
    "\n",
    "if data_source == 'alphavantage':\n",
    "    # ====================== Loading Data from Alpha Vantage ==================================\n",
    "\n",
    "    api_key = 'K5EBWCLPHLBDJ8QU'\n",
    "\n",
    "    # American Airlines stock market prices\n",
    "    ticker = \"AAL\"\n",
    "\n",
    "    # JSON file with all the stock market data for AAL from the last 20 years\n",
    "    url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(ticker,api_key)\n",
    "\n",
    "    # Save data to this file\n",
    "    file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "\n",
    "    # If you haven't already saved data,\n",
    "    # Go ahead and grab the data from the url\n",
    "    # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "    if not os.path.exists(file_to_save):\n",
    "        with urllib.request.urlopen(url_string) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            # extract stock market data\n",
    "            data = data['Time Series (Daily)']\n",
    "            df = pd.DataFrame(columns=['Date','Low','High','Close','Open'])\n",
    "            for k,v in data.items():\n",
    "                date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                data_row = [date.date(),float(v['3. low']),float(v['2. high']),\n",
    "                            float(v['4. close']),float(v['1. open'])]\n",
    "                df.loc[-1,:] = data_row\n",
    "                df.index = df.index + 1\n",
    "        print('Data saved to : %s'%file_to_save)        \n",
    "        df.to_csv(file_to_save)\n",
    "\n",
    "    # If the data is already there, just load it from the CSV\n",
    "    else:\n",
    "        print('File already exists. Loading data from CSV')\n",
    "        df = pd.read_csv(file_to_save)\n",
    "\n",
    "else:\n",
    "\n",
    "    # ====================== Loading Data from Kaggle ==================================\n",
    "    # You will be using HP's data. Feel free to experiment with other data.\n",
    "    # But while doing so, be careful to have a large enough dataset and also pay attention to the data normalization\n",
    "    df = pd.read_csv(os.path.join('Data','Stocks','hpq.us.txt'),delimiter=',',usecols=['Date','Open','High','Low',\n",
    "                                                                                       'Close'])\n",
    "    print('Loaded data from the Kaggle repository')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29dc10",
   "metadata": {},
   "source": [
    "## 2.- Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3759bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by date\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Double check the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b685a",
   "metadata": {},
   "source": [
    "### 2.1.- Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)\n",
    "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Average Price',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d16172",
   "metadata": {},
   "source": [
    "## 3.- Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90871a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b199ac5",
   "metadata": {},
   "source": [
    "## 6.- Normal averaging prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f95779",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "std_avg_predictions = []\n",
    "std_avg_x = []\n",
    "mse_errors = []\n",
    "\n",
    "for pred_idx in range(window_size,N):\n",
    "\n",
    "    if pred_idx >= N:\n",
    "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
    "    else:\n",
    "        date = df.loc[pred_idx,'Date']\n",
    "\n",
    "    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n",
    "    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    std_avg_x.append(date)\n",
    "\n",
    "print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793805ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
    "plt.plot(range(window_size,N),std_avg_predictions,color='orange',label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d2e40",
   "metadata": {},
   "source": [
    "## 7.- Exponential moving average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "\n",
    "run_avg_predictions = []\n",
    "run_avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "running_mean = 0.0\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred_idx in range(1,N):\n",
    "\n",
    "    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    run_avg_x.append(date)\n",
    "\n",
    "print('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaedb24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
    "plt.plot(range(0,N),run_avg_predictions,color='orange', label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e1186",
   "metadata": {},
   "source": [
    "## 8.- LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5abb53",
   "metadata": {},
   "source": [
    "### 8.1.- Definition of data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ecf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorSeq(object):\n",
    "\n",
    "    def __init__(self, prices, batch_size, num_unroll):\n",
    "        self._prices = prices\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length // self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "        \n",
    "        \n",
    "    def next_batch(self):\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size), dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size), dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b]+1>=self._prices_length:\n",
    "                #self._cursor[b] = b * self._segments\n",
    "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "    \n",
    "    \n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data, unroll_labels = [],[]\n",
    "        init_data, init_label = None, None\n",
    "        for ui in range(self._num_unroll):\n",
    "\n",
    "            data, labels = self.next_batch()    \n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "    \n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGeneratorSeq(train_data,5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    print('\\tInputs: ',dat )\n",
    "    print('\\n\\tOutput:',lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63946b6",
   "metadata": {},
   "source": [
    "### 8.2 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00be4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the mid prices from the highest and lowest\n",
    "high_prices = df.loc[:,'High'].as_matrix()\n",
    "low_prices = df.loc[:,'Low'].as_matrix()\n",
    "mid_prices = (high_prices+low_prices)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1b27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mid_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ace94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mid_prices[:11000]\n",
    "test_data = mid_prices[11000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6641b24",
   "metadata": {},
   "source": [
    "## 4.- Data normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7686450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to be between 0 and 1\n",
    "# When scaling remember! You normalize both test and train data with respect to training data\n",
    "# Because you are not supposed to have access to test data\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c709a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different time periods in the data have different value ranges, therefore data is normalized \n",
    "# by splitting the full series into windows. We will use a window size of 2500.\n",
    "\n",
    "# Train the scaler with training data and smooth data\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0,10000,smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208023d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26576eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'][0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0933976",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(df['Date'][0:len(train_data)], train_data)\n",
    "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Normalised Price',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad9b82",
   "metadata": {},
   "source": [
    "## 5.- Data smoothing"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
