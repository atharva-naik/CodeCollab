{
    "Modern Pandas": [
        [
            "This is part 1 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "Modern Pandas->Effective Pandas->Introduction": [
        [
            "This series is about how to make effective use of , a data analysis library for the Python programming language.\nIt\u2019s targeted at an intermediate level: people who have some experience with pandas, but are looking to improve.",
            "markdown"
        ]
    ],
    "Modern Pandas->Effective Pandas->Prior Art": [
        [
            "There are many great resources for learning pandas; this is not one of them.\nFor beginners, I typically recommend  , especially if they\u2019re familiar with SQL. Of course, there\u2019s the pandas  itself. I gave  at PyData Seattle targeted as an introduction if you prefer video form. Wes McKinney\u2019s  is still the goto book (and is also a really good introduction to NumPy as well). Jake VanderPlas\u2019s , in early release, is great too.\nKevin Markham has a  for beginners learning pandas.",
            "markdown"
        ],
        [
            "With all those resources (and many more that I\u2019ve slighted through omission), why write another? Surely the law of diminishing returns is kicking in by now.\nStill, I thought there was room for a guide that is up to date (as of March 2016) and emphasizes idiomatic pandas code (code that is <em>pandorable</em>).\nThis series probably won\u2019t be appropriate for people completely new to python\nor NumPy and pandas.\nBy luck, this first post happened to cover topics that are relatively introductory,\nso read some of the linked material and come back, or  if you\nhave questions.",
            "markdown"
        ]
    ],
    "Modern Pandas->Effective Pandas->Get the Data": [
        [
            "We\u2019ll be working with  from the BTS (R users can install Hadley\u2019s  dataset for similar data.",
            "markdown"
        ],
        [
            "import os\nimport zipfile\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep",
            "code"
        ],
        [
            "import requests\n\nheaders = {\n    'Referer': 'https://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&amp;DB_Short_Name=On-Time',\n    'Origin': 'https://www.transtats.bts.gov',\n    'Content-Type': 'application/x-www-form-urlencoded',\n}\n\nparams = (\n    ('Table_ID', '236'),\n    ('Has_Group', '3'),\n    ('Is_Zipped', '0'),\n)\n\nwith open('modern-1-url.txt', encoding='utf-8') as f:\n    data = f.read().strip()\n\nos.makedirs('data', exist_ok=True)\ndest = \"data/flights.csv.zip\"\n\nif not os.path.exists(dest):\n    r = requests.post('https://www.transtats.bts.gov/DownLoad_Table.asp',\n                      headers=headers, params=params, data=data, stream=True)\n\n    with open(\"data/flights.csv.zip\", 'wb') as f:\n        for chunk in r.iter_content(chunk_size=102400): \n            if chunk:\n                f.write(chunk)",
            "code"
        ],
        [
            "That download returned a ZIP file.\nThere\u2019s an open  for automatically decompressing ZIP archives with a single CSV,\nbut for now we have to extract it ourselves and then read it in.",
            "markdown"
        ],
        [
            "zf = zipfile.ZipFile(\"data/flights.csv.zip\")\nfp = zf.extract(zf.filelist[0].filename, path='data/')\ndf = pd.read_csv(fp, parse_dates=[\"FL_DATE\"]).rename(columns=str.lower)\n\ndf.info()",
            "code"
        ],
        [
            "&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 450017 entries, 0 to 450016\nData columns (total 33 columns):\nfl_date                  450017 non-null datetime64[ns]\nunique_carrier           450017 non-null object\nairline_id               450017 non-null int64\ntail_num                 449378 non-null object\nfl_num                   450017 non-null int64\norigin_airport_id        450017 non-null int64\norigin_airport_seq_id    450017 non-null int64\norigin_city_market_id    450017 non-null int64\norigin                   450017 non-null object\norigin_city_name         450017 non-null object\ndest_airport_id          450017 non-null int64\ndest_airport_seq_id      450017 non-null int64\ndest_city_market_id      450017 non-null int64\ndest                     450017 non-null object\ndest_city_name           450017 non-null object\ncrs_dep_time             450017 non-null int64\ndep_time                 441476 non-null float64\ndep_delay                441476 non-null float64\ntaxi_out                 441244 non-null float64\nwheels_off               441244 non-null float64\nwheels_on                440746 non-null float64\ntaxi_in                  440746 non-null float64\ncrs_arr_time             450017 non-null int64\narr_time                 440746 non-null float64\narr_delay                439645 non-null float64\ncancelled                450017 non-null float64\ncancellation_code        8886 non-null object\ncarrier_delay            97699 non-null float64\nweather_delay            97699 non-null float64\nnas_delay                97699 non-null float64\nsecurity_delay           97699 non-null float64\nlate_aircraft_delay      97699 non-null float64\nunnamed: 32              0 non-null float64\ndtypes: datetime64[ns](1), float64(15), int64(10), object(7)\nmemory usage: 113.3+ MB",
            "code"
        ]
    ],
    "Modern Pandas->Effective Pandas->Indexing": [
        [
            "Or, <em>explicit is better than implicit</em>.\nBy my count, 7 of the top-15 voted pandas questions on  are about indexing. This seems as good a place as any to start.",
            "markdown"
        ],
        [
            "By indexing, we mean the selection of subsets of a DataFrame or Series.\nDataFrames (and to a lesser extent, Series) provide a difficult set of challenges:Like lists, you can index by location.Like dictionaries, you can index by label.Like NumPy arrays, you can index by boolean masks.Any of these indexers could be scalar indexes, or they could be arrays, or they could be slices.Any of these should work on the index (row labels) or columns of a DataFrame.And any of these should work on hierarchical indexes.",
            "markdown"
        ],
        [
            "The complexity of pandas\u2019 indexing is a microcosm for the complexity of the pandas API in general.\nThere\u2019s a reason for the complexity (well, most of it), but that\u2019s not <em>much</em> consolation while you\u2019re learning.\nStill, all of these ways of indexing really are useful enough to justify their inclusion in the library.",
            "markdown"
        ]
    ],
    "Modern Pandas->Effective Pandas->Slicing": [
        [
            "Or, <em>explicit is better than implicit</em>.",
            "markdown"
        ],
        [
            "By my count, 7 of the top-15 voted pandas questions on  are about slicing. This seems as good a place as any to start.",
            "markdown"
        ],
        [
            "Brief history digression: For years the preferred method for row and/or column selection was .ix.",
            "markdown"
        ],
        [
            "df.ix[10:15, ['fl_date', 'tail_num']]",
            "code"
        ],
        [
            "/Users/taugspurger/Envs/blog/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n.ix is deprecated. Please use\n.loc for label based indexing or\n.iloc for positional indexing\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate_ix\n  \"\"\"Entry point for launching an IPython kernel.",
            "code"
        ],
        [
            "As you can see, this method is now deprecated. Why\u2019s that? This simple little operation hides some complexity. What if, rather than our default range(n) index, we had an integer index like",
            "markdown"
        ],
        [
            "# filter the warning for now on\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)",
            "code"
        ],
        [
            "first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()",
            "code"
        ],
        [
            "Can you predict ahead of time what our slice from above will give when passed to .ix?",
            "markdown"
        ],
        [
            "first.ix[10:15, ['fl_date', 'tail_num']]",
            "code"
        ],
        [
            "Surprise, an empty DataFrame! Which in data analysis is rarely a good thing. What happened?",
            "markdown"
        ],
        [
            "We had an integer index, so the call to .ix used its label-based mode. It was looking for integer <em>labels</em> between 10:15 (inclusive). It didn\u2019t find any. Since we sliced a range it returned an empty DataFrame, rather than raising a KeyError.",
            "markdown"
        ],
        [
            "By way of contrast, suppose we had a string index, rather than integers.",
            "markdown"
        ],
        [
            "first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "code"
        ],
        [
            "And it works again! Now that we had a string index, .ix used its positional-mode. It looked for <em>rows</em> 10-15 (exclusive on the right).",
            "markdown"
        ],
        [
            "But you can\u2019t reliably predict what the outcome of the slice will be ahead of time. It\u2019s on the <em>reader</em> of the code (probably your future self) to know the dtypes so you can reckon whether .ix will use label indexing (returning the empty DataFrame) or positional indexing (like the last example).\nIn general, methods whose behavior depends on the data, like .ix dispatching to label-based indexing on integer Indexes but location-based indexing on non-integer, are hard to use correctly. We\u2019ve been trying to stamp them out in pandas.",
            "markdown"
        ],
        [
            "Since pandas 0.12, these tasks have been cleanly separated into two methods:.loc for label-based indexing.iloc for positional indexing",
            "markdown"
        ],
        [
            "first.loc[['AA', 'AS', 'DL'], ['fl_date', 'tail_num']]",
            "code"
        ],
        [
            "first.iloc[[0, 1, 3], [0, 1]]",
            "code"
        ],
        [
            ".ix is deprecated, but will hang around for a little while.\nBut if you\u2019ve been using .ix out of habit, or if you didn\u2019t know any better, maybe give .loc and .iloc a shot. I\u2019d recommend carefully updating your code to decide if you\u2019ve been using positional or label indexing, and choose the appropriate indexer. For the intrepid reader, Joris Van den Bossche (a core pandas dev)  of the pandas __getitem__ API.\nA later post in this series will go into more detail on using Indexes effectively;\nthey are useful objects in their own right, but for now we\u2019ll move on to a closely related topic.",
            "markdown"
        ]
    ],
    "Modern Pandas->Effective Pandas->SettingWithCopy": [
        [
            "Pandas used to get <em>a lot</em> of questions about assignments seemingly not working. We\u2019ll take  question as a representative question.",
            "markdown"
        ],
        [
            "f = pd.DataFrame({'a':[1,2,3,4,5], 'b':[10,20,30,40,50]})\nf",
            "code"
        ],
        [
            "The user wanted to take the rows of b where a was 3 or less, and set them equal to b / 10\nWe\u2019ll use boolean indexing to select those rows f['a'] &lt;= 3,",
            "markdown"
        ],
        [
            "# ignore the context manager for now\nwith pd.option_context('mode.chained_assignment', None):\n    f[f['a'] &lt;= 3]['b'] = f[f['a'] &lt;= 3 ]['b'] / 10\nf",
            "code"
        ],
        [
            "And nothing happened. Well, something did happen, but nobody witnessed it. If an object without any references is modified, does it make a sound?",
            "markdown"
        ],
        [
            "The warning I silenced above with the context manager links to  that\u2019s quite helpful. I\u2019ll summarize the high points here.",
            "markdown"
        ],
        [
            "The \u201cfailure\u201d to update f comes down to what\u2019s called <em>chained indexing</em>, a practice to be avoided.\nThe \u201cchained\u201d comes from indexing multiple times, one after another, rather than one single indexing operation.\nAbove we had two operations on the left-hand side, one __getitem__ and one __setitem__ (in python, the square brackets are syntactic sugar for __getitem__ or __setitem__ if it\u2019s for assignment). So f[f['a'] &lt;= 3]['b'] becomesgetitem: f[f['a'] &lt;= 3]setitem: _['b'] = ... # using _ to represent the result of 1.",
            "markdown"
        ],
        [
            "In general, pandas can\u2019t guarantee whether that first __getitem__ returns a view or a copy of the underlying data.\nThe changes <em>will</em> be made to the thing I called _ above, the result of the __getitem__ in 1.\nBut we don\u2019t know that _ shares the same memory as our original f.\nAnd so we can\u2019t be sure that whatever changes are being made to _ will be reflected in f.",
            "markdown"
        ],
        [
            "Done properly, you would write",
            "markdown"
        ],
        [
            "f.loc[f['a'] &lt;= 3, 'b'] = f.loc[f['a'] &lt;= 3, 'b'] / 10\nf",
            "code"
        ],
        [
            "Now this is all in a single call to __setitem__ and pandas can ensure that the assignment happens properly.",
            "markdown"
        ],
        [
            "The rough rule is any time you see back-to-back square brackets, ][, you\u2019re in asking for trouble. Replace that with a .loc[..., ...] and you\u2019ll be set.",
            "markdown"
        ],
        [
            "The other bit of advice is that a SettingWithCopy warning is raised when the <em>assignment</em> is made.\nThe potential copy could be made earlier in your code.",
            "markdown"
        ]
    ],
    "Modern Pandas->Effective Pandas->Multidimensional Indexing": [
        [
            "MultiIndexes might just be my favorite feature of pandas.\nThey let you represent higher-dimensional datasets in a familiar two-dimensional table, which my brain can sometimes handle.\nEach additional level of the MultiIndex represents another dimension.\nThe cost of this is somewhat harder label indexing.",
            "markdown"
        ],
        [
            "My very first bug report to pandas, back in ,\nwas about indexing into a MultiIndex.\nI bring it up now because I genuinely couldn\u2019t tell whether the result I got was a bug or not.\nAlso, from that bug report<blockquote>",
            "markdown"
        ],
        [
            "Sorry if this isn\u2019t actually a bug. Still very new to python. Thanks!</blockquote>",
            "markdown"
        ],
        [
            "Adorable.",
            "markdown"
        ],
        [
            "That operation was made much easier by  addition in 2014, which lets you slice arbitrary levels of a MultiIndex..\nLet\u2019s make a MultiIndexed DataFrame to work with.",
            "markdown"
        ],
        [
            "hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "code"
        ],
        [
            "And just to clear up some terminology, the <em>levels</em> of a MultiIndex are the\nformer column names (unique_carrier, origin&amp;mldr;).\nThe labels are the actual values in a level, ('AA', 'ABQ', &amp;mldr;).\nLevels can be referred to by name or position, with 0 being the outermost level.",
            "markdown"
        ],
        [
            "Slicing the outermost index level is pretty easy, we just use our regular .loc[row_indexer, column_indexer]. We\u2019ll select the columns dep_time and dep_delay where the carrier was American Airlines, Delta, or US Airways.",
            "markdown"
        ],
        [
            "hdf.loc[['AA', 'DL', 'US'], ['dep_time', 'dep_delay']]",
            "code"
        ],
        [
            "142945 rows \u00d7 2 columns",
            "markdown"
        ],
        [
            "So far, so good. What if you wanted to select the rows whose origin was Chicago O\u2019Hare (ORD) or Des Moines International Airport (DSM).\nWell, .loc wants [row_indexer, column_indexer] so let\u2019s wrap the two elements of our row indexer (the list of carriers and the list of origins) in a tuple to make it a single unit:",
            "markdown"
        ],
        [
            "hdf.loc[(['AA', 'DL', 'US'], ['ORD', 'DSM']), ['dep_time', 'dep_delay']]",
            "code"
        ],
        [
            "5582 rows \u00d7 2 columns",
            "markdown"
        ],
        [
            "Now try to do any flight from ORD or DSM, not just from those carriers.\nThis used to be a pain.\nYou might have to turn to the .xs method, or pass in df.index.get_level_values(0) and zip that up with the indexers your wanted, or maybe reset the index and do a boolean mask, and set the index again&amp;mldr; ugh.",
            "markdown"
        ],
        [
            "But now, you can use an IndexSlice.",
            "markdown"
        ],
        [
            "hdf.loc[pd.IndexSlice[:, ['ORD', 'DSM']], ['dep_time', 'dep_delay']]",
            "code"
        ],
        [
            "19466 rows \u00d7 2 columns",
            "markdown"
        ],
        [
            "The : says include every label in this level.\nThe IndexSlice object is just sugar for the actual python slice object needed to remove slice each level.",
            "markdown"
        ],
        [
            "pd.IndexSlice[:, ['ORD', 'DSM']]",
            "code"
        ],
        [
            "(slice(None, None, None), ['ORD', 'DSM'])",
            "code"
        ],
        [
            "We\u2019ll talk more about working with Indexes (including MultiIndexes) in a later post. I have an unproven thesis that they\u2019re underused because IndexSlice is underused, causing people to think they\u2019re more unwieldy than they actually are. But let\u2019s close out part one.",
            "markdown"
        ]
    ],
    "Modern Pandas->Effective Pandas->WrapUp": [
        [
            "This first post covered Indexing, a topic that\u2019s central to pandas.\nThe power provided by the DataFrame comes with some unavoidable complexities.\nBest practices (using .loc and .iloc) will spare you many a headache.\nWe then toured a couple of commonly misunderstood sub-topics, setting with copy and Hierarchical Indexing.",
            "markdown"
        ]
    ],
    "Method Chaining": [
        [
            "This is part 2 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "Method Chaining->Method Chaining": [
        [
            "Method chaining, where you call methods on an object one after another, is in vogue at the moment.\nIt\u2019s always been a style of programming that\u2019s been possible with pandas,\nand over the past several releases, we\u2019ve added methods that enable even more chaining. (0.16.0): For adding new columns to a DataFrame in a chain (inspired by dplyr\u2019s mutate) (0.16.2): For including user-defined methods in method chains. (0.18.0): For altering axis names (in additional to changing the actual labels as before). (0.18): Took the top-level pd.rolling_* and pd.expanding_* functions and made them NDFrame methods with a groupby-like API. (0.18.0) Added a new groupby-like API (0.18.1): In the next release you\u2019ll be able to pass a callable to the indexing methods, to be evaluated within the DataFrame\u2019s context (like .query, but with code instead of strings).",
            "markdown"
        ],
        [
            "My scripts will typically start off with large-ish chain at the start getting things into a manageable state.\nIt\u2019s good to have the bulk of your munging done with right away so you can start to do Science\u2122:",
            "markdown"
        ],
        [
            "Here\u2019s a quick example:",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='ticks', context='talk')\n\nimport prep",
            "code"
        ],
        [
            "def read(fp):\n    df = (pd.read_csv(fp)\n            .rename(columns=str.lower)\n            .drop('unnamed: 36', axis=1)\n            .pipe(extract_city_name)\n            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time', 'crs_dep_time'])\n            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date']),\n                    dest=lambda x: pd.Categorical(x['dest']),\n                    origin=lambda x: pd.Categorical(x['origin']),\n                    tail_num=lambda x: pd.Categorical(x['tail_num']),\n                    unique_carrier=lambda x: pd.Categorical(x['unique_carrier']),\n                    cancellation_code=lambda x: pd.Categorical(x['cancellation_code'])))\n    return df\n\ndef extract_city_name(df):\n    '''\n    Chicago, IL -&gt; Chicago for origin_city_name and dest_city_name\n    '''\n    cols = ['origin_city_name', 'dest_city_name']\n    city = df[cols].apply(lambda x: x.str.extract(\"(.*), \\w{2}\", expand=False))\n    df = df.copy()\n    df[['origin_city_name', 'dest_city_name']] = city\n    return df\n\ndef time_to_datetime(df, columns):\n    '''\n    Combine all time items into datetimes.\n\n    2014-01-01,0914 -&gt; 2014-01-01 09:14:00\n    '''\n    df = df.copy()\n    def converter(col):\n        timepart = (col.astype(str)\n                       .str.replace('\\.0$', '')  # NaNs force float dtype\n                       .str.pad(4, fillchar='0'))\n        return pd.to_datetime(df['fl_date'] + ' ' +\n                               timepart.str.slice(0, 2) + ':' +\n                               timepart.str.slice(2, 4),\n                               errors='coerce')\n    df[columns] = df[columns].apply(converter)\n    return df\n\noutput = 'data/flights.h5'\n\nif not os.path.exists(output):\n    df = read(\"data/627361791_T_ONTIME.csv\")\n    df.to_hdf(output, 'flights', format='table')\nelse:\n    df = pd.read_hdf(output, 'flights', format='table')\ndf.info()",
            "code"
        ],
        [
            "&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 450017 entries, 0 to 450016\nData columns (total 33 columns):\nfl_date                  450017 non-null datetime64[ns]\nunique_carrier           450017 non-null category\nairline_id               450017 non-null int64\ntail_num                 449378 non-null category\nfl_num                   450017 non-null int64\norigin_airport_id        450017 non-null int64\norigin_airport_seq_id    450017 non-null int64\norigin_city_market_id    450017 non-null int64\norigin                   450017 non-null category\norigin_city_name         450017 non-null object\ndest_airport_id          450017 non-null int64\ndest_airport_seq_id      450017 non-null int64\ndest_city_market_id      450017 non-null int64\ndest                     450017 non-null category\ndest_city_name           450017 non-null object\ncrs_dep_time             450017 non-null datetime64[ns]\ndep_time                 441445 non-null datetime64[ns]\ndep_delay                441476 non-null float64\ntaxi_out                 441244 non-null float64\nwheels_off               441244 non-null float64\nwheels_on                440746 non-null float64\ntaxi_in                  440746 non-null float64\ncrs_arr_time             450017 non-null datetime64[ns]\narr_time                 440555 non-null datetime64[ns]\narr_delay                439645 non-null float64\ncancelled                450017 non-null float64\ncancellation_code        8886 non-null category\ncarrier_delay            97699 non-null float64\nweather_delay            97699 non-null float64\nnas_delay                97699 non-null float64\nsecurity_delay           97699 non-null float64\nlate_aircraft_delay      97699 non-null float64\nunnamed: 32              0 non-null float64\ndtypes: category(5), datetime64[ns](5), float64(13), int64(8), object(2)\nmemory usage: 103.2+ MB",
            "code"
        ],
        [
            "I find method chains readable, though some people don\u2019t.\nBoth the code and the flow of execution are from top to bottom, and the function parameters are always near the function itself, unlike with heavily nested function calls.",
            "markdown"
        ],
        [
            "My favorite example demonstrating this comes from  (pdf). Compare these two ways of telling the same story:",
            "markdown"
        ],
        [
            "tumble_after(\n    broke(\n        fell_down(\n            fetch(went_up(jack_jill, \"hill\"), \"water\"),\n            jack),\n        \"crown\"),\n    \"jill\"\n)",
            "code"
        ],
        [
            "and",
            "markdown"
        ],
        [
            "jack_jill %&gt;%\n    went_up(\"hill\") %&gt;%\n    fetch(\"water\") %&gt;%\n    fell_down(\"jack\") %&gt;%\n    broke(\"crown\") %&gt;%\n    tumble_after(\"jill\")",
            "code"
        ],
        [
            "Even if you weren\u2019t aware that in R %&gt;% (pronounced <em>pipe</em>) calls the function on the right with the thing on the left as an argument, you can still make out what\u2019s going on. Compare that with the first style, where you need to unravel the code to figure out the order of execution and which arguments are being passed where.",
            "markdown"
        ],
        [
            "Admittedly, you probably wouldn\u2019t write the first one.\nIt\u2019d be something like",
            "markdown"
        ],
        [
            "on_hill = went_up(jack_jill, 'hill')\nwith_water = fetch(on_hill, 'water')\nfallen = fell_down(with_water, 'jack')\nbroken = broke(fallen, 'jack')\nafter = tmple_after(broken, 'jill')",
            "code"
        ],
        [
            "I don\u2019t like this version because I have to spend time coming up with appropriate names for variables.\nThat\u2019s bothersome when we don\u2019t <em>really</em> care about the on_hill variable. We\u2019re just passing it into the next step.",
            "markdown"
        ],
        [
            "A fourth way of writing the same story may be available. Suppose you owned a JackAndJill object, and could define the methods on it. Then you\u2019d have something like R\u2019s %&gt;% example.",
            "markdown"
        ],
        [
            "jack_jill = JackAndJill()\n(jack_jill.went_up('hill')\n    .fetch('water')\n    .fell_down('jack')\n    .broke('crown')\n    .tumble_after('jill')\n)",
            "code"
        ],
        [
            "But the problem is you don\u2019t own the ndarray or DataFrame or , and the exact method you want may not exist.\nMonekypatching on your own methods is fragile.\nIt\u2019s not easy to correctly subclass pandas\u2019 DataFrame to extend it with your own methods.\nComposition, where you create a class that holds onto a DataFrame internally, may be fine for your own code, but it won\u2019t interact well with the rest of the ecosystem so your code will be littered with lines extracting and repacking the underlying DataFrame.",
            "markdown"
        ],
        [
            "Perhaps you could submit a pull request to pandas implementing your method.\nBut then you\u2019d need to convince the maintainers that it\u2019s broadly useful enough to merit its inclusion (and worth their time to maintain it). And DataFrame has something like 250+ methods, so we\u2019re reluctant to add more.",
            "markdown"
        ],
        [
            "Enter . All the benefits of having your specific function as a method on the DataFrame, without us having to maintain it, and without it overloading the already large pandas API. A win for everyone.",
            "markdown"
        ],
        [
            "jack_jill = pd.DataFrame()\n(jack_jill.pipe(went_up, 'hill')\n    .pipe(fetch, 'water')\n    .pipe(fell_down, 'jack')\n    .pipe(broke, 'crown')\n    .pipe(tumble_after, 'jill')\n)",
            "code"
        ],
        [
            "This really is just right-to-left function execution. The first argument to pipe, a callable, is called with the DataFrame on the left as its first argument, and any additional arguments you specify.",
            "markdown"
        ],
        [
            "I hope the analogy to data analysis code is clear.\nCode is read more often than it is written.\nWhen you or your coworkers or research partners have to go back in two months to update your script, having the story of raw data to results be told as clearly as possible will save you time.",
            "markdown"
        ]
    ],
    "Method Chaining->Method Chaining->Costs": [
        [
            "One drawback to excessively long chains is that debugging can be harder.\nIf something looks wrong at the end, you don\u2019t have intermediate values to inspect. There\u2019s a close parallel here to python\u2019s generators. Generators are great for keeping memory consumption down, but they can be hard to debug since values are consumed.",
            "markdown"
        ],
        [
            "For my typical exploratory workflow, this isn\u2019t really a big problem. I\u2019m working with a single dataset that isn\u2019t being updated, and the path from raw data to usuable data isn\u2019t so large that I can\u2019t drop an import pdb; pdb.set_trace() in the middle of my code to poke around.",
            "markdown"
        ],
        [
            "For large workflows, you\u2019ll probably want to move away from pandas to something more structured, like  or .",
            "markdown"
        ],
        [
            "When writing medium sized  jobs in python that will be run repeatedly, I\u2019ll use decorators to inspect and log properties about the DataFrames at each step of the process.",
            "markdown"
        ],
        [
            "from functools import wraps\nimport logging\n\ndef log_shape(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        logging.info(\"%s,%s\" % (func.__name__, result.shape))\n        return result\n    return wrapper\n\ndef log_dtypes(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        logging.info(\"%s,%s\" % (func.__name__, result.dtypes))\n        return result\n    return wrapper\n\n\n@log_shape\n@log_dtypes\ndef load(fp):\n    df = pd.read_csv(fp, index_col=0, parse_dates=True)\n\n@log_shape\n@log_dtypes\ndef update_events(df, new_events):\n    df.loc[new_events.index, 'foo'] = new_events\n    return df",
            "code"
        ],
        [
            "This plays nicely with , a little library I wrote to validate data as it flows through the pipeline (it essentialy turns those logging statements into excpetions if something looks wrong).",
            "markdown"
        ]
    ],
    "Method Chaining->Method Chaining->Inplace?": [
        [
            "Most pandas methods have an inplace keyword that\u2019s False by default.\nIn general, you shouldn\u2019t do inplace operations.",
            "markdown"
        ],
        [
            "First, if you like method chains then you simply can\u2019t use inplace since the return value is None, terminating the chain.",
            "markdown"
        ],
        [
            "Second, I suspect people have a mental model of inplace operations happening, you know, inplace. That is, extra memory doesn\u2019t need to be allocated for the result. .\nQuoting Jeff Reback from that answer<blockquote>",
            "markdown"
        ],
        [
            "Their is <strong>no guarantee</strong> that an inplace operation is actually faster. Often they are actually the same operation that works on a copy, but the top-level reference is reassigned.</blockquote>",
            "markdown"
        ],
        [
            "That is, the pandas code might look something like this",
            "markdown"
        ],
        [
            "def dataframe_method(self, inplace=False):\n    data = self.copy()  # regardless of inplace\n    result = ...\n    if inplace:\n        self._update_inplace(data)\n    else:\n        return result",
            "code"
        ],
        [
            "There\u2019s a lot of defensive copying in pandas.\nPart of this comes down to pandas being built on top of NumPy, and not having full control over how memory is handled and shared.\nWe saw it above when we defined our own functions extract_city_name and time_to_datetime.\nWithout the copy, adding the columns would modify the input DataFrame, which just isn\u2019t polite.",
            "markdown"
        ],
        [
            "Finally, inplace operations don\u2019t make sense in projects like  or , where you\u2019re manipulating expressions or building up a DAG of tasks to be executed, rather than manipulating the data directly.",
            "markdown"
        ]
    ],
    "Method Chaining->Method Chaining->Application": [
        [
            "I feel like we haven\u2019t done much coding, mostly just me shouting from the top of a soapbox (sorry about that).\nLet\u2019s do some exploratory analysis.",
            "markdown"
        ],
        [
            "What does the daily flight pattern look like?",
            "markdown"
        ],
        [
            "(df.dropna(subset=['dep_time', 'unique_carrier'])\n   .loc[df['unique_carrier']\n       .isin(df['unique_carrier'].value_counts().index[:5])]\n   .set_index('dep_time')\n   # TimeGrouper to resample &amp; groupby at once\n   .groupby(['unique_carrier', pd.TimeGrouper(\"H\")])\n   .fl_num.count()\n   .unstack(0)\n   .fillna(0)\n   .rolling(24)\n   .sum()\n   .rename_axis(\"Flights per Day\", axis=1)\n   .plot()\n)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_2_method_chaining_8_0.png\"/>",
            "markdown"
        ],
        [
            "import statsmodels.api as sm",
            "code"
        ],
        [
            "/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools",
            "code"
        ],
        [
            "Does a plane with multiple flights on the same day get backed up, causing later flights to be delayed more?",
            "markdown"
        ],
        [
            "%config InlineBackend.figure_format = 'png'\nflights = (df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n           .dropna()\n           .sort_values('dep_time')\n           .loc[lambda x: x.dep_delay &lt; 500]\n           .assign(turn = lambda x:\n                x.groupby(['fl_date', 'tail_num'])\n                 .dep_time\n                 .transform('rank').astype(int)))\n\nfig, ax = plt.subplots(figsize=(15, 5))\nsns.boxplot(x='turn', y='dep_delay', data=flights, ax=ax)\nax.set_ylim(-50, 50)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_2_method_chaining_11_0.png\"/>",
            "markdown"
        ],
        [
            "Doesn\u2019t really look like it. Maybe other planes are swapped in when one gets delayed,\nbut we don\u2019t have data on <em>scheduled</em> flights per plane.",
            "markdown"
        ],
        [
            "Do flights later in the day have longer delays?",
            "markdown"
        ],
        [
            "plt.figure(figsize=(15, 5))\n(df[['fl_date', 'tail_num', 'dep_time', 'dep_delay']]\n    .dropna()\n    .assign(hour=lambda x: x.dep_time.dt.hour)\n    .query('5 &lt; dep_delay &lt; 600')\n    .pipe((sns.boxplot, 'data'), 'hour', 'dep_delay'))\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_2_method_chaining_13_0.png\"/>",
            "markdown"
        ],
        [
            "There could be something here. I didn\u2019t show it here since I filtered them out,\nbut the vast majority of flights do leave on time.",
            "markdown"
        ],
        [
            "Thanks for reading!\nThis section was a bit more abstract, since we were talking about styles\nof coding rather than how to actually accomplish tasks.\nI\u2019m sometimes guilty of putting too much work into making my data wrangling code look nice and feel correct, at the expense of actually analyzing the data.\nThis isn\u2019t a competition to have the best or cleanest pandas code; pandas is always just a means to the end that is your research or business problem.\nThanks for indulging me.\nNext time we\u2019ll talk about a much more practical topic: performance.",
            "markdown"
        ]
    ],
    "Indexes": [
        [
            "This is part 3 in my series on writing modern idiomatic pandas.",
            "markdown"
        ],
        [
            "Indexes can be a difficult concept to grasp at first.\nI suspect this is partly becuase they\u2019re somewhat peculiar to pandas.\nThese aren\u2019t like the indexes put on relational database tables for performance optimizations.\nRather, they\u2019re more like the row_labels of an R DataFrame, but much more capable.",
            "markdown"
        ],
        [
            "Indexes offermetadata containereasy label-based row selectioneasy label-based alignment in operationslabel-based concatenation",
            "markdown"
        ],
        [
            "To demonstrate these, we\u2019ll first fetch some more data.\nThis will be weather data from sensors at a bunch of airports across the US.\nSee  for the example scraper I based this off of.",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport json\nimport glob\nimport datetime\nfrom io import StringIO\n\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('ticks')\n\n# States are broken into networks. The networks have a list of ids, each representing a station.\n# We will take that list of ids and pass them as query parameters to the URL we built up ealier.\nstates = \"\"\"AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME\n MI MN MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT\n WA WI WV WY\"\"\".split()\n\n# IEM has Iowa AWOS sites in its own labeled network\nnetworks = ['AWOS'] + ['{}_ASOS'.format(state) for state in states]",
            "code"
        ],
        [
            "def get_weather(stations, start=pd.Timestamp('2014-01-01'),\n                end=pd.Timestamp('2014-01-31')):\n    '''\n    Fetch weather data from MESONet between ``start`` and ``stop``.\n    '''\n    url = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n           \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n           \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n           \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n           \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\n    stations = \"&amp;\".join(\"station=%s\" % s for s in stations)\n    weather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                           comment=\"#\")\n                 .rename(columns={\"valid\": \"date\"})\n                 .rename(columns=str.strip)\n                 .assign(date=lambda df: pd.to_datetime(df['date']))\n                 .set_index([\"station\", \"date\"])\n                 .sort_index())\n    float_cols = ['tmpf', 'relh', 'sped', 'mslp', 'p01i', 'vsby', \"gust_mph\"]\n    weather[float_cols] = weather[float_cols].apply(pd.to_numeric, errors=\"corce\")\n    return weather",
            "code"
        ],
        [
            "def get_ids(network):\n    url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\n    r = requests.get(url.format(network))\n    md = pd.io.json.json_normalize(r.json()['features'])\n    md['network'] = network\n    return md",
            "code"
        ],
        [
            "Talk briefly about the gem of a method that is json_normalize.",
            "markdown"
        ],
        [
            "url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\nr = requests.get(url.format(\"AWOS\"))\njs = r.json()",
            "code"
        ],
        [
            "js['features'][:2]",
            "code"
        ],
        [
            "[{'geometry': {'coordinates': [-94.2723694444, 43.0796472222],\n   'type': 'Point'},\n  'id': 'AXA',\n  'properties': {'sid': 'AXA', 'sname': 'ALGONA'},\n  'type': 'Feature'},\n {'geometry': {'coordinates': [-93.569475, 41.6878083333], 'type': 'Point'},\n  'id': 'IKV',\n  'properties': {'sid': 'IKV', 'sname': 'ANKENY'},\n  'type': 'Feature'}]",
            "code"
        ],
        [
            "pd.DataFrame(js['features']).head().to_html()",
            "code"
        ],
        [
            "js['features'][0]\n{\n    'geometry': {\n        'coordinates': [-94.2723694444, 43.0796472222],\n        'type': 'Point'\n    },\n    'id': 'AXA',\n    'properties': {\n        'sid': 'AXA',\n        'sname': 'ALGONA'\n    },\n    'type': 'Feature'\n}",
            "code"
        ],
        [
            "js['features']\n\n[{'geometry': {'coordinates': [-94.2723694444, 43.0796472222],\n  'type': 'Point'},\n  'id': 'AXA',\n  'properties': {'sid': 'AXA', 'sname': 'ALGONA'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.569475, 41.6878083333], 'type': 'Point'},\n  'id': 'IKV',\n  'properties': {'sid': 'IKV', 'sname': 'ANKENY'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.0465277778, 41.4058805556],\n  'type': 'Point'},\n  'id': 'AIO',\n  'properties': {'sid': 'AIO', 'sname': 'ATLANTIC'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.9204416667, 41.6993527778],\n  'type': 'Point'},\n  'id': 'ADU',\n  'properties': {'sid': 'ADU', 'sname': 'AUDUBON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.848575, 42.0485694444], 'type': 'Point'},\n  'id': 'BNW',\n  'properties': {'sid': 'BNW', 'sname': 'BOONE MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.7888805556, 42.0443611111],\n  'type': 'Point'},\n  'id': 'CIN',\n  'properties': {'sid': 'CIN', 'sname': 'CARROLL'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.8983388889, 40.6831805556],\n  'type': 'Point'},\n  'id': 'TVK',\n  'properties': {'sid': 'TVK', 'sname': 'Centerville'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.3607694444, 41.0184305556],\n  'type': 'Point'},\n  'id': 'CNC',\n  'properties': {'sid': 'CNC', 'sname': 'CHARITON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.6132222222, 43.0730055556],\n  'type': 'Point'},\n  'id': 'CCY',\n  'properties': {'sid': 'CCY', 'sname': 'CHARLES CITY'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.553775, 42.7304194444], 'type': 'Point'},\n  'id': 'CKP',\n  'properties': {'sid': 'CKP', 'sname': 'Cherokee'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.0222722222, 40.7241527778],\n  'type': 'Point'},\n  'id': 'ICL',\n  'properties': {'sid': 'ICL', 'sname': 'CLARINDA'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.7592583333, 42.7430416667],\n  'type': 'Point'},\n  'id': 'CAV',\n  'properties': {'sid': 'CAV', 'sname': 'CLARION'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-90.332796, 41.829504], 'type': 'Point'},\n  'id': 'CWI',\n  'properties': {'sid': 'CWI', 'sname': 'CLINTON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.7604083333, 41.2611111111],\n  'type': 'Point'},\n  'id': 'CBF',\n  'properties': {'sid': 'CBF', 'sname': 'COUNCIL BLUFFS'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.3607972222, 41.0187888889],\n  'type': 'Point'},\n  'id': 'CSQ',\n  'properties': {'sid': 'CSQ', 'sname': 'CRESTON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.7433138889, 43.2755194444],\n  'type': 'Point'},\n  'id': 'DEH',\n  'properties': {'sid': 'DEH', 'sname': 'DECORAH'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.3799888889, 41.9841944444],\n  'type': 'Point'},\n  'id': 'DNS',\n  'properties': {'sid': 'DNS', 'sname': 'DENISON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.9834111111, 41.0520888889],\n  'type': 'Point'},\n  'id': 'FFL',\n  'properties': {'sid': 'FFL', 'sname': 'FAIRFIELD'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.6236694444, 43.2323166667],\n  'type': 'Point'},\n  'id': 'FXY',\n  'properties': {'sid': 'FXY', 'sname': 'Forest City'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.203203, 42.549741], 'type': 'Point'},\n  'id': 'FOD',\n  'properties': {'sid': 'FOD', 'sname': 'FORT DODGE'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.3267166667, 40.6614833333],\n  'type': 'Point'},\n  'id': 'FSW',\n  'properties': {'sid': 'FSW', 'sname': 'FORT MADISON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.7331972222, 41.7097305556],\n  'type': 'Point'},\n  'id': 'GGI',\n  'properties': {'sid': 'GGI', 'sname': 'Grinnell'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.3354555556, 41.5834194444],\n  'type': 'Point'},\n  'id': 'HNR',\n  'properties': {'sid': 'HNR', 'sname': 'HARLAN'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.9504, 42.4544277778], 'type': 'Point'},\n  'id': 'IIB',\n  'properties': {'sid': 'IIB', 'sname': 'INDEPENDENCE'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.2650805556, 42.4690972222],\n  'type': 'Point'},\n  'id': 'IFA',\n  'properties': {'sid': 'IFA', 'sname': 'Iowa Falls'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.4273916667, 40.4614611111],\n  'type': 'Point'},\n  'id': 'EOK',\n  'properties': {'sid': 'EOK', 'sname': 'KEOKUK MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.1113916667, 41.2984472222],\n  'type': 'Point'},\n  'id': 'OXV',\n  'properties': {'sid': 'OXV', 'sname': 'Knoxville'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-96.19225, 42.775375], 'type': 'Point'},\n  'id': 'LRJ',\n  'properties': {'sid': 'LRJ', 'sname': 'LE MARS'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.1604555556, 42.2203611111],\n  'type': 'Point'},\n  'id': 'MXO',\n  'properties': {'sid': 'MXO', 'sname': 'MONTICELLO MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.5122277778, 40.9452527778],\n  'type': 'Point'},\n  'id': 'MPZ',\n  'properties': {'sid': 'MPZ', 'sname': 'MOUNT PLEASANT'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.140575, 41.3669944444], 'type': 'Point'},\n  'id': 'MUT',\n  'properties': {'sid': 'MUT', 'sname': 'MUSCATINE'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.0190416667, 41.6701111111],\n  'type': 'Point'},\n  'id': 'TNU',\n  'properties': {'sid': 'TNU', 'sname': 'NEWTON MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.9759888889, 42.6831388889],\n  'type': 'Point'},\n  'id': 'OLZ',\n  'properties': {'sid': 'OLZ', 'sname': 'OELWEIN'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-96.0605861111, 42.9894916667],\n  'type': 'Point'},\n  'id': 'ORC',\n  'properties': {'sid': 'ORC', 'sname': 'Orange City'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.6876138889, 41.0471722222],\n  'type': 'Point'},\n  'id': 'I75',\n  'properties': {'sid': 'I75', 'sname': 'Osceola'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.4918666667, 41.227275], 'type': 'Point'},\n  'id': 'OOA',\n  'properties': {'sid': 'OOA', 'sname': 'Oskaloosa'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.9431083333, 41.3989138889],\n  'type': 'Point'},\n  'id': 'PEA',\n  'properties': {'sid': 'PEA', 'sname': 'PELLA'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-94.1637083333, 41.8277916667],\n  'type': 'Point'},\n  'id': 'PRO',\n  'properties': {'sid': 'PRO', 'sname': 'Perry'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.2624111111, 41.01065], 'type': 'Point'},\n  'id': 'RDK',\n  'properties': {'sid': 'RDK', 'sname': 'RED OAK'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.8353138889, 43.2081611111],\n  'type': 'Point'},\n  'id': 'SHL',\n  'properties': {'sid': 'SHL', 'sname': 'SHELDON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.4112333333, 40.753275], 'type': 'Point'},\n  'id': 'SDA',\n  'properties': {'sid': 'SDA', 'sname': 'SHENANDOAH MUNI'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-95.2399194444, 42.5972277778],\n  'type': 'Point'},\n  'id': 'SLB',\n  'properties': {'sid': 'SLB', 'sname': 'Storm Lake'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-92.0248416667, 42.2175777778],\n  'type': 'Point'},\n  'id': 'VTI',\n  'properties': {'sid': 'VTI', 'sname': 'VINTON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-91.6748111111, 41.2751444444],\n  'type': 'Point'},\n  'id': 'AWG',\n  'properties': {'sid': 'AWG', 'sname': 'WASHINGTON'},\n  'type': 'Feature'},\n{'geometry': {'coordinates': [-93.8690777778, 42.4392305556],\n  'type': 'Point'},\n  'id': 'EBS',\n  'properties': {'sid': 'EBS', 'sname': 'Webster City'},\n  'type': 'Feature'}]",
            "code"
        ],
        [
            "stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))",
            "code"
        ],
        [
            "import os\nids = pd.concat([get_ids(network) for network in networks], ignore_index=True)\ngr = ids.groupby('network')\n\nos.makedirs(\"weather\", exist_ok=True)\n\nfor i, (k, v) in enumerate(gr):\n    print(\"{}/{}\".format(i, len(network)), end='\\r')\n    weather = get_weather(v['id'])\n    weather.to_csv(\"weather/{}.csv\".format(k))\n\nweather = pd.concat([\n    pd.read_csv(f, parse_dates='date', index_col=['station', 'date'])\n    for f in glob.glob('weather/*.csv')])\n\nweather.to_hdf(\"weather.h5\", \"weather\")",
            "code"
        ],
        [
            "weather = pd.read_hdf(\"weather.h5\", \"weather\").sort_index()\n\nweather.head()",
            "code"
        ],
        [
            "OK, that was a bit of work. Here\u2019s a plot to reward ourselves.",
            "markdown"
        ],
        [
            "airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.png');",
            "code"
        ],
        [
            "airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.svg', transparent=True);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"Indexes_files/Indexes_18_0.png\"/>",
            "markdown"
        ]
    ],
    "Indexes->Set Operations": [
        [
            "Indexes are set-like (technically <em>multi</em>sets, since you can have duplicates), so they support most python set operations. Indexes are immutable so you won\u2019t find any of the inplace set operations.\nOne other difference is that since Indexes are also array like, you can\u2019t use some infix operators like - for difference. If you have a numeric index it is unclear whether you intend to perform math operations or set operations.\nYou can use &amp; for intersetion, | for union, and ^ for symmetric difference though, since there\u2019s no ambiguity.",
            "markdown"
        ],
        [
            "For example, lets find the set of airports that we have weather and flight information on. Since weather had a MultiIndex of airport,datetime, we\u2019ll use the levels attribute to get at the airport data, separate from the date data.",
            "markdown"
        ],
        [
            "# Bring in the flights data\n\nflights = pd.read_hdf('flights.h5', 'flights')\n\nweather_locs = weather.index.levels[0]\n# The `categories` attribute of a Categorical is an Index\norigin_locs = flights.origin.cat.categories\ndest_locs = flights.dest.cat.categories\n\nairports = weather_locs &amp; origin_locs &amp; dest_locs\nairports",
            "code"
        ],
        [
            "Index(['ABE', 'ABI', 'ABQ', 'ABR', 'ABY', 'ACT', 'ACV', 'AEX', 'AGS', 'ALB',\n       ...\n       'TUL', 'TUS', 'TVC', 'TWF', 'TXK', 'TYR', 'TYS', 'VLD', 'VPS', 'XNA'],\n      dtype='object', length=267)",
            "code"
        ],
        [
            "print(\"Weather, no flights:\\n\\t\", weather_locs.difference(origin_locs | dest_locs), end='\\n\\n')\n\nprint(\"Flights, no weather:\\n\\t\", (origin_locs | dest_locs).difference(weather_locs), end='\\n\\n')\n\nprint(\"Dropped Stations:\\n\\t\", (origin_locs | dest_locs) ^ weather_locs)",
            "code"
        ],
        [
            "Weather, no flights:\n\t Index(['01M', '04V', '04W', '05U', '06D', '08D', '0A9', '0CO', '0E0', '0F2',\n       ...\n       'Y50', 'Y51', 'Y63', 'Y70', 'YIP', 'YKM', 'YKN', 'YNG', 'ZPH', 'ZZV'],\n      dtype='object', length=1909)\n\nFlights, no weather:\n\t Index(['ADK', 'ADQ', 'ANC', 'BET', 'BKG', 'BQN', 'BRW', 'CDV', 'CLD', 'FAI',\n       'FCA', 'GUM', 'HNL', 'ITO', 'JNU', 'KOA', 'KTN', 'LIH', 'MQT', 'OGG',\n       'OME', 'OTZ', 'PPG', 'PSE', 'PSG', 'SCC', 'SCE', 'SIT', 'SJU', 'STT',\n       'STX', 'WRG', 'YAK', 'YUM'],\n      dtype='object')\n\nDropped Stations:\n\t Index(['01M', '04V', '04W', '05U', '06D', '08D', '0A9', '0CO', '0E0', '0F2',\n       ...\n       'Y63', 'Y70', 'YAK', 'YIP', 'YKM', 'YKN', 'YNG', 'YUM', 'ZPH', 'ZZV'],\n      dtype='object', length=1943)",
            "code"
        ]
    ],
    "Indexes->Flavors": [
        [
            "Pandas has many subclasses of the regular Index, each tailored to a specific kind of data.\nMost of the time these will be created for you automatically, so you don\u2019t have to worry about which one to choose.Int64IndexRangeIndex (Memory-saving special case of Int64Index)FloatIndexDatetimeIndex: Datetime64[ns] precision dataPeriodIndex: Regularly-spaced, arbitrary precision datetime data.TimedeltaIndex: Timedelta dataCategoricalIndex:",
            "markdown"
        ],
        [
            "Some of these are purely optimizations, others use information about the data to provide additional methods.\nAnd while sometimes you might work with indexes directly (like the set operations above), most of they time you\u2019ll be operating on a Series or DataFrame, which in turn makes use of its Index.",
            "markdown"
        ]
    ],
    "Indexes->Flavors->Row Slicing": [
        [
            "We saw in part one that they\u2019re great for making <em>row</em> subsetting as easy as column subsetting.",
            "markdown"
        ],
        [
            "weather.loc['DSM'].head()",
            "code"
        ],
        [
            "Without indexes we\u2019d probably resort to boolean masks.",
            "markdown"
        ],
        [
            "weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "code"
        ],
        [
            "Slightly less convenient, but still doable.",
            "markdown"
        ]
    ],
    "Indexes->Flavors->Indexes for Easier Arithmetic, Analysis": [
        [
            "It\u2019s nice to have your metadata (labels on each observation) next to you actual values. But if you store them in an array, they\u2019ll get in the way. Say we wanted to translate the farenheit temperature to celcius.",
            "markdown"
        ],
        [
            "# With indecies\ntemp = weather['tmpf']\n\nc = (temp - 32) * 5 / 9\nc.to_frame()",
            "code"
        ],
        [
            "3303647 rows \u00d7 1 columns",
            "markdown"
        ],
        [
            "# without\ntemp2 = weather.reset_index()[['station', 'date', 'tmpf']]\n\ntemp2['tmpf'] = (temp2['tmpf'] - 32) * 5 / 9\ntemp2.head()",
            "code"
        ],
        [
            "Again, not terrible, but not as good.\nAnd, what if you had wanted to keep farenheit around as well, instead of overwriting it like we did?\nThen you\u2019d need to make a copy of everything, including the station and date columns.\nWe don\u2019t have that problem, since indexes are mutable and safely shared between DataFrames / Series.",
            "markdown"
        ],
        [
            "temp.index is c.index",
            "code"
        ],
        [
            "True",
            "code"
        ]
    ],
    "Indexes->Flavors->Indexes for Alignment": [
        [
            "I\u2019ve saved the best for last.\nAutomatic alignment, or reindexing, is fundamental to pandas.",
            "markdown"
        ],
        [
            "All binary operations (add, multiply, etc&amp;mldr;) between Series/DataFrames first <em>align</em> and then proceed.",
            "markdown"
        ],
        [
            "Let\u2019s suppose we have hourly observations on temperature and windspeed.\nAnd suppose some of the observations were invalid, and not reported (simulated below by sampling from the full dataset). We\u2019ll assume the missing windspeed observations were potentially different from the missing temperature observations.",
            "markdown"
        ],
        [
            "dsm = weather.loc['DSM']\n\nhourly = dsm.resample('H').mean()\n\ntemp = hourly['tmpf'].sample(frac=.5, random_state=1).sort_index()\nsped = hourly['sped'].sample(frac=.5, random_state=2).sort_index()",
            "code"
        ],
        [
            "temp.head().to_frame()",
            "code"
        ],
        [
            "sped.head()",
            "code"
        ],
        [
            "date\n2014-01-01 01:00:00    11.4\n2014-01-01 02:00:00     8.0\n2014-01-01 03:00:00     9.1\n2014-01-01 04:00:00     9.1\n2014-01-01 05:00:00    10.3\nName: sped, dtype: float64",
            "code"
        ],
        [
            "Notice that the two indexes aren\u2019t identical.",
            "markdown"
        ],
        [
            "Suppose that the windspeed : temperature ratio is meaningful.\nWhen we go to compute that, pandas will automatically align the two by index label.",
            "markdown"
        ],
        [
            "sped / temp",
            "code"
        ],
        [
            "date\n2014-01-01 00:00:00         NaN\n2014-01-01 01:00:00         NaN\n2014-01-01 02:00:00    0.731261\n2014-01-01 03:00:00    0.831810\n2014-01-01 04:00:00    0.906375\n                         ...   \n2014-01-30 13:00:00         NaN\n2014-01-30 14:00:00    0.584712\n2014-01-30 17:00:00         NaN\n2014-01-30 21:00:00         NaN\n2014-01-30 23:00:00         NaN\ndtype: float64",
            "code"
        ],
        [
            "This lets you focus on doing the operation, rather than manually aligning things, ensuring that the arrays are the same length and in the same order.\nBy deault, missing values are inserted where the two don\u2019t align.\nYou can use the method version of any binary operation to specify a fill_value",
            "markdown"
        ],
        [
            "sped.div(temp, fill_value=1)",
            "code"
        ],
        [
            "date\n2014-01-01 00:00:00     0.091408\n2014-01-01 01:00:00    11.400000\n2014-01-01 02:00:00     0.731261\n2014-01-01 03:00:00     0.831810\n2014-01-01 04:00:00     0.906375\n                         ...    \n2014-01-30 13:00:00     0.027809\n2014-01-30 14:00:00     0.584712\n2014-01-30 17:00:00     0.023267\n2014-01-30 21:00:00     0.035663\n2014-01-30 23:00:00    13.700000\ndtype: float64",
            "code"
        ],
        [
            "And since I couldn\u2019t find anywhere else to put it, you can control the axis the operation is aligned along as well.",
            "markdown"
        ],
        [
            "hourly.div(sped, axis='index')",
            "code"
        ],
        [
            "720 rows \u00d7 7 columns",
            "markdown"
        ],
        [
            "The non row-labeled version of this is messy.",
            "markdown"
        ],
        [
            "temp2 = temp.reset_index()\nsped2 = sped.reset_index()\n\n# Find rows where the operation is defined\ncommon_dates = pd.Index(temp2.date) &amp; sped2.date\npd.concat([\n    # concat to not lose date information\n    sped2.loc[sped2['date'].isin(common_dates), 'date'],\n    (sped2.loc[sped2.date.isin(common_dates), 'sped'] /\n     temp2.loc[temp2.date.isin(common_dates), 'tmpf'])],\n    axis=1).dropna(how='all')",
            "code"
        ],
        [
            "170 rows \u00d7 2 columns",
            "markdown"
        ],
        [
            "Yeah, I prefer the temp / sped version.",
            "markdown"
        ],
        [
            "Alignment isn\u2019t limited to arithmetic operations, although those are the most obvious and easiest to demonstrate.",
            "markdown"
        ]
    ],
    "Indexes->Merging": [
        [
            "There are two ways of merging DataFrames / Series in pandasRelational Database style with pd.mergeArray style with pd.concat",
            "markdown"
        ],
        [
            "Personally, I think in terms of the concat style.\nI learned pandas before I ever really used SQL, so it comes more naturally to me I suppose.\npd.merge has more flexibilty, though I think <em>most</em> of the time you don\u2019t need this flexibilty.",
            "markdown"
        ]
    ],
    "Indexes->Merging->Concat Version": [
        [
            "pd.concat([temp, sped], axis=1).head()",
            "code"
        ],
        [
            "The axis parameter controls how the data should be stacked, 0 for vertically, 1 for horizontally.\nThe join parameter controls the merge behavior on the shared axis, (the Index for axis=1). By default it\u2019s like a union of the two indexes, or an outer join.",
            "markdown"
        ],
        [
            "pd.concat([temp, sped], axis=1, join='inner')",
            "code"
        ],
        [
            "170 rows \u00d7 2 columns",
            "markdown"
        ]
    ],
    "Indexes->Merging->Merge Version": [
        [
            "Since we\u2019re joining by index here the merge version is quite similar.\nWe\u2019ll see an example later of a one-to-many join where the two differ.",
            "markdown"
        ],
        [
            "pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True).head()",
            "code"
        ],
        [
            "pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True,\n         how='outer').head()",
            "code"
        ],
        [
            "Like I said, I typically prefer concat to merge.\nThe exception here is one-to-many type joins. Let\u2019s walk through one of those,\nwhere we join the flight data to the weather data.\nTo focus just on the merge, we\u2019ll aggregate hour weather data to be daily, rather than trying to find the closest recorded weather observation to each departure (you could do that, but it\u2019s not the focus right now). We\u2019ll then join the one (airport, date) record to the many (airport, date, flight) records.",
            "markdown"
        ],
        [
            "Quick tangent, to get the weather data to daily frequency, we\u2019ll need to resample (more on that in the timeseries section). The resample essentially involves breaking the recorded values into daily buckets and computing the aggregation function on each bucket. The only wrinkle is that we have to resample <em>by station</em>, so we\u2019ll use the pd.TimeGrouper helper.",
            "markdown"
        ],
        [
            "idx_cols = ['unique_carrier', 'origin', 'dest', 'tail_num', 'fl_num', 'fl_date']\ndata_cols = ['crs_dep_time', 'dep_delay', 'crs_arr_time', 'arr_delay',\n             'taxi_out', 'taxi_in', 'wheels_off', 'wheels_on', 'distance']\n\ndf = flights.set_index(idx_cols)[data_cols].sort_index()",
            "code"
        ],
        [
            "def mode(x):\n    '''\n    Arbitrarily break ties.\n    '''\n    return x.value_counts().index[0]\n\naggfuncs = {'tmpf': 'mean', 'relh': 'mean',\n            'sped': 'mean', 'mslp': 'mean',\n            'p01i': 'mean', 'vsby': 'mean',\n            'gust_mph': 'mean', 'skyc1': mode,\n            'skyc2': mode, 'skyc3': mode}\n# TimeGrouper works on a DatetimeIndex, so we move `station` to the\n# columns and then groupby it as well.\ndaily = (weather.reset_index(level=\"station\")\n                .groupby([pd.TimeGrouper('1d'), \"station\"])\n                .agg(aggfuncs))\n\ndaily.head()",
            "code"
        ]
    ],
    "Indexes->Merging->The merge version": [
        [
            "m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "code"
        ],
        [
            "5 rows \u00d7 40 columns",
            "markdown"
        ],
        [
            "m.sample(n=10000).pipe((sns.jointplot, 'data'), 'sped', 'dep_delay')\nplt.savefig('../content/images/indexes_sped_delay_join.svg', transparent=True)",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"Indexes_files/Indexes_63_0.png\"/>",
            "markdown"
        ],
        [
            "m.groupby('skyc1').dep_delay.agg(['mean', 'count']).sort_values(by='mean')",
            "code"
        ],
        [
            "import statsmodels.api as sm",
            "code"
        ],
        [
            "mod = sm.OLS.from_formula('dep_delay ~ C(skyc1) + distance + tmpf + relh + sped + mslp', data=m)\nres = mod.fit()\nres.summary()",
            "code"
        ],
        [
            "fig, ax = plt.subplots()\nax.scatter(res.fittedvalues, res.resid, color='k', marker='.', alpha=.25)\nax.set(xlabel='Predicted', ylabel='Residual')\nsns.despine()\nplt.savefig('../content/images/indexes_resid_fit.png', transparent=True)",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"Indexes_files/Indexes_67_0.png\"/>",
            "markdown"
        ],
        [
            "weather.head()",
            "code"
        ],
        [
            "import numpy as np\nimport pandas as pd\n\n\ndef read(fp):\n    df = (pd.read_csv(fp)\n            .rename(columns=str.lower)\n            .drop('unnamed: 36', axis=1)\n            .pipe(extract_city_name)\n            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time', 'crs_dep_time'])\n            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date']),\n                    dest=lambda x: pd.Categorical(x['dest']),\n                    origin=lambda x: pd.Categorical(x['origin']),\n                    tail_num=lambda x: pd.Categorical(x['tail_num']),\n                    unique_carrier=lambda x: pd.Categorical(x['unique_carrier']),\n                    cancellation_code=lambda x: pd.Categorical(x['cancellation_code'])))\n    return df\n\ndef extract_city_name(df):\n    '''\n    Chicago, IL -&gt; Chicago for origin_city_name and dest_city_name\n    '''\n    cols = ['origin_city_name', 'dest_city_name']\n    city = df[cols].apply(lambda x: x.str.extract(\"(.*), \\w{2}\", expand=False))\n    df = df.copy()\n    df[['origin_city_name', 'dest_city_name']] = city\n    return df\n\ndef time_to_datetime(df, columns):\n    '''\n    Combine all time items into datetimes.\n    \n    2014-01-01,0914 -&gt; 2014-01-01 09:14:00\n    '''\n    df = df.copy()\n    def converter(col):\n        timepart = (col.astype(str)\n                       .str.replace('\\.0$', '')  # NaNs force float dtype\n                       .str.pad(4, fillchar='0'))\n        return  pd.to_datetime(df['fl_date'] + ' ' +\n                               timepart.str.slice(0, 2) + ':' +\n                               timepart.str.slice(2, 4),\n                               errors='coerce')\n        return datetime_part\n    df[columns] = df[columns].apply(converter)\n    return df\n\n\nflights = read(\"878167309_T_ONTIME.csv\")",
            "code"
        ],
        [
            "locs = weather.index.levels[0] &amp; flights.origin.unique()",
            "code"
        ],
        [
            "(weather.reset_index(level='station')\n .query('station in @locs')\n .groupby(['station', pd.TimeGrouper('H')])).mean()",
            "code"
        ],
        [
            "191445 rows \u00d7 7 columns",
            "markdown"
        ],
        [
            "df = (flights.copy()[['unique_carrier', 'tail_num', 'origin', 'dep_time']]\n      .query('origin in @locs'))",
            "code"
        ],
        [
            "",
            "code"
        ],
        [
            "weather.loc['DSM']",
            "code"
        ],
        [
            "896 rows \u00d7 10 columns",
            "markdown"
        ],
        [
            "df = df",
            "code"
        ],
        [
            "471949 rows \u00d7 36 columns",
            "markdown"
        ],
        [
            "dep.head()",
            "code"
        ],
        [
            "0        2014-01-01 09:14:00\n1        2014-01-01 11:32:00\n2        2014-01-01 11:57:00\n3        2014-01-01 13:07:00\n4        2014-01-01 17:53:00\n                 ...        \n163906   2014-01-11 16:57:00\n163910   2014-01-11 11:04:00\n181062   2014-01-12 17:02:00\n199092   2014-01-13 23:36:00\n239150   2014-01-16 16:46:00\nName: dep_time, dtype: datetime64[ns]",
            "code"
        ],
        [
            "flights.dep_time",
            "code"
        ],
        [
            "0        2014-01-01 09:14:00\n1        2014-01-01 11:32:00\n2        2014-01-01 11:57:00\n3        2014-01-01 13:07:00\n4        2014-01-01 17:53:00\n                 ...        \n471944   2014-01-31 09:05:00\n471945   2014-01-31 09:24:00\n471946   2014-01-31 10:39:00\n471947   2014-01-31 09:28:00\n471948   2014-01-31 11:22:00\nName: dep_time, dtype: datetime64[ns]",
            "code"
        ],
        [
            "flights.dep_time.unique()",
            "code"
        ],
        [
            "array(['2014-01-01T03:14:00.000000000-0600',\n       '2014-01-01T05:32:00.000000000-0600',\n       '2014-01-01T05:57:00.000000000-0600', ...,\n       '2014-01-30T18:44:00.000000000-0600',\n       '2014-01-31T17:16:00.000000000-0600',\n       '2014-01-30T18:47:00.000000000-0600'], dtype='datetime64[ns]')",
            "code"
        ],
        [
            "stations",
            "code"
        ],
        [
            "flights.dep_time.head()",
            "code"
        ],
        [
            "0   2014-01-01 09:14:00\n1   2014-01-01 11:32:00\n2   2014-01-01 11:57:00\n3   2014-01-01 13:07:00\n4   2014-01-01 17:53:00\nName: dep_time, dtype: datetime64[ns]",
            "code"
        ]
    ],
    "Fast Pandas": [
        [
            "This is part 4 in my series on writing modern idiomatic pandas.",
            "markdown"
        ],
        [
            ", the creator of pandas, is kind of obsessed with performance. From micro-optimizations for element access, to  a fast hash table inside pandas, we all benefit from his and others\u2019 hard work.\nThis post will focus mainly on making efficient use of pandas and NumPy.",
            "markdown"
        ],
        [
            "One thing I\u2019ll explicitly not touch on is storage formats.\nPerformance is just one of many factors that go into choosing a storage format.\nJust know that pandas can talk to , and the format that strikes the right balance between performance, portability, data-types, metadata handling, etc., is an  topic of discussion.",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\nsns.set_style('ticks')\nsns.set_context('talk')\npd.options.display.max_rows = 10",
            "code"
        ]
    ],
    "Fast Pandas->Constructors": [
        [
            "It\u2019s pretty common to have many similar sources (say a bunch of CSVs) that need to be combined into a single DataFrame. There are two routes to the same end:Initialize one DataFrame and append to thatMake many smaller DataFrames and concatenate at the end",
            "markdown"
        ],
        [
            "For pandas, the second option is faster.\nDataFrame appends are expensive relative to a list append.\nDepending on the values, pandas might have to recast the data to a different type.\nAnd indexes are immutable, so each time you append pandas has to create an entirely new one.",
            "markdown"
        ],
        [
            "In the last section we downloaded a bunch of weather files, one per state, writing each to a separate CSV.\nOne could imagine coming back later to read them in, using the following code.",
            "markdown"
        ],
        [
            "The idiomatic python way",
            "markdown"
        ],
        [
            "files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "code"
        ],
        [
            "This is pretty standard code, quite similar to building up a list of tuples, say.\nThe only nitpick is that you\u2019d probably use a list-comprehension if you were just making a list.\nBut we don\u2019t have special syntax for DataFrame-comprehensions (if only), so you\u2019d fall back to the \u201cinitialize empty container, append to said container\u201d pattern.",
            "markdown"
        ],
        [
            "But there\u2019s a better, pandorable, way",
            "markdown"
        ],
        [
            "files = glob.glob('weather/*.csv')\nweather_dfs = [pd.read_csv(fp, names=columns) for fp in files]\nweather = pd.concat(weather_dfs)",
            "code"
        ],
        [
            "Subjectively this is cleaner and more beautiful.\nThere\u2019s fewer lines of code.\nYou don\u2019t have this extraneous detail of building an empty DataFrame.\nAnd objectively the pandorable way is faster, as we\u2019ll test next.",
            "markdown"
        ],
        [
            "We\u2019ll define two functions for building an identical DataFrame. The first append_df, creates an empty DataFrame and appends to it. The second, concat_df, creates many DataFrames, and concatenates them at the end. We also write a short decorator that runs the functions a handful of times and records the results.",
            "markdown"
        ],
        [
            "import time\n\nsize_per = 5000\nN = 100\ncols = list('abcd')\n\ndef timed(n=30):\n    '''\n    Running a microbenchmark. Never use this.\n    '''\n    def deco(func):\n        def wrapper(*args, **kwargs):\n            timings = []\n            for i in range(n):\n                t0 = time.time()\n                func(*args, **kwargs)\n                t1 = time.time()\n                timings.append(t1 - t0)\n            return timings\n        return wrapper\n    return deco\n    \n@timed(60)\ndef append_df():\n    '''\n    The pythonic (bad) way\n    '''\n    df = pd.DataFrame(columns=cols)\n    for _ in range(N):\n        df.append(pd.DataFrame(np.random.randn(size_per, 4), columns=cols))\n    return df\n\n@timed(60)\ndef concat_df():\n    '''\n    The pandorabe (good) way\n    '''\n    dfs = [pd.DataFrame(np.random.randn(size_per, 4), columns=cols)\n           for _ in range(N)]\n    return pd.concat(dfs, ignore_index=True)",
            "code"
        ],
        [
            "t_append = append_df()\nt_concat = concat_df()\n\ntimings = (pd.DataFrame({\"Append\": t_append, \"Concat\": t_concat})\n             .stack()\n             .reset_index()\n             .rename(columns={0: 'Time (s)',\n                              'level_1': 'Method'}))\ntimings.head()",
            "code"
        ],
        [
            "plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_4_performance_6_0.png\"/>",
            "markdown"
        ]
    ],
    "Fast Pandas->Datatypes": [
        [
            "The pandas type system essentially  with a few extensions (categorical, datetime64 with timezone, timedelta64).\nAn advantage of the DataFrame over a 2-dimensional NumPy array is that the DataFrame can have columns of various types within a single table.\nThat said, each column should have a specific dtype; you don\u2019t want to be mixing bools with ints with strings within a single column.\nFor one thing, this is slow.\nIt forces the column to be have an object dtype (the fallback python-object container type), which means you don\u2019t get any of the type-specific optimizations in pandas or NumPy.\nFor another, it means you\u2019re probably violating the maxims of tidy data, which we\u2019ll discuss next time.",
            "markdown"
        ],
        [
            "When should you have object columns?\nThere are a few places where the NumPy / pandas type system isn\u2019t as rich as you might like.\nThere\u2019s no integer NA (at the moment anyway), so if you have any missing values, represented by NaN, your otherwise integer column will be floats.\nThere\u2019s also no date dtype (distinct from datetime).\nConsider the needs of your application: can you treat an integer 1 as 1.0?\nCan you treat date(2016, 1, 1) as datetime(2016, 1, 1, 0, 0)?\nIn my experience, this is rarely a problem other than when writing to something with a stricter schema like a database.\nBut at that point it\u2019s fine to cast to one of the less performant types, since you\u2019re just not doing numeric operations anymore.",
            "markdown"
        ],
        [
            "The last case of object dtype data is text data.\nPandas doesn\u2019t have any fixed-width string dtypes, so you\u2019re stuck with python objects.\nThere is an important exception here, and that\u2019s low-cardinality text data, for which you\u2019ll want to use the category dtype (see below).",
            "markdown"
        ],
        [
            "If you have object data (either strings or python objects) that needs to be converted, checkout the ,  and  methods.",
            "markdown"
        ]
    ],
    "Fast Pandas->Iteration, Apply, And Vectorization": [
        [
            "We know that  (scare quotes since that statement is too broad to be meaningful).\nThere are various steps that can be taken to improve your code\u2019s performance from relatively simple changes, to rewriting your code in a lower-level language, to trying to parallelize it.\nAnd while you might have many options, there\u2019s typically an order you would proceed in.",
            "markdown"
        ],
        [
            "First (and I know it\u2019s clich\u00e9 to say so, but still) benchmark your code.\nMake sure you actually need to spend time optimizing it.\nThere are     and visualizing where things are slow.",
            "markdown"
        ],
        [
            "Second, consider your algorithm.\nMake sure you aren\u2019t doing more work than you need to.\nA common one I see is doing a full sort on an array, just to select the N largest or smallest items.\nPandas has methods for that.",
            "markdown"
        ],
        [
            "df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "code"
        ],
        [
            "# Select the 5 largest delays\ndelays.nlargest(5).sort_values()",
            "code"
        ],
        [
            "112623    1480.0\n158136    1545.0\n152911    1934.0\n60246     1970.0\n59719     2755.0\nName: DEP_DELAY, dtype: float64",
            "code"
        ],
        [
            "delays.nsmallest(5).sort_values()",
            "code"
        ],
        [
            "300895   -59.0\n235921   -58.0\n197897   -56.0\n332533   -56.0\n344542   -55.0\nName: DEP_DELAY, dtype: float64",
            "code"
        ],
        [
            "We follow up the nlargest or nsmallest with a sort (the result of nlargest/smallest is unordered), but it\u2019s much easier to sort 5 items that 500,000. The timings bear this out:",
            "markdown"
        ],
        [
            "%timeit delays.sort_values().tail(5)",
            "code"
        ],
        [
            "31 ms \u00b1 1.05 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)",
            "code"
        ],
        [
            "%timeit delays.nlargest(5).sort_values()",
            "code"
        ],
        [
            "7.87 ms \u00b1 113 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)",
            "code"
        ],
        [
            "\u201cUse the right algorithm\u201d is easy to say, but harder to apply in practice since you have to actually figure out the best algorithm to use.\nThat one comes down to experience.",
            "markdown"
        ],
        [
            "Assuming you\u2019re at a spot that needs optimizing, and you\u2019ve got the correct algorithm, <em>and</em> there isn\u2019t a readily available optimized version of what you need in pandas/numpy/scipy/scikit-learn/statsmodels/&amp;mldr;, then what?",
            "markdown"
        ],
        [
            "The first place to turn is probably a vectorized NumPy implementation.\nVectorization here means operating directly on arrays, rather than looping over lists scalars.\nThis is generally much less work than rewriting it in something like Cython, and you can get pretty good results just by making <em>effective</em> use of NumPy and pandas.\nWhile not every operation can be vectorized, many can.",
            "markdown"
        ],
        [
            "Let\u2019s work through an example calculating the  between airports.\nGrab the table of airport latitudes and longitudes from the  and extract it to a CSV.",
            "markdown"
        ],
        [
            "from utils import download_airports\nimport zipfile",
            "code"
        ],
        [
            "if not os.path.exists(\"data/airports.csv.zip\"):\n    download_airports()",
            "code"
        ],
        [
            "coord = (pd.read_csv(\"data/airports.csv.zip\", index_col=['AIRPORT'],\n                     usecols=['AIRPORT', 'LATITUDE', 'LONGITUDE'])\n           .groupby(level=0).first()\n           .dropna()\n           .sample(n=500, random_state=42)\n           .sort_index())\n\ncoord.head()",
            "code"
        ],
        [
            "For whatever reason, suppose we\u2019re interested in all the pairwise distances (I\u2019ve limited it to just a sample of 500 airports to make this manageable.\nIn the real world you <em>probably</em> don\u2019t need <em>all</em> the pairwise distances and would be better off with a . Remember: think about what you actually need, and find the right algorithm for that).",
            "markdown"
        ],
        [
            "MultiIndexes have an alternative from_product constructor for getting the  of the arrays you pass in.\nWe\u2019ll give it coords.index twice (to get its Cartesian product with itself).\nThat gives a MultiIndex of all the combination.\nWith some minor reshaping of coords we\u2019ll have a DataFrame with all the latitude/longitude pairs.",
            "markdown"
        ],
        [
            "idx = pd.MultiIndex.from_product([coord.index, coord.index],\n                                 names=['origin', 'dest'])\n\npairs = pd.concat([coord.add_suffix('_1').reindex(idx, level='origin'),\n                   coord.add_suffix('_2').reindex(idx, level='dest')],\n                  axis=1)\npairs.head()",
            "code"
        ],
        [
            "idx = idx[idx.get_level_values(0) &lt;= idx.get_level_values(1)]",
            "code"
        ],
        [
            "len(idx)",
            "code"
        ],
        [
            "125250",
            "code"
        ],
        [
            "We\u2019ll break that down a bit, but don\u2019t lose sight of the real target: our great-circle distance calculation.",
            "markdown"
        ],
        [
            "The add_suffix (and add_prefix) method is handy for quickly renaming the columns.",
            "markdown"
        ],
        [
            "coord.add_suffix('_1').head()",
            "code"
        ],
        [
            "Alternatively you could use the more general .rename like coord.rename(columns=lambda x: x + '_1').",
            "markdown"
        ],
        [
            "Next, we have the reindex.\nLike I mentioned in the prior chapter, indexes are crucial to pandas.\n.reindex is all about aligning a Series or DataFrame to a given index.\nIn this case we use .reindex to align our original DataFrame to the new\nMultiIndex of combinations.\nBy default, the output will have the original value if that index label was already present, and NaN otherwise.\nIf we just called coord.reindex(idx), with no additional arguments, we\u2019d get a DataFrame of all NaNs.",
            "markdown"
        ],
        [
            "coord.reindex(idx).head()",
            "code"
        ],
        [
            "That\u2019s because there weren\u2019t any values of idx that were in coord.index,\nwhich makes sense since coord.index is just a regular one-level Index, while idx is a MultiIndex.\nWe use the level keyword to handle the transition from the original single-level Index, to the two-leveled idx.<blockquote>",
            "markdown"
        ],
        [
            "level : int or name</blockquote>",
            "markdown"
        ],
        [
            "Broadcast across a level, matching Index values on the\npassed MultiIndex level",
            "markdown"
        ],
        [
            "coord.reindex(idx, level='dest').head()",
            "code"
        ],
        [
            "If you ever need to do an operation that mixes regular single-level indexes with Multilevel Indexes, look for a level keyword argument.\nFor example, all the arithmatic methods (.mul, .add, etc.) have them.",
            "markdown"
        ],
        [
            "This is a bit wasteful since the distance from airport A to B is the same as B to A.\nWe could easily fix this with a idx = idx[idx.get_level_values(0) &lt;= idx.get_level_values(1)], but we\u2019ll ignore that for now.",
            "markdown"
        ],
        [
            "Quick tangent, I got some&amp;mldr; let\u2019s say skepticism, on my last piece about the value of indexes.\nHere\u2019s an alternative version for the skeptics",
            "markdown"
        ],
        [
            "from itertools import product, chain\ncoord2 = coord.reset_index()",
            "code"
        ],
        [
            "x = product(coord2.add_suffix('_1').itertuples(index=False),\n            coord2.add_suffix('_2').itertuples(index=False))\ny = [list(chain.from_iterable(z)) for z in x]\n\ndf2 = (pd.DataFrame(y, columns=['origin', 'LATITUDE_1', 'LONGITUDE_1',\n                                'dest', 'LATITUDE_1', 'LONGITUDE_2'])\n       .set_index(['origin', 'dest']))\ndf2.head()",
            "code"
        ],
        [
            "It\u2019s also readable (it\u2019s Python after all), though a bit slower.\nTo me the .reindex method seems more natural.\nMy thought process was, \u201cI need all the combinations of origin &amp; destination (MultiIndex.from_product).\nNow I need to align this original DataFrame to this new MultiIndex (coords.reindex).\u201d",
            "markdown"
        ],
        [
            "With that diversion out of the way, let\u2019s turn back to our great-circle distance calculation.\nOur first implementation is pure python.\nThe algorithm itself isn\u2019t too important, all that matters is that we\u2019re doing math operations on scalars.",
            "markdown"
        ],
        [
            "import math\n\ndef gcd_py(lat1, lng1, lat2, lng2):\n    '''\n    Calculate great circle distance between two points.\n    http://www.johndcook.com/blog/python_longitude_latitude/\n    \n    Parameters\n    ----------\n    lat1, lng1, lat2, lng2: float\n    \n    Returns\n    -------\n    distance:\n      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.\n    '''\n    # python2 users will have to use ascii identifiers (or upgrade)\n    degrees_to_radians = math.pi / 180.0\n    \u03d51 = (90 - lat1) * degrees_to_radians\n    \u03d52 = (90 - lat2) * degrees_to_radians\n    \n    \u03b81 = lng1 * degrees_to_radians\n    \u03b82 = lng2 * degrees_to_radians\n    \n    cos = (math.sin(\u03d51) * math.sin(\u03d52) * math.cos(\u03b81 - \u03b82) +\n           math.cos(\u03d51) * math.cos(\u03d52))\n    # round to avoid precision issues on identical points causing ValueErrors\n    cos = round(cos, 8)\n    arc = math.acos(cos)\n    return arc * 6373  # radius of earth, in kilometers",
            "code"
        ],
        [
            "The second implementation uses NumPy.\nAside from numpy having a builtin deg2rad convenience function (which is probably a bit slower than multiplying by a constant $\\frac{\\pi}{180}$), basically all we\u2019ve done is swap the math prefix for np.\nThanks to NumPy\u2019s broadcasting, we can write code that works on scalars or arrays of conformable shape.",
            "markdown"
        ],
        [
            "def gcd_vec(lat1, lng1, lat2, lng2):\n    '''\n    Calculate great circle distance.\n    http://www.johndcook.com/blog/python_longitude_latitude/\n    \n    Parameters\n    ----------\n    lat1, lng1, lat2, lng2: float or array of float\n    \n    Returns\n    -------\n    distance:\n      distance from ``(lat1, lng1)`` to ``(lat2, lng2)`` in kilometers.\n    '''\n    # python2 users will have to use ascii identifiers\n    \u03d51 = np.deg2rad(90 - lat1)\n    \u03d52 = np.deg2rad(90 - lat2)\n    \n    \u03b81 = np.deg2rad(lng1)\n    \u03b82 = np.deg2rad(lng2)\n    \n    cos = (np.sin(\u03d51) * np.sin(\u03d52) * np.cos(\u03b81 - \u03b82) +\n           np.cos(\u03d51) * np.cos(\u03d52))\n    arc = np.arccos(cos)\n    return arc * 6373",
            "code"
        ],
        [
            "To use the python version on our DataFrame, we can either iterate&amp;mldr;",
            "markdown"
        ],
        [
            "%%time\npd.Series([gcd_py(*x) for x in pairs.itertuples(index=False)],\n          index=pairs.index)",
            "code"
        ],
        [
            "CPU times: user 833 ms, sys: 12.7 ms, total: 846 ms\nWall time: 847 ms\n\n\n\n\n\norigin  dest\n8F3     8F3         0.000000\n        A03      4744.967448\n        A09      4407.533212\n        A18      4744.593127\n        A24      3820.092688\n                    ...     \nZZU     YUY     12643.665960\n        YYL     13687.592278\n        ZBR      4999.647307\n        ZXO     14925.531303\n        ZZU         0.000000\nLength: 250000, dtype: float64",
            "code"
        ],
        [
            "Or use DataFrame.apply.",
            "markdown"
        ],
        [
            "%%time\nr = pairs.apply(lambda x: gcd_py(x['LATITUDE_1'], x['LONGITUDE_1'],\n                                 x['LATITUDE_2'], x['LONGITUDE_2']), axis=1);",
            "code"
        ],
        [
            "CPU times: user 14.4 s, sys: 61.2 ms, total: 14.4 s\nWall time: 14.4 s",
            "code"
        ],
        [
            "But as you can see, you don\u2019t want to use apply, especially with axis=1 (calling the function on each row). It\u2019s doing a lot more work handling dtypes in the background, and trying to infer the correct output shape that are pure overhead in this case. On top of that, it has to essentially use a for loop internally.",
            "markdown"
        ],
        [
            "You <em>rarely</em> want to use DataFrame.apply and almost never should use it with axis=1. Better to write functions that take arrays, and pass those in directly. Like we did with the vectorized version",
            "markdown"
        ],
        [
            "%%time\nr = gcd_vec(pairs['LATITUDE_1'], pairs['LONGITUDE_1'],\n            pairs['LATITUDE_2'], pairs['LONGITUDE_2'])",
            "code"
        ],
        [
            "CPU times: user 31.1 ms, sys: 26.4 ms, total: 57.5 ms\nWall time: 37.2 ms\n\n\n/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in arccos",
            "code"
        ],
        [
            "r.head()",
            "code"
        ],
        [
            "origin  dest\n8F3     8F3        0.000000\n        A03     4744.967484\n        A09     4407.533240\n        A18     4744.593111\n        A24     3820.092639\ndtype: float64",
            "code"
        ],
        [
            "I try not to use the word \u201ceasy\u201d when teaching, but that optimization was easy right?\nWhy then, do I come across uses of apply, in my code and others\u2019, even when the vectorized version is available?\nThe difficulty lies in knowing about broadcasting, and seeing where to apply it.",
            "markdown"
        ],
        [
            "For example, the README for  (by Cam Davidson Pilon, also author of , , and ) used to have an example of passing  into a DataFrame.apply.",
            "markdown"
        ],
        [
            "data.apply(lambda r: bgf.conditional_expected_number_of_purchases_up_to_time(\n    t, r['frequency'], r['recency'], r['T']), axis=1\n)",
            "code"
        ],
        [
            "If you look at the function , it\u2019s doing a fairly complicated computation involving a negative log likelihood and the Gamma function from scipy.special.\nBut crucially, it was already vectorized.\nWe were able to change the example to just pass the arrays (Series in this case) into the function, rather than applying the function to each row.",
            "markdown"
        ],
        [
            "bgf.conditional_expected_number_of_purchases_up_to_time(\n    t, data['frequency'], data['recency'], data['T']\n)",
            "code"
        ],
        [
            "This got us another 30x speedup on the example dataset.\nI bring this up because it\u2019s very natural to have to translate an equation to code and think, \u201cOk now I need to apply this function to each row\u201d, so you reach for DataFrame.apply.\nSee if you can just pass in the NumPy array or Series itself instead.",
            "markdown"
        ],
        [
            "Not all operations this easy to vectorize.\nSome operations are iterative by nature, and rely on the results of surrounding computations to proceed. In cases like this you can hope that one of the scientific python libraries has implemented it efficiently for you, or write your own solution using Numba / C / Cython / Fortran.",
            "markdown"
        ],
        [
            "Other examples take a bit more thought or knowledge to vectorize.\nLet\u2019s look at \nexample, taken from Jeff Reback\u2019s PyData London talk, that groupwise normalizes a dataset by subtracting the mean and dividing by the standard deviation for each group.",
            "markdown"
        ],
        [
            "import random\n\ndef create_frame(n, n_groups):\n    # just setup code, not benchmarking this\n    stamps = pd.date_range('20010101', periods=n, freq='ms')\n    random.shuffle(stamps.values)    \n    return pd.DataFrame({'name': np.random.randint(0,n_groups,size=n),\n                         'stamp': stamps,\n                         'value': np.random.randint(0,n,size=n),\n                         'value2': np.random.randn(n)})\n\n\ndf = create_frame(1000000,10000)\n\ndef f_apply(df):\n    # Typical transform\n    return df.groupby('name').value2.apply(lambda x: (x-x.mean())/x.std())\n\ndef f_unwrap(df):\n    # \"unwrapped\"\n    g = df.groupby('name').value2\n    v = df.value2\n    return (v-g.transform(np.mean))/g.transform(np.std)",
            "code"
        ],
        [
            "Timing it we see that the \u201cunwrapped\u201d version, get\u2019s quite a bit better performance.",
            "markdown"
        ],
        [
            "%timeit f_apply(df)",
            "code"
        ],
        [
            "4.28 s \u00b1 161 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)",
            "code"
        ],
        [
            "%timeit f_unwrap(df)",
            "code"
        ],
        [
            "53.3 ms \u00b1 1.97 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)",
            "code"
        ],
        [
            "Pandas GroupBy objects intercept calls for common functions like mean, sum, etc. and substitutes them with optimized Cython versions.\nSo the unwrapped .transform(np.mean) and .transform(np.std) are fast, while the x.mean and x.std in the .apply(lambda x: x - x.mean()/x.std()) aren\u2019t.",
            "markdown"
        ],
        [
            "Groupby.apply is always going to be around, beacuse it offers maximum flexibility. If you need to , it can handle that. It just might not be the fastest (which may be OK sometimes).",
            "markdown"
        ],
        [
            "This last example is admittedly niche.\nI\u2019d like to think that there aren\u2019t too many places in pandas where the natural thing to do .transform((x - x.mean()) / x.std()) is slower than the less obvious alternative.\nIdeally the user wouldn\u2019t have to know about GroupBy having special fast implementations of common methods.\nBut that\u2019s where we are now.",
            "markdown"
        ]
    ],
    "Fast Pandas->Categoricals": [
        [
            "Thanks to some great work by , , and others, pandas 0.15 gained a new  data type. Categoricals are nice for many reasons beyond just efficiency, but we\u2019ll focus on that here.",
            "markdown"
        ],
        [
            "Categoricals are an efficient way of representing data (typically strings) that have a low <em>cardinality</em>, i.e. relatively few distinct values relative to the size of the array. Internally, a Categorical stores the categories once, and an array of codes, which are just integers that indicate which category belongs there. Since it\u2019s cheaper to store a code than a category, we save on memory (shown next).",
            "markdown"
        ],
        [
            "import string\n\ns = pd.Series(np.random.choice(list(string.ascii_letters), 100000))\nprint('{:0.2f} KB'.format(s.memory_usage(index=False) / 1000))",
            "code"
        ],
        [
            "800.00 KB",
            "code"
        ],
        [
            "c = s.astype('category')\nprint('{:0.2f} KB'.format(c.memory_usage(index=False) / 1000))",
            "code"
        ],
        [
            "102.98 KB",
            "code"
        ],
        [
            "Beyond saving memory, having codes and a fixed set of categories offers up a bunch of algorithmic optimizations that pandas and others can take advantage of.",
            "markdown"
        ],
        [
            " has a very nice  on using categoricals, and optimizing code in general.",
            "markdown"
        ]
    ],
    "Fast Pandas->Going Further": [
        [
            "The pandas documentation has a section on , focusing on using Cython or numba to speed up a computation. I\u2019ve focused more on the lower-hanging fruit of picking the right algorithm, vectorizing your code, and using pandas or numpy more effetively. There are further optimizations availble if these aren\u2019t enough.",
            "markdown"
        ]
    ],
    "Fast Pandas->Summary": [
        [
            "This post was more about how to make effective use of numpy and pandas, than writing your own highly-optimized code.\nIn my day-to-day work of data analysis it\u2019s not worth the time to write and compile a cython extension.\nI\u2019d rather rely on pandas to be fast at what matters (label lookup on large arrays, factorizations for groupbys and merges, numerics).\nIf you want to learn more about what pandas does to make things fast, checkout Jeff Tratner\u2019 talk from PyData Seattle  on pandas\u2019 internals.",
            "markdown"
        ],
        [
            "Next time we\u2019ll look at a differnt kind of optimization: using the Tidy Data principles to facilitate efficient data analysis.",
            "markdown"
        ]
    ],
    "Tidy Data": [
        [
            "This is part 5 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "Tidy Data->Reshaping &amp; Tidy Data<blockquote>": [
        [
            "Structuring datasets to facilitate analysis </blockquote>",
            "markdown"
        ],
        [
            "So, you\u2019ve sat down to analyze a new dataset.\nWhat do you do first?",
            "markdown"
        ],
        [
            "In episode 11 of , Hilary and Roger discussed their typical approaches.\nI\u2019m with Hilary on this one, you should make sure your data is tidy.\nBefore you do any plots, filtering, transformations, summary statistics, regressions&amp;mldr;\nWithout a tidy dataset, you\u2019ll be fighting your tools to get the result you need.\nWith a tidy dataset, it\u2019s relatively easy to do all of those.",
            "markdown"
        ],
        [
            "Hadley Wickham kindly summarized tidiness as a dataset whereEach variable forms a columnEach observation forms a rowEach type of observational unit forms a table",
            "markdown"
        ],
        [
            "And today we\u2019ll only concern ourselves with the first two.\nAs quoted at the top, this really is about facilitating analysis: going as quickly as possible from question to answer.",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa\n\npd.options.display.max_rows = 10\nsns.set(style='ticks', context='talk')",
            "code"
        ]
    ],
    "Tidy Data->Reshaping &amp; Tidy Data<blockquote>->NBA Data": [
        [
            " StackOverflow question asked about calculating the number of days of rest NBA teams have between games.\nThe answer would have been difficult to compute with the raw data.\nAfter transforming the dataset to be tidy, we\u2019re able to quickly get the answer.",
            "markdown"
        ],
        [
            "We\u2019ll grab some NBA game data from basketball-reference.com using pandas\u2019 read_html function, which returns a list of DataFrames.",
            "markdown"
        ],
        [
            "fp = 'data/nba.csv'\n\nif not os.path.exists(fp):\n    tables = pd.read_html(\"http://www.basketball-reference.com/leagues/NBA_2016_games.html\")\n    games = tables[0]\n    games.to_csv(fp)\nelse:\n    games = pd.read_csv(fp)\ngames.head()",
            "code"
        ],
        [
            "Side note: pandas\u2019 read_html is pretty good. On simple websites it almost always works.\nIt provides a couple parameters for controlling what gets selected from the webpage if the defaults fail.\nI\u2019ll always use it first, before moving on to  or  if the page is more complicated.",
            "markdown"
        ],
        [
            "As you can see, we have a bit of general munging to do before tidying.\nEach month slips in an extra row of mostly NaNs, the column names aren\u2019t too useful, and we have some dtypes to fix up.",
            "markdown"
        ],
        [
            "column_names = {'Date': 'date', 'Start (ET)': 'start',\n                'Unamed: 2': 'box', 'Visitor/Neutral': 'away_team', \n                'PTS': 'away_points', 'Home/Neutral': 'home_team',\n                'PTS.1': 'home_points', 'Unamed: 7': 'n_ot'}\n\ngames = (games.rename(columns=column_names)\n    .dropna(thresh=4)\n    [['date', 'away_team', 'away_points', 'home_team', 'home_points']]\n    .assign(date=lambda x: pd.to_datetime(x['date'], format='%a, %b %d, %Y'))\n    .set_index('date', append=True)\n    .rename_axis([\"game_id\", \"date\"])\n    .sort_index())\ngames.head()",
            "code"
        ],
        [
            "A quick aside on that last block.dropna has a thresh argument. If at least thresh items are missing, the row is dropped. We used it to remove the \u201cMonth headers\u201d that slipped into the table.assign can take a callable. This lets us refer to the DataFrame in the previous step of the chain. Otherwise we would have to assign temp_df = games.dropna()... And then do the pd.to_datetime on that.set_index has an append keyword. We keep the original index around since it will be our unique identifier per game.We use .rename_axis to set the index names (this behavior is new in pandas 0.18; before .rename_axis only took a mapping for changing labels).",
            "markdown"
        ],
        [
            "The Question:<blockquote>",
            "markdown"
        ],
        [
            "<strong>How many days of rest did each team get between each game?</strong></blockquote>",
            "markdown"
        ],
        [
            "Whether or not your dataset is tidy depends on your question. Given our question, what is an observation?",
            "markdown"
        ],
        [
            "In this case, an observation is a (team, game) pair, which we don\u2019t have yet. Rather, we have two observations per row, one for home and one for away. We\u2019ll fix that with pd.melt.",
            "markdown"
        ],
        [
            "pd.melt works by taking observations that are spread across columns (away_team, home_team), and melting them down into one column with multiple rows. However, we don\u2019t want to lose the metadata (like game_id and date) that is shared between the observations. By including those columns as id_vars, the values will be repeated as many times as needed to stay with their observations.",
            "markdown"
        ],
        [
            "tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "code"
        ],
        [
            "The DataFrame tidy meets our rules for tidiness: each variable is in a column, and each observation (team, date pair) is on its own row.\nNow the translation from question (\u201cHow many days of rest between games\u201d) to operation (\u201cdate of today\u2019s game - date of previous game - 1\u201d) is direct:",
            "markdown"
        ],
        [
            "# For each team... get number of days between games\ntidy.groupby('team')['date'].diff().dt.days - 1",
            "code"
        ],
        [
            "0       NaN\n1       NaN\n2       NaN\n3       NaN\n4       NaN\n       ... \n2455    7.0\n2456    1.0\n2457    1.0\n2458    3.0\n2459    2.0\nName: date, Length: 2460, dtype: float64",
            "code"
        ],
        [
            "That\u2019s the essence of tidy data, the reason why it\u2019s worth considering what shape your data should be in.\nIt\u2019s about setting yourself up for success so that the answers naturally flow from the data (just kidding, it\u2019s usually still difficult. But hopefully less so).",
            "markdown"
        ],
        [
            "Let\u2019s assign that back into our DataFrame",
            "markdown"
        ],
        [
            "tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "code"
        ],
        [
            "To show the inverse of melt, let\u2019s take rest values we just calculated and place them back in the original DataFrame with a pivot_table.",
            "markdown"
        ],
        [
            "by_game = (pd.pivot_table(tidy, values='rest',\n                          index=['game_id', 'date'],\n                          columns='variable')\n             .rename(columns={'away_team': 'away_rest',\n                              'home_team': 'home_rest'}))\ndf = pd.concat([games, by_game], axis=1)\ndf.dropna().head()",
            "code"
        ],
        [
            "One somewhat subtle point: an \u201cobservation\u201d depends on the question being asked.\nSo really, we have two tidy datasets, tidy for answering team-level questions, and df for answering game-level questions.",
            "markdown"
        ],
        [
            "One potentially interesting question is \u201cwhat was each team\u2019s average days of rest, at home and on the road?\u201d With a tidy dataset (the DataFrame tidy, since it\u2019s team-level), seaborn makes this easy (more on seaborn in a future post):",
            "markdown"
        ],
        [
            "sns.set(style='ticks', context='paper')",
            "code"
        ],
        [
            "g = sns.FacetGrid(tidy, col='team', col_wrap=6, hue='team', size=2)\ng.map(sns.barplot, 'variable', 'rest');",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_17_0.png\"/>",
            "markdown"
        ],
        [
            "An example of a game-level statistic is the distribution of rest differences in games:",
            "markdown"
        ],
        [
            "df['home_win'] = df['home_points'] &gt; df['away_points']\ndf['rest_spread'] = df['home_rest'] - df['away_rest']\ndf.dropna().head()",
            "code"
        ],
        [
            "delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_20_0.png\"/>",
            "markdown"
        ],
        [
            "Or the win percent by rest difference",
            "markdown"
        ],
        [
            "fig, ax = plt.subplots(figsize=(12, 6))\nsns.barplot(x='rest_spread', y='home_win', data=df.query('-3 &lt;= rest_spread &lt;= 3'),\n            color='#4c72b0', ax=ax)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_22_0.png\"/>",
            "markdown"
        ]
    ],
    "Tidy Data->Reshaping &amp; Tidy Data<blockquote>->Stack / Unstack": [
        [
            "Pandas has two useful methods for quickly converting from wide to long format (stack) and long to wide (unstack).",
            "markdown"
        ],
        [
            "rest = (tidy.groupby(['date', 'variable'])\n            .rest.mean()\n            .dropna())\nrest.head()",
            "code"
        ],
        [
            "date        variable \n2015-10-28  away_team    0.000000\n            home_team    0.000000\n2015-10-29  away_team    0.333333\n            home_team    0.000000\n2015-10-30  away_team    1.083333\nName: rest, dtype: float64",
            "code"
        ],
        [
            "rest is in a \u201clong\u201d form since we have a single column of data, with multiple \u201ccolumns\u201d of metadata (in the MultiIndex). We use .unstack to move from long to wide.",
            "markdown"
        ],
        [
            "rest.unstack().head()",
            "code"
        ],
        [
            "unstack moves a level of a MultiIndex (innermost by default) up to the columns.\nstack is the inverse.",
            "markdown"
        ],
        [
            "rest.unstack().stack()",
            "code"
        ],
        [
            "date        variable \n2015-10-28  away_team    0.000000\n            home_team    0.000000\n2015-10-29  away_team    0.333333\n            home_team    0.000000\n2015-10-30  away_team    1.083333\n                           ...   \n2016-04-11  home_team    0.666667\n2016-04-12  away_team    1.000000\n            home_team    1.400000\n2016-04-13  away_team    0.500000\n            home_team    1.214286\nLength: 320, dtype: float64",
            "code"
        ],
        [
            "With .unstack you can move between those APIs that expect there data in long-format and those APIs that work with wide-format data. For example, DataFrame.plot(), works with wide-form data, one line per column.",
            "markdown"
        ],
        [
            "with sns.color_palette() as pal:\n    b, g = pal.as_hex()[:2]\n\nax=(rest.unstack()\n        .query('away_team &lt; 7')\n        .rolling(7)\n        .mean()\n        .plot(figsize=(12, 6), linewidth=3, legend=False))\nax.set(ylabel='Rest (7 day MA)')\nax.annotate(\"Home\", (rest.index[-1][0], 1.02), color=g, size=14)\nax.annotate(\"Away\", (rest.index[-1][0], 0.82), color=b, size=14)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_30_0.png\"/>",
            "markdown"
        ],
        [
            "The most conenient form will depend on exactly what you\u2019re doing.\nWhen interacting with databases you\u2019ll often deal with long form data.\nPandas\u2019 DataFrame.plot often expects wide-form data, while seaborn often expect long-form data. Regressions will expect wide-form data. Either way, it\u2019s good to be comfortable with stack and unstack (and MultiIndexes) to quickly move between the two.",
            "markdown"
        ]
    ],
    "Tidy Data->Reshaping &amp; Tidy Data<blockquote>->Mini Project: Home Court Advantage?": [
        [
            "We\u2019ve gone to all that work tidying our dataset, let\u2019s put it to use.\nWhat\u2019s the effect (in terms of probability to win) of being\nthe home team?",
            "markdown"
        ]
    ],
    "Tidy Data->Reshaping &amp; Tidy Data<blockquote>->Mini Project: Home Court Advantage?->Step 1: Create an outcome variable": [
        [
            "We need to create an indicator for whether the home team won.\nAdd it as a column called home_win in games.",
            "markdown"
        ],
        [
            "df['home_win'] = df.home_points &gt; df.away_points",
            "code"
        ]
    ],
    "Tidy Data->Reshaping &amp; Tidy Data<blockquote>->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team": [
        [
            "In the 10-minute literature review I did on the topic, it seems like people include a team-strength variable in their regressions.\nI suppose that makes sense; if stronger teams happened to play against weaker teams at home more often than away, it\u2019d look like the home-effect is stronger than it actually is.\nWe\u2019ll do a terrible job of controlling for team strength by calculating each team\u2019s win percent and using that as a predictor.\nIt\u2019d be better to use some kind of independent measure of team strength, but this will do for now.",
            "markdown"
        ],
        [
            "We\u2019ll use a similar melt operation as earlier, only now with the home_win variable we just created.",
            "markdown"
        ],
        [
            "wins = (\n    pd.melt(df.reset_index(),\n            id_vars=['game_id', 'date', 'home_win'],\n            value_name='team', var_name='is_home',\n            value_vars=['home_team', 'away_team'])\n   .assign(win=lambda x: x.home_win == (x.is_home == 'home_team'))\n   .groupby(['team', 'is_home'])\n   .win\n   .agg(['sum', 'count', 'mean'])\n   .rename(columns=dict(sum='n_wins',\n                        count='n_games',\n                        mean='win_pct'))\n)\nwins.head()",
            "code"
        ],
        [
            "Pause for visualiztion, because why not",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(wins.reset_index(), hue='team', size=7, aspect=.5, palette=['k'])\ng.map(sns.pointplot, 'is_home', 'win_pct').set(ylim=(0, 1));",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_38_0.png\"/>",
            "markdown"
        ],
        [
            "(It\u2019d be great if there was a library built on top of matplotlib that auto-labeled each point decently well. Apparently this is a difficult problem to do in general).",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(wins.reset_index(), col='team', hue='team', col_wrap=5, size=2)\ng.map(sns.pointplot, 'is_home', 'win_pct')",
            "code"
        ],
        [
            "&lt;seaborn.axisgrid.FacetGrid at 0x11a0fe588&gt;",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_40_1.png\"/>",
            "markdown"
        ],
        [
            "Those two graphs show that most teams have a higher win-percent at home than away. So we can continue to investigate.\nLet\u2019s aggregate over home / away to get an overall win percent per team.",
            "markdown"
        ],
        [
            "win_percent = (\n    # Use sum(games) / sum(games) instead of mean\n    # since I don't know if teams play the same\n    # number of games at home as away\n    wins.groupby(level='team', as_index=True)\n        .apply(lambda x: x.n_wins.sum() / x.n_games.sum())\n)\nwin_percent.head()",
            "code"
        ],
        [
            "team\nAtlanta Hawks        0.585366\nBoston Celtics       0.585366\nBrooklyn Nets        0.256098\nCharlotte Hornets    0.585366\nChicago Bulls        0.512195\ndtype: float64",
            "code"
        ],
        [
            "win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_43_1.png\"/>",
            "markdown"
        ],
        [
            "Is there a relationship between overall team strength and their home-court advantage?",
            "markdown"
        ],
        [
            "plt.figure(figsize=(8, 5))\n(wins.win_pct\n    .unstack()\n    .assign(**{'Home Win % - Away %': lambda x: x.home_team - x.away_team,\n               'Overall %': lambda x: (x.home_team + x.away_team) / 2})\n     .pipe((sns.regplot, 'data'), x='Overall %', y='Home Win % - Away %')\n)\nsns.despine()\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_5_tidy_45_0.png\"/>",
            "markdown"
        ],
        [
            "Let\u2019s get the team strength back into df.\nYou could you pd.merge, but I prefer .map when joining a Series.",
            "markdown"
        ],
        [
            "df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "code"
        ],
        [
            "import statsmodels.formula.api as sm\n\ndf['home_win'] = df.home_win.astype(int)  # for statsmodels",
            "code"
        ],
        [
            "mod = sm.logit('home_win ~ home_strength + away_strength + home_rest + away_rest', df)\nres = mod.fit()\nres.summary()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.552792\n         Iterations 6",
            "code"
        ],
        [
            "The strength variables both have large coefficeints (really we should be using some independent measure of team strength here, win_percent is showing up on the left and right side of the equation). The rest variables don\u2019t seem to matter as much.",
            "markdown"
        ],
        [
            "With .assign we can quickly explore variations in formula.",
            "markdown"
        ],
        [
            "(sm.Logit.from_formula('home_win ~ strength_diff + rest_spread',\n                       df.assign(strength_diff=df.home_strength - df.away_strength))\n    .fit().summary())",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.553499\n         Iterations 6",
            "code"
        ],
        [
            "mod = sm.Logit.from_formula('home_win ~ home_rest + away_rest', df)\nres = mod.fit()\nres.summary()",
            "code"
        ],
        [
            "Optimization terminated successfully.\n         Current function value: 0.676549\n         Iterations 4",
            "code"
        ],
        [
            "Overall not seeing to much support for rest mattering, but we got to see some more tidy data.",
            "markdown"
        ],
        [
            "That\u2019s it for today.\nNext time we\u2019ll look at data visualization.",
            "markdown"
        ]
    ],
    "Visualization": [
        [
            "This is part 6 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "Visualization->Visualization and Exploratory Analysis": [
        [
            "A few weeks ago, the R community went through some hand-wringing about plotting packages.\nFor outsiders (like me) the details aren\u2019t that important, but some brief background might be useful so we can transfer the takeaways to Python.\nThe competing systems are \u201cbase R\u201d, which is the plotting system built into the language, and ggplot2, Hadley Wickham\u2019s implementation of the grammar of graphics.\nFor those interested in more details, start with",
            "markdown"
        ],
        [
            "The most important takeaways are thatEither system is capable of producing anything the other canggplot2 is usually better for exploratory analysis",
            "markdown"
        ],
        [
            "Item 2 is not universally agreed upon, and it certainly isn\u2019t true for every type of chart, but we\u2019ll take it as fact for now.\nI\u2019m not foolish enough to attempt a formal analogy here, like \u201cmatplotlib is python\u2019s base R\u201d.\nBut there\u2019s at least a rough comparison:\nlike dplyr/tidyr and ggplot2, the combination of pandas and seaborn allows for fast iteration and exploration.\nWhen you need to, you can \u201cdrop down\u201d into matplotlib for further refinement.",
            "markdown"
        ]
    ],
    "Visualization->Visualization and Exploratory Analysis->Overview": [
        [
            "Here\u2019s a brief sketch of the plotting landscape as of April 2016.\nFor some reason, plotting tools feel a bit more personal than other parts of this series so far, so I feel the need to blanket this who discussion in a caveat: this is my personal take, shaped by my personal background and tastes.\nAlso, I\u2019m not at all an expert on visualization, just a consumer.\nFor real advice, you should  to the  in this .\nTake this all with an extra grain or two of salt.",
            "markdown"
        ]
    ],
    "Visualization->Visualization and Exploratory Analysis->": [
        [
            "Matplotlib is an amazing project, and is the foundation of pandas\u2019 built-in plotting and Seaborn.\nIt handles everything from the integration with various drawing backends, to several APIs handling drawing charts or adding and transforming individual glyphs (artists).\nI\u2019ve found knowing the  useful.\nYou\u2019re less likely to need things like  or , but when you do the documentation is there.",
            "markdown"
        ],
        [
            "Matplotlib has built up something of a bad reputation for being verbose.\nI think that complaint is valid, but misplaced.\nMatplotlib lets you control essentially anything on the figure.\nAn overly-verbose API just means there\u2019s an opportunity for a higher-level, domain specific, package to exist (like seaborn for statistical graphics).",
            "markdown"
        ],
        [
            "DataFrame and Series have a .plot namespace, with various chart types available (line, hist, scatter, etc.).\nPandas objects provide additional metadata that can be used to enhance plots (the Index for a better automatic x-axis then range(n) or Index names as axis labels for example).",
            "markdown"
        ],
        [
            "And since pandas had fewer backwards-compatibility constraints, it had a bit better default aesthetics.\nThe  will level this, and pandas has , in favor of matplotlib\u2019s (technically  it when fixing matplotlib 1.5 compatibility, so we deprecated it after the fact).",
            "markdown"
        ],
        [
            "At this point, I see pandas DataFrame.plot as a useful exploratory tool for quick throwaway plots.",
            "markdown"
        ],
        [
            ", created by Michael Waskom, \u201cprovides a high-level interface for drawing attractive statistical graphics.\u201d Seaborn gives a great API for quickly exploring different visual representations of your data. We\u2019ll be focusing on that today",
            "markdown"
        ],
        [
            " is a (still under heavy development) visualiztion library that targets the browser.",
            "markdown"
        ],
        [
            "Like matplotlib, Bokeh has a few APIs at various levels of abstraction.\nThey have a glyph API, which I suppose is most similar to matplotlib\u2019s Artists API, for drawing single or arrays of glpyhs (circles, rectangles, polygons, etc.).\nMore recently they introduced a Charts API, for producing canned charts from data structures like dicts or DataFrames.",
            "markdown"
        ]
    ],
    "Visualization->Visualization and Exploratory Analysis->Other Libraries": [
        [
            "This is a (probably incomplete) list of other visualization libraries that I don\u2019t know enough about to comment on",
            "markdown"
        ],
        [
            "It\u2019s also possible to use Javascript tools like D3 directly in the Jupyter notebook, but we won\u2019t go into those today.",
            "markdown"
        ]
    ],
    "Visualization->Visualization and Exploratory Analysis->Examples": [
        [
            "I do want to pause and explain the type of work I\u2019m doing with these packages.\nThe vast majority of plots I create are for exploratory analysis, helping me understand the dataset I\u2019m working with.\nThey aren\u2019t intended for the client (whoever that is) to see.\nOccasionally that exploratory plot will evolve towards a final product that will be used to explain things to the client.\nIn this case I\u2019ll either polish the exploratory plot, or rewrite it in another system more suitable for the final product (in D3 or Bokeh, say, if it needs to be an interactive document in the browser).",
            "markdown"
        ],
        [
            "Now that we have a feel for the overall landscape (from my point of view), let\u2019s delve into a few examples.\nWe\u2019ll use the diamonds dataset from ggplot2.\nYou could use Vincent Arelbundock\u2019s  to find it (pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/ggplot2/diamonds.csv')), but I wanted to checkout .",
            "markdown"
        ],
        [
            "import os\nimport feather\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa",
            "code"
        ],
        [
            "%load_ext rpy2.ipython",
            "code"
        ],
        [
            "%%R\nsuppressPackageStartupMessages(library(ggplot2))\nlibrary(feather)\nwrite_feather(diamonds, 'diamonds.fthr')",
            "code"
        ],
        [
            "import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "code"
        ],
        [
            "df.info()",
            "code"
        ],
        [
            "&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 53940 entries, 0 to 53939\nData columns (total 10 columns):\ncarat      53940 non-null float64\ncut        53940 non-null category\ncolor      53940 non-null category\nclarity    53940 non-null category\ndepth      53940 non-null float64\ntable      53940 non-null float64\nprice      53940 non-null int32\nx          53940 non-null float64\ny          53940 non-null float64\nz          53940 non-null float64\ndtypes: category(3), float64(6), int32(1)\nmemory usage: 2.8 MB",
            "code"
        ],
        [
            "It\u2019s not clear to me where the scientific community will come down on Bokeh for exploratory analysis.\nThe ability to share interactive graphics is compelling.\nThe trend towards more and more analysis and communication happening in the browser will only enhance this feature of Bokeh.",
            "markdown"
        ],
        [
            "Personally though, I have a lot of inertia behind matplotlib so I haven\u2019t switched to Bokeh for day-to-day exploratory analysis.",
            "markdown"
        ],
        [
            "I have greatly enjoyed Bokeh for building dashboards and  with Bokeh server.\nIt\u2019s still young, and I\u2019ve hit , but I\u2019m happy to put up with some awkwardness to avoid writing more javascript.",
            "markdown"
        ],
        [
            "sns.set(context='talk', style='ticks')\n\n%matplotlib inline",
            "code"
        ]
    ],
    "Visualization->Visualization and Exploratory Analysis->Matplotlib": [
        [
            "Since it\u2019s relatively new, I should point out that matplotlib 1.5 added support for plotting labeled data.",
            "markdown"
        ],
        [
            "fig, ax = plt.subplots()\n\nax.scatter(x='carat', y='depth', data=df, c='k', alpha=.15);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_13_0.png\"/>",
            "markdown"
        ],
        [
            "This isn\u2019t limited to just DataFrames.\nIt supports anything that uses __getitem__ (square-brackets) with string keys.\nOther than that, I don\u2019t have much to add to the .",
            "markdown"
        ]
    ],
    "Visualization->Visualization and Exploratory Analysis->Pandas Built-in Plotting": [
        [
            "The metadata in DataFrames gives a bit better defaults on plots.",
            "markdown"
        ],
        [
            "df.plot.scatter(x='carat', y='depth', c='k', alpha=.15)\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_17_0.png\"/>",
            "markdown"
        ],
        [
            "We get axis labels from the column names.\nNothing major, just nice.",
            "markdown"
        ],
        [
            "Pandas can be more convenient for plotting a bunch of columns with a shared x-axis (the index), say several timeseries.",
            "markdown"
        ],
        [
            "from pandas_datareader import fred\n\ngdp = fred.FredReader(['GCEC96', 'GPDIC96'], start='2000-01-01').read()\n\ngdp.rename(columns={\"GCEC96\": \"Government Expenditure\",\n                    \"GPDIC96\": \"Private Investment\"}).plot(figsize=(12, 6))\nplt.tight_layout()",
            "code"
        ],
        [
            "/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: pandas.core.common.is_list_like is deprecated. import from the public API: pandas.api.types.is_list_like instead\n  This is separate from the ipykernel package so we can avoid doing imports until",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_19_1.png\"/>",
            "markdown"
        ]
    ],
    "Visualization->Visualization and Exploratory Analysis->Seaborn": [
        [
            "The rest of this post will focus on seaborn, and why I think it\u2019s especially great for exploratory analysis.",
            "markdown"
        ],
        [
            "I would encourage you to read Seaborn\u2019s , which describe its design philosophy and attempted goals. Some highlights:<blockquote>",
            "markdown"
        ],
        [
            "Seaborn aims to make visualization a central part of exploring and understanding data.</blockquote>",
            "markdown"
        ],
        [
            "It does this through a consistent, understandable (to me anyway) API.<blockquote>",
            "markdown"
        ],
        [
            "The plotting functions try to do something useful when called with a minimal set of arguments, and they expose a number of customizable options through additional parameters.</blockquote>",
            "markdown"
        ],
        [
            "Which works great for exploratory analysis, with the option to turn that into something more polished if it looks promising.<blockquote>",
            "markdown"
        ],
        [
            "Some of the functions plot directly into a matplotlib axes object, while others operate on an entire figure and produce plots with several panels.</blockquote>",
            "markdown"
        ],
        [
            "The fact that seaborn is built on matplotlib means that if you are familiar with the pyplot API, your knowledge will still be useful.",
            "markdown"
        ],
        [
            "Most seaborn plotting functions (one per chart-type) take an x, y, hue, and data arguments (only some are required, depending on the plot type). If you\u2019re working with DataFrames, you\u2019ll pass in strings referring to column names, and the DataFrame for data.",
            "markdown"
        ],
        [
            "sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_22_0.png\"/>",
            "markdown"
        ],
        [
            "sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_23_0.png\"/>",
            "markdown"
        ],
        [
            "Bivariate relationships can easily be explored, either one at a time:",
            "markdown"
        ],
        [
            "sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_25_0.png\"/>",
            "markdown"
        ],
        [
            "Or many at once",
            "markdown"
        ],
        [
            "g = sns.pairplot(df, hue='cut')",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_27_0.png\"/>",
            "markdown"
        ],
        [
            "pairplot is a convenience wrapper around PairGrid, and offers our first look at an important seaborn abstraction, the Grid. <em>Seaborn Grids provide a link between a matplotlib Figure with multiple axes and features in your dataset.</em>",
            "markdown"
        ],
        [
            "There are two main ways of interacting with grids. First, seaborn provides convenience-wrapper functions like pairplot, that have good defaults for common tasks. If you need more flexibility, you can work with the Grid directly by mapping plotting functions over each axes.",
            "markdown"
        ],
        [
            "def core(df, \u03b1=.05):\n    mask = (df &gt; df.quantile(\u03b1)).all(1) &amp; (df &lt; df.quantile(1 - \u03b1)).all(1)\n    return df[mask]",
            "code"
        ],
        [
            "cmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True)\n\n(df.select_dtypes(include=[np.number])\n   .pipe(core)\n   .pipe(sns.PairGrid)\n   .map_upper(plt.scatter, marker='.', alpha=.25)\n   .map_diag(sns.kdeplot)\n   .map_lower(plt.hexbin, cmap=cmap, gridsize=20)\n);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_30_1.png\"/>",
            "markdown"
        ],
        [
            "This last example shows the tight integration with matplotlib. g.axes is an array of matplotlib.Axes and g.fig is a matplotlib.Figure.\nThis is a pretty common pattern when using seaborn: use a seaborn plotting method (or grid) to get a good start, and then adjust with matplotlib as needed.",
            "markdown"
        ],
        [
            "I <em>think</em> (not an expert on this at all) that one thing people like about the grammar of graphics is its flexibility.\nYou aren\u2019t limited to a fixed set of chart types defined by the library author.\nInstead, you construct your chart by layering scales, aesthetics and geometries.\nAnd using ggplot2 in R is a delight.",
            "markdown"
        ],
        [
            "That said, I wouldn\u2019t really call what seaborn / matplotlib offer that limited.\nYou can create pretty complex charts suited to your needs.",
            "markdown"
        ],
        [
            "agged = df.groupby(['cut', 'color']).mean().sort_index().reset_index()\n\ng = sns.PairGrid(agged, x_vars=agged.columns[2:], y_vars=['cut', 'color'],\n                 size=5, aspect=.65)\ng.map(sns.stripplot, orient=\"h\", size=10, palette='Blues_d');",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_32_1.png\"/>",
            "markdown"
        ],
        [
            "g = sns.FacetGrid(df, col='color', hue='color', col_wrap=4)\ng.map(sns.regplot, 'carat', 'price');",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_33_1.png\"/>",
            "markdown"
        ],
        [
            "Initially I had many more examples showing off seaborn, but I\u2019ll spare you.\nSeaborn\u2019s  is thorough (and just beautiful to look at).",
            "markdown"
        ],
        [
            "We\u2019ll end with a nice scikit-learn integration for exploring the parameter-space on a GridSearch object.",
            "markdown"
        ],
        [
            "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV",
            "code"
        ],
        [
            "For those unfamiliar with machine learning or scikit-learn, the basic idea is your algorithm (RandomForestClassifer) is trying to maximize some objective function (percent of correctly classified items in this case).\nThere are various <em>hyperparameters</em> that affect the fit.\nWe can search this space by trying out a bunch of possible values for each parameter with the GridSearchCV estimator.",
            "markdown"
        ],
        [
            "df = sns.load_dataset('titanic')\n\nclf = RandomForestClassifier()\nparam_grid = dict(max_depth=[1, 2, 5, 10, 20, 30, 40],\n                  min_samples_split=[2, 5, 10],\n                  min_samples_leaf=[2, 3, 5])\nest = GridSearchCV(clf, param_grid=param_grid, n_jobs=4)\n\ny = df['survived']\nX = df.drop(['survived', 'who', 'alive'], axis=1)\n\nX = pd.get_dummies(X, drop_first=True)\nX = X.fillna(value=X.median())\nest.fit(X, y);",
            "code"
        ],
        [
            "scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "code"
        ],
        [
            "sns.factorplot(x='param_max_depth', y='mean_test_score',\n               col='param_min_samples_split',\n               hue='param_min_samples_leaf',\n               data=scores);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_6_visualization_39_0.png\"/>",
            "markdown"
        ],
        [
            "Thanks for reading!\nI want to reiterate at the end that this is just <em>my</em> way of doing data visualization.\nYour needs might differ, meaning you might need different tools.\nYou can still use pandas to get it to the point where it\u2019s ready to be visualized!",
            "markdown"
        ],
        [
            "As always, .",
            "markdown"
        ]
    ],
    "Time Series": [
        [
            "This is part 7 in my series on writing modern idiomatic pandas.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries": [
        [
            "Pandas started out in the financial world, so naturally it has strong timeseries support.",
            "markdown"
        ],
        [
            "The first half of this post will look at pandas\u2019 capabilities for manipulating time series data.\nThe second half will discuss modelling time series data with statsmodels.",
            "markdown"
        ],
        [
            "%matplotlib inline\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader.data as web\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style='ticks', context='talk')\n\nif int(os.environ.get(\"MODERN_PANDAS_EPUB\", 0)):\n    import prep # noqa",
            "code"
        ],
        [
            "Let\u2019s grab some stock data for Goldman Sachs using the  package, which spun off of pandas:",
            "markdown"
        ],
        [
            "gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "code"
        ],
        [
            "There isn\u2019t a special data-container just for time series in pandas, they\u2019re just Series or DataFrames with a DatetimeIndex.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->Special Slicing": [
        [
            "Looking at the elements of gs.index, we see that DatetimeIndexes are made up of pandas.Timestamps:",
            "markdown"
        ],
        [
            "Looking at the elements of gs.index, we see that DatetimeIndexes are made up of pandas.Timestamps:",
            "markdown"
        ],
        [
            "gs.index[0]",
            "code"
        ],
        [
            "Timestamp('2006-01-03 00:00:00')",
            "code"
        ],
        [
            "A Timestamp is mostly compatible with the datetime.datetime class, but much amenable to storage in arrays.",
            "markdown"
        ],
        [
            "Working with Timestamps can be awkward, so Series and DataFrames with DatetimeIndexes have some special slicing rules.\nThe first special case is <em>partial-string indexing</em>. Say we wanted to select all the days in 2006. Even with Timestamp\u2019s convenient constructors, it\u2019s a pai",
            "markdown"
        ],
        [
            "gs.loc[pd.Timestamp('2006-01-01'):pd.Timestamp('2006-12-31')].head()",
            "code"
        ],
        [
            "Thanks to partial-string indexing, it\u2019s as simple as",
            "markdown"
        ],
        [
            "gs.loc['2006'].head()",
            "code"
        ],
        [
            "Since label slicing is inclusive, this slice selects any observation where the year is 2006.",
            "markdown"
        ],
        [
            "The second \u201cconvenience\u201d is __getitem__ (square-bracket) fall-back indexing. I\u2019m only going to mention it here, with the caveat that you should never use it.\nDataFrame __getitem__ typically looks in the column: gs['2006'] would search gs.columns for '2006', not find it, and raise a KeyError. But DataFrames with a DatetimeIndex catch that KeyError and try to slice the index.\nIf it succeeds in slicing the index, the result like gs.loc['2006'] is returned.\nIf it fails, the KeyError is re-raised.\nThis is confusing because in pretty much every other case DataFrame.__getitem__ works on columns, and it\u2019s fragile because if you happened to have a column '2006' you <em>would</em> get just that column, and no fall-back indexing would occur. Just use gs.loc['2006'] when slicing DataFrame indexes.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->Special Methods->Resampling": [
        [
            "Resampling is similar to a groupby: you split the time series into groups (5-day buckets below), apply a function to each group (mean), and combine the result (one row per group).",
            "markdown"
        ],
        [
            "gs.resample(\"5d\").mean().head()",
            "code"
        ],
        [
            "gs.resample(\"W\").agg(['mean', 'sum']).head()",
            "code"
        ],
        [
            "You can up-sample to convert to a higher frequency.\nThe new points are filled with NaNs.",
            "markdown"
        ],
        [
            "gs.resample(\"6H\").mean().head()",
            "code"
        ]
    ],
    "Time Series->Timeseries->Special Methods->Rolling / Expanding / EW": [
        [
            "These methods aren\u2019t unique to DatetimeIndexes, but they often make sense with time series, so I\u2019ll show them here.",
            "markdown"
        ],
        [
            "gs.Close.plot(label='Raw')\ngs.Close.rolling(28).mean().plot(label='28D MA')\ngs.Close.expanding().mean().plot(label='Expanding Average')\ngs.Close.ewm(alpha=0.03).mean().plot(label='EWMA($\\\\alpha=.03$)')\n\nplt.legend(bbox_to_anchor=(1.25, .5))\nplt.tight_layout()\nplt.ylabel(\"Close ($)\")\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_23_0.png\"/>",
            "markdown"
        ],
        [
            "Each of .rolling, .expanding, and .ewm return a deferred object, similar to a GroupBy.",
            "markdown"
        ],
        [
            "roll = gs.Close.rolling(30, center=True)\nroll",
            "code"
        ],
        [
            "Rolling [window=30,center=True,axis=0]",
            "code"
        ],
        [
            "m = roll.agg(['mean', 'std'])\nax = m['mean'].plot()\nax.fill_between(m.index, m['mean'] - m['std'], m['mean'] + m['std'],\n                alpha=.25)\nplt.tight_layout()\nplt.ylabel(\"Close ($)\")\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_26_0.png\"/>",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->Grab Bag->Offsets": [
        [
            "These are similar to dateutil.relativedelta, but works with arrays.",
            "markdown"
        ],
        [
            "gs.index + pd.DateOffset(months=3, days=-2)",
            "code"
        ],
        [
            "DatetimeIndex(['2006-04-01', '2006-04-02', '2006-04-03', '2006-04-04',\n               '2006-04-07', '2006-04-08', '2006-04-09', '2006-04-10',\n               '2006-04-11', '2006-04-15',\n               ...\n               '2010-03-15', '2010-03-16', '2010-03-19', '2010-03-20',\n               '2010-03-21', '2010-03-22', '2010-03-26', '2010-03-27',\n               '2010-03-28', '2010-03-29'],\n              dtype='datetime64[ns]', name='Date', length=1007, freq=None)",
            "code"
        ]
    ],
    "Time Series->Timeseries->Grab Bag->Holiday Calendars": [
        [
            "There are a whole bunch of special calendars, useful for traders probabaly.",
            "markdown"
        ],
        [
            "from pandas.tseries.holiday import USColumbusDay",
            "code"
        ],
        [
            "USColumbusDay.dates('2015-01-01', '2020-01-01')",
            "code"
        ],
        [
            "DatetimeIndex(['2015-10-12', '2016-10-10', '2017-10-09', '2018-10-08',\n               '2019-10-14'],\n              dtype='datetime64[ns]', freq='WOM-2MON')",
            "code"
        ]
    ],
    "Time Series->Timeseries->Grab Bag->Timezones": [
        [
            "Pandas works with pytz for nice timezone-aware datetimes.\nThe typical workflow islocalize timezone-naive timestamps to some timezoneconvert to desired timezone",
            "markdown"
        ],
        [
            "If you already have timezone-aware Timestamps, there\u2019s no need for step one.",
            "markdown"
        ],
        [
            "# tz naiive -&gt; tz aware..... to desired UTC\ngs.tz_localize('US/Eastern').tz_convert('UTC').head()",
            "code"
        ]
    ],
    "Time Series->Timeseries->Modeling Time Series": [
        [
            "The rest of this post will focus on time series in the econometric sense.\nMy indented reader for this section isn\u2019t all that clear, so I apologize upfront for any sudden shifts in complexity.\nI\u2019m roughly targeting material that could be presented in a first or second semester applied statisctics course.\nWhat follows certainly isn\u2019t a replacement for that.\nAny formality will be restricted to footnotes for the curious.\nI\u2019ve put a whole bunch of resources at the end for people earger to learn more.",
            "markdown"
        ],
        [
            "We\u2019ll focus on modelling Average Monthly Flights. Let\u2019s download the data.\nIf you\u2019ve been following along in the series, you\u2019ve seen most of this code before, so feel free to skip.",
            "markdown"
        ],
        [
            "import os\nimport io\nimport glob\nimport zipfile\nfrom utils import download_timeseries\n\nimport statsmodels.api as sm\n\n\ndef download_many(start, end):\n    months = pd.period_range(start, end=end, freq='M')\n    # We could easily parallelize this loop.\n    for i, month in enumerate(months):\n        download_timeseries(month)\n\n\ndef time_to_datetime(df, columns):\n    '''\n    Combine all time items into datetimes.\n\n    2014-01-01,1149.0 -&gt; 2014-01-01T11:49:00\n    '''\n    def converter(col):\n        timepart = (col.astype(str)\n                       .str.replace('\\.0$', '')  # NaNs force float dtype\n                       .str.pad(4, fillchar='0'))\n        return  pd.to_datetime(df['fl_date'] + ' ' +\n                               timepart.str.slice(0, 2) + ':' +\n                               timepart.str.slice(2, 4),\n                               errors='coerce')\n        return datetime_part\n    df[columns] = df[columns].apply(converter)\n    return df\n\n\ndef read_one(fp):\n    df = (pd.read_csv(fp, encoding='latin1')\n            .rename(columns=str.lower)\n            .drop('unnamed: 6', axis=1)\n            .pipe(time_to_datetime, ['dep_time', 'arr_time', 'crs_arr_time',\n                                     'crs_dep_time'])\n            .assign(fl_date=lambda x: pd.to_datetime(x['fl_date'])))\n    return df",
            "code"
        ],
        [
            "/Users/taugspurger/miniconda3/envs/modern-pandas/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools",
            "code"
        ],
        [
            "store = 'data/ts.hdf5'\n\nif not os.path.exists(store):\n    download_many('2000-01-01', '2016-01-01')\n\n    zips = glob.glob(os.path.join('data', 'timeseries', '*.zip'))\n    csvs = [unzip_one(fp) for fp in zips]\n    dfs = [read_one(fp) for fp in csvs]\n    df = pd.concat(dfs, ignore_index=True)\n\n    df['origin'] = df['origin'].astype('category')\n    df.to_hdf(store, 'ts', format='table')\nelse:\n    df = pd.read_hdf(store, 'ts')",
            "code"
        ],
        [
            "with pd.option_context('display.max_rows', 100):\n    print(df.dtypes)",
            "code"
        ],
        [
            "fl_date         datetime64[ns]\norigin                category\ncrs_dep_time    datetime64[ns]\ndep_time        datetime64[ns]\ncrs_arr_time    datetime64[ns]\narr_time        datetime64[ns]\ndtype: object",
            "code"
        ],
        [
            "We can calculate the historical values with a resample.",
            "markdown"
        ],
        [
            "daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "code"
        ],
        [
            "2000-01-01    15176.677419\n2000-02-01    15327.551724\n2000-03-01    15578.838710\n2000-04-01    15442.100000\n2000-05-01    15448.677419\nFreq: MS, Name: fl_date, dtype: float64",
            "code"
        ],
        [
            "Note that I use the \"MS\" frequency code there.\nPandas defaults to end of month (or end of year).\nAppend an 'S' to get the start.",
            "markdown"
        ],
        [
            "ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_41_0.png\"/>",
            "markdown"
        ],
        [
            "import statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm",
            "code"
        ],
        [
            "One note of warning: I\u2019m using the development version of statsmodels (commit de15ec8 to be precise).\nNot all of the items I\u2019ve shown here are available in the currently-released version.",
            "markdown"
        ],
        [
            "Think back to a typical regression problem, ignoring anything to do with time series for now.\nThe usual task is to predict some value $y$ using some a linear combination of features in $X$.",
            "markdown"
        ],
        [
            "$$y = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p + \\epsilon$$",
            "markdown"
        ],
        [
            "When working with time series, some of the most important (and sometimes <em>only</em>) features are the previous, or <em>lagged</em>, values of $y$.",
            "markdown"
        ],
        [
            "Let\u2019s start by trying just that \u201cmanually\u201d: running a regression of y on lagged values of itself.\nWe\u2019ll see that this regression suffers from a few problems: multicollinearity, autocorrelation, non-stationarity, and seasonality.\nI\u2019ll explain what each of those are in turn and why they\u2019re problems.\nAfterwards, we\u2019ll use a second model, seasonal ARIMA, which handles those problems for us.",
            "markdown"
        ],
        [
            "First, let\u2019s create a dataframe with our lagged values of y using the .shift method, which shifts the index i periods, so it lines up with that observation.",
            "markdown"
        ],
        [
            "X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "code"
        ],
        [
            "We can fit the lagged model using statsmodels (which uses  to translate the formula string to a design matrix).",
            "markdown"
        ],
        [
            "mod_lagged = smf.ols('y ~ trend + L1 + L2 + L3 + L4 + L5',\n                     data=X.assign(trend=np.arange(len(X))))\nres_lagged = mod_lagged.fit()\nres_lagged.summary()",
            "code"
        ],
        [
            "There are a few problems with this approach though.\nSince our lagged values are highly correlated with each other, our regression suffers from .\nThat ruins our estimates of the slopes.",
            "markdown"
        ],
        [
            "sns.heatmap(X.corr());",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_48_0.png\"/>",
            "markdown"
        ],
        [
            "Second, we\u2019d intuitively expect the $\\beta_i$s to gradually decline to zero.\nThe immediately preceding period <em>should</em> be most important ($\\beta_1$ is the largest coefficient in absolute value), followed by $\\beta_2$, and $\\beta_3$&amp;mldr;\nLooking at the regression summary and the bar graph below, this isn\u2019t the case (the cause is related to multicollinearity).",
            "markdown"
        ],
        [
            "ax = res_lagged.params.drop(['Intercept', 'trend']).plot.bar(rot=0)\nplt.ylabel('Coefficeint')\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_50_0.png\"/>",
            "markdown"
        ],
        [
            "Finally, our degrees of freedom drop since we lose two for each variable (one for estimating the coefficient, one for the lost observation as a result of the shift).\nAt least in (macro)econometrics, each observation is precious and we\u2019re loath to throw them away, though sometimes that\u2019s unavoidable.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->Modeling Time Series->Autocorrelation": [
        [
            "Another problem our lagged model suffered from is  (also know as serial correlation).\nRoughly speaking, autocorrelation is when there\u2019s a clear pattern in the residuals of your regression (the observed minus the predicted).\nLet\u2019s fit a simple model of $y = \\beta_0 + \\beta_1 T + \\epsilon$, where T is the time trend (np.arange(len(y))).",
            "markdown"
        ],
        [
            "# `Results.resid` is a Series of residuals: y - \u0177\nmod_trend = sm.OLS.from_formula(\n    'y ~ trend', data=y.to_frame(name='y')\n                       .assign(trend=np.arange(len(y))))\nres_trend = mod_trend.fit()",
            "code"
        ],
        [
            "Residuals (the observed minus the expected, or $\\hat{e_t} = y_t - \\hat{y_t}$) are supposed to be .\nThat\u2019s  many of the properties of linear regression are founded upon.\nIn this case there\u2019s a correlation between one residual and the next: if the residual at time $t$ was above expectation, then the residual at time $t + 1$ is <em>much</em> more likely to be above average as well ($e_t &gt; 0 \\implies E_t[e_{t+1}] &gt; 0$).",
            "markdown"
        ],
        [
            "We\u2019ll define a helper function to plot the residuals time series, and some diagnostics about them.",
            "markdown"
        ],
        [
            "def tsplot(y, lags=None, figsize=(10, 8)):\n    fig = plt.figure(figsize=figsize)\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n    \n    y.plot(ax=ts_ax)\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    [ax.set_xlim(1.5) for ax in [acf_ax, pacf_ax]]\n    sns.despine()\n    plt.tight_layout()\n    return ts_ax, acf_ax, pacf_ax",
            "code"
        ],
        [
            "Calling it on the residuals from the linear trend:",
            "markdown"
        ],
        [
            "tsplot(res_trend.resid, lags=36);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_58_0.png\"/>",
            "markdown"
        ],
        [
            "The top subplot shows the time series of our residuals $e_t$, which should be white noise (but it isn\u2019t).\nThe bottom shows the  of the residuals as a correlogram.\nIt measures the correlation between a value and it\u2019s lagged self, e.g. $corr(e_t, e_{t-1}), corr(e_t, e_{t-2}), \\ldots$.\nThe partial autocorrelation plot in the bottom-right shows a similar concept.\nIt\u2019s partial in the sense that the value for $corr(e_t, e_{t-k})$ is the correlation between those two periods, after controlling for the values at all shorter lags.",
            "markdown"
        ],
        [
            "Autocorrelation is a problem in regular regressions like above, but we\u2019ll use it to our advantage when we setup an ARIMA model below.\nThe basic idea is pretty sensible: if your regression residuals have a clear pattern, then there\u2019s clearly some structure in the data that you aren\u2019t taking advantage of.\nIf a positive residual today means you\u2019ll likely have a positive residual tomorrow, why not incorporate that information into your forecast, and lower your forecasted value for tomorrow?\nThat\u2019s pretty much what ARIMA does.",
            "markdown"
        ],
        [
            "It\u2019s important that your dataset be stationary, otherwise you run the risk of finding .\nA common example is the relationship between number of TVs per person and life expectancy.\nIt\u2019s not likely that there\u2019s an actual causal relationship there.\nRather, there could be a third variable that\u2019s driving both (wealth, say).\n had some stern words for the econometrics literature on this.<blockquote>",
            "markdown"
        ],
        [
            "We find it very curious that whereas virtually every textbook on econometric methodology contains explicit warnings of the dangers of autocorrelated errors, this phenomenon crops up so frequently in well-respected applied work.</blockquote>",
            "markdown"
        ],
        [
            "(:fire:), but in that academic passive-aggressive way.",
            "markdown"
        ],
        [
            "The typical way to handle non-stationarity is to difference the non-stationary variable until is is stationary.",
            "markdown"
        ],
        [
            "y.to_frame(name='y').assign(\u0394y=lambda x: x.y.diff()).plot(subplots=True)\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_61_0.png\"/>",
            "markdown"
        ],
        [
            "Our original series actually doesn\u2019t look <em>that</em> bad.\nIt doesn\u2019t look like nominal GDP say, where there\u2019s a clearly rising trend.\nBut we have more rigorous methods for detecting whether a series is non-stationary than simply plotting and squinting at it.\nOne popular method is the Augmented Dickey-Fuller test.\nIt\u2019s a statistical hypothesis test that roughly says:",
            "markdown"
        ],
        [
            "$H_0$ (null hypothesis): $y$ is non-stationary, needs to be differenced",
            "markdown"
        ],
        [
            "$H_A$ (alternative hypothesis): $y$ is stationary, doesn\u2019t need to be differenced",
            "markdown"
        ],
        [
            "I don\u2019t want to get into the weeds on exactly what the test statistic is, and what the distribution looks like.\nThis is implemented in statsmodels as .\nThe return type is a bit busy for me, so we\u2019ll wrap it in a namedtuple.",
            "markdown"
        ],
        [
            "from collections import namedtuple\n\nADF = namedtuple(\"ADF\", \"adf pvalue usedlag nobs critical icbest\")",
            "code"
        ],
        [
            "ADF(*smt.adfuller(y))._asdict()",
            "code"
        ],
        [
            "OrderedDict([('adf', -1.3206520699512339),\n             ('pvalue', 0.61967180643147923),\n             ('usedlag', 15),\n             ('nobs', 177),\n             ('critical',\n              {'1%': -3.4678453197999071,\n               '10%': -2.575551186759871,\n               '5%': -2.8780117454974392}),\n             ('icbest', 2710.6120408261486)])",
            "code"
        ],
        [
            "So we failed to reject the null hypothesis that the original series was non-stationary.\nLet\u2019s difference it.",
            "markdown"
        ],
        [
            "ADF(*smt.adfuller(y.diff().dropna()))._asdict()",
            "code"
        ],
        [
            "OrderedDict([('adf', -3.6412428797327996),\n             ('pvalue', 0.0050197770854934548),\n             ('usedlag', 14),\n             ('nobs', 177),\n             ('critical',\n              {'1%': -3.4678453197999071,\n               '10%': -2.575551186759871,\n               '5%': -2.8780117454974392}),\n             ('icbest', 2696.3891181091631)])",
            "code"
        ],
        [
            "This looks better.\nIt\u2019s not statistically significant at the 5% level, but who cares what statisticins say anyway.",
            "markdown"
        ],
        [
            "We\u2019ll fit another OLS model of $\\Delta y = \\beta_0 + \\beta_1 L \\Delta y_{t-1} + e_t$",
            "markdown"
        ],
        [
            "data = (y.to_frame(name='y')\n         .assign(\u0394y=lambda df: df.y.diff())\n         .assign(L\u0394y=lambda df: df.\u0394y.shift()))\nmod_stationary = smf.ols('\u0394y ~ L\u0394y', data=data.dropna())\nres_stationary = mod_stationary.fit()",
            "code"
        ],
        [
            "tsplot(res_stationary.resid, lags=24);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_69_0.png\"/>",
            "markdown"
        ],
        [
            "So we\u2019ve taken care of multicolinearity, autocorelation, and stationarity, but we still aren\u2019t done.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->Seasonality": [
        [
            "We have strong monthly seasonality:",
            "markdown"
        ],
        [
            "smt.seasonal_decompose(y).plot();",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_73_0.png\"/>",
            "markdown"
        ],
        [
            "There are a few ways to handle seasonality.\nWe\u2019ll just rely on the SARIMAX method to do it for us.\nFor now, recognize that it\u2019s a problem to be solved.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->ARIMA": [
        [
            "So, we\u2019ve sketched the problems with regular old regression: multicollinearity, autocorrelation, non-stationarity, and seasonality.\nOur tool of choice, smt.SARIMAX, which stands for Seasonal ARIMA with eXogenous regressors, can handle all these.\nWe\u2019ll walk through the components in pieces.",
            "markdown"
        ],
        [
            "ARIMA stands for AutoRegressive Integrated Moving Average.\nIt\u2019s a relatively simple yet flexible way of modeling univariate time series.\nIt\u2019s made up of three components, and is typically written as $\\mathrm{ARIMA}(p, d, q)$.",
            "markdown"
        ],
        [
            "ARIMA stands for AutoRegressive Integrated Moving Average, and it\u2019s a relatively simple way of modeling univariate time series.\nIt\u2019s made up of three components, and is typically written as $\\mathrm{ARIMA}(p, d, q)$.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->ARIMA->": [
        [
            "The idea is to predict a variable by a linear combination of its lagged values (<em>auto</em>-regressive as in regressing a value on its past <em>self</em>).\nAn AR(p), where $p$ represents the number of lagged values used, is written as",
            "markdown"
        ],
        [
            "$$y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\ldots + \\phi_p y_{t-p} + e_t$$",
            "markdown"
        ],
        [
            "$c$ is a constant and $e_t$ is white noise.\nThis looks a lot like a linear regression model with multiple predictors, but the predictors happen to be lagged values of $y$ (though they are estimated differently).",
            "markdown"
        ],
        [
            "MA models look somewhat similar to the AR component, but it\u2019s dealing with different values.",
            "markdown"
        ],
        [
            "$$y_t = c + e_t + \\theta_1 e_{t-1} + \\theta_2 e_{t-2} + \\ldots + \\theta_q e_{t-q}$$",
            "markdown"
        ],
        [
            "$c$ again is a constant and $e_t$ again is white noise.\nBut now the coefficients are the <em>residuals</em> from previous predictions.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->ARIMA->Integrated": [
        [
            "Integrated is like the opposite of differencing, and is the part that deals with stationarity.\nIf you have to difference your dataset 1 time to get it stationary, then $d=1$.\nWe\u2019ll introduce one bit of notation for differencing: $\\Delta y_t = y_t - y_{t-1}$ for $d=1$.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->ARIMA->Combining": [
        [
            "Putting that together, an ARIMA(1, 1, 1) process is written as",
            "markdown"
        ],
        [
            "$$\\Delta y_t = c + \\phi_1 \\Delta y_{t-1} + \\theta_t e_{t-1} + e_t$$",
            "markdown"
        ],
        [
            "Using <em>lag notation</em>, where $L y_t = y_{t-1}$, i.e. y.shift() in pandas, we can rewrite that as",
            "markdown"
        ],
        [
            "$$(1 - \\phi_1 L) (1 - L)y_t = c + (1 + \\theta L)e_t$$",
            "markdown"
        ],
        [
            "That was for our specific $\\mathrm{ARIMA}(1, 1, 1)$ model. For the general $\\mathrm{ARIMA}(p, d, q)$, that becomes",
            "markdown"
        ],
        [
            "$$(1 - \\phi_1 L - \\ldots - \\phi_p L^p) (1 - L)^d y_t = c + (1 + \\theta L + \\ldots + \\theta_q L^q)e_t$$",
            "markdown"
        ],
        [
            "We went through that <em>extremely</em> quickly, so don\u2019t feel bad if things aren\u2019t clear.\nFortunately, the model is pretty easy to use with statsmodels (using it <em>correctly</em>, in a statistical sense, is another matter).",
            "markdown"
        ],
        [
            "mod = smt.SARIMAX(y, trend='c', order=(1, 1, 1))\nres = mod.fit()\ntsplot(res.resid[2:], lags=24);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_81_0.png\"/>",
            "markdown"
        ],
        [
            "res.summary()",
            "code"
        ],
        [
            "There\u2019s a bunch of output there with various tests, estimated parameters, and information criteria.\nLet\u2019s just say that things are looking better, but we still haven\u2019t accounted for seasonality.",
            "markdown"
        ],
        [
            "A seasonal ARIMA model is written as $\\mathrm{ARIMA}(p,d,q)\u00d7(P,D,Q)_s$.\nLowercase letters are for the non-seasonal component, just like before. Upper-case letters are a similar specification for the seasonal component, where $s$ is the periodicity (4 for quarterly, 12 for monthly).",
            "markdown"
        ],
        [
            "It\u2019s like we have two processes, one for non-seasonal component and one for seasonal components, and we multiply them together with regular algebra rules.",
            "markdown"
        ],
        [
            "The general form of that looks like (quoting the  here)",
            "markdown"
        ],
        [
            "$$\\phi_p(L)\\tilde{\\phi}_P(L^S)\\Delta^d\\Delta_s^D y_t = A(t) + \\theta_q(L)\\tilde{\\theta}_Q(L^s)e_t$$",
            "markdown"
        ],
        [
            "where$\\phi_p(L)$ is the non-seasonal autoregressive lag polynomial$\\tilde{\\phi}_P(L^S)$ is the seasonal autoregressive lag polynomial$\\Delta^d\\Delta_s^D$ is the time series, differenced $d$ times, and seasonally differenced $D$ times.$A(t)$ is the trend polynomial (including the intercept)$\\theta_q(L)$ is the non-seasonal moving average lag polynomial$\\tilde{\\theta}_Q(L^s)$ is the seasonal moving average lag polynomial",
            "markdown"
        ],
        [
            "I don\u2019t find that to be very clear, but maybe an example will help.\nWe\u2019ll fit a seasonal ARIMA$(1,1,2)\u00d7(0, 1, 2)_{12}$.",
            "markdown"
        ],
        [
            "So the nonseasonal component is$p=1$: period autoregressive: use $y_{t-1}$$d=1$: one first-differencing of the data (one month)$q=2$: use the previous two non-seasonal residual, $e_{t-1}$ and $e_{t-2}$, to forecast",
            "markdown"
        ],
        [
            "And the seasonal component is$P=0$: Don\u2019t use any previous seasonal values$D=1$: Difference the series 12 periods back: y.diff(12)$Q=2$: Use the two previous seasonal residuals",
            "markdown"
        ],
        [
            "mod_seasonal = smt.SARIMAX(y, trend='c',\n                           order=(1, 1, 2), seasonal_order=(0, 1, 2, 12),\n                           simple_differencing=False)\nres_seasonal = mod_seasonal.fit()",
            "code"
        ],
        [
            "res_seasonal.summary()",
            "code"
        ],
        [
            "tsplot(res_seasonal.resid[12:], lags=24);",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_86_0.png\"/>",
            "markdown"
        ],
        [
            "Things look much better now.",
            "markdown"
        ],
        [
            "One thing I didn\u2019t really talk about is order selection. How to choose $p, d, q, P, D$ and $Q$.\nR\u2019s forecast package does have a handy auto.arima function that does this for you.\nPython / statsmodels don\u2019t have that at the minute.\nThe alternative seems to be experience (boo), intuition (boo), and good-old grid-search.\nYou can fit a bunch of models for a bunch of combinations of the parameters and use the  or  to choose the best.\n is a useful reference, and  StackOverflow answer recommends a few options.",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->Forecasting": [
        [
            "Now that we fit that model, let\u2019s put it to use.\nFirst, we\u2019ll make a bunch of one-step ahead forecasts.\nAt each point (month), we take the history up to that point and make a forecast for the next month.\nSo the forecast for January 2014 has available all the data up through December 2013.",
            "markdown"
        ],
        [
            "pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "code"
        ],
        [
            "ax = y.plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_ylabel(\"Monthly Flights\")\nplt.legend()\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_91_0.png\"/>",
            "markdown"
        ],
        [
            "There are a few places where the observed series slips outside the 95% confidence interval.\nThe series seems especially unstable before 2005.",
            "markdown"
        ],
        [
            "Alternatively, we can make <em>dynamic</em> forecasts as of some month (January 2013 in the example below).\nThat means the forecast from that point forward only use information available as of January 2013.\nThe predictions are generated in a similar way: a bunch of one-step forecasts.\nOnly instead of plugging in the <em>actual</em> values beyond January 2013, we plug in the <em>forecast</em> values.",
            "markdown"
        ],
        [
            "pred_dy = res_seasonal.get_prediction(start='2002-03-01', dynamic='2013-01-01')\npred_dy_ci = pred_dy.conf_int()",
            "code"
        ],
        [
            "ax = y.plot(label='observed')\npred_dy.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_dy_ci.index,\n                pred_dy_ci.iloc[:, 0],\n                pred_dy_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_ylabel(\"Monthly Flights\")\n\n# Highlight the forecast area\nax.fill_betweenx(ax.get_ylim(), pd.Timestamp('2013-01-01'), y.index[-1],\n                 alpha=.1, zorder=-1)\nax.annotate('Dynamic $\\\\longrightarrow$', (pd.Timestamp('2013-02-01'), 550))\n\nplt.legend()\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern_7_timeseries_94_0.png\"/>",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->Resources": [
        [
            "This is a collection of links for those interested.",
            "markdown"
        ],
        [
            "Time series modeling in Python",
            "markdown"
        ],
        [
            "General Textbooks: A great introduction: Readable undergraduate resource, has a few chapters on time series: My favorite PhD level textbook: A classic: Extremely dry, but useful if you\u2019re implementing this stuff",
            "markdown"
        ]
    ],
    "Time Series->Timeseries->Conclusion": [
        [
            "Congratulations if you made it this far, this piece just kept growing (and I still had to cut stuff).\nThe main thing cut was talking about how SARIMAX is implemented on top of using statsmodels\u2019 statespace framework.\nThe statespace framework, developed mostly by Chad Fulton over the past couple years, is really nice.\nYou can pretty easily  with custom models, but still get all the benefits of the framework\u2019s estimation and results facilities.\nI\u2019d recommend reading the .\nWe also didn\u2019t get to talk at all about Skipper Seabold\u2019s work on VARs, but maybe some other time.",
            "markdown"
        ],
        [
            "As always, .",
            "markdown"
        ],
        [
            "",
            "code"
        ]
    ],
    "Scaling": [
        [
            "This is part 1 in my series on writing modern idiomatic pandas.",
            "markdown"
        ],
        [
            "As I sit down to write this, the third-most popular pandas question on StackOverflow covers . This is in tension with the fact that a pandas DataFrame is an in memory container. <em>You can\u2019t have a DataFrame larger than your machine\u2019s RAM</em>. In practice, your available RAM should be several times the size of your dataset, as you or pandas will have to make intermediate copies as part of the analysis.",
            "markdown"
        ],
        [
            "Historically, pandas users have scaled to larger datasets by switching away from pandas or using iteration. Both of these are perfectly valid approaches, but changing your workflow in response to scaling data is unfortunate. I use pandas because it\u2019s a pleasant experience, and I would like that experience to scale to larger datasets. That\u2019s what , a parallel computing library, enables. We\u2019ll discuss Dask in detail later. But first, let\u2019s work through scaling a simple analysis to a larger than memory dataset.",
            "markdown"
        ],
        [
            "Our task is to find the 100 most-common occupations reported in the FEC\u2019s . The files are split by election cycle (2007-2008, 2009-2010, &amp;mldr;). You can find some scripts for downloading the data in . My laptop can read in each cycle\u2019s file individually, but the full dataset is too large to read in at once. Let\u2019s read in just 2010\u2019s file, and do the \u201csmall data\u201d version.",
            "markdown"
        ],
        [
            "from pathlib import Path\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_parquet(\"data/indiv-10.parq\", columns=['occupation'], engine='pyarrow')\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "code"
        ],
        [
            "    RETIRED                    279775\n    ATTORNEY                   166768\n    PRESIDENT                   81336\n    PHYSICIAN                   73015\n    HOMEMAKER                   66057\n                                ...  \n    C.E.O.                       1945\n    EMERGENCY PHYSICIAN          1944\n    BUSINESS EXECUTIVE           1924\n    BUSINESS REPRESENTATIVE      1879\n    GOVERNMENT AFFAIRS           1867\n    Name: occupation, Length: 100, dtype: int64",
            "code"
        ],
        [
            "After reading in the file, our actual analysis is a simple 1-liner using two operations built into pandas. Truly, the best of all possible worlds.",
            "markdown"
        ],
        [
            "Next, we\u2019ll do the analysis for the entire dataset, which is larger than memory, in two ways. First we\u2019ll use just pandas and iteration. Then we\u2019ll use Dask.",
            "markdown"
        ]
    ],
    "Scaling->Using Iteration": [
        [
            "To do this with just pandas we have to rewrite our code, taking care to never have too much data in RAM at once. We willCreate a global total_counts Series that contains the counts from all of the files processed so farRead in a fileCompute a temporary variable counts with the counts for just this fileAdd that temporary counts into the global total_countsSelect the 100 largest with .nlargest",
            "markdown"
        ],
        [
            "This works since the total_counts Series is relatively small, and each year\u2019s data fits in RAM individually. Our peak memory usage should be the size of the largest individual cycle (2015-2016) plus the size of total_counts (which we can essentially ignore).",
            "markdown"
        ],
        [
            "files = sorted(Path(\"data/\").glob(\"indiv-*.parq\"))\n\ntotal_counts = pd.Series()\n\nfor year in files:\n    df = pd.read_parquet(year, columns=['occupation'],\n                         engine=\"pyarrow\")\n    counts = df.occupation.value_counts()\n    total_counts = total_counts.add(counts, fill_value=0)\n\ntotal_counts = total_counts.nlargest(100).sort_values(ascending=False)",
            "code"
        ],
        [
            "RETIRED                    4769520\nNOT EMPLOYED               2656988\nATTORNEY                   1340434\nPHYSICIAN                   659082\nHOMEMAKER                   494187\n                            ...   \nCHIEF EXECUTIVE OFFICER      26551\nSURGEON                      25521\nEDITOR                       25457\nOPERATOR                     25151\nORTHOPAEDIC SURGEON          24384\nName: occupation, Length: 100, dtype: int64",
            "code"
        ],
        [
            "While this works, our small one-liner has ballooned in size (and complexity; should you <em>really</em> have to know about Series.add\u2019s fill_value parameter for this simple analysis?). If only there was a better way&amp;mldr;",
            "markdown"
        ]
    ],
    "Scaling->Using Dask": [
        [
            "With Dask, we essentially recover our original code. We\u2019ll change our import to use dask.dataframe.read_parquet, which returns a Dask DataFrame.",
            "markdown"
        ],
        [
            "import dask.dataframe as dd",
            "code"
        ],
        [
            "df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "code"
        ],
        [
            "RETIRED                    4769520\nNOT EMPLOYED               2656988\nATTORNEY                   1340434\nPHYSICIAN                   659082\nHOMEMAKER                   494187\n                            ...   \nCHIEF EXECUTIVE OFFICER      26551\nSURGEON                      25521\nEDITOR                       25457\nOPERATOR                     25151\nORTHOPAEDIC SURGEON          24384\nName: occupation, Length: 100, dtype: int64",
            "code"
        ],
        [
            "There are a couple differences from the original pandas version, which we\u2019ll discuss next, but overall I hope you agree that the Dask version is nicer than the version using iteration.",
            "markdown"
        ]
    ],
    "Scaling->Dask": [
        [
            "Now that we\u2019ve seen dask.dataframe in action, let\u2019s step back and discuss Dask a bit. Dask is an open-source project that natively parallizes Python. I\u2019m a happy user of and contributor to Dask.",
            "markdown"
        ],
        [
            "At a high-level, Dask provides familiar APIs for , , and  ways to parallelize .",
            "markdown"
        ],
        [
            "At a low-level, each of these is built on high-performance  that executes operations in parallel. The  aren\u2019t too important; all we care about is thatDask works with <em>task graphs</em> (<em>tasks</em>: functions to call on data, and <em>graphs</em>: the relationships between tasks).This is a flexible and performant way to parallelize many different kinds of problems.",
            "markdown"
        ],
        [
            "To understand point 1, let\u2019s examine the difference between a Dask DataFrame and a pandas DataFrame. When we read in df with dd.read_parquet, we received a Dask DataFrame.",
            "markdown"
        ],
        [
            "df\n<strong>Dask DataFrame Structure:</strong>Dask Name: read-parquet, 35 tasks",
            "code"
        ],
        [
            "A Dask DataFrame consists of many pandas DataFrames arranged by the index. Dask is really just coordinating these pandas DataFrames.<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"50%\"/>",
            "markdown"
        ],
        [
            "All the actual computation (reading from disk, computing the value counts, etc.) eventually use pandas internally. If I do df.occupation.str.len, Dask will coordinate calling pandas.Series.str.len on each of the pandas DataFrames.",
            "markdown"
        ],
        [
            "Those reading carefully will notice a problem with the statement \u201cA Dask DataFrame consists of many pandas DataFrames\u201d. Our initial problem was that we didn\u2019t have enough memory for those DataFrames! How can Dask be coordinating DataFrames if there isn\u2019t enough memory? This brings us to the second major difference: Dask DataFrames (and arrays) are lazy. Operations on them don\u2019t execute and produce the final result immediately. Rather, calling methods on them builds up a task graph.",
            "markdown"
        ],
        [
            "We can visualize task graphs using graphviz. For the blog, I\u2019ve trimmed down the example to be a subset of the entire graph.",
            "markdown"
        ],
        [
            "df.visualize(rankdir='LR')",
            "code"
        ],
        [
            "<img alt=\"\" loading=\"lazy\" src=\"/images/scalable-read-simple.svg\"/>",
            "markdown"
        ],
        [
            "df (the dask DataFrame consisting of many pandas DataFrames) has a task graph with 5 calls to a parquet reader (one for each file), each of which produces a DataFrame when called.",
            "markdown"
        ],
        [
            "Calling additional methods on df adds additional tasks to this graph. For example, our most_common Series has three additional callsSelect the occupation column (__getitem__)Perform the value countsSelect the 100 largest values",
            "markdown"
        ],
        [
            "most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "code"
        ],
        [
            "    Dask Series Structure:\n    npartitions=1\n        int64\n          ...\n    Name: occupation, dtype: int64\n    Dask Name: series-nlargest-agg, 113 tasks",
            "code"
        ],
        [
            "Which we can visualize.",
            "markdown"
        ],
        [
            "most_common.visualize(rankdir='LR')",
            "code"
        ],
        [
            "<img alt=\"\" loading=\"lazy\" src=\"/images/scalable-most-common.svg\"/>",
            "markdown"
        ],
        [
            "So most_common doesn\u2019t hold the actual answer yet. Instead, it holds a recipe for the answer; a list of all the steps to take to get the concrete result. One way to ask for the result is with the compute method.",
            "markdown"
        ],
        [
            "most_common.compute()",
            "code"
        ],
        [
            "    RETIRED                    4769520\n    NOT EMPLOYED               2656988\n    ATTORNEY                   1340434\n    PHYSICIAN                   659082\n    HOMEMAKER                   494187\n                                ...   \n    CHIEF EXECUTIVE OFFICER      26551\n    SURGEON                      25521\n    EDITOR                       25457\n    OPERATOR                     25151\n    ORTHOPAEDIC SURGEON          24384\n    Name: occupation, Length: 100, dtype: int64",
            "code"
        ],
        [
            "At this point, the task graph is handed to a , which is responsible for executing a task graph. Schedulers can analyze a task graph and find sections that can run <em>in parallel</em>. (Dask includes several schedulers. See  for how to choose, though Dask has good defaults.)",
            "markdown"
        ],
        [
            "So that\u2019s a high-level tour of how Dask works:",
            "markdown"
        ],
        [
            "<img alt=\"collections, schedulers\" loading=\"lazy\" src=\"http://dask.pydata.org/en/latest/_images/collections-schedulers.png\"/>Various collections collections like dask.dataframe and dask.array\nprovide users familiar APIs for working with large datasets.Computations are represented as a task graph. These graphs could be built by\nhand, or more commonly built by one of the collections.Dask schedulers run task graphs in parallel (potentially distributed across\na cluster), reusing libraries like NumPy and pandas to do the computations.",
            "markdown"
        ],
        [
            "Let\u2019s finish off this post by continuing to explore the FEC dataset with Dask. At this point, we\u2019ll use the distributed scheduler for it\u2019s nice diagnostics.",
            "markdown"
        ],
        [
            "import dask.dataframe as dd\nfrom dask import compute\nfrom dask.distributed import Client\nimport seaborn as sns\n\nclient = Client(processes=False)",
            "code"
        ],
        [
            "Calling Client without providing a scheduler address will make a local \u201ccluster\u201d of threads or processes on your machine. There are  to deploy a Dask cluster onto an actual cluster of machines, though we\u2019re particularly fond of . This highlights one of my favorite features of Dask: it scales down to use a handful of threads on a laptop <em>or</em> up to a cluster with thousands of nodes. Dask can comfortably handle medium-sized datasets (dozens of GBs, so larger than RAM) on a laptop. Or it can scale up to very large datasets with a cluster.",
            "markdown"
        ],
        [
            "individual_cols = ['cmte_id', 'entity_tp', 'employer', 'occupation',\n                   'transaction_dt', 'transaction_amt']\n\nindiv = dd.read_parquet('data/indiv-*.parq',\n                        columns=individual_cols,\n                        engine=\"pyarrow\")\nindiv\n<strong>Dask DataFrame Structure:</strong>Dask Name: read-parquet, 5 tasks",
            "code"
        ],
        [
            "We can compute summary statistics like the average mean and standard deviation of the transaction amount:",
            "markdown"
        ],
        [
            "avg_transaction = indiv.transaction_amt.mean()",
            "code"
        ],
        [
            "We can answer questions like \u201cWhich employer\u2019s employees donated the most?\u201d",
            "markdown"
        ],
        [
            "total_by_employee = (\n    indiv.groupby('employer')\n        .transaction_amt.sum()\n        .nlargest(10)\n)",
            "code"
        ],
        [
            "Or \u201cwhat is the average amount donated per occupation?\u201d",
            "markdown"
        ],
        [
            "avg_by_occupation = (\n    indiv.groupby(\"occupation\")\n        .transaction_amt.mean()\n        .nlargest(10)\n)",
            "code"
        ],
        [
            "Since Dask is lazy, we haven\u2019t actually computed anything.",
            "markdown"
        ],
        [
            "total_by_employee",
            "code"
        ],
        [
            "    Dask Series Structure:\n    npartitions=1\n        int64\n          ...\n    Name: transaction_amt, dtype: int64\n    Dask Name: series-nlargest-agg, 13 tasks",
            "code"
        ],
        [
            "avg_transaction, avg_by_occupation and total_by_employee are three separate computations (they have different task graphs), but we know they share some structure: they\u2019re all reading in the same data, they might select the same subset of columns, and so on. Dask is able to avoid redundant computation when you use the top-level dask.compute function.",
            "markdown"
        ],
        [
            "%%time\navg_transaction, by_employee, by_occupation = compute(\n    avg_transaction, total_by_employee, avg_by_occupation\n)",
            "code"
        ],
        [
            "    CPU times: user 57.5 s, sys: 14.4 s, total: 1min 11s\n    Wall time: 54.9 s",
            "code"
        ],
        [
            "avg_transaction",
            "code"
        ],
        [
            "    566.0899206077507",
            "code"
        ],
        [
            "by_employee",
            "code"
        ],
        [
            "    employer\n    RETIRED                1019973117\n    SELF-EMPLOYED           834547641\n    SELF                    537402882\n    SELF EMPLOYED           447363032\n    NONE                    418011322\n    HOMEMAKER               355195126\n    NOT EMPLOYED            345770418\n    FAHR, LLC               166679844\n    CANDIDATE                75186830\n    ADELSON DRUG CLINIC      53358500\n    Name: transaction_amt, dtype: int64",
            "code"
        ],
        [
            "by_occupation",
            "code"
        ],
        [
            "    occupation\n    CHAIRMAN CEO &amp; FOUNDER                   1,023,333.33\n    PAULSON AND CO., INC.                    1,000,000.00\n    CO-FOUNDING DIRECTOR                       875,000.00\n    CHAIRMAN/CHIEF TECHNOLOGY OFFICER          750,350.00\n    CO-FOUNDER, DIRECTOR, CHIEF INFORMATIO     675,000.00\n    CO-FOUNDER, DIRECTOR                       550,933.33\n    MOORE CAPITAL GROUP, LP                    500,000.00\n    PERRY HOMES                                500,000.00\n    OWNER, FOUNDER AND CEO                     500,000.00\n    CHIEF EXECUTIVE OFFICER/PRODUCER           500,000.00\n    Name: transaction_amt, dtype: float64",
            "code"
        ],
        [
            "Things like filtering work well. Let\u2019s find the 10 most common occupations and filter the dataset down to just those.",
            "markdown"
        ],
        [
            "top_occupations = (\n    indiv.occupation.value_counts()\n        .nlargest(10).index\n).compute()\ntop_occupations",
            "code"
        ],
        [
            "    Index(['RETIRED', 'NOT EMPLOYED', 'ATTORNEY', 'PHYSICIAN', 'HOMEMAKER',\n           'PRESIDENT', 'PROFESSOR', 'CONSULTANT', 'EXECUTIVE', 'ENGINEER'],\n          dtype='object')",
            "code"
        ],
        [
            "We\u2019ll filter the raw records down to just the ones from those occupations. Then we\u2019ll compute a few summary statistics on the transaction amounts for each group.",
            "markdown"
        ],
        [
            "donations = (\n    indiv[indiv.occupation.isin(top_occupations)]\n        .groupby(\"occupation\")\n        .transaction_amt\n        .agg(['count', 'mean', 'sum', 'max'])\n)",
            "code"
        ],
        [
            "total_avg, occupation_avg = compute(indiv.transaction_amt.mean(),\n                                    donations['mean'])",
            "code"
        ],
        [
            "These are small, concrete results so we can turn to familiar tools like matplotlib to visualize the result.",
            "markdown"
        ],
        [
            "ax = occupation_avg.sort_values(ascending=False).plot.barh(color='k', width=0.9);\nlim = ax.get_ylim()\nax.vlines(total_avg, *lim, color='C1', linewidth=3)\nax.legend(['Average donation'])\nax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\")\nsns.despine()",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern-pandas-08_49_0.png\"/>",
            "markdown"
        ],
        [
            "Dask inherits all of pandas\u2019 great time-series support. We can get the total amount donated per day using a .",
            "markdown"
        ],
        [
            "daily = (\n    indiv[['transaction_dt', 'transaction_amt']].dropna()\n        .set_index('transaction_dt')['transaction_amt']\n        .resample(\"D\")\n        .sum()\n).compute()\ndaily",
            "code"
        ],
        [
            "    1916-01-23    1000\n    1916-01-24       0\n    1916-01-25       0\n    1916-01-26       0\n    1916-01-27       0\n                  ... \n    2201-05-29       0\n    2201-05-30       0\n    2201-05-31       0\n    2201-06-01       0\n    2201-06-02    2000\n    Name: transaction_amt, Length: 104226, dtype: int64",
            "code"
        ],
        [
            "It seems like we have some bad data. This should just be 2007-2016. We\u2019ll filter it down to the real subset before plotting.\nNotice that the seamless transition from dask.dataframe operations above, to pandas operations below.",
            "markdown"
        ],
        [
            "subset = daily.loc['2011':'2016']\nax = subset.div(1000).plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",)\nsns.despine();",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern-pandas-08_54_0.png\"/>",
            "markdown"
        ]
    ],
    "Scaling->Joining": [
        [
            "Like pandas, Dask supports joining together multiple datasets.",
            "markdown"
        ],
        [
            "Individual donations are made to <em>committees</em>. Committees are what make the actual expenditures (buying a TV ad).\nSome committees are directly tied to a candidate (this are campaign committees). Other committees are tied to a group (like the Republican National Committee). Either may be tied to a party.",
            "markdown"
        ],
        [
            "Let\u2019s read in the committees. The total number of committees is small, so we\u2019ll .compute immediately to get a pandas DataFrame (the reads still happen in parallel!).",
            "markdown"
        ],
        [
            "committee_cols = ['cmte_id', 'cmte_nm', 'cmte_tp', 'cmte_pty_affiliation']\ncm = dd.read_parquet(\"data/cm-*.parq\",\n                     columns=committee_cols).compute()\n\n# Some committees change thier name, but the ID stays the same\ncm = cm.groupby('cmte_id').last()\ncm",
            "code"
        ],
        [
            "28612 rows \u00d7 3 columns",
            "markdown"
        ],
        [
            "We\u2019ll use dd.merge, which is analogous to pd.merge for joining a Dask DataFrame with a pandas or Dask DataFrame.",
            "markdown"
        ],
        [
            "indiv = indiv[(indiv.transaction_dt &gt;= pd.Timestamp(\"2007-01-01\")) &amp;\n              (indiv.transaction_dt &lt;= pd.Timestamp(\"2018-01-01\"))]\n\ndf2 = dd.merge(indiv, cm.reset_index(), on='cmte_id')\ndf2\n<strong>Dask DataFrame Structure:</strong>Dask Name: merge, 141 tasks",
            "code"
        ],
        [
            "Now we can find which party raised more over the course of each election. We\u2019ll group by the day and party and sum the transaction amounts.",
            "markdown"
        ],
        [
            "indiv = indiv.repartition(npartitions=10)\ndf2 = dd.merge(indiv, cm.reset_index(), on='cmte_id')\ndf2\n<strong>Dask DataFrame Structure:</strong>Dask Name: merge, 141 tasks",
            "code"
        ],
        [
            "party_donations = (\n    df2.groupby([df2.transaction_dt, 'cmte_pty_affiliation'])\n       .transaction_amt.sum()\n).compute().sort_index()",
            "code"
        ],
        [
            "We\u2019ll filter that down to just Republican and Democrats and plot.",
            "markdown"
        ],
        [
            "ax = (\n    party_donations.loc[:, ['REP', 'DEM']]\n        .unstack(\"cmte_pty_affiliation\").iloc[1:-2]\n        .rolling('30D').mean().plot(color=['C0', 'C3'], figsize=(12, 6),\n                                    linewidth=3)\n)\nsns.despine()\nax.set(title=\"Daily Donations (30-D Moving Average)\", xlabel=\"Date\");",
            "code"
        ],
        [
            "<img alt=\"png\" loading=\"lazy\" src=\"/images/modern-pandas-08_64_0.png\"/>",
            "markdown"
        ]
    ],
    "Scaling->Try It Out!": [
        [
            "So that\u2019s a taste of Dask. Next time you hit a scaling problem with pandas (or NumPy, scikit-learn, or your custom code), feel free to",
            "markdown"
        ],
        [
            "pip install dask[complete]",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "conda install dask",
            "code"
        ],
        [
            "The  has links to all the relevant documentation, and  where you can try out Dask before installing.",
            "markdown"
        ],
        [
            "As always, reach out to me on  or in the comments if you have anything to share.",
            "markdown"
        ]
    ]
}