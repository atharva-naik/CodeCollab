{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f330da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Newer than 13000 minutes:\\n')\n",
    "\n",
    "for df in new_dfs:\n",
    "    \n",
    "    print(df.query)\n",
    "    print(len(df.df))\n",
    "    \n",
    "print('\\nOlder than 13000 minutes:\\n')\n",
    "\n",
    "for df in old_dfs:\n",
    "    \n",
    "    print(df.query)\n",
    "    print(len(df.df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10397f2",
   "metadata": {},
   "source": [
    "<font size=\"3\">I'll also take this opportunity to make a list of dataframes which contain only positive tweets about the query as determined by the polarity measured by TextBlob. After all, I want to predict the type of fast food that a tweeter likes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3cc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_dfs = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for result in query_results:\n",
    "\n",
    "    good_dfs.append(make_query_result(\n",
    "        result.query, result.df[result.df['polarity'].values>0]))\n",
    "    \n",
    "    print(len(good_dfs[i].df))\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8f904a",
   "metadata": {},
   "source": [
    "<font size=\"3\">Most of the tweets were after the 13000 minute mark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669e51d",
   "metadata": {},
   "source": [
    "<font size=\"3\">We can also examine the correlation between different features of each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05baab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.viridis\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.title('Pearson Correlation: ' + result.query, y=1.05, size=15)\n",
    "    sns.heatmap(result.df.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de7250",
   "metadata": {},
   "source": [
    "<font size=\"3\">Some differences between queries can be seen. For instance, there is a high correlation between polarity and subjectivity for javascript (0.44). For McDonald's and Chick-fil-A this correlation is lower (0.2 and 0.13) and for KFC and Burger King there is no correlation (less than 0.1).<br><br>\n",
    "\n",
    "Another example is the relationship between retweets, likes, and replies. The correlation between these three features is very high for javascript queries and less so for fast food tweets.<br><br>\n",
    "\n",
    "These types of feature relationships are the ones that I'm relying on for determining the fast food preferences of tweets despite ignoring NLP data of tweets outside of sentiment analysis.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cb05c",
   "metadata": {},
   "source": [
    "<font size=\"3\">Another type of visualization I wanted to produce was a scatter plot to look at the clustering of different queries. I wanted to do this with a plotly interactive plot and was successful but found the large number of samples made the notebook slow and unresponsive. I instead define a \"num_samples\" variable which is the number of randomly chosen samples taken from each query dataframe to be plotted. This produces a less dense but similarly populated plot.<br><br>\n",
    "\n",
    "Non-numerical features are dropped from the dataframes prior to principal component analysis (PCA). PCA breaks down the features of each query into (in my case) 2 or 3 linear combinations of each feature. This allows me to plot the data points on a 2d or 3d plot to examine the clustering of different queries and how far they are from each other in this space.<br><br>\n",
    "\n",
    "I'll first make a function that creates an interactive 2d scatter plot with Plotly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make2dplot(df_list, num_samples):\n",
    "    \n",
    "\n",
    "    X = pd.DataFrame([])\n",
    "    Y = pd.DataFrame([])\n",
    "\n",
    "    for result in df_list:\n",
    "    \n",
    "        temp_df = result.df.drop(['raw_tweet', 'fullname', 'username',\n",
    "                                'time', 'hashtags', 'atreplies',\n",
    "                                'badges', 'tweet', 'elapsedtime',\n",
    "                                'weekday', 'tokens', 'query'], axis=1)\n",
    "    \n",
    "        temp_query = pd.DataFrame(result.df['query'])\n",
    "    \n",
    "        #need to reset index otherwise finding index with \n",
    "        #random numbers will fail for indexes which were \n",
    "        #sent to the new_dfs instead of old_dfs\n",
    "    \n",
    "        temp_df = temp_df.reset_index(drop=True)\n",
    "        temp_query = temp_query.reset_index(drop=True)\n",
    "    \n",
    "        index = random.sample(range(len(temp_df)), num_samples)\n",
    "    \n",
    "        X = X.append(temp_df.iloc[index])\n",
    "\n",
    "        Y = Y.append(temp_query.iloc[index])\n",
    "\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "    sklearn_pca = sklearnPCA(n_components=2)\n",
    "    Y_sklearn = sklearn_pca.fit_transform(X_std)\n",
    "\n",
    "    traces = []\n",
    "\n",
    "    j = 0\n",
    "\n",
    "    for result in df_list:\n",
    "\n",
    "        trace = go.Scatter(\n",
    "            x=Y_sklearn[j:j+num_samples,0],\n",
    "            y=Y_sklearn[j:j+num_samples,1],\n",
    "            mode='markers',\n",
    "            name=result.query,\n",
    "            #text=[username.decode() for username in result.df['username']],\n",
    "            marker=Marker(\n",
    "                size=4,\n",
    "                line=Line(\n",
    "                    color='rgba(217, 217, 217, 0.14)',\n",
    "                    width=0.5),\n",
    "                opacity=0.8))\n",
    "        traces.append(trace)\n",
    "        j = j + num_samples\n",
    "\n",
    "    data = Data(traces)\n",
    "    layout = Layout(xaxis=XAxis(title='PC1', showline=False),\n",
    "                    yaxis=YAxis(title='PC2', showline=False))\n",
    "    fig = Figure(data=data, layout=layout)\n",
    "    offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb3234",
   "metadata": {},
   "source": [
    "<font size=\"3\">Then, before using my previous function, I'll use a similar script to generate a 3d scatter plot of the PCA of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f531259",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame([])\n",
    "Y = pd.DataFrame([])\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    temp_df = result.df.drop(['raw_tweet', 'fullname', 'username',\n",
    "                            'time', 'hashtags', 'atreplies',\n",
    "                            'badges', 'tweet', 'elapsedtime',\n",
    "                            'weekday', 'tokens', 'query'], axis=1)\n",
    "    \n",
    "    temp_query = pd.DataFrame(result.df['query'])\n",
    "    \n",
    "    index = random.sample(range(len(temp_df)), num_samples)\n",
    "    \n",
    "    X = X.append(temp_df.iloc[index])\n",
    "\n",
    "    Y = Y.append(temp_query.iloc[index])\n",
    "    \n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "sklearn_pca = sklearnPCA(n_components=3)\n",
    "Y_sklearn = sklearn_pca.fit_transform(X_std)\n",
    "\n",
    "traces = []\n",
    "\n",
    "j = 0\n",
    "\n",
    "for result in query_results:\n",
    "\n",
    "    trace = go.Scatter3d(\n",
    "        x=Y_sklearn[j:j+num_samples,0],\n",
    "        y=Y_sklearn[j:j+num_samples,1],\n",
    "        z=Y_sklearn[j:j+num_samples,2],\n",
    "        mode='markers',\n",
    "        name=result.query,\n",
    "        #text=[username.decode() for username in result.df['username']],\n",
    "        marker=Marker(\n",
    "            size=4,\n",
    "            line=Line(\n",
    "                color='rgba(217, 217, 217, 0.14)',\n",
    "                width=0.5),\n",
    "            opacity=0.8))\n",
    "    traces.append(trace)\n",
    "    j = j + num_samples\n",
    "\n",
    "data = Data(traces)\n",
    "\n",
    "layout = go.Layout(scene=\n",
    "                   dict(\n",
    "                        xaxis=dict(ticks='', showticklabels=False),\n",
    "                        yaxis=dict(ticks='', showticklabels=False),\n",
    "                        zaxis=dict(ticks='', showticklabels=False),\n",
    "                       )\n",
    "                  )\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ffaef",
   "metadata": {},
   "source": [
    "<font size=\"3\">Above is the PCA of tweets plotted in 3d and below in 2d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772f3596",
   "metadata": {},
   "source": [
    "<font size=\"3\">Below are tweets from the old_dfs dataframes which contain only tweets older than 13000 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "make2dplot(old_dfs, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd0989",
   "metadata": {},
   "source": [
    "<font size=\"3\">Below are tweets from the new_dfs dataframes which contain only tweets younger than 13000 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "make2dplot(new_dfs, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25301df",
   "metadata": {},
   "source": [
    "<font size=\"3\">And below are tweets from the good_dfs dataframes which contain only tweets with a positive polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "make2dplot(good_dfs, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aad491",
   "metadata": {},
   "source": [
    "<font size=\"3\">A common theme running through the PCA graphs is the separation of the javascript cluster from the fast food clusters. This is what I hoped for and expected by picking it as a control query. I suspect that in the machine learnign and prediction section the javascript tweets will be easier to distinguish from the others because of their different features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd885df3",
   "metadata": {},
   "source": [
    "<font size=\"5\">4. Machine Learning and Prediction<br><br>\n",
    "\n",
    "<font size=\"3\">Here we begin the fun part: predicting what type of query someone is talking about, or what type of fast food they like.<br><br>\n",
    "\n",
    "I begin by creating a test data set by siphoning off some amount of tweets from the dataframe. This number, defined by the variable num_samples, is removed from the training data so that the algorithm will not see them until it is asked to predict what query they were part of later during the testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = pd.DataFrame([])\n",
    "Y_train = pd.DataFrame([])\n",
    "\n",
    "X_test = pd.DataFrame([])\n",
    "Y_test = pd.DataFrame([])\n",
    "\n",
    "#number of random samples from each query to remove from\n",
    "#the training data and add to the test data\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    temp_df = result.df.drop(['raw_tweet', 'fullname', 'username',\n",
    "                            'time', 'hashtags', 'atreplies',\n",
    "                            'badges', 'tweet', 'elapsedtime',\n",
    "                            'weekday', 'tokens', 'query'], axis=1)\n",
    "    \n",
    "    temp_query = pd.DataFrame(result.df['query'])\n",
    "    \n",
    "    index = random.sample(range(len(temp_df)), num_samples)\n",
    "    \n",
    "    X_test = X_test.append(temp_df.iloc[index])\n",
    "\n",
    "    Y_test = Y_test.append(temp_query.iloc[index])\n",
    "\n",
    "    temp_df = temp_df.drop(index)\n",
    "    \n",
    "    temp_query = temp_query.drop(index)\n",
    "    \n",
    "    X_train = X_train.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    Y_train = Y_train.append(temp_query, ignore_index=True)\n",
    "    \n",
    "\n",
    "Y_train = Y_train['query']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28690358",
   "metadata": {},
   "source": [
    "<font size=\"3\">We can examine the new train and test data and see that 5 x 100 samples have been removed from the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c20d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sum = 0\n",
    "\n",
    "for result in query_results:\n",
    "    #print(len(result.df))\n",
    "    len_sum = len_sum + len(result.df)\n",
    "\n",
    "print(\"Total samples: \" + str(len_sum))\n",
    "print(\"X_train samples: \" + str(len(X_train)))\n",
    "print(\"Y_train samples: \" + str(len(Y_train)))\n",
    "print(\"X_test samples: \" + str(len(X_test)))\n",
    "print(\"Y_test samples: \" + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2698f8d",
   "metadata": {},
   "source": [
    "<font size=\"3\">I'll run a logistic regression and a random forest classifier using sklearn to first train the algorithm then make a prediction of the test data. Displayed is the training score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c16d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first find length of longest list\n",
    "#of query results\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for result in query_results:\n",
    "    if len(result.df) > max_len:\n",
    "        max_len = len(result.df)\n",
    "\n",
    "#initialize dataframe for hist display\n",
    "\n",
    "df = pd.DataFrame(None, index=[i for i in range(max_len)], \n",
    "                  columns=[result.query for result in query_results])\n",
    "\n",
    "#fill the dataframe\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    df[result.query] = result.df[['polarity']]\n",
    "    \n",
    "#plot\n",
    "\n",
    "df.iplot(kind='histogram', bins=21, filename='cufflinks/multiple-histograms')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54bcd8c",
   "metadata": {},
   "source": [
    "<font size=\"3\">If a histogram is used to contrast the fast food queries against javascript the difference in sentiment becomes apparent. For example, when comparing KFC and javascript the majority of negative sentiment tweets are about KFC. This histogram is not normalized to the amount of tweets per query, but if it were the results would be more obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d805a3a",
   "metadata": {},
   "source": [
    "<font size=\"3\">Another thing I was interested in was evaluating tweet sentiment of a brand over time. With tweets this is limited to the last ~9 days (~14000 minutes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7858fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in query_results:\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.regplot(result.df['elapsedtime'], result.df['polarity'])\n",
    "    plt.xlabel('Elapsed Time (minutes)')\n",
    "    plt.ylabel('Polarity')\n",
    "    plt.title(result.query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212bd05",
   "metadata": {},
   "source": [
    "<font size=\"3\">There is a large cluster of tweets just past the 13000 minute mark. I can separate out tweets that are newer or older than this into new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d09695",
   "metadata": {},
   "outputs": [],
   "source": [
    "class query_result(object):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    the results of a query\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "        query: name of query\n",
    "        df: pandas dataframe containing:\n",
    "        \n",
    "            tweet: tweet (str)\n",
    "            realname: real names of tweeter (str)\n",
    "            username: user name of tweeter (str)\n",
    "            time: time of tweet (unix time, int)\n",
    "            retweets: number of retweets (int)\n",
    "            likes: number of likes (int)\n",
    "            replies: number of replies (int)\n",
    "            hashtags: list of hashtags (list of str)\n",
    "            atreplies: list of at-replies (list of str)\n",
    "            badges: list of badges (such as verified account) (list of str)\n",
    "            \n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, query, tweet_df):\n",
    "        self.query = query\n",
    "        self.df = tweet_df\n",
    "\n",
    "def make_query_result(query, tweet_df):\n",
    "    return query_result(query, tweet_df)\n",
    "\n",
    "new_dfs = []\n",
    "old_dfs = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    new_dfs.append(make_query_result(\n",
    "        result.query, result.df[result.df['elapsedtime'].values<13000]))\n",
    "\n",
    "    old_dfs.append(make_query_result(\n",
    "        result.query, result.df[result.df['elapsedtime'].values>13000]))\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f354411",
   "metadata": {},
   "source": [
    "<font size=\"3\">I'll examine the amount of tweets before the 13000 minute mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3925d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import TwitterScraper\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.tools as tls\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "offline.init_notebook_mode()\n",
    "\n",
    "import cufflinks as cf\n",
    "\n",
    "cf.go_offline()\n",
    "\n",
    "cf.set_config_file(offline=True, world_readable=True, theme='pearl')\n",
    "\n",
    "\n",
    "base_url = u'https://twitter.com/search?q='\n",
    "\n",
    "\n",
    "query_list = [u'burger%20king', u'mcdonalds', u'chick-fil-a',\n",
    "              u'kfc', u'javascript']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Alternatively, instead of using the elapsed time cutoff as I do later,\n",
    "you can tell Twitter to only search for tweets within a time range\n",
    "as shown in comments below\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#query_list = [u'burger%20king since%3A2017-09-05 until%3A2017-09-12',\n",
    "#              u'mcdonalds since%3A2017-09-05 until%3A2017-09-12',\n",
    "#              u'chick-fil-a since%3A2017-09-05 until%3A2017-09-12',\n",
    "#              u'kfc since%3A2017-09-05 until%3A2017-09-12',\n",
    "#              u'javascript since%3A2017-09-05 until%3A2017-09-12']\n",
    "\n",
    "\n",
    "#This calls my TwitterScraper three times in parallel. It passes the base_url, query,\n",
    "#and the number of times to scroll down to load new tweets\n",
    "\n",
    "query_results = Parallel(n_jobs=3)(delayed(TwitterScraper.scrape)(base_url, query, 200) for query in query_list)\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    print(result.query)\n",
    "    print(len(result.df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670fac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results[0].df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4e1bc",
   "metadata": {},
   "source": [
    "<font size=\"3\">Here is the raw scraped data. The raw_tweet contains all links, pictures, and non-ASCII characters. The time is in unix time and can be converted using the Python datetime module. Retweets, replies, and likes are integers representing how many of each the tweet has. Hashtags and atreplies is a list containing strings of each hashtag and atreply mentioned in the tweet. Currently the only badge that I am aware of is from a verified member, and so that information would appear in the badges column if the tweeter was verified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c673a34",
   "metadata": {},
   "source": [
    "<font size=\"5\">2. Feature Engineering<br><br>\n",
    "\n",
    "<font size=\"3\">Next up is feature engineering. Some simple features that I thought of were:<br><br>\n",
    "\n",
    "Web links and pictures: does that tweet contain one?<br><br>\n",
    "\n",
    "Length of the tweet.<br><br>\n",
    "\n",
    "Is tweeter verified?<br><br>\n",
    "\n",
    "Time:<br>\n",
    "Time elapsed between tweet and now (tweet age).<br>\n",
    "Numerical (0=Monday, 6 = Sunday) and alphabetical weekday.<br>\n",
    "Time of day<br><br>\n",
    "\n",
    "Number of hashtags and atreplies.<br><br>\n",
    "\n",
    "Sentiment analysis with TextBlob to determine the polarity and subjectivity of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185fb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.utcnow().replace(microsecond=0)\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    #Does the tweet contain a picture or web link?\n",
    "    \n",
    "    result.df['pic'] = [int(bool(re.search(r\"(pic.twitter.com)\", result.df['raw_tweet'][i].decode())))\n",
    "     for i in range(len(result.df['raw_tweet']))]\n",
    "    \n",
    "    result.df['link'] = [int(bool(re.search(r\"(http)\", result.df['raw_tweet'][i].decode())))\n",
    "     for i in range(len(result.df['raw_tweet']))]\n",
    "\n",
    "    #Clean up the tweet by removing picture links which\n",
    "    #do not count toward tweet length, then calculate\n",
    "    #tweet length.\n",
    "\n",
    "    tweet_column = [0]*len(result.df['raw_tweet'])\n",
    "    tweet_length_column = [0]*len(result.df['raw_tweet'])\n",
    "    \n",
    "    for i in range(len(result.df['raw_tweet'])):\n",
    "\n",
    "        tweet_column[i] = re.sub(r\"pic.twitter.com\\S+\", \"\", \n",
    "                                 result.df['raw_tweet'][i].decode())\n",
    "        \n",
    "        tweet_length_column[i] = len(tweet_column[i])\n",
    "        \n",
    "    result.df['tweet'] = tweet_column\n",
    "    result.df['tweet_length'] = tweet_length_column\n",
    "    \n",
    "    #Detect if tweeter is verified.\n",
    "\n",
    "    verified_column = [0]*len(result.df['badges'])\n",
    "    \n",
    "    for i in range(len(result.df['badges'])):\n",
    "        \n",
    "        if any('Verified account' in badge for badge in result.df['badges'][i]):\n",
    "    \n",
    "            verified_column[i] = 1\n",
    "        \n",
    "    result.df['verified'] = verified_column\n",
    "    \n",
    "    #Create several time columns such as time elapsed between\n",
    "    #tweet and now, time of the day tweeted, and day of the \n",
    "    #week tweeted.  \n",
    "\n",
    "    elapsed_time_column = [0]*len(result.df['time'])\n",
    "    weekday_column = [0]*len(result.df['time'])\n",
    "    numerical_weekday_column = [0]*len(result.df['time'])\n",
    "    day_time_column = [0]*len(result.df['time'])\n",
    "    \n",
    "    for i in range(len(result.df['time'])):\n",
    "        \n",
    "        time_elapsed = (now - datetime.utcfromtimestamp(int(result.df['time'][i])))\n",
    "\n",
    "        elapsed_time_column[i] = time_elapsed.total_seconds()/60\n",
    "        \n",
    "        weekday_column[i] = datetime.utcfromtimestamp(int(result.df['time'][i])).strftime(\"%A\")\n",
    "\n",
    "        numerical_weekday_column[i] = datetime.utcfromtimestamp(int(result.df['time'][i])).weekday()\n",
    "        \n",
    "        day_time_column[i] = (datetime.utcfromtimestamp(int(result.df['time'][i])).hour \n",
    "                           + datetime.utcfromtimestamp(int(result.df['time'][i])).minute/60)\n",
    "        \n",
    "    result.df['elapsedtime'] = elapsed_time_column\n",
    "    result.df['weekday'] = weekday_column\n",
    "    result.df['num_weekday'] = numerical_weekday_column\n",
    "    result.df['day_time'] = day_time_column\n",
    "\n",
    "    #Count the number of hashtags and atreplies from\n",
    "    #the tweet.\n",
    "\n",
    "    result.df['num_hashtags'] = [len(result.df['hashtags'][i]) for i in \n",
    "                                range(len(result.df['hashtags']))]\n",
    "    \n",
    "    result.df['num_atreplies'] = [len(result.df['atreplies'][i]) for i in \n",
    "                                range(len(result.df['atreplies']))]\n",
    "    \n",
    "    #Remove web links then run sentiment analysis\n",
    "    #with TextBlob.\n",
    "\n",
    "    tweet_stripped = [re.sub(r\"http\\S+\", \"\", result.df['tweet'][i])\n",
    "                      for i in range(len(result.df['tweet']))]\n",
    "    \n",
    "    tweet_senti = [TextBlob(tweet_stripped[i]).sentiment \n",
    "                    for i in range(len(tweet_stripped))]\n",
    "    \n",
    "    result.df['polarity'] = pd.DataFrame([x[0] for x in tweet_senti])\n",
    "    result.df['subjectivity'] = pd.DataFrame([x[1] for x in tweet_senti])\n",
    "\n",
    "    #Add the query associated with the tweet.\n",
    "    result.df['query'] = [result.query for i in range(len(result.df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36bca9",
   "metadata": {},
   "source": [
    "<font size=\"5\">2.5 Tokenization<br><br>\n",
    "\n",
    "<font size=\"3\">Here I use Spacy to tokenize the tweets. I will use this in a later project for natural language processing (NLP) which I think has a better chance of separating differences between people who like different brands or different types of fast food, based on the language of their tweets.<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    #remove urls from tweets\n",
    "    \n",
    "    tweet_stripped = [re.sub(r\"http\\S+\", \"\", result.df['tweet'][i])\n",
    "                      for i in range(len(result.df['tweet']))]\n",
    "    \n",
    "    result.df['tokens'] = [nlp(tweet) for tweet in tweet_stripped]\n",
    "    \n",
    "    #Spacy does not see a hashtag followed by a phrase as a single entity\n",
    "    #therefore all hashtags are searched out and merged with the following\n",
    "    #phrase to form a single entity\n",
    "    \n",
    "    indexes = [[m.span() for m in re.finditer('#\\w+', tweet, flags=re.IGNORECASE)]\n",
    "               for tweet in tweet_stripped]\n",
    "    \n",
    "    for j in range(len(result.df['tokens'])):\n",
    "        \n",
    "        for start,end in indexes[j]:\n",
    "            \n",
    "            result.df['tokens'][j].merge(start_idx=start,end_idx=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of tokenized tweets\n",
    "\n",
    "for i in range(2):\n",
    "    print([(w.text, w.pos_) for w in query_results[0].df['tokens'][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e53ca2",
   "metadata": {},
   "source": [
    "<font size=\"5\">3. Preliminary Visualization<br><br>\n",
    "\n",
    "<font size=\"3\">Here are some beginning studies of the data. We can first examine the most common hashtags per query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e11b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in query_results:\n",
    "    \n",
    "    print(\"Most common hashtags for: \" + result.query)\n",
    "    \n",
    "    all_hashtags = [hashtag.upper() for hashtags in result.df['hashtags']\n",
    "                    for hashtag in hashtags]\n",
    "\n",
    "    freqs = Counter(all_hashtags)\n",
    "\n",
    "    print(freqs.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d16cf",
   "metadata": {},
   "source": [
    "<font size=\"3\">And the most common atreplies for each query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4098f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in query_results:\n",
    "    \n",
    "    print(\"Most common atreplies for: \" + result.query)\n",
    "    \n",
    "    all_atreplies = [atreply.upper() for atreplies in result.df['atreplies']\n",
    "                     for atreply in atreplies]\n",
    "\n",
    "    freqs = Counter(all_atreplies)\n",
    "\n",
    "    print(freqs.most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a802daf",
   "metadata": {},
   "source": [
    "<font size=\"3\">As one might expect, for each fast food query the most common atreply is toward\n",
    "the official account of the fast food chain. The most common atreply for javascript is toward\n",
    "YouTube, which is also a common atreply for Burger King, McDonald's, and KFC.<br><br>\n",
    "\n",
    "Next we can examine the polarity of tweets for each query using TextBlob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "make2dplot(query_results, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f534b8",
   "metadata": {},
   "source": [
    "<font size=\"3\">I'll then compare the matching score, or what fraction of the test data was predicted accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa89bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_matches = [1 if i==j else 0 for i,j in zip(Y_pred_logreg, np.array(Y_test))]\n",
    "\n",
    "print(str(sum(logreg_matches)/len(logreg_matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_matches = [1 if i==j else 0 for i,j in zip(Y_pred_rf, np.array(Y_test))]\n",
    "\n",
    "print(str(sum(rf_matches)/len(rf_matches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb75388",
   "metadata": {},
   "source": [
    "<font size=\"3\">Looks like the random forest made better predictions, although both did better than randomly guessing (0.2). This is better than I had expected given that I disregarded NLP outside of sentiment analysis with TextBlob. Next I'll examine the matching frequency for each specific query to see if any queries where harder or easier to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    print(result.query + \" accuracy: \")\n",
    "    \n",
    "    print(str(sum(logreg_matches[j:j+num_samples])\n",
    "              /len(logreg_matches[j:j+num_samples])))\n",
    "    \n",
    "    j = j + num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266a0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    print(result.query + \" accuracy: \")\n",
    "    \n",
    "    print(str(sum(rf_matches[j:j+num_samples])\n",
    "              /len(rf_matches[j:j+num_samples])))\n",
    "    \n",
    "    j = j + num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d026e",
   "metadata": {},
   "source": [
    "<font size=\"3\">Both the logistic regression and random forest were better at predicting javascript tweets versus the fast food tweets. I had suspected this to be the case and is the reason I added it as a control. There were many differences between fast food queries and javascript queries illustrated earlier.<br><br>\n",
    "\n",
    "I'll now run the same algorithms on the new_dfs dataset. This dataset has less data, both in terms of fewer tweets as well as less time being posted on Twitter for others to engage with. Perhaps this will result in worse predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b319d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame([])\n",
    "Y_train = pd.DataFrame([])\n",
    "\n",
    "X_test = pd.DataFrame([])\n",
    "Y_test = pd.DataFrame([])\n",
    "\n",
    "#number of random samples from each query to remove from\n",
    "#the training data and add to the test data\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "for result in new_dfs:\n",
    "    \n",
    "    temp_df = result.df.drop(['raw_tweet', 'fullname', 'username',\n",
    "                            'time', 'hashtags', 'atreplies',\n",
    "                            'badges', 'tweet', 'elapsedtime',\n",
    "                            'weekday', 'tokens', 'query'], axis=1)\n",
    "    \n",
    "    temp_query = pd.DataFrame(result.df['query'])\n",
    "    \n",
    "    index = random.sample(range(len(temp_df)), num_samples)\n",
    "    \n",
    "    X_test = X_test.append(temp_df.iloc[index])\n",
    "\n",
    "    Y_test = Y_test.append(temp_query.iloc[index])\n",
    "\n",
    "    temp_df = temp_df.drop(index)\n",
    "    \n",
    "    temp_query = temp_query.drop(index)\n",
    "    \n",
    "    X_train = X_train.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    Y_train = Y_train.append(temp_query, ignore_index=True)\n",
    "    \n",
    "\n",
    "Y_train = Y_train['query']\n",
    "\n",
    "len_sum = 0\n",
    "\n",
    "for result in new_dfs:\n",
    "    #print(len(result.df))\n",
    "    len_sum = len_sum + len(result.df)\n",
    "\n",
    "print(\"Total samples: \" + str(len_sum))\n",
    "print(\"X_train samples: \" + str(len(X_train)))\n",
    "print(\"Y_train samples: \" + str(len(Y_train)))\n",
    "print(\"X_test samples: \" + str(len(X_test)))\n",
    "print(\"Y_test samples: \" + str(len(Y_test)))\n",
    "\n",
    "#logistic regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "print(\"LogReg training score: \" +\n",
    "      str(logreg.score(X_train, Y_train)))\n",
    "\n",
    "logreg_matches = [1 if i==j else 0 for i,j in zip(Y_pred_logreg, np.array(Y_test))]\n",
    "\n",
    "print(\"LogReg match score: \" + \n",
    "      str(sum(logreg_matches)/len(logreg_matches)))\n",
    "\n",
    "print(\"LogReg query match scores:\")\n",
    "\n",
    "j = 0\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    print(result.query + \" accuracy: \")\n",
    "    \n",
    "    print(str(sum(logreg_matches[j:j+num_samples])\n",
    "              /len(logreg_matches[j:j+num_samples])))\n",
    "    \n",
    "    j = j + num_samples\n",
    "    \n",
    "    \n",
    "    \n",
    "#random forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_rf = random_forest.predict(X_test)\n",
    "\n",
    "print(\"Random Forest training score: \" +\n",
    "      str(random_forest.score(X_train, Y_train)))\n",
    "\n",
    "\n",
    "rf_matches = [1 if i==j else 0 for i,j in zip(Y_pred_rf, np.array(Y_test))]\n",
    "\n",
    "print(\"Random Forest match score: \" + \n",
    "      str(sum(rf_matches)/len(rf_matches)))\n",
    "\n",
    "print(\"Random Forest query match scores:\")\n",
    "\n",
    "j = 0\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    print(result.query + \" accuracy: \")\n",
    "    \n",
    "    print(str(sum(rf_matches[j:j+num_samples])\n",
    "              /len(rf_matches[j:j+num_samples])))\n",
    "    \n",
    "    j = j + num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815465f",
   "metadata": {},
   "source": [
    "<font size=\"3\">Looks like predictions were worse on average. Random forest took a big hit in predictive power when compared to the more data rich full dataset.<br><br>\n",
    "\n",
    "Next I'll attempt to answer the question that led me to begin this project: Can I predict a tweeter's fast food preference?<br><br>\n",
    "\n",
    "I've simplified this question quite a bit and have narrowed it into a few classifications: They like 1 of 4 fast food queries presented here (Burger King, McDonald's, KFC, Chick-fil-A), or this tweet is unassociated with fast food (javascript). I'll run the same analysis as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame([])\n",
    "Y_train = pd.DataFrame([])\n",
    "\n",
    "X_test = pd.DataFrame([])\n",
    "Y_test = pd.DataFrame([])\n",
    "\n",
    "#number of random samples from each query to remove from\n",
    "#the training data and add to the test data\n",
    "\n",
    "num_samples = 50\n",
    "\n",
    "#remove num_samples random samples from training\n",
    "#and add to test data\n",
    "\n",
    "for result in good_dfs:\n",
    "    \n",
    "    temp_df = result.df.drop(['raw_tweet', 'fullname', 'username',\n",
    "                            'time', 'hashtags', 'atreplies',\n",
    "                            'badges', 'tweet', 'elapsedtime',\n",
    "                            'weekday', 'tokens', 'query'], axis=1)\n",
    "    \n",
    "    temp_query = pd.DataFrame(result.df['query'])\n",
    "    \n",
    "    #need to reset index otherwise finding index with \n",
    "    #random numbers will fail for indexes which were \n",
    "    #not sent to good_dfs\n",
    "    \n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    temp_query = temp_query.reset_index(drop=True)\n",
    "    \n",
    "    index = random.sample(range(len(temp_df)), num_samples)\n",
    "    \n",
    "    X_test = X_test.append(temp_df.iloc[index])\n",
    "\n",
    "    Y_test = Y_test.append(temp_query.iloc[index])\n",
    "\n",
    "    temp_df = temp_df.drop(index)\n",
    "    \n",
    "    temp_query = temp_query.drop(index)\n",
    "    \n",
    "    X_train = X_train.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    Y_train = Y_train.append(temp_query, ignore_index=True)\n",
    "    \n",
    "\n",
    "    \n",
    "Y_train = Y_train['query']\n",
    "\n",
    "len_sum = 0\n",
    "\n",
    "for result in good_dfs:\n",
    "    #print(len(result.df))\n",
    "    len_sum = len_sum + len(result.df)\n",
    "\n",
    "print(\"Total samples: \" + str(len_sum))\n",
    "print(\"X_train samples: \" + str(len(X_train)))\n",
    "print(\"Y_train samples: \" + str(len(Y_train)))\n",
    "print(\"X_test samples: \" + str(len(X_test)))\n",
    "print(\"Y_test samples: \" + str(len(Y_test)))\n",
    "\n",
    "#logistic regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "print(\"LogReg training score: \" +\n",
    "      str(logreg.score(X_train, Y_train)))\n",
    "\n",
    "logreg_matches = [1 if i==j else 0 for i,j in zip(Y_pred_logreg, np.array(Y_test))]\n",
    "\n",
    "print(\"LogReg match score: \" + \n",
    "      str(sum(logreg_matches)/len(logreg_matches)))\n",
    "\n",
    "print(\"LogReg query match scores:\")\n",
    "\n",
    "j = 0\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    print(result.query + \" accuracy: \")\n",
    "    \n",
    "    print(str(sum(logreg_matches[j:j+num_samples])\n",
    "              /len(logreg_matches[j:j+num_samples])))\n",
    "    \n",
    "    j = j + num_samples\n",
    "    \n",
    "    \n",
    "#random forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_rf = random_forest.predict(X_test)\n",
    "\n",
    "print(\"Random Forest training score: \" +\n",
    "      str(random_forest.score(X_train, Y_train)))\n",
    "\n",
    "\n",
    "rf_matches = [1 if i==j else 0 for i,j in zip(Y_pred_rf, np.array(Y_test))]\n",
    "\n",
    "print(\"Random Forest match score: \" + \n",
    "      str(sum(rf_matches)/len(rf_matches)))\n",
    "\n",
    "print(\"Random Forest query match scores:\")\n",
    "\n",
    "j = 0\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    print(result.query + \" accuracy: \")\n",
    "    \n",
    "    print(str(sum(rf_matches[j:j+num_samples])\n",
    "              /len(rf_matches[j:j+num_samples])))\n",
    "    \n",
    "    j = j + num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae299a3",
   "metadata": {},
   "source": [
    "<font size=\"3\">Looks like both the logistic regression and the random forest can tell you with more certainty than guessing what the fast food preference of a tweeter is. The random forest is better and has an accurate prediction rate of 48% or more for each query. This is much better than randomly guessing for which you would expect a predictive success rate of about 20%. This is more than I had expected coming into this project. If I were to add in NLP looking for certain phrases, hashtags, atreplies, or other language nuances, I assume that the accuracy rate would be much higher. However, that is currently out of my scope of understanding but is very high on the to-do list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ed5a9",
   "metadata": {},
   "source": [
    "<font size=\"5\">5. Conclusion<br><br>\n",
    "\n",
    "<font size=\"3\">The algorithm is capable of predicting with better accuracy than random whether or not a person is likely to prefer Burger King, McDonald's, KFC, Chick-fil-A, or whether they're not really a good target for fast food analysis. The point of this in a business sense was to mimic targeted advertising. You could advertise your business to those whom would be most receptive toward it. If a person's tweets closely matched the tweets of people who saw the business favorably you would advertise to them and not to someone who may not fit the profile so well. This would save on advertising while being more effective with it. At least, this is how I see it with my limited business experience.<br><br>\n",
    "\n",
    "The training data included a very small portion of the data available in a tweet. As I mentioned a few times previously the only language data included in the machine learning and prediction portion was the sentiment analysis from TextBlob. I disregarded all other language data, even the hashtags and atreplies. I suspect the algorithm would improve greatly if this were included. I do include tweet length, which one could argue is related to the language of the tweet, but is not particularly sophisticated.<br><br>\n",
    "\n",
    "I also disregarded the elapsedtime column for PCA and machine learning because I felt it would not distinguish tweets from one another, or that if it did it would do so falsely. All tweets follow the same progression of elapsed time as the others and it is mostly a reflection of when I collected the data. I did keep the weekday and time of day information columns because it could be that people who really like McDonald's post mainly at 2 AM on Wednesday and Friday.<br><br>\n",
    "\n",
    "I also chose to use some rather simple machine learning techniques. A better and more complex choice would involve ensemble learning with multiple different classifiers, or simply looking at different classifiers to see if one did really well.<br><br>\n",
    "\n",
    "Don't forget to save the data to re-analyze later when you have better ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa11baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MEANS\\n')\n",
    "for result in query_results:\n",
    "    print(result.query + ':\\t'+ str(np.mean(result.df['polarity'])))\n",
    "\n",
    "print('\\nMEDIANS\\n')\n",
    "for result in query_results:\n",
    "    print(result.query + ':\\t'+ str(np.median(result.df['polarity'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edcce8e",
   "metadata": {},
   "source": [
    "<font size=\"3\">Javascript has the most positive tweets on average. We can examine the distribution of tweets between neutral, positive, and negative polarity for each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fraction polarity:\n",
    "\n",
    "print('\\nFRACTION NEUTRAL\\n')\n",
    "\n",
    "for result in query_results:\n",
    "    \n",
    "    frac_neut = sum(result.df['polarity'].values==0)/len(result.df['polarity'])\n",
    "    print(result.query + ':\\t'+ str(frac_neut))\n",
    "\n",
    "print('\\nFRACTION POSITIVE\\n')    \n",
    "    \n",
    "for result in query_results:    \n",
    "    \n",
    "    frac_pos = sum(result.df['polarity'].values>0)/len(result.df['polarity'])\n",
    "    print(result.query + ':\\t'+ str(frac_pos))\n",
    "\n",
    "print('\\nFRACTION NEGATIVE\\n')\n",
    "    \n",
    "for result in query_results:\n",
    "    \n",
    "    frac_neg = sum(result.df['polarity'].values<0)/len(result.df['polarity'])\n",
    "    print(result.query + ':\\t' + str(frac_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f1672",
   "metadata": {},
   "source": [
    "<font size=\"3\">Perhaps unsurprisingly, javascript has the lowest fraction of negative tweets and the highest fraction of neutral and positive tweets. It seems that Twitter users don't feel as strongly about a programming language as they do about fast food restaurants. Overall this distinguishes fast food tweets from a more professional subject such as javascript where people may feel less inclined to make negative statements. However it does not help separate different types of fast food from each other.<br><br>\n",
    "\n",
    "The previous analysis is like examining a 3 bin histogram. We can instead look at an actual histogram with 21 bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a22898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "logreg.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d5a97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "random_forest.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred_rf = random_forest.predict(X_test)\n",
    "\n",
    "random_forest.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052f26c1",
   "metadata": {},
   "source": [
    "<font size=\"6\">Predicting Fast Food Preferences From Twitter Data<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb8acf5",
   "metadata": {},
   "source": [
    "<font size=\"5\">1. Scraping<br><br>\n",
    "\n",
    "<font size=\"3\">Here I will use my Twitter scraper to obtain some tweets from search queries involving fast food. I have added in a \"javascript\" search query as a control. I hope to be able to determine differences in fast food preferences.<br><br>\n",
    "\n",
    "I perform the scraping in parallel using joblib. My laptop has 4 cores so I use 3 cores for the parallel processing."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
