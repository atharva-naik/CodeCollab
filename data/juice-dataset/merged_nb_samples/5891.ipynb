{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2113a7d",
   "metadata": {},
   "source": [
    "Semblance, Coherence, and other Discontinuity Attributes\n",
    "==============================\n",
    "\n",
    "Introduction\n",
    "-------------\n",
    "\n",
    "First off, this tutorial is a companion to the short\n",
    "[article/tutorial](writeup.md) published in The Leading Edge.  If you haven't\n",
    "yet, have a look at the more detailed introduction there.\n",
    "\n",
    "Modern 3D seismic interpretation often uses attributes that compare the local\n",
    "similarity/variance of traces within a seismic volume.  If you're more familiar\n",
    "with image processing than seismic interpretation, think of them as a class of\n",
    "3D edge filters adapted for the periodic nature of seismic data (i.e. \"wiggles\"\n",
    "instead of more \"smooth\" data).  These attributes are referred to as\n",
    "\"semblance\", \"coherence\", \"similarity\", \"discontinuity\", and many similar\n",
    "terms, including a number of \"trade names\" applied to specific implementations\n",
    "in proprietary software packages.  Here, we'll use the term \"discontinuity\" to\n",
    "refer to the general class of attribute, and the others to refer to specific\n",
    "algorithms.\n",
    "\n",
    "In practice, you'd typically calculate a discontinuity attribute within a\n",
    "seismic interpretation package.  In many cases, the algorithms that a\n",
    "particular software package uses are either considered trade secrets or are not\n",
    "documented in detail. This leads to discontinuity calculations being treated as\n",
    "a \"black box\". Most interpreters use them, but aren't familiar with how they're\n",
    "calculated. Of course, knowing algorithmic details is not at all necessary to\n",
    "use discontinuity for interpretation, but it is useful when comparing different\n",
    "discontinuity attributes or when implementing new methods.\n",
    "\n",
    "In this tutorial I'll demonstrate algorithms for a few common discontinuity\n",
    "attributes. I'm going to show simple implementations of a few algorithms with\n",
    "no regard for practical considerations such as memory use, speed, etc.  The\n",
    "goal is to show how these types of attributes can be implemented with high-level\n",
    "\"building-blocks\". I'm not going to spend much time discussing how, when, or\n",
    "why one should use one over another.  Which particular algorithm answers a\n",
    "particular question best is usually specific to the dataset and question at\n",
    "hand.  Instead, I'll focus on general tradeoffs, such as theoretical\n",
    "sensitivity to amplitude, noise, etc.  In practice, it's usually best to\n",
    "compare multiple attributes.  Therefore, the goal of this tutorial is to give you the knowledge to implmenent some of these algorithms and to understand why different discontinuity algorithms give slightly different results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbaeac",
   "metadata": {},
   "source": [
    "Our Dataset\n",
    "-------------\n",
    "\n",
    "The data we'll be working with is a small subset of the Penobscot 3D seismic dataset [from offshore Nova Scotia](https://opendtect.org/osr/uploads/Main/Penobscot-3D_Loc.jpg), owned by the Nova\n",
    "Scotia Department of Energy and distributed by dGB Earth Sciences under an\n",
    "CC-BY-SA license. (Full dataset available at: https://opendtect.org/osr/pmwiki.php/Main/PENOBSCOT3DSABLEISLAND)  \n",
    "\n",
    "The exact inline/crossline/time ranges of this subset are given in `data/penobscot_subset.hdr`, which is a simple ascii file describing the \"raw\" data stored in `data/penobscot_subset.bin`.\n",
    "\n",
    "Stratigraphic Context\n",
    "----------------------\n",
    "\n",
    "For context, we're going to be looking at a small region (about 3km by 5km) of the [late Cretaceous to Paleogene section](http://www.callforbids.cnsopb.ns.ca/2013/01/sites/default/files/node/Figure%202.1.jpg) on the Nova Scotian shelf.  The base of our dataset is near the top of the delatic Dawson Canyon formation. The bright, continuous reflectors are within the late Cretaceous Wayndot Chalk.  At the top of these is a major sequence boundary, which incises into the Wayndot Chalk.  The chaotic section at the top of our subset is a distal deltaic section within the Maastrichtian-Paleogene portion of the Banquereau formation, which in this exact area, happens to include a number of mass transport complexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71461b",
   "metadata": {},
   "source": [
    "Setup\n",
    "-------\n",
    "\n",
    "First off, we'll be working in Python.  However, most of these examples focus on using linear algebra and image processing libraries and the concepts should be easily transferrable to other high-level scientific programming languages such as Matlab/Octave or Julia. \n",
    "\n",
    "The code in this tutorial depends on Python (2.7 or 3.x) and recent-ish versions of numpy, scipy, and matplotlib.  For 3D visualization, there's an optional dependence on mayavi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e56daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use non-interactive matplotlib figures embedded in the notebook, for the moment\n",
    "# Comment this line out or change it to another backend if you'd prefer interactive figures.\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import scipy.signal\n",
    "\n",
    "# Local files included with the notebook\n",
    "# Note that the \"data\" folder also has an __init__.py file.\n",
    "# Therefore the \"data\" folder is treated as a module with functions\n",
    "# defined in __init__, like \"data.load_seismic\" and \"data.load_horizon\"\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9a8c30",
   "metadata": {},
   "source": [
    "Let's define a simple plotting fuction to view a crossline and time slice through a 3D numpy array (we'll use this extensively later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data, title=''):\n",
    "    # We'll take slices half-way through the volume (// is integer division)\n",
    "    j0 = data.shape[1] // 2\n",
    "    k0 = data.shape[2] // 2\n",
    "    \n",
    "    # Setup subplots where one is 3x the height of the other\n",
    "    # Our data has a fairly narrow time range, so we'll make a cross section \n",
    "    # that's 1/3 the height of the time slice\n",
    "    gs = plt.GridSpec(4, 1)\n",
    "    fig = plt.figure(figsize=plt.figaspect(1.1))\n",
    "    ax1 = fig.add_subplot(gs[:-1], anchor='S')\n",
    "    ax2 = fig.add_subplot(gs[-1], anchor='N')\n",
    "    \n",
    "    # Plot the sections\n",
    "    ax1.imshow(data[:,:,k0].T, cmap='gray')\n",
    "    ax2.imshow(data[:,j0,:].T, cmap='gray')\n",
    "    \n",
    "    # Mark the cross section locations...\n",
    "    for ax, loc in zip([ax1, ax2], [j0, k0]):\n",
    "        ax.axhline(loc, color='red')\n",
    "        ax.set(xticks=[], yticks=[])\n",
    "    \n",
    "    ax1.set(title=title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e76ef",
   "metadata": {},
   "source": [
    "Now let's take a look at our input dataset. Once again, this is a small subset of the Penobscot 3D seismic dataset. We'll load it into memory as a 3D numpy array and display a timeslice and inline section using the `plot` function above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b863ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(marfurt, 'Marfurt et al (1998)')\n",
    "plot(gersztenkorn, 'Gersztenkorn & Marfurt (1999)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d49d73",
   "metadata": {},
   "source": [
    "Note that Marfurt et al's (1998) method shows more \"bold\" edges of the channels on the left side of the timeslice.  Because these channels are partly defined by amplitude changes, Gersztenkorn & Marfurt's (1999) approach actually dims the channel edges slightly.  However, it sharpens and more clearly shows fault locations and the internal structure of the mass transport complex at the bottom of the timeslice. This is a good example of the subtle trade-offs between different discontinuity algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3354856",
   "metadata": {},
   "source": [
    "Dip-correction\n",
    "----------------\n",
    "\n",
    "A drawback to Marfurt, et al's (1998) and Gersztenkorn and Marfurt's (1999)\n",
    "approaches is that dipping reflectors will have a uniformly higher\n",
    "discontinuity (lower coherence/similarity) than non-dipping reflectors. In\n",
    "other words, these attributes don't distinguish between regional structural dip and\n",
    "localized discontinuities due to faulting, etc. Therefore, Marfurt\n",
    "(2006) proposed calculating and correcting for structural dip when performing\n",
    "discontinuity calculations.   This correction can be applied to any of the\n",
    "different algorithms discussed so far.  While there are a number of different\n",
    "methods that can be used to both calculate and correct for structural dip (see\n",
    "Ch. 2 of Chopra and Marfurt (2007) for a review), dip calculations are beyond\n",
    "the scope of this tutorial.  Therefore, we'll approximate a dip correction by\n",
    "flattening on a pre-picked horizon in the interval of interest before applying\n",
    "a discontinuity calculation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7281482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(data, surface, window):\n",
    "    # Smooth the surface slightly. The surface consists of \"raw\" integer\n",
    "    # indices into the seismic cube. If we don't do this, we'll wind up with\n",
    "    # banding due to the integer nature of the surface.\n",
    "    surface = scipy.ndimage.gaussian_filter(surface.astype(float), 1)\n",
    "\n",
    "    ni, nj, nk = data.shape\n",
    "    ik = np.arange(nk)\n",
    "    out_ik = np.arange(window) - window // 2\n",
    "\n",
    "    out = np.zeros((ni, nj, window))\n",
    "    for i, j in np.ndindex(ni, nj):\n",
    "        trace = data[i,j,:]\n",
    "        k = surface[i, j]\n",
    "        shifted = np.interp(out_ik + k, ik, trace)\n",
    "\n",
    "        out[i,j,:] = shifted\n",
    "\n",
    "    return out\n",
    "\n",
    "def unflatten(data, surface, orig_shape):\n",
    "    out = np.zeros(orig_shape)\n",
    "    surface = np.clip(surface, 0, orig_shape[-1] - 1)\n",
    "\n",
    "    win = data.shape[-1] // 2\n",
    "    for i, j in np.ndindex(orig_shape[0], orig_shape[1]):\n",
    "        k = int(surface[i,j])\n",
    "\n",
    "        outmin, outmax = max(0, k - win), min(orig_shape[-1], k + win + 1)\n",
    "        inmin, inmax = outmin - (k - win), k + win + 1 - outmax\n",
    "        inmax = data.shape[-1] - abs(inmax)\n",
    "\n",
    "        out[i, j, outmin:outmax] = data[i, j, inmin:inmax]\n",
    "\n",
    "    return out\n",
    "\n",
    "def dip_corrected(seismic, window, func):\n",
    "    surface = data.load_horizon()\n",
    "    flat = flatten(seismic, surface, seismic.shape[-1])\n",
    "    sembl = moving_window(flat, window, func)\n",
    "    return unflatten(sembl, surface, seismic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2397cdc",
   "metadata": {},
   "source": [
    "Now we can apply it using Gersztenkorn & Marfurt's (1999) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dip_corrected_gersztenkorn = dip_corrected(seismic, (3,3,9), gersztenkorn_eigenstructure)\n",
    "plot(dip_corrected_gersztenkorn, 'Dip Corrected\\nGersztenkorn & Marfurt (1999)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b7c2b",
   "metadata": {},
   "source": [
    "Or apply it using Marfurt, et al's (1998) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9daffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dip_corrected_marfurt = dip_corrected(seismic, (3,3,9), marfurt_semblance)\n",
    "plot(dip_corrected_marfurt, 'Dip Corrected\\nMarfurt, et al (1998)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81546f8",
   "metadata": {},
   "source": [
    "The effects of dip correction are less\n",
    "apparent in our examples due to the relatively low structural dips in the area.\n",
    "However, dip correction significantly enhances resolution of subtle features in\n",
    "more structurally complex areas (Marfurt, 2006; Chopra and Marfurt, 2007)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa831870",
   "metadata": {},
   "source": [
    "Using the Analytic Trace\n",
    "------------------------\n",
    "\n",
    "In addition to dip correction, many authors recommend including the analytic trace in the discontinuity calculation (e.g.\n",
    "Marfurt, et al, 1999, 2006; Chopra and Marfurt, 2007).  In principle, this\n",
    "makes the discontinuity estimation less sensitive to zero-crossings in the\n",
    "original dataset. \n",
    "\n",
    "Here's an example of applying this to Gersztenkorn & Marfurt's (1999) algorithm.  We're basically appending the imaginary portion of the analytic trace to the end of the \"regular\" trace to make each trace twice as long. \n",
    "\n",
    "We're also going to be rather inefficent here, and compute the Hilbert transform repeatedly inside the moving window to work around `scipy.ndimage.generic_filter` not supporting complex types. Because of that, this example will take awhile (5-10 minutes) to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb109716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_eigenstructure(region):\n",
    "    region = region.reshape(-1, region.shape[-1])\n",
    "\n",
    "    region = scipy.signal.hilbert(region, axis=-1)\n",
    "    region = np.hstack([region.real, region.imag])\n",
    "\n",
    "    cov = region.dot(region.T)\n",
    "    vals = np.linalg.eigvals(cov)\n",
    "    return np.abs(vals.max() / vals.sum())\n",
    "\n",
    "complex_gersztenkorn = moving_window(seismic, (3,3,9), complex_eigenstructure)\n",
    "plot(complex_gersztenkorn, 'Including Analytic Trace\\nGersztenkorn & Marfurt (1999)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293749d9",
   "metadata": {},
   "source": [
    "In practice, including the analytic trace information often has a very minor effect, unless an unusually small window is chosen.  For example, let's replot the \"regular\" version of Gersztenkorn & Marfurt's (1999) algorithm applied to this dataset. The differences between these two are almost unnoticable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495818e",
   "metadata": {},
   "source": [
    "Now we're ready to explore some various discontinuity attributes. We'll review things in roughly historical order, starting with the earliest approach: Bahorich and Farmer (1995)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547457a",
   "metadata": {},
   "source": [
    "Early Algorithms - Cross-correlation\n",
    "----------------\n",
    "\n",
    "The earliest discontinuity algorithm was developed by Bahorich and Farmer\n",
    "(1995) and used the maximum cross correlation value of three traces.  Bahorich\n",
    "and Farmer coined the term \"coherence\" for the attribute, based on its\n",
    "conceptual similarity to pre-stack methods for estimating stacking velocities.\n",
    "While this exact approach is computationally expensive and not widely used\n",
    "today, it provides a good starting point to understand later algorithms.\n",
    "\n",
    "The key here is that the entirety of each trace is correlated with a\n",
    "\"moving-window\" subset of two neighboring traces. This has some advantages, such as automatic dip correction, and disadvantages, such as being computationally expensive and sensitive to noise. \n",
    "\n",
    "The python function shown below gives a basic example of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bahorich_coherence(data, zwin):\n",
    "    ni, nj, nk = data.shape\n",
    "    out = np.zeros_like(data)\n",
    "    \n",
    "    # Pad the input to make indexing simpler. We're not concerned about memory usage.\n",
    "    # We'll handle the boundaries by \"reflecting\" the data at the edge.\n",
    "    padded = np.pad(data, ((0, 1), (0, 1), (zwin//2, zwin//2)), mode='reflect')\n",
    "\n",
    "    for i, j, k in np.ndindex(ni, nj, nk):\n",
    "        # Extract the \"full\" center trace\n",
    "        center_trace = data[i,j,:]\n",
    "        \n",
    "        # Use a \"moving window\" portion of the adjacent traces\n",
    "        x_trace = padded[i+1, j, k:k+zwin]\n",
    "        y_trace = padded[i, j+1, k:k+zwin]\n",
    "\n",
    "        # Cross correlate. `xcor` & `ycor` will be 1d arrays of length\n",
    "        # `center_trace.size - x_trace.size + 1`\n",
    "        xcor = np.correlate(center_trace, x_trace)\n",
    "        ycor = np.correlate(center_trace, y_trace)\n",
    "        \n",
    "        # The result is the maximum normalized cross correlation value\n",
    "        center_std = center_trace.std()\n",
    "        px = xcor.max() / (xcor.size * center_std * x_trace.std())\n",
    "        py = ycor.max() / (ycor.size * center_std * y_trace.std())\n",
    "        out[i,j,k] = np.sqrt(px * py)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66956741",
   "metadata": {},
   "source": [
    "Now, let's apply this to our test dataset.  Note that we'll need to use a relatively long ``zwin`` window, and that this will take quite some time (5-15 minutes) to compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82851bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bahorich = bahorich_coherence(seismic, 21)\n",
    "plot(bahorich, 'Bahorich & Farmer (1995)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24bdfdc",
   "metadata": {},
   "source": [
    "Compared to the \"raw\" seismic data, this time slice is much easier to interpret. Notice the sinuous features on the left.  While there is likely some minor faulting in addition to the large fault on the right, these sinuous features are easily interpretable as channels in this view, while they were unclear in the timeslice of the \"raw\" data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a833b",
   "metadata": {},
   "source": [
    "Similar to before, we can view it in 3D if Mayavi is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e489ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore3d(bahorich)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a832c74",
   "metadata": {},
   "source": [
    "If you have time, try varying the ``zwin`` value from 21 to something shorter and longer.  Note that with short windows, the result quickly becomes unclear.  This is because the cross-correlation implictly centers the portion of the trace we're using on its mean, instead of the \"true\" zero.  For longer traces, the mean is usually zero, but short regions of a trace often hae a non-zero mean.  Due to this, Bahorich and Farmer's algorithm requires large windows and is expensive to compute. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62e500",
   "metadata": {},
   "source": [
    "Generalization to an Arbitrary Number of Traces\n",
    "-----------------------------------------------\n",
    "Bahorich and Farmer's (1995) approach was very successful, but it is\n",
    "sensitive to noise because only three traces are used. Marfurt, et al (1998)\n",
    "generalized Bahorich and Farmer's cross-correlation approach to an arbitrary\n",
    "number of input traces, referred to by the authors as \"semblance-based\n",
    "coherence\". Marfurt et al's approach also exploits our knowledge that the data\n",
    "is zero-centered, and avoids centering on the mean of the trace, rather than\n",
    "the \"true\" mean of 0. As an example (Note: we'll reuse the ``moving_window``\n",
    "function in future examples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_window(data, window, func):\n",
    "    # `generic_filter` will give the function 1D input. We'll reshape it for convinence\n",
    "    wrapped = lambda region: func(region.reshape(window))\n",
    "    \n",
    "    # Instead of an explicit for loop, we'll use a scipy function to do the same thing\n",
    "    # The boundaries will be handled by \"reflecting\" the data, by default\n",
    "    return scipy.ndimage.generic_filter(data, wrapped, window)\n",
    "\n",
    "def marfurt_semblance(region):\n",
    "    # We'll need an ntraces by nsamples array\n",
    "    region = region.reshape(-1, region.shape[-1])\n",
    "    ntraces, nsamples = region.shape\n",
    "\n",
    "    square_of_sums = np.sum(region, axis=0)**2\n",
    "    sum_of_squares = np.sum(region**2, axis=0)\n",
    "    return square_of_sums.sum() / sum_of_squares.sum() / ntraces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130ca80",
   "metadata": {},
   "source": [
    "Now we can apply this to our data (note that it will take a minute or two to compute):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92c4a06",
   "metadata": {},
   "source": [
    "However, if we use a very short time window, we'll start to see significant differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = (3,3,3)\n",
    "short_complex = moving_window(seismic, window, complex_eigenstructure)\n",
    "short_normal = moving_window(seismic, window, gersztenkorn_eigenstructure)\n",
    "\n",
    "plot(short_complex, 'Complex Trace')\n",
    "plot(short_normal, 'No Complex Trace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc369b0",
   "metadata": {},
   "source": [
    "Notice the \"banding\" in the bottom cross section of the \"No Complex Trace\" plot.  Let's plot only the cross sections and make them a bit bigger to show the differences clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2622545",
   "metadata": {},
   "outputs": [],
   "source": [
    "j0 = short_complex.shape[1] // 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, figsize=plt.figaspect(0.5))\n",
    "axes[0].imshow(short_complex[:,j0,:].T, cmap='gray')\n",
    "axes[1].imshow(short_normal[:,j0,:].T, cmap='gray')\n",
    "\n",
    "axes[0].set(xticks=[], yticks=[], title='Complex Included')\n",
    "axes[1].set(xticks=[], yticks=[], title='Complex Not Included')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08dbbc",
   "metadata": {},
   "source": [
    "You can see much more pronounced banding parallel to stratigraphy in the bottom image. This is because most covariance-based discontinuity algorithms are sensitive to zero-crossings in the input data.  The imaginary portion of the analytic trace is basically a 90-degree phase shifted version of the original data.  Therefore, it's at a maximum or minimum whenever the real portion crosses zero.  By including the analytic trace, we essentially cancel out the effects of the zero crossings on the discontinuity calculation.  \n",
    "\n",
    "However, if you're already using a relative long time window, zero-crossings are usually not an issue.  Therefore, it's often better to use a slightly longer window and not include the analytic trace information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf4c09",
   "metadata": {},
   "source": [
    "Gradient Structure Tensor\n",
    "--------------------------\n",
    "\n",
    "So far, we've mostly focused on algorithms authored by a single group of researchers.  However, there are a number of different discontinuity algorithms that take a very different approach.  A good example of this is the various Gradient Structure Tensor (GST) attributes developed by Randen, et al (2000).  The GST attributes analyze the variability in the local gradient within a moving window.  We'll focus on GST coherence here, but multiple attributes can be derived from the gradient structure tensor, resulting in local dip estimates, texture attributes, and different discontinuity attributes (Randen, et al, 2000).  \n",
    "\n",
    "Conceptually, the gradient structure tensor measures the three-dimensional change in the local gradient of the seismic data.  Therefore, a dip estimate is inherent in the calculation, and no separate dip-correction is required.  However, similar to semblance-based coherence (Marfurt, et al, 1998) it is sensitive to amplitude changes as well as phase changes.  GST coherence differs from the other discontinuity attributes in what it measures.  The other discontinuity attributes we've discussed so far measure similarity of adjacent waveforms.  GST coherence measures how constant the local gradient/slope of the seismic data is.  In that sense, it's more similar to the various volumetric curvature attributes than to other discontinuity attributes.  \n",
    "\n",
    "Let's take a look at a simple implementation built on top of `scipy.ndimage`.  Randen, et al (2000) use a Gaussian gradient operator to estimate the local gradient, and we'll follow suit here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ac9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(seismic, sigma):\n",
    "    \"\"\"Builds a 4-d array of the gaussian gradient of *seismic*.\"\"\"\n",
    "    grads = []\n",
    "    for axis in range(3):\n",
    "        # Gaussian filter with order=1 is a gaussian gradient operator\n",
    "        grad = scipy.ndimage.gaussian_filter1d(seismic, sigma, axis=axis, order=1)\n",
    "        grads.append(grad[..., np.newaxis])\n",
    "    return np.concatenate(grads, axis=3)\n",
    "\n",
    "def moving_window4d(grad, window, func):\n",
    "    \"\"\"Applies the given function *func* over a moving *window*, reducing \n",
    "    the input *grad* array from 4D to 3D.\"\"\"\n",
    "    # Pad in the spatial dimensions, but leave the gradient dimension unpadded.\n",
    "    half_window = [(x // 2, x // 2) for x in window] + [(0, 0)]\n",
    "    padded = np.pad(grad, half_window, mode='reflect')\n",
    "    \n",
    "    out = np.empty(grad.shape[:3], dtype=float)\n",
    "    for i, j, k in np.ndindex(out.shape):\n",
    "        region = padded[i:i+window[0], j:j+window[1], k:k+window[2], :]\n",
    "        out[i,j,k] = func(region)\n",
    "    return out\n",
    "\n",
    "def gst_coherence_calc(region):\n",
    "    \"\"\"Calculate gradient structure tensor coherence on a local region.\n",
    "    Intended to be applied with *moving_window4d*.\"\"\"\n",
    "    region = region.reshape(-1, 3)\n",
    "    gst = region.T.dot(region) # This is the 3x3 gradient structure tensor\n",
    "    \n",
    "    # Reverse sort of eigenvalues of the GST (largest first)\n",
    "    eigs = np.sort(np.linalg.eigvalsh(gst))[::-1]\n",
    "    \n",
    "    return (eigs[0] - eigs[1]) / (eigs[0] + eigs[1])\n",
    "        \n",
    "def gst_coherence(seismic, window, sigma=1):\n",
    "    \"\"\"Randen, et al's (2000) Gradient Structure Tensor based coherence.\"\"\"\n",
    "    # 4-d gradient array (ni x nj x nk x 3)\n",
    "    grad = gradients(seismic, sigma)\n",
    "    return moving_window4d(grad, window, gst_coherence_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd451eb3",
   "metadata": {},
   "source": [
    "Now let's apply this to our seismic cube. Note that this will take a bit to run (~5 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2df781",
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_coh = gst_coherence(seismic, (3, 3, 9), sigma=1)\n",
    "plot(gst_coh, r'GST Coherence ($\\sigma=1$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38ff640",
   "metadata": {},
   "source": [
    "One additional \"knob\" that this algorithm has is the width of the gaussian gradient operator that's applied to calculate the gradient structure tensor.  In the example above, that's the `sigma` parameter, which in this specific case is in \"voxel\" units (i.e. raw indices). Varying this parameter changes the distance that the gradient is calculated over, and therefore the wavelength of features that the attribute is sensitive to.  \n",
    "\n",
    "In the case of the other discontinuity attributes we've discussed, the spatial resolution and size of the geologic features the attribute is sensitive to are tightly coupled through the size of the `window` over which the attribute is calculated.  In this case, we can in theory calculate an attribute that's sensitive to larger features without reducting the lateral/vertical resolution.\n",
    "\n",
    "For example, if we increase `sigma` by a factor of two from the figure above, we'll better image many of the major geologic features that we were seeing in the previous discontinuity attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_coh = gst_coherence(seismic, (3, 3, 9), sigma=2)\n",
    "plot(gst_coh, r'GST Coherence ($\\sigma=2$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce1f0c4",
   "metadata": {},
   "source": [
    "If we reduce `sigma` to half of the original value, the attribute will be sensitive only to short-wavelength changes, and we'll miss many of the larger channels and faults in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f728f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_coh = gst_coherence(seismic, (3, 3, 9), sigma=0.5)\n",
    "plot(gst_coh, r'GST Coherence ($\\sigma=0.5$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfaca4b",
   "metadata": {},
   "source": [
    "However, because the GST operates on the \"raw\" seismic data, if we make `sigma` too large, we'll start to have issues with phase-skipping. In other words, we'll see broad regions of low or high coherence due to a peak being compared to an adjacent trough or a peak a stratigraphically different reflector. This manifests itself as broad regions of high/low discontinuity that don't directly correspond to geologic features.\n",
    "\n",
    "For example, if we calculate the attribute with `sigma=3`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151468d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gst_coh = gst_coherence(seismic, (3, 3, 9), sigma=3)\n",
    "plot(gst_coh, r'GST Coherence ($\\sigma=3$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed861998",
   "metadata": {},
   "source": [
    "Effect of Window Size and Shape\n",
    "------------------------------------\n",
    "\n",
    "All discontinuity algorithms we've discussed are applied over a moving window.  The size of this window controls both the spatial resolution of the attribute and the size of features that the attribute is sensitive to.  Therefore, the size and shape of the window over which an attribute is applied is a key component of any of these discontinuity measurements.  \n",
    "\n",
    "The scipy ecosystem gives us the tools to easily apply non-rectangular filters. `scipy.ndimage.generic_filter` takes a `footprint` argument which determines the shape of the region that the given function is applied over.  For example, if we wanted to apply Marfurt, et al (1998)'s discontinuity algorithm over a broadly elliptical window with the long-axis parallel to the crossline direction, we could do something similar to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af8070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arbitrary_moving_window(seismic, window, func, footprint):\n",
    "    footprint = np.asarray(footprint)\n",
    "    ntraces = footprint.sum()\n",
    "    wrapped = lambda region: func(region.reshape(ntraces, -1))\n",
    "    \n",
    "    if footprint.ndim == 2:\n",
    "        footprint = np.tile(footprint[..., np.newaxis], (1, 1, window[0]))\n",
    "    return scipy.ndimage.generic_filter(seismic, wrapped, window, footprint=footprint)\n",
    "\n",
    "footprint = [[0, 0, 1, 0, 0],\n",
    "             [0, 1, 1, 1, 0],\n",
    "             [0, 1, 1, 1, 0],\n",
    "             [1, 1, 1, 1, 1],\n",
    "             [0, 1, 1, 1, 0],\n",
    "             [0, 1, 1, 1, 0],\n",
    "             [0, 0, 1, 0, 0]]\n",
    "result = arbitrary_moving_window(seismic, (7, 7, 9), marfurt_semblance, footprint)\n",
    "plot(result, 'Elliptical Window\\nMarfurt, et al, 1998')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e0ec1",
   "metadata": {},
   "source": [
    "However, a more basic consideration than kernel shape in most cases is the overall size of the kernel/moving-window over which the attribute is calculated.  This controls both the spatial resolution of the calculation and the size of the geologic feature which it is sensitive to.\n",
    "\n",
    "To help us build up a bit of intuition about how the size of the moving window affects the discontinuity attribute, let's set up an interactive widget to vary window size and see the result.  To speed up the calculation, we'll apply it only to a timeslice of our data.  Because we're working in a Jupyter notebook, we'll use IPython widgets instead of using an interactive matplotlib slider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46939d81",
   "metadata": {},
   "source": [
    "However, as you've probably noticed, most of these calculations are quite slow. It is possible to speed things up dramatically without leaving Python, but it makes the implmenation considerably less understandable.  Therefore, to make the interactive slider responsive, we'll only apply the calculation to a small portion of our data.  Let's plot that portion to get a sense of what we're looking at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baab2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = np.s_[10:100, :100, :]\n",
    "tslice = result[..., seismic.shape[-1] // 2]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(tslice.T, cmap='gray')\n",
    "ax.bar(sub[0].start, sub[1].stop, sub[0].stop - sub[0].start, 0,\n",
    "       facecolor='none', edgecolor='red')\n",
    "ax.axis('image')\n",
    "plt.show()\n",
    "\n",
    "subset = data.load_seismic()[sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75326533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_timeslice(seismic, window, func):\n",
    "    k0 = int(seismic.shape[-1]) // 2\n",
    "    iwin, jwin, kwin = window\n",
    "    \n",
    "    padded = np.pad(subset, [(win//2, win//2) for win in window], mode='reflect')\n",
    "    \n",
    "    out = np.zeros(seismic.shape[:2], dtype=float)\n",
    "    for i, j in np.ndindex(out.shape):\n",
    "        region = padded[i:i+iwin, j:j+jwin, k0:k0+kwin]\n",
    "        out[i,j] = func(region)\n",
    "    return out\n",
    "\n",
    "def plot_timeslice(attr):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(attr.T, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5915b6",
   "metadata": {},
   "source": [
    "Okay, so now we're going to apply Marfurt, et al's (1998) discontinutiy algorithm to a small region (red box above) of our data.  Let's set up an interactive slider to vary the xy window size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "\n",
    "def change_window(xwin):\n",
    "    attr = apply_timeslice(subset, (xwin, xwin, 9), marfurt_semblance)\n",
    "    plot_timeslice(attr)\n",
    "\n",
    "ipywidgets.interact(change_window, xwin=(2, 11, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce6354",
   "metadata": {},
   "source": [
    "Now let's set up a similar slider to vary the z-window size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757143a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "marfurt = moving_window(seismic, (3, 3, 9), marfurt_semblance)\n",
    "plot(marfurt, 'Marfurt et al (1998)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1fc976",
   "metadata": {},
   "source": [
    "Note that both the channels on the left and the fault on the right have become much more clear.  Also, we can now see a discontinuous region (a mass transport complex) in the lower right portion of the timeslice.  We're even starting to get hints of other channels near the upper-left portion of the figure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad5388",
   "metadata": {},
   "source": [
    "Once again, we can visualize this in 3D, if desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30654001",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore3d(marfurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df7611",
   "metadata": {},
   "source": [
    "From an algorithmic perspective, this is similar to Bahorich and Farmer's (1995) approach if the\n",
    "cross-correlation is only measured at a lag of zero, instead of the maximum\n",
    "cross correlation. However, Marfurt et al's (1998) method does not require a long\n",
    "vertical window to produce clear results, leading to faster computational times\n",
    "and better resolution in the time-direction.  Furthermore, because more\n",
    "adjacent traces are used, this method is less sensitive to noise, resulting in\n",
    "a more interpretable result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598cd451",
   "metadata": {},
   "source": [
    "Conceptually, Marfurt, et al's (1998) algorithm treats each seismic trace\n",
    "within the moving window as a separate dimension and measures how close the\n",
    "resulting point cloud is to a hyper-plane with a slope of 1 in all directions\n",
    "and intercept of 0.  \n",
    "\n",
    "It's easiest to visualize for the case of two traces.  For two traces, semblance-based coherence\n",
    "computes how close the points fall to a line with a slope of 1 and intercept of 0. In the figures below, we'll visualize this by crossplotting two traces. The discretely-sampled amplitude values (blue circles) from each trace will crossplotted on the right-hand portion of the figure.  We'll then measure how close they fall to our line.\n",
    "\n",
    "Let's start with the case of two identical traces:\n",
    "\n",
    "<img src=\"images/semblance_Identical_Traces.png\">\n",
    "\n",
    "When both traces are identical, the values fall perfectly on our line, so the semblance-based coherence is exactly 1.  This is the maximum possible value.\n",
    "\n",
    "However, if we shift one of the traces down slightly, the samples will have a non-linear pattern:\n",
    "\n",
    "<img src=\"images/semblance_Shifted_Traces.png\">\n",
    "\n",
    "Even though the line with a slope of 1 is a good fit to the scatter of points, the points don't fall directly on the line, so the semblance-based coherence is less than 1.\n",
    "\n",
    "Finally, let's consider what happens when we have two traces with an identical shape, but different amplitudes:\n",
    "\n",
    "<img src=\"images/semblance_Different_Amplitude.png\">\n",
    "\n",
    "The samples fall on a perfect line, but the slope of the line is not 1.  Thefore, the semblance-based coherence is below 1.  This is a key theoretical difference between Marfurt, et al's (1998) approach and some other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595b971",
   "metadata": {},
   "source": [
    "It is also possible to express this algorithm in terms of a covariance matrix,\n",
    "where each trace is treated as a separate dimension.  This is mathematically\n",
    "identical to the example given above, but as we'll see shortly, computing the\n",
    "covariance matrix allows for other approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0cb841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marfurt_semblance2(region):\n",
    "    region = region.reshape(-1, region.shape[-1])\n",
    "    ntraces, nsamples = region.shape\n",
    "\n",
    "    cov = region.dot(region.T)\n",
    "    return cov.sum() / cov.diagonal().sum() / ntraces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abeea3",
   "metadata": {},
   "source": [
    "This yields a completely identical result as the implementation shown before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cf25a",
   "metadata": {},
   "source": [
    "Removing Amplitude Sensitivity\n",
    "------------------------------\n",
    "\n",
    "One caveat to both Marfurt, et al's (1998) and Bahorich and Farmer's (1995)\n",
    "method is that they're sensitive to lateral differences in amplitude as well as\n",
    "differences in phase.  While this is desirable for detecting stratigraphic\n",
    "features, differences due to lateral changes in amplitude can obscure subtle\n",
    "structural features.  Gersztenkorn and Marfurt (1999) proposed an\n",
    "implementation that is sensitive only to lateral changes in phase of the input\n",
    "waveforms, and not to changes in amplitude: \"eigenstructure-based coherence\".\n",
    "\n",
    "Eigenstructure-based coherence (Gersztenkorn and Marfurt, 1999) computes the\n",
    "covariance matrix of the input region, similar to the previous example.\n",
    "However, it uses the ratio of the largest eigenvalue of the covariance matrix\n",
    "to the sum of the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa3028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gersztenkorn_eigenstructure(region):\n",
    "    region = region.reshape(-1, region.shape[-1])\n",
    "\n",
    "    cov = region.dot(region.T)\n",
    "    vals = np.linalg.eigvalsh(cov)\n",
    "    return vals.max() / vals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gersztenkorn = moving_window(seismic, (3, 3, 9), gersztenkorn_eigenstructure)\n",
    "plot(gersztenkorn, 'Gersztenkorn & Marfurt (1999)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275a1809",
   "metadata": {},
   "source": [
    "Conceptually, this is similar to treating each seismic trace within the moving\n",
    "window as a separate dimension and calculating how well the resulting point\n",
    "cloud is fit by a plane.  To contrast with Marfurt, et al's (1998)\n",
    "method, for the case of two traces, this measures the scatter about the\n",
    "best-fit line instead of a line with a slope of 1, as shown below:\n",
    "\n",
    "Once again, let's start with the case of two identical traces:\n",
    "\n",
    "<img src=\"images/eigenstructure_Identical_Traces.png\">\n",
    "\n",
    "When both traces are identical, the values fall perfectly on our line, so the eigenstructure-based coherence is exactly 1, identical to the semblance-based coherence.\n",
    "\n",
    "When we shift the traces down slightly, we'll change the slop:\n",
    "\n",
    "<img src=\"images/eigenstructure_Shifted_Traces.png\">\n",
    "\n",
    "However, even though the slope of best-fit line has changed, the overall scatter around this line is almost identical to a line with a slope of one, so eigenstructure-based and semblance-based coherence estimations give a very similar value.\n",
    "\n",
    "Finally, let's consider what happens when we have two traces with an identical shape, but different amplitudes:\n",
    "\n",
    "<img src=\"images/eigenstructure_Different_Amplitude.png\">\n",
    "\n",
    "The samples fall on a perfect line, but the slope of the line is not 1.  However, unlike semblance-based coherence, eigenstructure-based coherence measures how close the scatter of samples is to the best-fit line. Therefore, Gersztenkorn & Marfurt's (1999) method gives a perfect coherence of 1 in this case.  In other words, eigenstructure-based coherence is only sensitive to phase changes, in theory.  In practice, stratigraphic and structural features are usually expressed through both amplitude and phase changes, so separating the two out is not always advantageous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af08dc",
   "metadata": {},
   "source": [
    "Let's look at this side-by-side with Marfurt, et al's (1998) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(gersztenkorn, 'Gersztenkorn & Marfurt (1999)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic = data.load_seismic()\n",
    "plot(seismic, 'Input Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e93894d",
   "metadata": {},
   "source": [
    "On the right side of the figure, there's a normal fault that tips out towards the top of the time slice.  There are additional features visible in both the time slice and cross section, however, they're difficult to interpret without more detailed investigation.\n",
    "\n",
    "Speaking of more detailed investigation, you can also explore this dataset in a 3D viewer if you have Mayavi installed. (Click and drag on the secions to move them through the volume.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66267141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mayavi import mlab\n",
    "\n",
    "def explore3d(data_cube):\n",
    "    source = mlab.pipeline.scalar_field(data_cube)\n",
    "    source.spacing = [1, 1, -1]\n",
    "\n",
    "    nx, ny, nz = data_cube.shape\n",
    "    mlab.pipeline.image_plane_widget(source, plane_orientation='x_axes', \n",
    "                                     slice_index=nx//2, colormap='gray')\n",
    "    mlab.pipeline.image_plane_widget(source, plane_orientation='y_axes', \n",
    "                                     slice_index=ny//2, colormap='gray')\n",
    "    mlab.pipeline.image_plane_widget(source, plane_orientation='z_axes', \n",
    "                                     slice_index=nz//2, colormap='gray')\n",
    "    mlab.show()\n",
    "\n",
    "explore3d(seismic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4840c4",
   "metadata": {},
   "source": [
    "One quick comment on execuction speed: I'm deliberately using rather inefficient techniques in most of these examples to keep the code as concise and understandable as possible. Because of that, many of the examples below will take several minutes to run.  If you don't like waiting, consider using a small subset of the data, similar to the subset shown below. (Note: skip the dip-correction example, if you do, as it assumes you're using the full dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ff698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seismic = data.load_seismic()[10:40, :20, 10:-10]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
