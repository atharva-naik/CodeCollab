{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7495bd8a",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "This notebook demonstrates the stacking technique with the data in <a href=https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries>Two Sigma Connect: Rental Listing Inquiries</a> from <a href=https://www.kaggle.com/competitions>Kaggle competition</a>. Reading the detail is left to the audience, but to summarise:\n",
    "\n",
    "- The data is from <a href=https://www.renthop.com/>RentHop</a>, which is a web and mobile-based search engine that allows users to search for apartments in major cities.\n",
    "- Base on the data for each listing the purpose is to predict the interest level of each listing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6403b",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "The data from Kaggle comes with the training data and the test data, the latter has no correct label. For the purpose of demonstration, we will use only the training data. The test data is loaded though.\n",
    "\n",
    "- Below we print out the columns from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b183a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "with open('./data/train.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "raw_train = pd.DataFrame(data)\n",
    "\n",
    "with open('./data/test.json') as data_file:    \n",
    "    data = json.load(data_file)\n",
    "\n",
    "raw_test = pd.DataFrame(data)\n",
    "\n",
    "for item in raw_train.columns:\n",
    "    print item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f5761",
   "metadata": {},
   "source": [
    "The id is used for identifying the listing in the competition. For simplicity, we reindex the data by integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc44189",
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = raw_train.index\n",
    "raw_train=raw_train.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d89f97",
   "metadata": {},
   "source": [
    "We are going to train a popular model: <a href=https://github.com/dmlc/xgboost>xgboost</a>. We will start with only the numerical feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price']\n",
    "X = raw_train[col]\n",
    "y = raw_train['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0a9a9",
   "metadata": {},
   "source": [
    "## Folding \n",
    "\n",
    "To be able to evaluate our model, we use the package `model_selection` to split the indices into:\n",
    "\n",
    "- Training data (used to train the models)\n",
    "- Validation data (used to evaluate the models. We avoid using the term \"test data\" to differentiate this from the test data from Kaggle)\n",
    "\n",
    "For the later usage in stacking, we also split the training data into two folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "skf = model_selection.StratifiedKFold(n_splits=3)\n",
    "folds = skf.split(X, y)\n",
    "_, fold1 = folds.next()\n",
    "_, fold2 = folds.next()\n",
    "_, validation_idx = folds.next()\n",
    "\n",
    "train_idx = np.concatenate([fold1, fold2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392d01b",
   "metadata": {},
   "source": [
    "Below we initialize the parameters for xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param = {}\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['eta'] = 0.02\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric'] = \"mlogloss\"\n",
    "param['min_child_weight'] = 3\n",
    "param['subsample'] = 0.7\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['seed'] = 321\n",
    "num_rounds = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43aa2bb",
   "metadata": {},
   "source": [
    "Below we train and evaluate the xgboost model. We use the logloss as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ = time.time()\n",
    "X_train = np.array(X.iloc[train_idx])\n",
    "y_train = np.array(y.iloc[train_idx])\n",
    "xgtrain = xgb.DMatrix(X_train, label= y_train)\n",
    "\n",
    "clf = xgb.train(param, xgtrain, num_rounds)\n",
    "\n",
    "X_validation = np.array(X.iloc[validation_idx])\n",
    "y_validation = np.array(y.iloc[validation_idx])\n",
    "xgvalidation = xgb.DMatrix(X_validation)\n",
    "y_prob = clf.predict(xgvalidation)\n",
    "\n",
    "print 'The log loss is: %.3f' % sklearn.metrics.log_loss(y_validation, y_prob)\n",
    "print 'Time elapsed: %.2f seconds' % (time.time() - start_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a369a0",
   "metadata": {},
   "source": [
    "## Adding the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86320f0d",
   "metadata": {},
   "source": [
    "### Clean the text\n",
    "\n",
    "The code below cleans the \"description\" column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b7d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def cleaning_text(sentence):\n",
    "    sentence = sentence.encode('ascii', errors='replace')\n",
    "    sentence=sentence.lower()\n",
    "    sentence=re.sub('[^\\w\\s]',' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('_', ' ', sentence) #removes punctuations\n",
    "    sentence=re.sub('\\d+',' ', sentence) #removes digits\n",
    "    cleaned=' '.join([w for w in sentence.split() if not w in stop]) \n",
    "    #removes english stopwords\n",
    "    cleaned=' '.join([w for w , pos in pos_tag(cleaned.split()) if (pos == 'NN' or pos=='JJ' or pos=='JJR' or pos=='JJS' )])\n",
    "    #selecting only nouns and adjectives\n",
    "    \n",
    "    cleaned=' '.join([w for w in cleaned.split() if not len(w)<=2 ]) \n",
    "    #removes single lettered words and digits\n",
    "    cleaned=cleaned.strip()\n",
    "    return cleaned\n",
    "\n",
    "start_ = time.time()\n",
    "raw_train['cleaned_txt'] = raw_train['description'].apply(lambda x: cleaning_text(x))\n",
    "print 'Time elapsed: %.2f' % (time.time()-start_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1e6db",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "To take the text data into account, we need to somehow encoding the chategorical features into numerical features. Dummification (or one-hot encoding) is a popular way for general chategorical features. For text data, we would use a much more efficient method: <a gref=https://en.wikipedia.org/wiki/Tf%E2%80%93idf>tf-idf</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c9abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "n_features = 500\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "text_train = np.array(raw_train.loc[train_idx, 'cleaned_txt'].replace(np.nan, ''))\n",
    "tf = tf_vectorizer.fit_transform(text_train)\n",
    "TF = TfidfTransformer()\n",
    "tf_idf_train = TF.fit_transform(tf)\n",
    "tf_idf_train = tf_idf_train.toarray()\n",
    "X_train_des = np.concatenate([X_train, tf_idf_train], axis=1)\n",
    "\n",
    "\n",
    "text_validation = np.array(raw_train.loc[validation_idx, 'cleaned_txt'].replace(np.nan, ''))\n",
    "tf = tf_vectorizer.transform(text_validation)\n",
    "tf_idf_validation = TF.transform(tf)\n",
    "tf_idf_validation = tf_idf_validation.toarray()\n",
    "X_validation_des = np.concatenate([X_validation, tf_idf_validation], axis=1)\n",
    "\n",
    "text_fold1 = np.array(raw_train.loc[fold1, 'cleaned_txt'].replace(np.nan, ''))\n",
    "text_fold2 = np.array(raw_train.loc[fold2, 'cleaned_txt'].replace(np.nan, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bb6e59",
   "metadata": {},
   "source": [
    "Below we trained the model with both the numerical feature we used before, and the new text data.\n",
    "\n",
    "We see that the performance is improved with no surprise -- we took more feature into account. However, the improvement takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0fb75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ = time.time()\n",
    "xgtrain_des = xgb.DMatrix(X_train_des, label= y_train)\n",
    "clf = xgb.train(param, xgtrain_des, num_rounds)\n",
    "xgvalidation_des = xgb.DMatrix(X_validation_des)\n",
    "y_prob_des = clf.predict(xgvalidation_des)\n",
    "print 'Time elapsed: %.2f seconds' % (time.time() - start_)\n",
    "print 'The log loss is: %.3f' % sklearn.metrics.log_loss(y_validation, y_prob_des)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97807a4a",
   "metadata": {},
   "source": [
    "## Stacking models with resampling\n",
    "\n",
    "In the demo above we saw a drawback of the tree-based models: inefficiency when dealing with large cardinality of a chategorical feature.\n",
    "\n",
    "One thing we can do is to train some simpler models first on the text data, and then stack back with the numerical predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff8a01",
   "metadata": {},
   "source": [
    "### Using sparse matrix\n",
    "\n",
    "The text data contains a lot of zeros, one simple way to gain efficiency is to use the sparse matrices."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
