{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa81e2b2",
   "metadata": {},
   "source": [
    "## This notebook contains our summarization techniques that we tried ranging from Keyphrase Extraction (using frequency, collocation, chunking and wordnet) to Key sentences extraction ( sentences with most frequent words, Gensim summarizer, Classification and Gensim summarizer). The final approach that we used was Key sentences extraction using Classification and Gensim Summarization that uses TextRank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51732022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import nltk, re, numpy as np\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import string\n",
    "from gensim.summarization import summarize, keywords\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ceeb25",
   "metadata": {},
   "source": [
    "## Reading in the files\n",
    "The following sections read in the text from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f763af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loadText\n",
    "def loadText(text):\n",
    "    f = open(text)\n",
    "    raw = f.read().decode('utf8');\n",
    "    return raw\n",
    "\n",
    "tos_text = loadText(\"static/data/Google.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b21626",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tos_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150988e6",
   "metadata": {},
   "source": [
    "# Keyphrase Extraction Approach\n",
    "\n",
    "In the following approaches - frequency, collocation and chunking - for each, the steps followed are: 1. Identifying candidates for keyphrases and 2. Keyphrase selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72487b5",
   "metadata": {},
   "source": [
    "## Approach 1 - Frequency\n",
    "\n",
    "The following segment attempts using Approach 1 - Frequent Terms\n",
    "-- Frequent Unigrams (with or without stemming, stopwords, and other normalization)\n",
    "-- Frequent Bigram frequencies (with or without stemming, stopwords, and other normalization)\n",
    "-- Other variations on frequent n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6c94f",
   "metadata": {},
   "source": [
    "### 1. Candidates identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using FreqDist\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "text_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "fdist = nltk.FreqDist(text_tokens)\n",
    "print(\"20 Most frequent tokens: \")\n",
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be28b3",
   "metadata": {},
   "source": [
    "#### Normalizing\n",
    "Normalizing - convert to lower case, remove punctuation and stop words, stem words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267109b5",
   "metadata": {},
   "source": [
    "### 1. Candidate Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44764347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b84a5",
   "metadata": {},
   "source": [
    "### 2. Candidate Selection - Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440783c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Post normalizing\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(norm)\n",
    "\n",
    "# only bigrams that appear atleast 3 times\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "# returns the 10 bigrams with the highest PMI\n",
    "print(\"\\nTop 10 bigrams using PMI:\")\n",
    "print(finder.nbest(bigram_measures.pmi, 10))\n",
    "#finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "# Finds top 10 bigrams using the Pearson's Chi-squared test\n",
    "print(\"\\nTop 10 bigrams using Pearson's Chi-Squared Test:\")\n",
    "print(finder.nbest(bigram_measures.chi_sq, 10))\n",
    "#finder.score_ngrams(bigram_measures.chi_sq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ce05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds top 10 bigrams using the likelihood ratio\n",
    "\n",
    "print(\"\\nTop 10 bigrams using Maximum Likelihood Ratio:\")\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 10))\n",
    "#finder.score_ngrams(bigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc7fb6",
   "metadata": {},
   "source": [
    "I also used collocations on text that was not normalized but that did not yield in meaningful results since the stop words were included and ended up being among the most frequent.\n",
    "Eg: ('of', 'the'), ('in', 'the'), ('the', 'jungle'), ('he', 'was'), etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c76ce",
   "metadata": {},
   "source": [
    "### 2. Candidate Selection - Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e674f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post normalizing\n",
    "\n",
    "finder = TrigramCollocationFinder.from_words(norm)\n",
    "\n",
    "# only trigrams that appear atleast 3 times\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "# return the 10 trigrams with the highest PMI\n",
    "print(\"\\nTop 10 trigrams using PMI:\")\n",
    "print(finder.nbest(trigram_measures.pmi, 10))\n",
    "#finder.score_ngrams(trigram_measures.pmi)\n",
    "\n",
    "# Finds top 10 trigrams using the Pearson's Chi-squared test\n",
    "print(\"\\nTop 10 trigrams using Pearson's Chi-Squared Test:\")\n",
    "print(finder.nbest(trigram_measures.chi_sq, 10))\n",
    "#finder.score_ngrams(trigram_measures.chi_sq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c5856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds top 10 trigrams using the likelihood ratio\n",
    "print(\"\\nTop 10 trigrams using Maximum Likelihood Ratio:\")\n",
    "print(finder.nbest(trigram_measures.likelihood_ratio, 10))\n",
    "#finder.score_ngrams(trigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7c850",
   "metadata": {},
   "source": [
    "## Aproach 3 - Chunking\n",
    "\n",
    "### 1. Candidate Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f88c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(corpus):\n",
    "    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sents = sent_tokenizer.tokenize(corpus) # Split text into sentences  \n",
    "    return [nltk.word_tokenize(word) for word in raw_sents]\n",
    "\n",
    "def create_chunker(grammar):\n",
    "    return nltk.RegexpParser(grammar)\n",
    "\n",
    "def run_chunker(ch, sentences):\n",
    "    return [ch.parse(sent) for sent in sentences]\n",
    "\n",
    "# Defining the grammar for the chunker\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+}  # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "\"\"\"\n",
    "tagged_sentences = nltk.pos_tag_sents(tokenize_text(text))\n",
    "\n",
    "clause_chunker = create_chunker(grammar)\n",
    "\n",
    "# Collecting the clauses\n",
    "clauses = []\n",
    "for sent in tagged_sentences:\n",
    "    tree = clause_chunker.parse(sent)\n",
    "    for st in tree.subtrees():\n",
    "        if st.label() == 'CLAUSE': clauses.append(st)\n",
    "\n",
    "#print(clauses)\n",
    "\n",
    "#Collecting the proper nouns\n",
    "proper_nouns = []\n",
    "for sent in tagged_sentences:\n",
    "    tree = clause_chunker.parse(sent)\n",
    "    for st in tree.subtrees():\n",
    "        if st.label() == 'NP': proper_nouns.append(st.leaves()[0][0])\n",
    "#print(proper_nouns)\n",
    "\n",
    "candidates = []\n",
    "for clause in clauses:\n",
    "    for s in clause.subtrees():\n",
    "        if(s.label() == 'NP'):\n",
    "            for l in s.leaves():\n",
    "                for word in l:\n",
    "                    if(word in proper_nouns):\n",
    "                        candidates.append(clause)\n",
    "                        next\n",
    "\n",
    "candidate_sentences = []\n",
    "for candidate in candidates:\n",
    "    candidate_sentences.append(' '.join([l[0] for l in candidate.leaves()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf2f04",
   "metadata": {},
   "source": [
    "### 2. Candidate Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "#To remove specific punctuation and possessive found in this book\n",
    "all_punct=string.punctuation + \"--\" + \".\\\"\" + \",\\\"\" + \"?\\\"\" + \"\\'s\" + \"!\\\"\"+\"\\\"\"\n",
    "text_nopunct = [x.lower() for x in text_tokens if x not in all_punct]      #Convert to lower & punctuation\n",
    "#print(\"Total number of words: \", len(text_nopunct),\"\\n\")\n",
    "\n",
    "norm_fdist = nltk.FreqDist(text_nopunct)\n",
    "unique_incl_stop = len(norm_fdist.keys())\n",
    "#print(\"Total number of unique words - without removing stop words: \", unique_incl_stop)\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "norm = [i for i in text_nopunct if i not in stop]    #Remove stopwords\n",
    "norm_fdist = nltk.FreqDist(norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83a50f",
   "metadata": {},
   "source": [
    "### 2. Keyphrase selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nNormalized Text:\")\n",
    "#print(\"Total number of unique words post normalizing: \",len(norm_fdist.keys()))\n",
    "print(\"\\n20 Most frequent words: \")\n",
    "print(norm_fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4997cb1c",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "Using stemming on unigrams, post normalizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c13d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#Results post stemming - Using Snowball stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "text_stem = [stemmer.stem(i) for i in norm]\n",
    "stem_fdist = nltk.FreqDist(text_stem)\n",
    "print(\"\\nUsing stemmed words: \\n\")\n",
    "print(\"20 Most frequent words: \")\n",
    "print(stem_fdist.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2804b4c",
   "metadata": {},
   "source": [
    "The above output is not very useful. It basically used all the words and even post normalizing did not provide a very meaningful output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed02ae",
   "metadata": {},
   "source": [
    "Repeating the above, but using bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk import bigrams\n",
    "from nltk import trigrams\n",
    "\n",
    "bi = bigrams(norm)\n",
    "bi_list = [bigram for bigram in bi]\n",
    "tri = trigrams(norm)\n",
    "tri_list = [trigram for trigram in tri]\n",
    "\n",
    "bi_fdist = nltk.FreqDist(bi_list)\n",
    "print(\"\\n20 Most frequent bigrams: \\n\")\n",
    "print(bi_fdist.most_common(20))\n",
    "print(\"\\n\\n20 Most frequent trigrams: \\n\")\n",
    "tri_fdist = nltk.FreqDist(tri_list)\n",
    "print(tri_fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0b5b5",
   "metadata": {},
   "source": [
    "## Approach 2 - Collocation (Words)\n",
    "\n",
    "Collocation can be done either with words or with parts of speech. Here, I have attempted with words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lchar = 0\n",
    "finlist = []\n",
    "\n",
    "for sentence in set(candidate_sentences):\n",
    "    if lchar<2000:\n",
    "        ls = len(sentence)\n",
    "        if (lchar + ls)<2000:\n",
    "            finlist.append(sentence)\n",
    "            lchar +=ls\n",
    "        \n",
    "print(\"\\n\".join(ph for ph in finlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683a8f6",
   "metadata": {},
   "source": [
    "## Aproach 4 - Wordnet "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
