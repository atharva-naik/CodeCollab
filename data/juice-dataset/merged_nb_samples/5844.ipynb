{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea238929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter plot of points with vectors\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "ax = plt.gca()\n",
    "ax.quiver(0,0,xs,ys,angles='xy',scale_units='xy',scale=1, linewidth = .01)\n",
    "ax.set_xlim([-1,1])\n",
    "ax.set_ylim([-1,1])\n",
    "xlabel('First principal component')\n",
    "ylabel('Second principal component')\n",
    "title('Plot of points against LSA principal components')\n",
    "plt.draw()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(numpy.asmatrix(dtm_lsa) * numpy.asmatrix(dtm_lsa).T)\n",
    "pd.DataFrame(similarity,index=example, columns=example).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101a1d3",
   "metadata": {},
   "source": [
    "### <a href = http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/> LDA for Grandmas (15 Minute Read) </a>\n",
    "\n",
    "<a href=https://www.youtube.com/watch?v=3mHy4OSyRf0> LDA Introduction </a>\n",
    "\n",
    "## First, choose K topics\n",
    "\n",
    "Kind of like KNN but we are deciding, up front, on a preset number of topics.\n",
    "\n",
    "## Calculate Word / Topic Probabilities\n",
    "\n",
    "![](https://snag.gy/yx9grm.jpg)\n",
    "\n",
    "**For each possible topic Z, and each word:**\n",
    "1. Multiply the frequency of the word by the total number of words already in topic Z\n",
    "\n",
    "This gives us a probability that each word exists in the preset number of topics.\n",
    "\n",
    "> The term here that is unfamilliar is a hyperparameter, *alpha*.  In this case, alpha is a scaler that helps minimize an error term.  Thankfully, most LDA models that are implented will set this automatically and it's usually, 95% of the time, a fine solution.  To really get a strong handle of the math behind this model, there are whitepapers you can read.  Also, having a strong handle on Bayesian statistis is a must to really grasp this model at it's lowest levels.  We are not going there today!\n",
    "\n",
    "> One problem **alpha** solves is leaving the window of opportunity open when a word may only belong in a single topic, that doesn't exist in any others.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae078d8",
   "metadata": {},
   "source": [
    "## Calculate Word / Topic Probabilities\n",
    "\n",
    "![](https://snag.gy/yx9grm.jpg)\n",
    "\n",
    "**For each possible topic Z, and each word:**\n",
    "1. Multiply the frequency of the word by the total number of words already in topic Z\n",
    "\n",
    "This gives us a probability that each word exists in the preset number of topics.\n",
    "\n",
    "> The term here that is unfamilliar is a hyperparameter, *alpha*.  In this case, alpha is a scaler that helps minimize an error term.  Thankfully, most LDA models that are implented will set this automatically and it's usually, 95% of the time, a fine solution.  To really get a strong handle of the math behind this model, there are whitepapers you can read.  Also, having a strong handle on Bayesian statistis is a must to really grasp this model at it's lowest levels.  We are not going there today!\n",
    "\n",
    "> One problem **alpha** solves is leaving the window of opportunity open when a word may only belong in a single topic, that doesn't exist in any others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53de4b53",
   "metadata": {},
   "source": [
    "## LDA Intuition\n",
    " \n",
    "Using the previous assumption about how words are assigned to topics, as we iterate through each word in our corpus, and assign to topics:\n",
    "\n",
    "1. Words become more common in topics where they are already common.\n",
    "1. Topics will become more common in documents where they are already common.\n",
    "\n",
    "**With LDA**:\n",
    "- Words are assigned to topics randomly at first\n",
    "- As words are found to be consistently distributed within topics, the model achieves a sort of balance based on the distribution of words accross all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e7527",
   "metadata": {},
   "source": [
    "### (Optional) LDA Math...\n",
    "\n",
    "<img src= http://i.imgur.com/HRRK6k0.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd1eedc",
   "metadata": {},
   "source": [
    "### Parameters of LDA\n",
    "\n",
    "$\\alpha$ and $\\beta$ Hyperparameters: \n",
    "- $\\alpha$ represents document-topic density and Beta represents topic-word density. Higher the value of $\\alpha$, documents are composed of more topics and lower the value of $\\alpha$, documents contain fewer topics. \n",
    "\n",
    "- On the other hand, higher the $\\beta$, topics are composed of a large number of words in the corpus, and with the lower value of beta, they are composed of few words.\n",
    "\n",
    "Number of Topics – Number of topics to be extracted from the corpus. Researchers have developed approaches to obtain an optimal number of topics by using Kullback Leibler Divergence Score. \n",
    "\n",
    "Number of Topic Terms – Number of terms composed in a single topic. It is generally decided according to the requirement. If the problem statement talks about extracting themes or concepts, it is recommended to choose a higher number, if problem statement talks about extracting features or terms, a low number is recommended.\n",
    "\n",
    "Number of Iterations / passes – Maximum number of iterations allowed to LDA algorithm for convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e9849f",
   "metadata": {},
   "source": [
    "## LDA Challenges\n",
    "\n",
    "1. **There's a bit of entropy to topics.**\n",
    "There can be between %1-10 shift in what is generated in LDA models.  You may not get the same thing 2x!\n",
    "\n",
    "1. **Can be very difficult to assess.**\n",
    "If you have a large corpus, with many topics (>10), it's damn near impossible to visualize the distribution of documents to topics.\n",
    "\n",
    "1. **Preprocessing is heavy.**\n",
    "To get the most out of LDA, cleaning stopwords, and specific language can be a challenging task.  Sometimes it's difficult to avoid the noise involved with this model.\n",
    "\n",
    "1. **SME is necessary for accurate topic assessment.**\n",
    "The more straight forward your text is, the less subject matter expertise is required.  A more advanced use of LDA would involve assessing documents with lots of idiomtic language. Knowing what topics are found, can be subjective.\n",
    "\n",
    "1.  **Determining what topics mean, is tricky.**\n",
    "A collection of world probabiltiies generally isn't very intuitive.  You could take the first word and use that as your topic \"label\".  Hence, subject matter expertise.\n",
    "\n",
    "1. **LDA is unsupervised.**\n",
    "It's not possible to know what is \"correct\".  The repsonse topics are generated, hence this is why LDA is known as a \"generative\" model. \n",
    "\n",
    "1. **Tuning your LDA model can be tough.\"**\n",
    "With other unsupervised models, it's possible to tune for the parameter **K** *number of topics*, but it's not necessarily a very accurate method.  There are things you can do to assess the main paramter **K**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6fbb64",
   "metadata": {},
   "source": [
    "## LDA Strengths\n",
    "\n",
    "1. **It can be very strong performer in production.**\n",
    "After you build the model, it can easily be used \"online\".\n",
    "> \"Online\" training allows you to update your model with more training data without having to refit all your data.  Only new data can be fit globally.\n",
    "\n",
    "1.  **It's easy to get a quick sense of what a large body of text is broadly \"about\", without having to read all of it.** Rather than reading 12k PDF's on corproate policies, you could extract the text, and run LDA to see what generalities it finds.\n",
    "1.  **Easily classify / tag documents by topic.**\n",
    "1.  **It can \"just work\" out of the box.**  However, your mileage will vary depending on your preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921b393",
   "metadata": {},
   "source": [
    "## Other Types of LDA models\n",
    "\n",
    "- Topics Over Time\n",
    "- Dynamic Topic Modeling\n",
    "- Hierarchical LDA\n",
    "- Pachinko Allocation \n",
    "\n",
    "A cool new LDA model to look out for:\n",
    "\n",
    "- LDA2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460e41de",
   "metadata": {},
   "source": [
    "### Evaluating LDA: \n",
    "- Perplexity to Evaluate: <a href=https://www.youtube.com/watch?v=OHyVNCvnsTo> Stanford NLP </a> \n",
    "- Extrinsic Evaluations: Human Evaluation, Mechanical Turkers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05d689",
   "metadata": {},
   "source": [
    "## LDA Codealong"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
