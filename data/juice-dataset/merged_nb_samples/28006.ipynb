{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_color_codes()\n",
    "\n",
    "mpl.rc('figure', figsize=(8, 5))\n",
    "mpl.rc('figure', dpi=300)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc580f84",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    ">## 1. Activation Function\n",
    "* 1.1. Vanishing Gradient Problem\n",
    "* 1.2. Vanishing Gradient Problem - Visualization\n",
    "* 1.3. $\\text{tanh}(a)$\n",
    "* 1.4. Rectified Linear Unit (ReLU)\n",
    "* 1.5. `keras` - `activation`\n",
    "\n",
    ">## 2. Cost Function\n",
    "* 2.1. Cross-Entropy\n",
    "* 2.2. `keras` - `loss`\n",
    "\n",
    ">## 3. Weight Initialization\n",
    "* 3.1. Glorot Initialization\n",
    "* 3.2. LeCun Initialization\n",
    "* 3.3. `keras` - `kernel_initializer`\n",
    "\n",
    ">## 4. Optimization Algorithm\n",
    "\n",
    "\n",
    ">## 5. Regularization\n",
    "* 5.1. Overfitting\n",
    "* 5.2. L1 & L2 Regularization\n",
    "* 5.3. `keras` - `kernel_regularizer`\n",
    "* 5.4. Dropout Regularization\n",
    "* 5.5. `keras` - `Dropout`\n",
    "\n",
    ">## 6. Softmax\n",
    "* 6.1. `keras` - `activation='softmax'`\n",
    "\n",
    ">## 7. Batch Normalization\n",
    "* 7.1. `keras` - `BatchNormalization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a1b07",
   "metadata": {},
   "source": [
    "오차(목적) 함수 개선: cross-entropy cost function\n",
    "가중치 초기값: weight initialization\n",
    "정규화: regularization\n",
    "Softmax 출력\n",
    "Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e4b844",
   "metadata": {},
   "source": [
    "# 1. Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de313bb6",
   "metadata": {},
   "source": [
    "## 1.1. Vanishing Gradient Problem\n",
    "* **No. of Hidden Layers $\\uparrow$ $\\rightarrow$ Performance $\\downarrow$**\n",
    "* **WHY?** \n",
    ">* As the error backpropagate,\n",
    ">* The gradient of the activation fn. is multiplied\n",
    ">* This value is close to zero $\\rightarrow$ gradient vanishes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802733f0",
   "metadata": {},
   "source": [
    "## 1.2. Vanishing Gradient Problem - Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf897943",
   "metadata": {},
   "source": [
    "### Gradient of the Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "sigmoid_prime = lambda x: sigmoid(x)*(1-sigmoid(x))\n",
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, sigmoid(xx))\n",
    "plt.plot(xx, sigmoid_prime(xx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ff7a6",
   "metadata": {},
   "source": [
    "### Compare 4 Models with 1~4 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c606ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model1.add(Dense(10, activation=\"sigmoid\"))\n",
    "model1.compile(optimizer=SGD(lr=0.2), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5af870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist1 = model1.fit(X_train, Y_train, epochs=30, batch_size=100, validation_data=(X_test, Y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc85175c",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist0.history['val_acc'], ls=\":\", label=\"mean squared error\")\n",
    "plt.plot(hist1.history['val_acc'], label=\"cross entropy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51128ee5",
   "metadata": {},
   "source": [
    "# 3. Weight Initialization\n",
    "* $n_{\\text{in}}$: No. of neurons in input layer\n",
    "* $n_{\\text{out}}$: No. of neurons in output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e772a",
   "metadata": {},
   "source": [
    "## 3.1. Glorot Initialization\n",
    "\n",
    "#### Glorot Normal: \n",
    "\n",
    ">$$\\text{Mean}(W)=0$$\n",
    ">$$$$\n",
    ">$$\\text{Var}(W)=\\dfrac{2}{n_{\\text{in}} + n_{\\text{out}}}$$\n",
    "\n",
    "#### Glorot Uniform:\n",
    "\n",
    ">$$\\text{High}(W) = + \\sqrt{\\dfrac{6}{(n_{\\text{in}} + n_{\\text{out}})}}$$\n",
    ">$$$$\n",
    ">$$\\text{Low}(W) = - \\sqrt{\\dfrac{6}{(n_{\\text{in}} + n_{\\text{out}})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07812932",
   "metadata": {},
   "source": [
    "## 3.2. LeCun Initialization\n",
    "\n",
    "#### LeCun Normal: \n",
    "\n",
    ">$$\\text{Mean}(W)=0$$\n",
    ">$$$$\n",
    ">$$\\text{Var}(W)=\\dfrac{1}{n_{\\text{in}}}$$\n",
    "\n",
    "#### LeCun Uniform:\n",
    "\n",
    ">$$\\text{High}(W) = + \\sqrt{\\dfrac{3}{n_{\\text{in}}}}$$\n",
    ">$$$$\n",
    ">$$\\text{Low}(W) = - \\sqrt{\\dfrac{3}{n_{\\text{in}}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911b8222",
   "metadata": {},
   "source": [
    "## 3.3. `keras` - `kernel_initializer`\n",
    "* `random_uniform`\n",
    "* `random_normal`\n",
    "* `glorot_uniform`\n",
    "* `glorot_normal`\n",
    "* `lecun_uniform`\n",
    "* `lecun_normal`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fb4bf",
   "metadata": {},
   "source": [
    "### `kernel_initializer='normal'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7467f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(100, kernel_initializer=\"normal\", activation=\"sigmoid\", input_dim=784))\n",
    "model4.add(Dense(10, kernel_initializer=\"normal\", activation=\"sigmoid\"))\n",
    "model4.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist4 = model4.fit(X_train, Y_train, epochs=10, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4330d2",
   "metadata": {},
   "source": [
    "### `kernel_initializer='glorot_uniform'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74280b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "callback3 = WeightHistory(model3, num_epoch)\n",
    "hist3 = model3.fit(X_train, Y_train, epochs=num_epoch, batch_size=100, callbacks=[callback3], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39d8d3e",
   "metadata": {},
   "source": [
    "### Result - 3 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2ffd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(hist3.history['acc'])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(callback3.weight_change[0], 'r', label=\"hidden 1\")\n",
    "plt.plot(callback3.weight_change[1], 'g', label=\"hidden 2\")\n",
    "plt.plot(callback3.weight_change[2], 'b', label=\"hidden 3\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Average Weight Change\")\n",
    "plt.ylim(0, np.max(callback3.weight_change[-1][5:]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist3.history['acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f77888",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "callback4 = WeightHistory(model4, num_epoch)\n",
    "hist4 = model4.fit(X_train, Y_train, epochs=num_epoch, batch_size=100, callbacks=[callback4], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f71b4",
   "metadata": {},
   "source": [
    "### Result - 4 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ac13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(hist4.history['acc'])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(callback4.weight_change[0], 'r', label=\"hidden 1\")\n",
    "plt.plot(callback4.weight_change[1], 'g', label=\"hidden 2\")\n",
    "plt.plot(callback4.weight_change[2], 'b', label=\"hidden 3\")\n",
    "plt.plot(callback4.weight_change[3], 'k', label=\"hidden 4\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Average Weight Change\")\n",
    "plt.ylim(0, np.max(callback4.weight_change[-1][5:]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc13369",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist4.history['acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5bdb7",
   "metadata": {},
   "source": [
    "## 1.3. $\\text{tanh}(a)$\n",
    "* Maximum Gradient: $1$ *(4 times bigger than that of Sigmoid)*\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "  \\tanh(a) \\equiv \\frac{e^a-e^{-a}}{e^a+e^{-a}} = 2\\sigma(2a) - 1\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405a92cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = lambda x: np.tanh(x)\n",
    "tanh_prime = lambda x: 1 - np.tanh(x) ** 2\n",
    "xx = np.linspace(-10, 10, 1000)\n",
    "plt.plot(xx, tanh(xx), 'b:', label=\"tanh(a)\")\n",
    "plt.plot(xx, tanh_prime(xx), 'g-', label=\"derivative of tanh(a)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c982448",
   "metadata": {},
   "source": [
    "## 1.4. Rectified Linear Unit (ReLu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d62d8",
   "metadata": {},
   "source": [
    "* Solves vanishing gradient problem\n",
    "* Widely used for NN with large no. of hidden layers\n",
    "* **ReLU**\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "  \\max(0, a)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "* **Leaky ReLU** (Non-zero gradient for $a<0$\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "  \\max(\\alpha a, a) \\;\\; (0 < \\alpha < 1)\n",
    "\\end{eqnarray}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96888fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model5 = Sequential()\n",
    "model5.add(Dense(100, input_dim=784, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\"))\n",
    "model5.add(Dense(10, activation=\"sigmoid\", kernel_initializer=\"glorot_uniform\"))\n",
    "model5.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17592cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist5 = model5.fit(X_train, Y_train, epochs=10, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7447e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist4.history['acc'], label=\"normal\")\n",
    "plt.plot(hist5.history['acc'], label=\"glorot_uniform\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455e9eed",
   "metadata": {},
   "source": [
    "# 4. Optimization Alorithm\n",
    "* [Reference](http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html)\n",
    "\n",
    "## 4.1. Gradient Descent (default)\n",
    "\n",
    ">$$ w_{k+1} = w_k - \\mu_k g(w_k) = w_k - v_k $$\n",
    "\n",
    "## 4.2. Decay\n",
    "* Reduce step-size\n",
    "\n",
    ">$$ \\mu_{k+1} = \\mu_{k} \\dfrac{1}{1 + \\text{decay}} $$\n",
    "\n",
    "## 4.3. Momentum\n",
    "* Inertia (tries to move towards its original direction)\n",
    "\n",
    ">$$ v_{k+1} = \\text{momentum} \\cdot v_k - \\mu_k g(w_k) $$\n",
    "\n",
    "\n",
    "## 4.4. Nesterov Momentum\n",
    "* Move along momentum & Then use gradient\n",
    "\n",
    ">$$ v_{k+1} = \\text{momentum} \\cdot v_k - \\mu_k g(w_k - \\text{momentum} \\cdot v_k) $$\n",
    "\n",
    "## 4.5. Adagrad\n",
    "* Adaptive Gradient\n",
    "\n",
    ">$$ G_{k+1} = G_k + g^2 $$\n",
    ">\n",
    ">$$ w_{k+1} = w_k - \\dfrac{\\mu_k}{\\sqrt{G_k + \\epsilon}} g(w_k) $$\n",
    "\n",
    "## 4.6. RMSProp\n",
    "\n",
    ">$$ G_{k+1} = \\gamma G_k + (1 - \\gamma) g^2 $$\n",
    ">\n",
    ">$$ w_{k+1} = w_k - \\dfrac{\\mu_k}{\\sqrt{G_k + \\epsilon}} g(w_k) $$\n",
    "\n",
    "## 4.7. AdaDelta\n",
    "\n",
    ">$$ G_{k+1} = \\gamma G_k + (1 - \\gamma) g^2 $$\n",
    ">\n",
    ">$$ \\mu_{k+1} = \\gamma \\mu_k + (1 - \\gamma) \\Delta_k^2 $$\n",
    ">\n",
    ">$$ \\Delta_k = \\dfrac{\\sqrt{\\mu_k + \\epsilon}}{\\sqrt{G_k + \\epsilon}} g(w_k) $$\n",
    ">\n",
    ">$$ w_{k+1} = w_k - \\Delta_k $$\n",
    "\n",
    "## 4.8. Adam\n",
    "* Adaptive Momentum\n",
    "\n",
    ">$$ G_{k+1} = \\gamma G_k + (1 - \\gamma) g^2 $$\n",
    ">\n",
    ">$$ v_{k+1} = \\gamma_v v_k + (1 - \\gamma_v) g_k^2 $$\n",
    ">\n",
    ">$$ \\hat{G}_k = \\dfrac{G_k}{1 - \\beta_1} $$\n",
    ">\n",
    ">$$ \\hat{v}_k = \\dfrac{v_k}{1 - \\beta_2} $$\n",
    ">\n",
    ">$$ w_{k+1} = w_k - \\dfrac{\\mu_k}{\\sqrt{\\hat{G}_k + \\epsilon}} \\hat{v}_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d78d8e5",
   "metadata": {},
   "source": [
    "# 5. Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f91c3b",
   "metadata": {},
   "source": [
    "## 5.1. Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model6 = Sequential()\n",
    "model6.add(Dense(30, input_dim=784, activation=\"sigmoid\"))\n",
    "model6.add(Dense(10, activation=\"sigmoid\"))\n",
    "model6.compile(optimizer=SGD(lr=0.5), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a447e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist6 = model6.fit(X_train, Y_train, epochs=30, batch_size=10, validation_data=(X_test, Y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606872b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(hist6.history['loss'], label=\"loss\")\n",
    "plt.legend()\n",
    "plt.title(\"loss\")\n",
    "plt.subplot(212)\n",
    "plt.plot(hist6.history['acc'], label=\"training accuracy\")\n",
    "plt.plot(hist6.history['val_acc'], label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1fdf17",
   "metadata": {},
   "source": [
    "## 5.2. L1 & L2 Regularization\n",
    "\n",
    "#### L1 Regularization\n",
    "\n",
    ">$$\n",
    "C = - (y \\log z+(1-y) \\log(1-z)) + \\lambda \\sum_i |w_i|\n",
    "$$\n",
    "\n",
    "#### L2 Regularization\n",
    "\n",
    ">$$\n",
    "C = - (y \\log z+(1-y) \\log(1-z)) + \\lambda \\sum_i w_i^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8f796",
   "metadata": {},
   "source": [
    "## 5.3. `keras` - `kernel_regularizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc691ca",
   "metadata": {},
   "source": [
    "### Result - 2 Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a005700",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(hist2.history['acc'])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(callback2.weight_change[0], 'r', label=\"hidden 1\")\n",
    "plt.plot(callback2.weight_change[1], 'g', label=\"hidden 2\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Average Weight Change\")\n",
    "plt.ylim(0, np.max(callback2.weight_change[-1][5:]))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2.history['acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f70df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(-5, 5, 100)\n",
    "relu = np.maximum(0, a)\n",
    "leakyrelu = np.maximum(0.1 * a, a)\n",
    "plt.plot(a, relu, 'r-', label=\"ReLU\")\n",
    "plt.plot(a, leakyrelu, 'g--', label=\"Leaky ReLU ($\\\\alpha=0.1$)\")\n",
    "plt.title(\"ReLU and Leaky ReLU\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba1178",
   "metadata": {},
   "source": [
    "## 1.5. `keras` - `activation`\n",
    "* `sigmoid`\n",
    "* `tanh`\n",
    "* `relu`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bcf21d",
   "metadata": {},
   "source": [
    "### `activation='sigmoid'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa0692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "np.random.seed(0)\n",
    "model7 = Sequential()\n",
    "model7.add(Dense(30, input_dim=784, activation=\"sigmoid\", kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model7.add(Dense(10, activation=\"sigmoid\"))\n",
    "model7.compile(optimizer=SGD(lr=0.5), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78cd704",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist7 = model7.fit(X_train, Y_train, epochs=30, batch_size=10, validation_data=(X_test, Y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(hist7.history['loss'], label=\"loss\")\n",
    "plt.legend()\n",
    "plt.title(\"loss\")\n",
    "plt.subplot(212)\n",
    "plt.plot(hist7.history['acc'], label=\"training accuracy\")\n",
    "plt.plot(hist7.history['val_acc'], label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c23e70",
   "metadata": {},
   "source": [
    "## 5.4. Dropout Regularization\n",
    "* For each epoch drop $100 \\times p %$ of hidden layer neurons\n",
    "* Helps prevent overfitting\n",
    "\n",
    "<img src=\"http://neuralnetworksanddeeplearning.com/images/tikz31.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62133637",
   "metadata": {},
   "source": [
    "## 5.5. `keras` - `Dropout`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc16d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "np.random.seed(0)\n",
    "model8 = Sequential()\n",
    "model8.add(Dense(30, input_dim=784, activation=\"sigmoid\"))\n",
    "model8.add(Dropout(0.1))\n",
    "model8.add(Dense(10, activation=\"sigmoid\"))\n",
    "model8.compile(optimizer=SGD(lr=0.5), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist8 = model8.fit(X_train, Y_train, epochs=30, batch_size=10, validation_data=(X_test, Y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(hist8.history['loss'], label=\"loss\")\n",
    "plt.legend()\n",
    "plt.title(\"loss\")\n",
    "plt.subplot(212)\n",
    "plt.plot(hist8.history['acc'], label=\"training accuracy\")\n",
    "plt.plot(hist8.history['val_acc'], label=\"test accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bf5ec3",
   "metadata": {},
   "source": [
    "# 6. Softmax\n",
    "* Softmax fn. $\\rightarrow$ $\\text{sum of output} = 1$\n",
    "* Then, the output values can be interpreted as probability mass\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray} \n",
    "  y^L_j = \\frac{e^{a^L_j}}{\\sum_k e^{a^L_k}},\n",
    "\\end{eqnarray}\n",
    "$$\n",
    ">\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "  \\sum_j y^L_j & = & \\frac{\\sum_j e^{a^L_j}}{\\sum_k e^{a^L_k}} = 1\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "><img src=\"https://www.tensorflow.org/versions/master/images/softmax-regression-scalargraph.png\" style=\"width:60%;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0768925",
   "metadata": {},
   "source": [
    "## 6.1. `keras` - `activation='softmax'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf53dce",
   "metadata": {},
   "source": [
    "### `activation='tanh'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb810bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(15, input_dim=784, activation=\"tanh\"))\n",
    "model2.add(Dense(10, activation=\"sigmoid\"))\n",
    "model2.compile(optimizer=SGD(lr=0.2), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae79615",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist2 = model2.fit(X_train, Y_train, epochs=30, batch_size=100, validation_data=(X_test, Y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f952fc",
   "metadata": {},
   "source": [
    "### `activation='relu'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512eba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train0, y_train0), (X_test0, y_test0) = mnist.load_data()\n",
    "\n",
    "X_train = X_train0.reshape(60000, 784).astype('float32') / 255.0\n",
    "X_test = X_test0.reshape(10000, 784).astype('float32') / 255.0\n",
    "Y_train = np_utils.to_categorical(y_train0, 10)\n",
    "Y_test = np_utils.to_categorical(y_test0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53702219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model1.add(Dense(10, activation=\"sigmoid\"))\n",
    "model1.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf23dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model2.add(Dense(15, activation=\"sigmoid\"))\n",
    "model2.add(Dense(10, activation=\"sigmoid\"))\n",
    "model2.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c9547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model3.add(Dense(15, activation=\"sigmoid\"))\n",
    "model3.add(Dense(15, activation=\"sigmoid\"))\n",
    "model3.add(Dense(10, activation=\"sigmoid\"))\n",
    "model3.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model4.add(Dense(15, activation=\"sigmoid\"))\n",
    "model4.add(Dense(15, activation=\"sigmoid\"))\n",
    "model4.add(Dense(15, activation=\"sigmoid\"))\n",
    "model4.add(Dense(10, activation=\"sigmoid\"))\n",
    "model4.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554880a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class WeightHistory(Callback):\n",
    "    \n",
    "    def __init__(self, model, num_epoch):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_hidden = len(model.layers) - 1\n",
    "        self.weight_old = [self.model.layers[i].get_weights()[0] for i in range(self.num_hidden)]\n",
    "        self.weight = [0.0 for i in range(self.num_hidden)]\n",
    "        self.weight_change = np.zeros((self.num_hidden, num_epoch))\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        for i in range(self.num_hidden):\n",
    "            self.weight[i] = self.model.layers[i].get_weights()[0]\n",
    "            self.weight_change[i, epoch] = np.abs((self.weight[i] - self.weight_old[i]).mean())\n",
    "            self.weight_old[i] = self.weight[i]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e977f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "callback1 = WeightHistory(model1, num_epoch)\n",
    "hist1 = model1.fit(X_train, Y_train, epochs=num_epoch, batch_size=100, callbacks=[callback1], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896f785",
   "metadata": {},
   "source": [
    "### Result - 1 Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e359ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(15, input_dim=784, activation=\"relu\"))\n",
    "model3.add(Dense(10, activation=\"sigmoid\"))\n",
    "model3.compile(optimizer=SGD(lr=0.2), loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist3 = model3.fit(X_train, Y_train, epochs=30, batch_size=100, validation_data=(X_test, Y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ae539",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist1.history['acc'], 'b:', label=\"logistic\")\n",
    "plt.plot(hist2.history['acc'], 'g--', label=\"tanh\")\n",
    "plt.plot(hist3.history['acc'], 'r-', label=\"relu\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bdfed8",
   "metadata": {},
   "source": [
    "# 2. Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e30169",
   "metadata": {},
   "source": [
    "## 2.1. Cross-Entropy\n",
    "* Cross-Entropy: offset $\\sigma'(a)$\n",
    "\n",
    ">$$\n",
    "\\begin{eqnarray} \n",
    "  C = y \\log z^{(L)} + (1-y) \\log (1-z^{(L)})\n",
    "\\end{eqnarray}\n",
    "$$\n",
    ">$$$$\n",
    ">$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial C}{\\partial w_j^{(L)}} \n",
    "&=& \\left(\\frac{y }{z^{(L)}} - \\frac{(1-y)}{1-z^{(L)}} \\right) \\frac{\\partial z^{(L)}}{\\partial w_j^{(L)}} \\\\\n",
    "&=& \\left(\\frac{y}{\\sigma(a)} - \\frac{(1-y)}{1-\\sigma(a)} \\right)\\sigma'(a) z^{(l-1)}_j \\\\\n",
    "&=& \\frac{\\sigma'(a) }{\\sigma(a) (1-\\sigma(a))} (\\sigma(a)-y) z^{(l-1)}_j  \\\\\n",
    "&=& (\\sigma(a)-y) z^{(l-1)}_j \\\\ \n",
    "&=& (z^{(L)}-y) z^{(l-1)}_j \\\\ \n",
    "\\\\\n",
    "\\frac{\\partial C}{\\partial b^{(L)}} &=&  z^{(L)} - y\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07735eae",
   "metadata": {},
   "source": [
    "## 2.2. `keras` - `loss`\n",
    "* ``mean_squared_error``\n",
    "* ``mean_squared_logarithmic_error``\n",
    "* ``mean_absolute_error``\n",
    "* ``mean_absolute_percentage_error``\n",
    "* ``binary_crossentropy``\n",
    "* ``categorical_crossentropy``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de839f",
   "metadata": {},
   "source": [
    "### `loss='mean_squared_error'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9112720",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model0 = Sequential()\n",
    "model0.add(Dense(15, input_dim=784, activation=\"sigmoid\"))\n",
    "model0.add(Dense(10, activation=\"sigmoid\"))\n",
    "model0.compile(optimizer=SGD(lr=0.2), loss='mean_squared_error', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba92ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hist0 = model0.fit(X_train, Y_train, epochs=30, batch_size=100, validation_data=(X_test, Y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9177e95",
   "metadata": {},
   "source": [
    "### `loss='categorical_crossentropy'`"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
