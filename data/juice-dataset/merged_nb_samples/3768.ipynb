{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df181c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_type(df):\n",
    "    \"\"\"house properties include yes/no flags such as \"hashottuborspa\" (whether the house has hot\n",
    "       tub or spa). Convert these fields to boolean. Note that the raw values are \"yes\" against \n",
    "       NaN. NaN's are converted to False. \n",
    "    \"\"\"\n",
    "    df['latitude'] = df['latitude'] / 1e6\n",
    "    df['longitude'] = df['longitude'] / 1e6\n",
    "    \n",
    "    df['tax_rate'] = df['taxamount'] / df['taxvaluedollarcnt']\n",
    "    \n",
    "    if len(missing_data_features_set) > 0:\n",
    "        df = df.drop(missing_data_features_set, axis=1)\n",
    "    \n",
    "    if len(flag_features) > 0:\n",
    "        for col in flag_features:\n",
    "            df[col] = df[col].fillna(value=False).apply(bool).copy()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labeled_train = df_labeled_train.drop(['logerror', 'transactiondate'], axis=1)\n",
    "X_labeled_val = df_labeled_val.drop(['logerror', 'transactiondate'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19dbcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labeled_train = convert_type(X_labeled_train)\n",
    "X_no_hash_labeled_train = X_labeled_train.copy()\n",
    "\n",
    "# this is for CatBoost, NaN is now a category in these features\n",
    "X_no_hash_labeled_train[list(categorical_features_names)] = X_no_hash_labeled_train[list(categorical_features_names)]\\\n",
    "                                                            .astype(str)\n",
    "X_no_hash_labeled_train.to_pickle(working_dir + 'X_no_hash_labeled_train.pkl')\n",
    "\n",
    "X_labeled_val = convert_type(X_labeled_val)\n",
    "X_no_hash_labeled_val = X_labeled_val.copy()\n",
    "X_no_hash_labeled_val[list(categorical_features_names)] = X_no_hash_labeled_val[list(categorical_features_names)]\\\n",
    "                                                          .astype(str)\n",
    "X_no_hash_labeled_val.to_pickle(working_dir + 'X_no_hash_labeled_val.pkl')\n",
    "\n",
    "# this for feature engineering, add y variables to dataframe\n",
    "pd.concat([X_no_hash_labeled_train, df_labeled_train[['logerror', 'transactiondate']]], axis=1)\\\n",
    ".to_pickle(working_dir + 'df_no_hash_labeled_clean_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b371d6",
   "metadata": {},
   "source": [
    "## Hash Categorical Features ##\n",
    "The data dictionary clarifies which features are categorical. Now process the training data with the hashing trick. The goal of this block of code is to output a sparse matrix of the transformed features and save it to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59deb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_features(df):\n",
    "    \"\"\"\n",
    "    Feature-hash all the categorical features to a sparse matrix.\n",
    "    The continuous features form a dense matrix; stack this dense matrix with the sparse matrix.\n",
    "    Finally it returns a scipy csr_matrix.\n",
    "    \"\"\"\n",
    "  \n",
    "    n_features = 2**20\n",
    "    D = df.filter(items=categorical_features_names).to_dict(orient='records')\n",
    "    hash_X = FeatureHasher(n_features=n_features).transform(D)\n",
    "    del n_features, D\n",
    "    gc.collect()\n",
    "    \n",
    "    X_all = scipy.sparse.hstack((df.filter(items=continuous_features).values.astype('float'), hash_X))\n",
    "    return X_all\n",
    "\n",
    "scipy.sparse.save_npz(working_dir+\"X_hashed_remove_miss_labeled_train.npz\", hash_features(X_labeled_train))\n",
    "scipy.sparse.save_npz(working_dir+\"X_hashed_remove_miss_labeled_val.npz\", hash_features(X_labeled_val))\n",
    "\n",
    "del X_labeled_train, X_labeled_val, X_no_hash_labeled_train, X_no_hash_labeled_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944b582",
   "metadata": {},
   "source": [
    "## Apply Process to Test Data ##\n",
    "\n",
    "For the submission we are asked to predict prices for 6 time points for all properties. I process these time points separately. The same process done to the training data is applied to the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_matrix(target_df, year, month, add_hash_features=False):\n",
    "    \n",
    "    dft = eval(\"df_properties_\" + year + \"[df_properties_\" + year + \".index.isin(target_df.index)].copy()\")\n",
    "    # above evaluates to this expression:\n",
    "    # dft = df_properties_2016[df_properties_2016.index.isin(target_df.index)].copy()\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    dft['transaction_year'] = int(year)\n",
    "    dft['transaction_month'] = int(month)\n",
    "\n",
    "    df_no_hash = convert_type(dft)\n",
    "    exec(\"df_no_hash.to_pickle(working_dir+'X_no_hash_test_\" + year + month + \".pkl')\")\n",
    "    # above evaluates to this expression:\n",
    "    # dft.to_pickle(working_dir+'X_no_hash_test_201610.pkl')\n",
    "        \n",
    "    if add_hash_features == True:\n",
    "        features_mat = hash_features(df_no_hash)\n",
    "        exec(\"scipy.sparse.save_npz(working_dir+'X_test_all_\" + year + month + \"_\" + str(subset_num) \\\n",
    "             + \".npz', features_mat)\")\n",
    "        # above evaluates to this expression:\n",
    "        # scipy.sparse.save_npz(working_dir+'X_test_all_201610_0.npz', features_mat)\n",
    "    \n",
    "    del dft\n",
    "\n",
    "    try:\n",
    "        del df_no_hash\n",
    "    except NameError: \n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        del features_mat\n",
    "    except NameError: \n",
    "        pass\n",
    "    \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e52fb0",
   "metadata": {},
   "source": [
    "### Load Test Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da96de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(working_dir+'sample_submission.csv', header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ec13c",
   "metadata": {},
   "source": [
    "### Apply Data Manipulation without Hashing to Test Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c54f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in ('2016', '2017'):\n",
    "    for month in ('10', '11', '12'):\n",
    "        make_test_matrix(sample_submission, year, month, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a088fca",
   "metadata": {},
   "source": [
    "### Apply Hashing to Test Data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf087b",
   "metadata": {},
   "source": [
    "## Load Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4f299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/home/lee/Documents/Datasets for GitHub/kaggle_zillow_home_value_prediction/\"\n",
    "\n",
    "df_properties_2016 = pd.read_csv(working_dir+'properties_2016.csv', header=0, index_col=0, low_memory=False)\n",
    "df_transaction_2016 = pd.read_csv(working_dir+'train_2016_v2.csv', header=0, index_col=0, low_memory=False)\n",
    "\n",
    "df_properties_2017 = pd.read_csv(working_dir+'properties_2017.csv', header=0, index_col=0, low_memory=False)\n",
    "df_transaction_2017 = pd.read_csv(working_dir+'train_2017.csv', header=0, index_col=0, low_memory=False)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71279a58",
   "metadata": {},
   "source": [
    "Output is very long; comment out after viewing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587ab4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_dataset(df):\n",
    "\n",
    "    print(\"dataframe shape: {}\".format(df.shape))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"preview data: \\n\")\n",
    "    for i in list(range(0, len(df.columns), 8)):\n",
    "        print(df.iloc[0:5, i:i+8])\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"summarize data: \\n\")\n",
    "    for i in list(range(0, len(df.columns), 8)):\n",
    "        print(df.iloc[:, i:i+8].describe())\n",
    "\n",
    "# look_at_dataset(df_properties_2016)\n",
    "# look_at_dataset(df_transaction_2016)\n",
    "# look_at_dataset(df_properties_2017)\n",
    "# look_at_dataset(df_transaction_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699a720",
   "metadata": {},
   "source": [
    "## Merge Dataframes ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_create_date(year):\n",
    "    \"\"\"\n",
    "    join transaction and properties data\n",
    "    convert transaction date field to datetime\n",
    "    extract transaction year from transaction date\n",
    "    extract transaction month from transaction date\n",
    "    \"\"\"\n",
    "    df = eval(\"df_transaction_\" + year + \".join(df_properties_\" + year + \", how='left')\")\n",
    "    # above evaluates to this expression:\n",
    "    # df = df_transaction_2017.join(df_properties_2017, how='left')\n",
    "    \n",
    "    df['transactiondate'] = df['transactiondate'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d'))\n",
    "    df['transaction_year'] = df['transactiondate'].dt.year\n",
    "    df['transaction_month'] = df['transactiondate'].dt.month\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train_2016 = join_create_date('2016')\n",
    "df_train_2017 = join_create_date('2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eabaea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_transaction_2016, df_transaction_2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train_2016, df_train_2017])\n",
    "# df_train.to_pickle(working_dir+'df_train_raw_all.pkl')\n",
    "del df_train_2016, df_train_2017\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49235e4",
   "metadata": {},
   "source": [
    "## Save Raw Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_train, df_labeled_val = train_test_split(df_train, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430fc4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled_train.to_pickle(working_dir + 'df_labeled_raw_train.pkl')\n",
    "df_labeled_val.to_pickle(working_dir + 'df_labeled_raw_val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ac89d",
   "metadata": {},
   "source": [
    "## Set Target Aside ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labeled_train = df_labeled_train['logerror']\n",
    "y_labeled_train.to_pickle(working_dir + 'y_labeled_train.pkl')\n",
    "del y_labeled_train; gc.collect()\n",
    "\n",
    "y_labeled_val = df_labeled_val['logerror']\n",
    "y_labeled_val.to_pickle(working_dir + 'y_labeled_val.pkl')\n",
    "del y_labeled_val; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652c663",
   "metadata": {},
   "source": [
    "## Drop Columns with High % of Missing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116defec",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0.8\n",
    "# column names of all columns with too many missing values\n",
    "missing_data_features_set = set(tuple(df_labeled_train.columns[df_labeled_train.isnull().mean() > missing_threshold]))\n",
    "del missing_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dceae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all column names, including target ##\n",
    "columns_all = tuple(df_labeled_train.columns)\n",
    "\n",
    "# binary column names, convert to boolean in next step\n",
    "flag_features_set = set(('fireplaceflag', 'hashottuborspa', 'pooltypeid10', 'pooltypeid2', \\\n",
    "                         'pooltypeid7', 'taxdelinquencyflag')) - missing_data_features_set\n",
    "flag_features = tuple(col for col in columns_all \\\n",
    "                            if (col in flag_features_set) == True)\n",
    "\n",
    "# categorical \n",
    "categorical_features_set = set(('airconditioningtypeid', 'architecturalstyletypeid', \\\n",
    "                                'buildingclasstypeid', 'decktypeid', 'fips', 'heatingorsystemtypeid', \\\n",
    "                                'propertycountylandusecode', 'propertylandusetypeid', \\\n",
    "                                'propertyzoningdesc', 'rawcensustractandblock', 'censustractandblock', \\\n",
    "                                'regionidcounty', 'regionidcity', 'regionidzip', \\\n",
    "                                'regionidneighborhood', 'typeconstructiontypeid', 'assessmentyear', \\\n",
    "                                'taxdelinquencyyear', 'transaction_year', 'transaction_month'))\\\n",
    "                           - missing_data_features_set \n",
    "categorical_features_index = list(icol for icol, col in enumerate(columns_all) \\\n",
    "                                  if (col in categorical_features_set) == True)\n",
    "categorical_features_names = tuple(col for col in columns_all \\\n",
    "                                   if (col in categorical_features_set) == True)\n",
    "\n",
    "# numercial\n",
    "continuous_features_set = set(columns_all) - categorical_features_set\\\n",
    "                          - set(['logerror', 'transactiondate']) - missing_data_features_set\n",
    "continuous_features = tuple(col for col in columns_all \\\n",
    "                            if (col in continuous_features_set) == True)\n",
    "\n",
    "del flag_features_set, continuous_features_set, categorical_features_set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1040f009",
   "metadata": {},
   "source": [
    "# Zillow's Home Value Prediction (Zestimate) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b280fc",
   "metadata": {},
   "source": [
    "## Load Packages ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import gc\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "               \n",
    "np.random.seed(0)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
