{
    "Introduction": [
        [
            "Contents",
            "markdown"
        ],
        [
            "Introduction",
            "markdown"
        ],
        [
            "SciPy Organization",
            "markdown"
        ],
        [
            "Finding Documentation",
            "markdown"
        ],
        [
            "SciPy is a collection of mathematical algorithms and convenience\nfunctions built on the NumPy extension of Python. It adds\nsignificant power to the interactive Python session by providing the\nuser with high-level commands and classes for manipulating and\nvisualizing data. With SciPy, an interactive Python session\nbecomes a data-processing and system-prototyping environment rivaling\nsystems, such as MATLAB, IDL, Octave, R-Lab, and SciLab.",
            "markdown"
        ],
        [
            "The additional benefit of basing SciPy on Python is that this also makes a\npowerful programming language available for use in developing\nsophisticated programs and specialized applications. Scientific\napplications using SciPy benefit from the development of\nadditional modules in numerous niches of the software landscape by\ndevelopers across the world. Everything from parallel programming to\nweb and data-base subroutines and classes have been made available to\nthe Python programmer. All of this power is available in addition to\nthe mathematical libraries in SciPy.",
            "markdown"
        ],
        [
            "This tutorial will acquaint the first-time user of SciPy with some of its most\nimportant features. It assumes that the user has already installed the SciPy\npackage. Some general Python facility is also assumed, such as could be\nacquired by working through the Python distribution\u00e2\u0080\u0099s Tutorial. For further\nintroductory help the user is directed to the NumPy documentation.",
            "markdown"
        ]
    ],
    "Introduction->SciPy Organization": [
        [
            "SciPy is organized into subpackages covering different scientific\ncomputing domains. These are summarized in the following table:",
            "markdown"
        ],
        [
            "SciPy sub-packages need to be imported separately, for example:",
            "markdown"
        ],
        [
            "from scipy import linalg, optimize",
            "code"
        ],
        [
            "Because of their ubiquitousness, some of the functions in these\nsubpackages are also made available in the scipy namespace to ease\ntheir use in interactive sessions and programs. In addition, many\nbasic array functions from numpy are also available at the\ntop-level of the scipy package. Before looking at the\nsub-packages individually, we will first look at some of these common\nfunctions.",
            "markdown"
        ]
    ],
    "Introduction->Finding Documentation": [
        [
            "SciPy and NumPy have documentation versions in both HTML and PDF format\navailable at https://docs.scipy.org/, that cover nearly\nall available functionality. However, this documentation is still\nwork-in-progress and some parts may be incomplete or sparse. As\nwe are a volunteer organization and depend on the community for\ngrowth, your participation - everything from providing feedback to\nimproving the documentation and code - is welcome and actively\nencouraged.",
            "markdown"
        ],
        [
            "Python\u00e2\u0080\u0099s documentation strings are used in SciPy for on-line\ndocumentation. There are two methods for reading them and\ngetting help. One is Python\u00e2\u0080\u0099s command help in the pydoc\nmodule. Entering this command with no arguments (i.e. >>> help )\nlaunches an interactive help session that allows searching through the\nkeywords and modules available to all of Python. Secondly, running the command\n<em class=\"xref py py-obj\">help(obj) with an object as the argument displays that object\u00e2\u0080\u0099s calling\nsignature, and documentation string.",
            "markdown"
        ],
        [
            "The pydoc method of help is sophisticated but uses a pager to display\nthe text. Sometimes this can interfere with the terminal within which you are\nrunning the interactive session. A numpy/scipy-specific help system\nis also available under the command numpy.info. The signature and\ndocumentation string for the object passed to the help command are\nprinted to standard output (or to a writeable object passed as the\nthird argument). The second keyword argument of numpy.info defines\nthe maximum width of the line for printing. If a module is passed as\nthe argument to help then a list of the functions and classes defined\nin that module is printed. For example:",
            "markdown"
        ],
        [
            "np.info(optimize.fmin)\n fmin(func, x0, args=(), xtol=0.0001, ftol=0.0001, maxiter=None, maxfun=None,\n      full_output=0, disp=1, retall=0, callback=None)\n\nMinimize a function using the downhill simplex algorithm.\n\nParameters\n----------\nfunc : callable func(x,*args)\n    The objective function to be minimized.\nx0 : ndarray\n    Initial guess.\nargs : tuple\n    Extra arguments passed to func, i.e. ``f(x,*args)``.\ncallback : callable\n    Called after each iteration, as callback(xk), where xk is the\n    current parameter vector.\n\nReturns\n-------\nxopt : ndarray\n    Parameter that minimizes function.\nfopt : float\n    Value of function at minimum: ``fopt = func(xopt)``.\niter : int\n    Number of iterations performed.\nfuncalls : int\n    Number of function calls made.\nwarnflag : int\n    1 : Maximum number of function evaluations made.\n    2 : Maximum number of iterations reached.\nallvecs : list\n    Solution at each iteration.\n\nOther parameters\n----------------\nxtol : float\n    Relative error in xopt acceptable for convergence.\nftol : number\n    Relative error in func(xopt) acceptable for convergence.\nmaxiter : int\n    Maximum number of iterations to perform.\nmaxfun : number\n    Maximum number of function evaluations to make.\nfull_output : bool\n    Set to True if fopt and warnflag outputs are desired.\ndisp : bool\n    Set to True to print convergence messages.\nretall : bool\n    Set to True to return list of solutions at each iteration.\n\nNotes\n-----\nUses a Nelder-Mead simplex algorithm to find the minimum of function of\none or more variables.",
            "code"
        ],
        [
            "Another useful command is dir,\nwhich can be used to look at the namespace of a module or package.",
            "markdown"
        ]
    ],
    "Special functions (scipy.special)": [
        [
            "The main feature of the scipy.special package is the definition of\nnumerous special functions of mathematical physics. Available\nfunctions include airy, elliptic, bessel, gamma, beta, hypergeometric,\nparabolic cylinder, mathieu, spheroidal wave, struve, and\nkelvin. There are also some low-level stats functions that are not\nintended for general use as an easier interface to these functions is\nprovided by the stats module. Most of these functions can take\narray arguments and return array results following the same\nbroadcasting rules as other math functions in Numerical Python. Many\nof these functions also accept complex numbers as input. For a\ncomplete list of the available functions with a one-line description\ntype >>> help(special). Each function also has its own\ndocumentation accessible using help.  If you don\u00e2\u0080\u0099t see a function you\nneed, consider writing it and contributing it to the library. You can\nwrite the function in either C, Fortran, or Python. Look in the source\ncode of the library for examples of each of these kinds of functions.",
            "markdown"
        ]
    ],
    "Special functions (scipy.special)->Bessel functions of real order(jv, jn_zeros)": [
        [
            "Bessel functions are a family of solutions to Bessel\u00e2\u0080\u0099s differential equation\nwith real or complex order alpha:\n\n\\[x^2 \\frac{d^2 y}{dx^2} + x \\frac{dy}{dx} + (x^2 - \\alpha^2)y = 0\\]",
            "markdown"
        ],
        [
            "Among other uses, these functions arise in wave propagation problems, such as\nthe vibrational modes of a thin drum head.  Here is an example of a circular\ndrum head anchored at the edge:",
            "markdown"
        ],
        [
            "from scipy import special\n import numpy as np\n def drumhead_height(n, k, distance, angle, t):\n...    kth_zero = special.jn_zeros(n, k)[-1]\n...    return np.cos(t) * np.cos(n*angle) * special.jn(n, distance*kth_zero)\n theta = np.r_[0:2*np.pi:50j]\n radius = np.r_[0:1:50j]\n x = np.array([r * np.cos(theta) for r in radius])\n y = np.array([r * np.sin(theta) for r in radius])\n z = np.array([drumhead_height(1, 1, r, theta, 0.5) for r in radius])",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n fig = plt.figure()\n ax = fig.add_axes(rect=(0, 0.05, 0.95, 0.95), projection='3d')\n ax.plot_surface(x, y, z, rstride=1, cstride=1, cmap='RdBu_r', vmin=-0.5, vmax=0.5)\n ax.set_xlabel('X')\n ax.set_ylabel('Y')\n ax.set_xticks(np.arange(-1, 1.1, 0.5))\n ax.set_yticks(np.arange(-1, 1.1, 0.5))\n ax.set_zlabel('Z')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates a 3-D representation of the vibrational modes on a drum head viewed at a three-quarter angle. A circular region on the X-Y plane is defined with a Z value of 0 around the edge. Within the circle a single smooth valley exists on the -X side and a smooth peak exists on the +X side. The image resembles a yin-yang at this angle.\"' class=\"plot-directive\" src=\"../_images/special-1.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Special functions (scipy.special)->Cython Bindings for Special Functions (scipy.special.cython_special)": [
        [
            "SciPy also offers Cython bindings for scalar, typed versions of many\nof the functions in special. The following Cython code gives a simple\nexample of how to use these functions:",
            "markdown"
        ],
        [
            "cimport scipy.special.cython_special as csc\n\ncdef:\n    double x = 1\n    double complex z = 1 + 1j\n    double si, ci, rgam\n    double complex cgam\n\nrgam = csc.gamma(x)\nprint(rgam)\ncgam = csc.gamma(z)\nprint(cgam)\ncsc.sici(x, &si, &ci)\nprint(si, ci)",
            "code"
        ],
        [
            "(See the Cython documentation for help with compiling Cython.) In\nthe example the function csc.gamma works essentially like its\nufunc counterpart gamma, though it takes C types as arguments\ninstead of NumPy arrays. Note, in particular, that the function is\noverloaded to support real and complex arguments; the correct variant\nis selected at compile time. The function csc.sici works slightly\ndifferently from sici; for the ufunc we could write ai, bi =\nsici(x), whereas in the Cython version multiple return values are\npassed as pointers. It might help to think of this as analogous to\ncalling a ufunc with an output array: sici(x, out=(si, ci)).",
            "markdown"
        ],
        [
            "There are two potential advantages to using the Cython bindings:",
            "markdown"
        ],
        [
            "they avoid Python function overhead",
            "markdown"
        ],
        [
            "they do not require the Python Global Interpreter Lock (GIL)",
            "markdown"
        ],
        [
            "The following sections discuss how to use these advantages to\npotentially speed up your code, though, of course, one should always\nprofile the code first to make sure putting in the extra effort will\nbe worth it.",
            "markdown"
        ]
    ],
    "Special functions (scipy.special)->Cython Bindings for Special Functions (scipy.special.cython_special)->Avoiding Python Function Overhead": [
        [
            "For the ufuncs in special, Python function overhead is avoided by\nvectorizing, that is, by passing an array to the function. Typically,\nthis approach works quite well, but sometimes it is more convenient to\ncall a special function on scalar inputs inside a loop, for example,\nwhen implementing your own ufunc. In this case, the Python function\noverhead can become significant. Consider the following example:",
            "markdown"
        ],
        [
            "import scipy.special as sc\ncimport scipy.special.cython_special as csc\n\ndef python_tight_loop():\n    cdef:\n        int n\n        double x = 1\n\n    for n in range(100):\n        sc.jv(n, x)\n\ndef cython_tight_loop():\n    cdef:\n        int n\n        double x = 1\n\n    for n in range(100):\n        csc.jv(n, x)",
            "code"
        ],
        [
            "On one computer python_tight_loop took about 131 microseconds to\nrun and cython_tight_loop took about 18.2 microseconds to\nrun. Obviously this example is contrived: one could just call\nspecial.jv(np.arange(100), 1) and get results just as fast as in\ncython_tight_loop. The point is that if Python function overhead\nbecomes significant in your code, then the Cython bindings might be\nuseful.",
            "markdown"
        ]
    ],
    "Special functions (scipy.special)->Cython Bindings for Special Functions (scipy.special.cython_special)->Releasing the GIL": [
        [
            "One often needs to evaluate a special function at many points, and\ntypically the evaluations are trivially parallelizable. Since the\nCython bindings do not require the GIL, it is easy to run them in\nparallel using Cython\u00e2\u0080\u0099s prange function. For example, suppose that\nwe wanted to compute the fundamental solution to the Helmholtz\nequation:\n\n\\[\\Delta_x G(x, y) + k^2G(x, y) = \\delta(x - y),\\]",
            "markdown"
        ],
        [
            "where \\(k\\) is the wavenumber and \\(\\delta\\) is the Dirac\ndelta function. It is known that in two dimensions the unique\n(radiating) solution is\n\n\\[G(x, y) = \\frac{i}{4}H_0^{(1)}(k|x - y|),\\]",
            "markdown"
        ],
        [
            "where \\(H_0^{(1)}\\) is the Hankel function of the first kind,\ni.e., the function hankel1. The following example shows how we could\ncompute this function in parallel:",
            "markdown"
        ],
        [
            "from libc.math cimport fabs\ncimport cython\nfrom cython.parallel cimport prange\n\nimport numpy as np\nimport scipy.special as sc\ncimport scipy.special.cython_special as csc\n\ndef serial_G(k, x, y):\n    return 0.25j*sc.hankel1(0, k*np.abs(x - y))\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ncdef void _parallel_G(double k, double[:,:] x, double[:,:] y,\n                      double complex[:,:] out) nogil:\n    cdef int i, j\n\n    for i in prange(x.shape[0]):\n        for j in range(y.shape[0]):\n            out[i,j] = 0.25j*csc.hankel1(0, k*fabs(x[i,j] - y[i,j]))\n\ndef parallel_G(k, x, y):\n    out = np.empty_like(x, dtype='complex128')\n    _parallel_G(k, x, y, out)\n    return out",
            "code"
        ],
        [
            "(For help with compiling parallel code in Cython see here.) If the\nabove Cython code is in a file test.pyx, then we can write an\ninformal benchmark which compares the parallel and serial versions of\nthe function:",
            "markdown"
        ],
        [
            "import timeit\n\nimport numpy as np\n\nfrom test import serial_G, parallel_G\n\ndef main():\n    k = 1\n    x, y = np.linspace(-100, 100, 1000), np.linspace(-100, 100, 1000)\n    x, y = np.meshgrid(x, y)\n\n    def serial():\n        serial_G(k, x, y)\n\n    def parallel():\n        parallel_G(k, x, y)\n\n    time_serial = timeit.timeit(serial, number=3)\n    time_parallel = timeit.timeit(parallel, number=3)\n    print(\"Serial method took {:.3} seconds\".format(time_serial))\n    print(\"Parallel method took {:.3} seconds\".format(time_parallel))\n\nif __name__ == \"__main__\":\n    main()",
            "code"
        ],
        [
            "On one quad-core computer the serial method took 1.29 seconds and the\nparallel method took 0.29 seconds.",
            "markdown"
        ]
    ],
    "Special functions (scipy.special)->Functions not in scipy.special": [
        [
            "Some functions are not included in special because they are\nstraightforward to implement with existing functions in NumPy and\nSciPy. To prevent reinventing the wheel, this section provides\nimplementations of several such functions, which hopefully illustrate\nhow to handle similar functions. In all examples NumPy is imported as\nnp and special is imported as sc.",
            "markdown"
        ],
        [
            "The binary entropy function:",
            "markdown"
        ],
        [
            "def binary_entropy(x):\n    return -(sc.xlogy(x, x) + sc.xlog1py(1 - x, -x))/np.log(2)",
            "code"
        ],
        [
            "A rectangular step function on [0, 1]:",
            "markdown"
        ],
        [
            "def step(x):\n    return 0.5*(np.sign(x) + np.sign(1 - x))",
            "code"
        ],
        [
            "Translating and scaling can be used to get an arbitrary step function.",
            "markdown"
        ],
        [
            "The ramp function:",
            "markdown"
        ],
        [
            "def ramp(x):\n    return np.maximum(0, x)",
            "code"
        ]
    ],
    "Integration (scipy.integrate)": [
        [
            "The scipy.integrate sub-package provides several integration\ntechniques including an ordinary differential equation integrator. An\noverview of the module is provided by the help command:",
            "markdown"
        ],
        [
            "help(integrate)\n Methods for Integrating Functions given function object.\n\n   quad          -- General purpose integration.\n   dblquad       -- General purpose double integration.\n   tplquad       -- General purpose triple integration.\n   fixed_quad    -- Integrate func(x) using Gaussian quadrature of order n.\n   quadrature    -- Integrate with given tolerance using Gaussian quadrature.\n   romberg       -- Integrate func using Romberg integration.\n\n Methods for Integrating Functions given fixed samples.\n\n   trapezoid            -- Use trapezoidal rule to compute integral.\n   cumulative_trapezoid -- Use trapezoidal rule to cumulatively compute integral.\n   simpson              -- Use Simpson's rule to compute integral from samples.\n   romb                 -- Use Romberg Integration to compute integral from\n                        -- (2**k + 1) evenly-spaced samples.\n\n   See the special module's orthogonal polynomials (special) for Gaussian\n      quadrature roots and weights for other weighting factors and regions.\n\n Interface to numerical integrators of ODE systems.\n\n   odeint        -- General integration of ordinary differential equations.\n   ode           -- Integrate ODE using VODE and ZVODE routines.",
            "code"
        ]
    ],
    "Integration (scipy.integrate)->General integration (quad)": [
        [
            "The function quad is provided to integrate a function of one\nvariable between two points. The points can be \\(\\pm\\infty\\)\n(\\(\\pm\\) inf) to indicate infinite limits. For example,\nsuppose you wish to integrate a bessel function jv(2.5, x) along\nthe interval \\([0, 4.5].\\)\n\n\\[I=\\int_{0}^{4.5}J_{2.5}\\left(x\\right)\\, dx.\\]",
            "markdown"
        ],
        [
            "This could be computed using quad:",
            "markdown"
        ],
        [
            "import scipy.integrate as integrate\n import scipy.special as special\n result = integrate.quad(lambda x: special.jv(2.5,x), 0, 4.5)\n result\n(1.1178179380783249, 7.8663172481899801e-09)",
            "code"
        ],
        [
            "from numpy import sqrt, sin, cos, pi\n I = sqrt(2/pi)*(18.0/27*sqrt(2)*cos(4.5) - 4.0/27*sqrt(2)*sin(4.5) +\n...                 sqrt(2*pi) * special.fresnel(3/sqrt(pi))[0])\n I\n1.117817938088701",
            "code"
        ],
        [
            "print(abs(result[0]-I))\n1.03761443881e-11",
            "code"
        ],
        [
            "The first argument to quad is a \u00e2\u0080\u009ccallable\u00e2\u0080\u009d Python object (i.e., a\nfunction, method, or class instance). Notice the use of a lambda-\nfunction in this case as the argument. The next two arguments are the\nlimits of integration. The return value is a tuple, with the first\nelement holding the estimated value of the integral and the second\nelement holding an upper bound on the error. Notice, that in this\ncase, the true value of this integral is\n\n\\[I=\\sqrt{\\frac{2}{\\pi}}\\left(\\frac{18}{27}\\sqrt{2}\\cos\\left(4.5\\right)-\\frac{4}{27}\\sqrt{2}\\sin\\left(4.5\\right)+\\sqrt{2\\pi}\\textrm{Si}\\left(\\frac{3}{\\sqrt{\\pi}}\\right)\\right),\\]",
            "markdown"
        ],
        [
            "where\n\n\\[\\textrm{Si}\\left(x\\right)=\\int_{0}^{x}\\sin\\left(\\frac{\\pi}{2}t^{2}\\right)\\, dt.\\]",
            "markdown"
        ],
        [
            "is the Fresnel sine integral. Note that the numerically-computed integral is\nwithin \\(1.04\\times10^{-11}\\) of the exact result \u00e2\u0080\u0094 well below the\nreported error bound.",
            "markdown"
        ],
        [
            "If the function to integrate takes additional parameters, they can be provided\nin the <em class=\"xref py py-obj\">args argument. Suppose that the following integral shall be calculated:\n\n\\[I(a,b)=\\int_{0}^{1} ax^2+b \\, dx.\\]",
            "markdown"
        ],
        [
            "This integral can be evaluated by using the following code:",
            "markdown"
        ],
        [
            "from scipy.integrate import quad\n def integrand(x, a, b):\n...     return a*x**2 + b\n...\n a = 2\n b = 1\n I = quad(integrand, 0, 1, args=(a,b))\n I\n(1.6666666666666667, 1.8503717077085944e-14)",
            "code"
        ],
        [
            "Infinite inputs are also allowed in quad by using \\(\\pm\\)\ninf as one of the arguments. For example, suppose that a numerical\nvalue for the exponential integral:\n\n\\[E_{n}\\left(x\\right)=\\int_{1}^{\\infty}\\frac{e^{-xt}}{t^{n}}\\, dt.\\]",
            "markdown"
        ],
        [
            "is desired (and the fact that this integral can be computed as\nspecial.expn(n,x) is forgotten). The functionality of the function\nspecial.expn can be replicated by defining a new function\nvec_expint based on the routine quad:",
            "markdown"
        ],
        [
            "from scipy.integrate import quad\n import numpy as np\n def integrand(t, n, x):\n...     return np.exp(-x*t) / t**n\n...",
            "code"
        ],
        [
            "def expint(n, x):\n...     return quad(integrand, 1, np.inf, args=(n, x))[0]\n...",
            "code"
        ],
        [
            "vec_expint = np.vectorize(expint)",
            "code"
        ],
        [
            "vec_expint(3, np.arange(1.0, 4.0, 0.5))\narray([ 0.1097,  0.0567,  0.0301,  0.0163,  0.0089,  0.0049])\n import scipy.special as special\n special.expn(3, np.arange(1.0,4.0,0.5))\narray([ 0.1097,  0.0567,  0.0301,  0.0163,  0.0089,  0.0049])",
            "code"
        ],
        [
            "The function which is integrated can even use the quad argument (though the\nerror bound may underestimate the error due to possible numerical error in the\nintegrand from the use of quad ). The integral in this case is\n\n\\[I_{n}=\\int_{0}^{\\infty}\\int_{1}^{\\infty}\\frac{e^{-xt}}{t^{n}}\\, dt\\, dx=\\frac{1}{n}.\\]",
            "markdown"
        ],
        [
            "result = quad(lambda x: expint(3, x), 0, np.inf)\n print(result)\n(0.33333333324560266, 2.8548934485373678e-09)",
            "code"
        ],
        [
            "I3 = 1.0/3.0\n print(I3)\n0.333333333333",
            "code"
        ],
        [
            "print(I3 - result[0])\n8.77306560731e-11",
            "code"
        ],
        [
            "This last example shows that multiple integration can be handled using\nrepeated calls to quad.",
            "markdown"
        ]
    ],
    "Integration (scipy.integrate)->General multiple integration (dblquad, tplquad, nquad)": [
        [
            "The mechanics for double and triple integration have been wrapped up into the\nfunctions dblquad and tplquad. These functions take the function\nto  integrate and four, or six arguments, respectively. The limits of all\ninner integrals need to be defined as functions.",
            "markdown"
        ],
        [
            "An example of using double integration to compute several values of\n\\(I_{n}\\) is shown below:",
            "markdown"
        ],
        [
            "from scipy.integrate import quad, dblquad\n def I(n):\n...     return dblquad(lambda t, x: np.exp(-x*t)/t**n, 0, np.inf, lambda x: 1, lambda x: np.inf)\n...",
            "code"
        ],
        [
            "print(I(4))\n(0.2500000000043577, 1.29830334693681e-08)\n print(I(3))\n(0.33333333325010883, 1.3888461883425516e-08)\n print(I(2))\n(0.4999999999985751, 1.3894083651858995e-08)",
            "code"
        ],
        [
            "As example for non-constant limits consider the integral\n\n\\[I=\\int_{y=0}^{1/2}\\int_{x=0}^{1-2y} x y \\, dx\\, dy=\\frac{1}{96}.\\]",
            "markdown"
        ],
        [
            "This integral can be evaluated using the expression below (Note the use of the\nnon-constant lambda functions for the upper limit of the inner integral):",
            "markdown"
        ],
        [
            "from scipy.integrate import dblquad\n area = dblquad(lambda x, y: x*y, 0, 0.5, lambda x: 0, lambda x: 1-2*x)\n area\n(0.010416666666666668, 1.1564823173178715e-16)",
            "code"
        ],
        [
            "For n-fold integration, scipy provides the function nquad. The\nintegration bounds are an iterable object: either a list of constant bounds,\nor a list of functions for the non-constant integration bounds. The order of\nintegration (and therefore the bounds) is from the innermost integral to the\noutermost one.",
            "markdown"
        ],
        [
            "The integral from above\n\n\\[I_{n}=\\int_{0}^{\\infty}\\int_{1}^{\\infty}\\frac{e^{-xt}}{t^{n}}\\, dt\\, dx=\\frac{1}{n}\\]",
            "markdown"
        ],
        [
            "can be calculated as",
            "markdown"
        ],
        [
            "from scipy import integrate\n N = 5\n def f(t, x):\n...    return np.exp(-x*t) / t**N\n...\n integrate.nquad(f, [[1, np.inf],[0, np.inf]])\n(0.20000000000002294, 1.2239614263187945e-08)",
            "code"
        ],
        [
            "Note that the order of arguments for <em class=\"xref py py-obj\">f must match the order of the\nintegration bounds; i.e., the inner integral with respect to \\(t\\) is on\nthe interval \\([1, \\infty]\\) and the outer integral with respect to\n\\(x\\) is on the interval \\([0, \\infty]\\).",
            "markdown"
        ],
        [
            "Non-constant integration bounds can be treated in a similar manner; the\nexample from above\n\n\\[I=\\int_{y=0}^{1/2}\\int_{x=0}^{1-2y} x y \\, dx\\, dy=\\frac{1}{96}.\\]",
            "markdown"
        ],
        [
            "can be evaluated by means of",
            "markdown"
        ],
        [
            "from scipy import integrate\n def f(x, y):\n...     return x*y\n...\n def bounds_y():\n...     return [0, 0.5]\n...\n def bounds_x(y):\n...     return [0, 1-2*y]\n...\n integrate.nquad(f, [bounds_x, bounds_y])\n(0.010416666666666668, 4.101620128472366e-16)",
            "code"
        ],
        [
            "which is the same result as before.",
            "markdown"
        ]
    ],
    "Integration (scipy.integrate)->Gaussian quadrature": [
        [
            "A few functions are also provided in order to perform simple Gaussian\nquadrature over a fixed interval. The first is fixed_quad, which\nperforms fixed-order Gaussian quadrature. The second function is\nquadrature, which performs Gaussian quadrature of multiple\norders until the difference in the integral estimate is beneath some\ntolerance supplied by the user. These functions both use the module\nscipy.special.orthogonal, which can calculate the roots and quadrature\nweights of a large variety of orthogonal polynomials (the polynomials\nthemselves are available as special functions returning instances of\nthe polynomial class \u00e2\u0080\u0094 e.g., special.legendre).",
            "markdown"
        ]
    ],
    "Integration (scipy.integrate)->Romberg Integration": [
        [
            "Romberg\u00e2\u0080\u0099s method [WPR] is another method for numerically evaluating an\nintegral. See the help function for romberg for further details.",
            "markdown"
        ]
    ],
    "Integration (scipy.integrate)->Integrating using Samples": [
        [
            "If the samples are equally-spaced and the number of samples available\nis \\(2^{k}+1\\) for some integer \\(k\\), then Romberg romb\nintegration can be used to obtain high-precision estimates of the\nintegral using the available samples. Romberg integration uses the\ntrapezoid rule at step-sizes related by a power of two and then\nperforms Richardson extrapolation on these estimates to approximate\nthe integral with a higher degree of accuracy.",
            "markdown"
        ],
        [
            "In case of arbitrary spaced samples, the two functions trapezoid\nand simpson are available. They are using Newton-Coates formulas\nof order 1 and 2 respectively to perform integration. The trapezoidal rule\napproximates the function as a straight line between adjacent points, while\nSimpson\u00e2\u0080\u0099s rule approximates the function between three adjacent points as a\nparabola.",
            "markdown"
        ],
        [
            "For an odd number of samples that are equally spaced Simpson\u00e2\u0080\u0099s rule is exact\nif the function is a polynomial of order 3 or less. If the samples are not\nequally spaced, then the result is exact only if the function is a polynomial\nof order 2 or less.",
            "markdown"
        ],
        [
            "import numpy as np\n def f1(x):\n...    return x**2\n...\n def f2(x):\n...    return x**3\n...\n x = np.array([1,3,4])\n y1 = f1(x)\n from scipy import integrate\n I1 = integrate.simpson(y1, x)\n print(I1)\n21.0",
            "code"
        ],
        [
            "This corresponds exactly to\n\n\\[\\int_{1}^{4} x^2 \\, dx = 21,\\]",
            "markdown"
        ],
        [
            "whereas integrating the second function",
            "markdown"
        ],
        [
            "y2 = f2(x)\n I2 = integrate.simpson(y2, x)\n print(I2)\n61.5",
            "code"
        ],
        [
            "does not correspond to\n\n\\[\\int_{1}^{4} x^3 \\, dx = 63.75\\]",
            "markdown"
        ],
        [
            "because the order of the polynomial in f2 is larger than two.",
            "markdown"
        ]
    ],
    "Integration (scipy.integrate)->Faster integration using low-level callback functions": [
        [
            "A user desiring reduced integration times may pass a C function\npointer through scipy.LowLevelCallable to quad, dblquad,\ntplquad or nquad and it will be integrated and return a result in\nPython.  The performance increase here arises from two factors.  The\nprimary improvement is faster function evaluation, which is provided\nby compilation of the function itself.  Additionally we have a speedup\nprovided by the removal of function calls between C and Python in\nquad.  This method may provide a speed improvements of ~2x for\ntrivial functions such as sine but can produce a much more noticeable\nimprovements (10x+) for more complex functions.  This feature then, is\ngeared towards a user with numerically intensive integrations willing\nto write a little C to reduce computation time significantly.",
            "markdown"
        ],
        [
            "The approach can be used, for example, via ctypes in a few simple steps:",
            "markdown"
        ],
        [
            "1.) Write an integrand function in C with the function signature\ndouble f(int n, double *x, void *user_data), where x is an\narray containing the point the function f is evaluated at, and user_data\nto arbitrary additional data you want to provide.",
            "markdown"
        ],
        [
            "/* testlib.c */\ndouble f(int n, double *x, void *user_data) {\n    double c = *(double *)user_data;\n    return c + x[0] - x[1] * x[2]; /* corresponds to c + x - y * z */\n}",
            "code"
        ],
        [
            "2.) Now compile this file to a shared/dynamic library (a quick search will help\nwith this as it is OS-dependent). The user must link any math libraries,\netc., used.  On linux this looks like:",
            "markdown"
        ],
        [
            "$ gcc -shared -fPIC -o testlib.so testlib.c",
            "code"
        ],
        [
            "The output library will be referred to as testlib.so, but it may have a\ndifferent file extension. A library has now been created that can be loaded\ninto Python with ctypes.",
            "markdown"
        ],
        [
            "3.) Load shared library into Python using ctypes and set restypes and\nargtypes - this allows SciPy to interpret the function correctly:",
            "markdown"
        ],
        [
            "import os, ctypes\nfrom scipy import integrate, LowLevelCallable\n\nlib = ctypes.CDLL(os.path.abspath('testlib.so'))\nlib.f.restype = ctypes.c_double\nlib.f.argtypes = (ctypes.c_int, ctypes.POINTER(ctypes.c_double), ctypes.c_void_p)\n\nc = ctypes.c_double(1.0)\nuser_data = ctypes.cast(ctypes.pointer(c), ctypes.c_void_p)\n\nfunc = LowLevelCallable(lib.f, user_data)",
            "code"
        ],
        [
            "The last void *user_data in the function is optional and can be omitted\n(both in the C function and ctypes argtypes) if not needed. Note that the\ncoordinates are passed in as an array of doubles rather than a separate argument.",
            "markdown"
        ],
        [
            "4.) Now integrate the library function as normally, here using nquad:",
            "markdown"
        ],
        [
            "integrate.nquad(func, [[0, 10], [-10, 0], [-1, 1]])\n(1200.0, 1.1102230246251565e-11)",
            "code"
        ],
        [
            "The Python tuple is returned as expected in a reduced amount of time.  All\noptional parameters can be used with this method including specifying\nsingularities, infinite bounds, etc.",
            "markdown"
        ]
    ],
    "Integration (scipy.integrate)->Ordinary differential equations (solve_ivp)": [
        [
            "Integrating a set of ordinary differential equations (ODEs) given\ninitial conditions is another useful example. The function\nsolve_ivp is available in SciPy for integrating a first-order\nvector differential equation:\n\n\\[\\frac{d\\mathbf{y}}{dt}=\\mathbf{f}\\left(\\mathbf{y},t\\right),\\]",
            "markdown"
        ],
        [
            "given initial conditions \\(\\mathbf{y}\\left(0\\right)=y_{0}\\), where\n\\(\\mathbf{y}\\) is a length \\(N\\) vector and \\(\\mathbf{f}\\)\nis a mapping from \\(\\mathcal{R}^{N}\\) to \\(\\mathcal{R}^{N}.\\)\nA higher-order ordinary differential equation can always be reduced to\na differential equation of this type by introducing intermediate\nderivatives into the \\(\\mathbf{y}\\) vector.",
            "markdown"
        ],
        [
            "For example, suppose it is desired to find the solution to the\nfollowing second-order differential equation:\n\n\\[\\frac{d^{2}w}{dz^{2}}-zw(z)=0\\]",
            "markdown"
        ],
        [
            "with initial conditions \\(w\\left(0\\right)=\\frac{1}{\\sqrt[3]{3^{2}}\\Gamma\\left(\\frac{2}{3}\\right)}\\) and \\(\\left.\\frac{dw}{dz}\\right|_{z=0}=-\\frac{1}{\\sqrt[3]{3}\\Gamma\\left(\\frac{1}{3}\\right)}.\\) It is known that the solution to this differential equation with these\nboundary conditions is the Airy function\n\n\\[w=\\textrm{Ai}\\left(z\\right),\\]",
            "markdown"
        ],
        [
            "which gives a means to check the integrator using special.airy.",
            "markdown"
        ],
        [
            "First, convert this ODE into standard form by setting\n\\(\\mathbf{y}=\\left[\\frac{dw}{dz},w\\right]\\) and \\(t=z\\). Thus,\nthe differential equation becomes\n\n\\[\\begin{split}\\frac{d\\mathbf{y}}{dt}=\\left[\\begin{array}{c} ty_{1}\\\\ y_{0}\\end{array}\\right]=\\left[\\begin{array}{cc} 0 & t\\\\ 1 & 0\\end{array}\\right]\\left[\\begin{array}{c} y_{0}\\\\ y_{1}\\end{array}\\right]=\\left[\\begin{array}{cc} 0 & t\\\\ 1 & 0\\end{array}\\right]\\mathbf{y}.\\end{split}\\]",
            "markdown"
        ],
        [
            "In other words,\n\n\\[\\mathbf{f}\\left(\\mathbf{y},t\\right)=\\mathbf{A}\\left(t\\right)\\mathbf{y}.\\]",
            "markdown"
        ],
        [
            "As an interesting reminder, if \\(\\mathbf{A}\\left(t\\right)\\)\ncommutes with \\(\\int_{0}^{t}\\mathbf{A}\\left(\\tau\\right)\\, d\\tau\\)\nunder matrix multiplication, then this linear differential equation\nhas an exact solution using the matrix exponential:\n\n\\[\\mathbf{y}\\left(t\\right)=\\exp\\left(\\int_{0}^{t}\\mathbf{A}\\left(\\tau\\right)d\\tau\\right)\\mathbf{y}\\left(0\\right),\\]",
            "markdown"
        ],
        [
            "However, in this case, \\(\\mathbf{A}\\left(t\\right)\\) and its integral do not commute.",
            "markdown"
        ],
        [
            "This differential equation can be solved using the function solve_ivp.\nIt requires the derivative, fprime, the time span <em class=\"xref py py-obj\">[t_start, t_end]\nand the initial conditions vector, y0, as input arguments and returns\nan object whose y field is an array with consecutive solution values as\ncolumns. The initial conditions are therefore given in the first output column.",
            "markdown"
        ],
        [
            "from scipy.integrate import solve_ivp\n from scipy.special import gamma, airy\n y1_0 = +1 / 3**(2/3) / gamma(2/3)\n y0_0 = -1 / 3**(1/3) / gamma(1/3)\n y0 = [y0_0, y1_0]\n def func(t, y):\n...     return [t*y[1],y[0]]\n...\n t_span = [0, 4]\n sol1 = solve_ivp(func, t_span, y0)\n print(\"sol1.t: {}\".format(sol1.t))\nsol1.t:    [0.         0.10097672 1.04643602 1.91060117 2.49872472 3.08684827\n 3.62692846 4.        ]",
            "code"
        ],
        [
            "As it can be seen solve_ivp determines its time steps automatically if not\nspecified otherwise. To compare the solution of solve_ivp with the <em class=\"xref py py-obj\">airy\nfunction the time vector created by solve_ivp is passed to the <em class=\"xref py py-obj\">airy function.",
            "markdown"
        ],
        [
            "print(\"sol1.y[1]: {}\".format(sol1.y[1]))\nsol1.y[1]: [0.35502805 0.328952   0.12801343 0.04008508 0.01601291 0.00623879\n 0.00356316 0.00405982]\n print(\"airy(sol.t)[0]:  {}\".format(airy(sol1.t)[0]))\nairy(sol.t)[0]: [0.35502805 0.328952   0.12804768 0.03995804 0.01575943 0.00562799\n 0.00201689 0.00095156]",
            "code"
        ],
        [
            "The solution of solve_ivp with its standard parameters shows a big deviation\nto the airy function. To minimize this deviation, relative and absolute\ntolerances can be used.",
            "markdown"
        ],
        [
            "rtol, atol = (1e-8, 1e-8)\n sol2 = solve_ivp(func, t_span, y0, rtol=rtol, atol=atol)\n print(\"sol2.y[1][::6]: {}\".format(sol2.y[1][0::6]))\nsol2.y[1][::6]: [0.35502805 0.19145234 0.06368989 0.0205917  0.00554734 0.00106409]\n print(\"airy(sol2.t)[0][::6]: {}\".format(airy(sol2.t)[0][::6]))\nairy(sol2.t)[0][::6]: [0.35502805 0.19145234 0.06368989 0.0205917  0.00554733 0.00106406]",
            "code"
        ],
        [
            "To specify user defined time points for the solution of solve_ivp, solve_ivp\noffers two possibilities that can also be used complementarily. By passing the <em class=\"xref py py-obj\">t_eval\noption to the function call solve_ivp returns the solutions of these time points\nof <em class=\"xref py py-obj\">t_eval in its output.",
            "markdown"
        ],
        [
            "import numpy as np\n t = np.linspace(0, 4, 100)\n sol3 = solve_ivp(func, t_span, y0, t_eval=t)",
            "code"
        ],
        [
            "If the jacobian matrix of function is known, it can be passed to the solve_ivp\nto achieve better results. Please be aware however that the default integration method\nRK45 does not support jacobian matrices and thereby another integration method has\nto be chosen. One of the integration methods that support a jacobian matrix is the for\nexample the Radau method of following example.",
            "markdown"
        ],
        [
            "def gradient(t, y):\n...     return [[0,t], [1,0]]\n sol4 = solve_ivp(func, t_span, y0, method='Radau', jac=gradient)",
            "code"
        ]
    ],
    "Integration (scipy.integrate)->Ordinary differential equations (solve_ivp)->Solving a system with a banded Jacobian matrix": [
        [
            "odeint can be told that the Jacobian is banded.  For a large\nsystem of differential equations that are known to be stiff, this\ncan improve performance significantly.",
            "markdown"
        ],
        [
            "As an example, we\u00e2\u0080\u0099ll solve the 1-D Gray-Scott partial\ndifferential equations using the method of lines [MOL].  The Gray-Scott equations\nfor the functions \\(u(x, t)\\) and \\(v(x, t)\\) on the interval\n\\(x \\in [0, L]\\) are\n\n\\[\\begin{split}\\begin{split}\n\\frac{\\partial u}{\\partial t} = D_u \\frac{\\partial^2 u}{\\partial x^2} - uv^2 + f(1-u) \\\\\n\\frac{\\partial v}{\\partial t} = D_v \\frac{\\partial^2 v}{\\partial x^2} + uv^2 - (f + k)v \\\\\n\\end{split}\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(D_u\\) and \\(D_v\\) are the diffusion coefficients of the\ncomponents \\(u\\) and \\(v\\), respectively, and \\(f\\) and \\(k\\)\nare constants.  (For more information about the system, see\nhttp://groups.csail.mit.edu/mac/projects/amorphous/GrayScott/)",
            "markdown"
        ],
        [
            "We\u00e2\u0080\u0099ll assume Neumann (i.e., \u00e2\u0080\u009cno flux\u00e2\u0080\u009d) boundary conditions:\n\n\\[\\frac{\\partial u}{\\partial x}(0,t) = 0, \\quad\n\\frac{\\partial v}{\\partial x}(0,t) = 0, \\quad\n\\frac{\\partial u}{\\partial x}(L,t) = 0, \\quad\n\\frac{\\partial v}{\\partial x}(L,t) = 0\\]",
            "markdown"
        ],
        [
            "To apply the method of lines, we discretize the \\(x\\) variable by defining\nthe uniformly spaced grid of \\(N\\) points \\(\\left\\{x_0, x_1, \\ldots, x_{N-1}\\right\\}\\), with\n\\(x_0 = 0\\) and \\(x_{N-1} = L\\).\nWe define \\(u_j(t) \\equiv u(x_k, t)\\) and \\(v_j(t) \\equiv v(x_k, t)\\), and\nreplace the \\(x\\) derivatives with finite differences.  That is,\n\n\\[\\frac{\\partial^2 u}{\\partial x^2}(x_j, t) \\rightarrow\n    \\frac{u_{j-1}(t) - 2 u_{j}(t) + u_{j+1}(t)}{(\\Delta x)^2}\\]",
            "markdown"
        ],
        [
            "We then have a system of \\(2N\\) ordinary differential equations:\n\n(1)#\\[\\begin{split} \\begin{split}\n \\frac{du_j}{dt} = \\frac{D_u}{(\\Delta x)^2} \\left(u_{j-1} - 2 u_{j} + u_{j+1}\\right)\n       -u_jv_j^2 + f(1 - u_j) \\\\\n \\frac{dv_j}{dt} = \\frac{D_v}{(\\Delta x)^2} \\left(v_{j-1} - 2 v_{j} + v_{j+1}\\right)\n       + u_jv_j^2 - (f + k)v_j\n \\end{split}\\end{split}\\]",
            "markdown"
        ],
        [
            "For convenience, the \\((t)\\) arguments have been dropped.",
            "markdown"
        ],
        [
            "To enforce the boundary conditions, we introduce \u00e2\u0080\u009cghost\u00e2\u0080\u009d points\n\\(x_{-1}\\) and \\(x_N\\), and define \\(u_{-1}(t) \\equiv u_1(t)\\),\n\\(u_N(t) \\equiv u_{N-2}(t)\\); \\(v_{-1}(t)\\) and \\(v_N(t)\\)\nare defined analogously.",
            "markdown"
        ],
        [
            "Then\n\n(2)#\\[\\begin{split} \\begin{split}\n \\frac{du_0}{dt} = \\frac{D_u}{(\\Delta x)^2} \\left(2u_{1} - 2 u_{0}\\right)\n       -u_0v_0^2 + f(1 - u_0) \\\\\n \\frac{dv_0}{dt} = \\frac{D_v}{(\\Delta x)^2} \\left(2v_{1} - 2 v_{0}\\right)\n       + u_0v_0^2 - (f + k)v_0\n \\end{split}\\end{split}\\]",
            "markdown"
        ],
        [
            "and\n\n(3)#\\[\\begin{split} \\begin{split}\n \\frac{du_{N-1}}{dt} = \\frac{D_u}{(\\Delta x)^2} \\left(2u_{N-2} - 2 u_{N-1}\\right)\n       -u_{N-1}v_{N-1}^2 + f(1 - u_{N-1}) \\\\\n \\frac{dv_{N-1}}{dt} = \\frac{D_v}{(\\Delta x)^2} \\left(2v_{N-2} - 2 v_{N-1}\\right)\n       + u_{N-1}v_{N-1}^2 - (f + k)v_{N-1}\n \\end{split}\\end{split}\\]",
            "markdown"
        ],
        [
            "Our complete system of \\(2N\\) ordinary differential equations is (1)\nfor \\(k = 1, 2, \\ldots, N-2\\), along with (2) and (3).",
            "markdown"
        ],
        [
            "We can now starting implementing this system in code.  We must combine\n\\(\\{u_k\\}\\) and \\(\\{v_k\\}\\) into a single vector of length \\(2N\\).\nThe two obvious choices are\n\\(\\{u_0, u_1, \\ldots, u_{N-1}, v_0, v_1, \\ldots, v_{N-1}\\}\\)\nand\n\\(\\{u_0, v_0, u_1, v_1, \\ldots, u_{N-1}, v_{N-1}\\}\\).\nMathematically, it does not matter, but the choice affects how\nefficiently odeint can solve the system.  The reason is in how\nthe order affects the pattern of the nonzero elements of the Jacobian matrix.",
            "markdown"
        ],
        [
            "When the variables are ordered\nas \\(\\{u_0, u_1, \\ldots, u_{N-1}, v_0, v_1, \\ldots, v_{N-1}\\}\\),\nthe pattern of nonzero elements of the Jacobian matrix is\n\n\\[\\begin{split}\\begin{smallmatrix}\n   * & * & 0 & 0 & 0 & 0 & 0  &  * & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   * & * & * & 0 & 0 & 0 & 0  &  0 & * & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & * & * & * & 0 & 0 & 0  &  0 & 0 & * & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & * & * & * & 0 & 0  &  0 & 0 & 0 & * & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & * & * & * & 0  &  0 & 0 & 0 & 0 & * & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & * & * & *  &  0 & 0 & 0 & 0 & 0 & * & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & * & *  &  0 & 0 & 0 & 0 & 0 & 0 & * \\\\\n   * & 0 & 0 & 0 & 0 & 0 & 0  &  * & * & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & * & 0 & 0 & 0 & 0 & 0  &  * & * & * & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & * & 0 & 0 & 0 & 0  &  0 & * & * & * & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & * & 0 & 0 & 0  &  0 & 0 & * & * & * & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & * & 0 & 0  &  0 & 0 & 0 & * & * & * & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & * & 0  &  0 & 0 & 0 & 0 & * & * & * \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & *  &  0 & 0 & 0 & 0 & ) & * & * \\\\\n\\end{smallmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "The Jacobian pattern with variables interleaved\nas \\(\\{u_0, v_0, u_1, v_1, \\ldots, u_{N-1}, v_{N-1}\\}\\) is\n\n\\[\\begin{split}\\begin{smallmatrix}\n   * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * & 0 & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * & * & 0 \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * & 0 & * \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & 0 & * & * \\\\\n   0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * & * & * \\\\\n\\end{smallmatrix}\\end{split}\\]",
            "markdown"
        ],
        [
            "In both cases, there are just five nontrivial diagonals, but\nwhen the variables are interleaved, the bandwidth is much\nsmaller.\nThat is, the main diagonal and the two diagonals immediately\nabove and the two immediately below the main diagonal\nare the nonzero diagonals.\nThis is important, because the inputs mu and ml\nof odeint are the upper and lower bandwidths of the\nJacobian matrix.  When the variables are interleaved,\nmu and ml are 2.  When the variables are stacked\nwith \\(\\{v_k\\}\\) following \\(\\{u_k\\}\\), the upper\nand lower bandwidths are \\(N\\).",
            "markdown"
        ],
        [
            "With that decision made, we can write the function that\nimplements the system of differential equations.",
            "markdown"
        ],
        [
            "First, we define the functions for the source and reaction\nterms of the system:",
            "markdown"
        ],
        [
            "def G(u, v, f, k):\n    return f * (1 - u) - u*v**2\n\ndef H(u, v, f, k):\n    return -(f + k) * v + u*v**2",
            "code"
        ],
        [
            "Next, we define the function that computes the right-hand side\nof the system of differential equations:",
            "markdown"
        ],
        [
            "def grayscott1d(y, t, f, k, Du, Dv, dx):\n    \"\"\"\n    Differential equations for the 1-D Gray-Scott equations.\n\n    The ODEs are derived using the method of lines.\n    \"\"\"\n    # The vectors u and v are interleaved in y.  We define\n    # views of u and v by slicing y.\n    u = y[::2]\n    v = y[1::2]\n\n    # dydt is the return value of this function.\n    dydt = np.empty_like(y)\n\n    # Just like u and v are views of the interleaved vectors\n    # in y, dudt and dvdt are views of the interleaved output\n    # vectors in dydt.\n    dudt = dydt[::2]\n    dvdt = dydt[1::2]\n\n    # Compute du/dt and dv/dt.  The end points and the interior points\n    # are handled separately.\n    dudt[0]    = G(u[0],    v[0],    f, k) + Du * (-2.0*u[0] + 2.0*u[1]) / dx**2\n    dudt[1:-1] = G(u[1:-1], v[1:-1], f, k) + Du * np.diff(u,2) / dx**2\n    dudt[-1]   = G(u[-1],   v[-1],   f, k) + Du * (- 2.0*u[-1] + 2.0*u[-2]) / dx**2\n    dvdt[0]    = H(u[0],    v[0],    f, k) + Dv * (-2.0*v[0] + 2.0*v[1]) / dx**2\n    dvdt[1:-1] = H(u[1:-1], v[1:-1], f, k) + Dv * np.diff(v,2) / dx**2\n    dvdt[-1]   = H(u[-1],   v[-1],   f, k) + Dv * (-2.0*v[-1] + 2.0*v[-2]) / dx**2\n\n    return dydt",
            "code"
        ],
        [
            "We won\u00e2\u0080\u0099t implement a function to compute the Jacobian, but we will tell\nodeint that the Jacobian matrix is banded.  This allows the underlying\nsolver (LSODA) to avoid computing values that it knows are zero.  For a large\nsystem, this improves the performance significantly, as demonstrated in the\nfollowing ipython session.",
            "markdown"
        ],
        [
            "First, we define the required inputs:",
            "markdown"
        ],
        [
            "In [30]: rng = np.random.default_rng()\n\nIn [31]: y0 = rng.standard_normal(5000)\n\nIn [32]: t = np.linspace(0, 50, 11)\n\nIn [33]: f = 0.024\n\nIn [34]: k = 0.055\n\nIn [35]: Du = 0.01\n\nIn [36]: Dv = 0.005\n\nIn [37]: dx = 0.025",
            "code"
        ],
        [
            "Time the computation without taking advantage of the banded structure\nof the Jacobian matrix:",
            "markdown"
        ],
        [
            "In [38]: %timeit sola = odeint(grayscott1d, y0, t, args=(f, k, Du, Dv, dx))\n1 loop, best of 3: 25.2 s per loop",
            "code"
        ],
        [
            "Now set ml=2 and mu=2, so odeint knows that the Jacobian matrix\nis banded:",
            "markdown"
        ],
        [
            "In [39]: %timeit solb = odeint(grayscott1d, y0, t, args=(f, k, Du, Dv, dx), ml=2, mu=2)\n10 loops, best of 3: 191 ms per loop",
            "code"
        ],
        [
            "That is quite a bit faster!",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s ensure that they have computed the same result:",
            "markdown"
        ],
        [
            "In [41]: np.allclose(sola, solb)\nOut[41]: True",
            "code"
        ]
    ],
    "Integration (scipy.integrate)->Ordinary differential equations (solve_ivp)->References": [
        [
            "https://en.wikipedia.org/wiki/Romberg\u00e2\u0080\u0099s_method\n\n\n[MOL]",
            "markdown"
        ],
        [
            "https://en.wikipedia.org/wiki/Method_of_lines",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)": [
        [
            "Contents",
            "markdown"
        ],
        [
            "Optimization (scipy.optimize)",
            "markdown"
        ],
        [
            "Unconstrained minimization of multivariate scalar functions (minimize)",
            "markdown"
        ],
        [
            "Nelder-Mead Simplex algorithm (method='Nelder-Mead')",
            "markdown"
        ],
        [
            "Broyden-Fletcher-Goldfarb-Shanno algorithm (method='BFGS')",
            "markdown"
        ],
        [
            "Newton-Conjugate-Gradient algorithm (method='Newton-CG')",
            "markdown"
        ],
        [
            "Full Hessian example:",
            "markdown"
        ],
        [
            "Hessian product example:",
            "markdown"
        ],
        [
            "Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')",
            "markdown"
        ],
        [
            "Full Hessian example:",
            "markdown"
        ],
        [
            "Hessian product example:",
            "markdown"
        ],
        [
            "Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm (method='trust-krylov')",
            "markdown"
        ],
        [
            "Full Hessian example:",
            "markdown"
        ],
        [
            "Hessian product example:",
            "markdown"
        ],
        [
            "Trust-Region Nearly Exact Algorithm (method='trust-exact')",
            "markdown"
        ],
        [
            "Constrained minimization of multivariate scalar functions (minimize)",
            "markdown"
        ],
        [
            "Trust-Region Constrained Algorithm (method='trust-constr')",
            "markdown"
        ],
        [
            "Defining Bounds Constraints:",
            "markdown"
        ],
        [
            "Defining Linear Constraints:",
            "markdown"
        ],
        [
            "Defining Nonlinear Constraints:",
            "markdown"
        ],
        [
            "Solving the Optimization Problem:",
            "markdown"
        ],
        [
            "Sequential Least SQuares Programming (SLSQP) Algorithm (method='SLSQP')",
            "markdown"
        ],
        [
            "Global optimization",
            "markdown"
        ],
        [
            "Least-squares minimization (least_squares)",
            "markdown"
        ],
        [
            "Example of solving a fitting problem",
            "markdown"
        ],
        [
            "Further examples",
            "markdown"
        ],
        [
            "Univariate function minimizers (minimize_scalar)",
            "markdown"
        ],
        [
            "Unconstrained minimization (method='brent')",
            "markdown"
        ],
        [
            "Bounded minimization (method='bounded')",
            "markdown"
        ],
        [
            "Custom minimizers",
            "markdown"
        ],
        [
            "Root finding",
            "markdown"
        ],
        [
            "Scalar functions",
            "markdown"
        ],
        [
            "Fixed-point solving",
            "markdown"
        ],
        [
            "Sets of equations",
            "markdown"
        ],
        [
            "Root finding for large problems",
            "markdown"
        ],
        [
            "Still too slow? Preconditioning.",
            "markdown"
        ],
        [
            "Linear programming (linprog)",
            "markdown"
        ],
        [
            "Linear programming example",
            "markdown"
        ],
        [
            "Assignment problems",
            "markdown"
        ],
        [
            "Linear sum assignment problem example",
            "markdown"
        ],
        [
            "Mixed integer linear programming",
            "markdown"
        ],
        [
            "Knapsack problem example",
            "markdown"
        ],
        [
            "The scipy.optimize package provides several commonly used\noptimization algorithms. A detailed listing is available:\nscipy.optimize (can also be found by help(scipy.optimize)).",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)": [
        [
            "The minimize function provides a common interface to unconstrained\nand constrained minimization algorithms for multivariate scalar functions\nin scipy.optimize. To demonstrate the minimization function, consider the\nproblem of minimizing the Rosenbrock function of \\(N\\) variables:\n\n\\[f\\left(\\mathbf{x}\\right)=\\sum_{i=1}^{N-1}100\\left(x_{i+1}-x_{i}^{2}\\right)^{2}+\\left(1-x_{i}\\right)^{2}.\\]",
            "markdown"
        ],
        [
            "The minimum value of this function is 0 which is achieved when\n\\(x_{i}=1.\\)",
            "markdown"
        ],
        [
            "Note that the Rosenbrock function and its derivatives are included in\nscipy.optimize. The implementations shown in the following sections\nprovide examples of how to define an objective function as well as its\njacobian and hessian functions. Objective functions in scipy.optimize\nexpect a numpy array as their first parameter which is to be optimized\nand must return a float value. The exact calling signature must be\nf(x, *args) where x represents a numpy array and args\na tuple of additional arguments supplied to the objective function.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')": [
        [
            "In the example below, the minimize routine is used\nwith the Nelder-Mead simplex algorithm (selected through the method\nparameter):",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.optimize import minimize",
            "code"
        ],
        [
            "def rosen(x):\n...     \"\"\"The Rosenbrock function\"\"\"\n...     return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)",
            "code"
        ],
        [
            "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n res = minimize(rosen, x0, method='nelder-mead',\n...                options={'xatol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 339\n         Function evaluations: 571",
            "code"
        ],
        [
            "print(res.x)\n[1. 1. 1. 1. 1.]",
            "code"
        ],
        [
            "The simplex algorithm is probably the simplest way to minimize a fairly\nwell-behaved function. It requires only function evaluations and is a good\nchoice for simple minimization problems. However, because it does not use\nany gradient evaluations, it may take longer to find the minimum.",
            "markdown"
        ],
        [
            "Another optimization algorithm that needs only function calls to find\nthe minimum is Powell\u00e2\u0080\u0099s method available by setting method='powell' in\nminimize.",
            "markdown"
        ],
        [
            "To demonstrate how to supply additional arguments to an objective function,\nlet us minimize the Rosenbrock function with an additional scaling factor <em class=\"xref py py-obj\">a\nand an offset <em class=\"xref py py-obj\">b:\n\n\\[f\\left(\\mathbf{x}, a, b\\right)=\\sum_{i=1}^{N-1}a\\left(x_{i+1}-x_{i}^{2}\\right)^{2}+\\left(1-x_{i}\\right)^{2} + b.\\]",
            "markdown"
        ],
        [
            "Again using the minimize routine this can be solved by the following\ncode block for the example parameters <em class=\"xref py py-obj\">a=0.5 and <em class=\"xref py py-obj\">b=1.",
            "markdown"
        ],
        [
            "def rosen_with_args(x, a, b):\n...     \"\"\"The Rosenbrock function with additional arguments\"\"\"\n...     return sum(a*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0) + b",
            "code"
        ],
        [
            "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n res = minimize(rosen_with_args, x0, method='nelder-mead',\n...                args=(0.5, 1.), options={'xatol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 1.000000\n         Iterations: 319\n         Function evaluations: 525",
            "code"
        ],
        [
            "print(res.x)\n[1.         1.         1.         1.         0.99999999]",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Broyden-Fletcher-Goldfarb-Shanno algorithm (method='BFGS')": [
        [
            "In order to converge more quickly to the solution, this routine uses\nthe gradient of the objective function. If the gradient is not given\nby the user, then it is estimated using first-differences. The\nBroyden-Fletcher-Goldfarb-Shanno (BFGS) method typically requires\nfewer function calls than the simplex algorithm even when the gradient\nmust be estimated.",
            "markdown"
        ],
        [
            "To demonstrate this algorithm, the Rosenbrock function is again used.\nThe gradient of the Rosenbrock function is the vector:\n\n \\begin{eqnarray*} \\frac{\\partial f}{\\partial x_{j}} & = & \\sum_{i=1}^{N}200\\left(x_{i}-x_{i-1}^{2}\\right)\\left(\\delta_{i,j}-2x_{i-1}\\delta_{i-1,j}\\right)-2\\left(1-x_{i-1}\\right)\\delta_{i-1,j}.\\\\  & = & 200\\left(x_{j}-x_{j-1}^{2}\\right)-400x_{j}\\left(x_{j+1}-x_{j}^{2}\\right)-2\\left(1-x_{j}\\right).\\end{eqnarray*}",
            "markdown"
        ],
        [
            "This expression is valid for the interior derivatives. Special cases\nare\n\n \\begin{eqnarray*} \\frac{\\partial f}{\\partial x_{0}} & = & -400x_{0}\\left(x_{1}-x_{0}^{2}\\right)-2\\left(1-x_{0}\\right),\\\\ \\frac{\\partial f}{\\partial x_{N-1}} & = & 200\\left(x_{N-1}-x_{N-2}^{2}\\right).\\end{eqnarray*}",
            "markdown"
        ],
        [
            "A Python function which computes this gradient is constructed by the\ncode-segment:",
            "markdown"
        ],
        [
            "def rosen_der(x):\n...     xm = x[1:-1]\n...     xm_m1 = x[:-2]\n...     xm_p1 = x[2:]\n...     der = np.zeros_like(x)\n...     der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n...     der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n...     der[-1] = 200*(x[-1]-x[-2]**2)\n...     return der",
            "code"
        ],
        [
            "This gradient information is specified in the minimize function\nthrough the jac parameter as illustrated below.",
            "markdown"
        ],
        [
            "res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n...                options={'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 25                     # may vary\n         Function evaluations: 30\n         Gradient evaluations: 30\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ],
        [
            "Another way to supply gradient information is to write a single\nfunction which returns both the objective and the gradient: this is\nindicated by setting jac=True. In this case, the Python function\nto be optimized must return a tuple whose first value is the objective\nand whose second value represents the gradient. For this example, the\nobjective can be specified in the following way:",
            "markdown"
        ],
        [
            "def rosen_and_der(x):\n...     objective = sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n...     xm = x[1:-1]\n...     xm_m1 = x[:-2]\n...     xm_p1 = x[2:]\n...     der = np.zeros_like(x)\n...     der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n...     der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n...     der[-1] = 200*(x[-1]-x[-2]**2)\n...     return objective, der",
            "code"
        ],
        [
            "res = minimize(rosen_and_der, x0, method='BFGS', jac=True,\n...                options={'disp': True})\n         Current function value: 0.000000\n         Iterations: 25                     # may vary\n         Function evaluations: 30\n         Gradient evaluations: 30",
            "code"
        ],
        [
            "Supplying objective and gradient in a single function can help to avoid\nredundant computations and therefore speed up the optimization significantly.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Newton-Conjugate-Gradient algorithm (method='Newton-CG')": [
        [
            "Newton-Conjugate Gradient algorithm is a modified Newton\u00e2\u0080\u0099s\nmethod and uses a conjugate gradient algorithm to (approximately) invert\nthe local Hessian [NW].  Newton\u00e2\u0080\u0099s method is based on fitting the function\nlocally to a quadratic form:\n\n\\[f\\left(\\mathbf{x}\\right)\\approx f\\left(\\mathbf{x}_{0}\\right)+\\nabla f\\left(\\mathbf{x}_{0}\\right)\\cdot\\left(\\mathbf{x}-\\mathbf{x}_{0}\\right)+\\frac{1}{2}\\left(\\mathbf{x}-\\mathbf{x}_{0}\\right)^{T}\\mathbf{H}\\left(\\mathbf{x}_{0}\\right)\\left(\\mathbf{x}-\\mathbf{x}_{0}\\right).\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{H}\\left(\\mathbf{x}_{0}\\right)\\) is a matrix of second-derivatives (the Hessian). If the Hessian is\npositive definite then the local minimum of this function can be found\nby setting the gradient of the quadratic form to zero, resulting in\n\n\\[\\mathbf{x}_{\\textrm{opt}}=\\mathbf{x}_{0}-\\mathbf{H}^{-1}\\nabla f.\\]",
            "markdown"
        ],
        [
            "The inverse of the Hessian is evaluated using the conjugate-gradient\nmethod. An example of employing this method to minimizing the\nRosenbrock function is given below. To take full advantage of the\nNewton-CG method, a function which computes the Hessian must be\nprovided. The Hessian matrix itself does not need to be constructed,\nonly a vector which is the product of the Hessian with an arbitrary\nvector needs to be available to the minimization routine. As a result,\nthe user can provide either a function to compute the Hessian matrix,\nor a function to compute the product of the Hessian with an arbitrary\nvector.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Newton-Conjugate-Gradient algorithm (method='Newton-CG')->Full Hessian example:": [
        [
            "The Hessian of the Rosenbrock function is\n\n \\begin{eqnarray*} H_{ij}=\\frac{\\partial^{2}f}{\\partial x_{i}\\partial x_{j}} & = & 200\\left(\\delta_{i,j}-2x_{i-1}\\delta_{i-1,j}\\right)-400x_{i}\\left(\\delta_{i+1,j}-2x_{i}\\delta_{i,j}\\right)-400\\delta_{i,j}\\left(x_{i+1}-x_{i}^{2}\\right)+2\\delta_{i,j},\\\\  & = & \\left(202+1200x_{i}^{2}-400x_{i+1}\\right)\\delta_{i,j}-400x_{i}\\delta_{i+1,j}-400x_{i-1}\\delta_{i-1,j},\\end{eqnarray*}",
            "markdown"
        ],
        [
            "if \\(i,j\\in\\left[1,N-2\\right]\\) with \\(i,j\\in\\left[0,N-1\\right]\\) defining the \\(N\\times N\\) matrix. Other non-zero entries of the matrix are\n\n \\begin{eqnarray*} \\frac{\\partial^{2}f}{\\partial x_{0}^{2}} & = & 1200x_{0}^{2}-400x_{1}+2,\\\\ \\frac{\\partial^{2}f}{\\partial x_{0}\\partial x_{1}}=\\frac{\\partial^{2}f}{\\partial x_{1}\\partial x_{0}} & = & -400x_{0},\\\\ \\frac{\\partial^{2}f}{\\partial x_{N-1}\\partial x_{N-2}}=\\frac{\\partial^{2}f}{\\partial x_{N-2}\\partial x_{N-1}} & = & -400x_{N-2},\\\\ \\frac{\\partial^{2}f}{\\partial x_{N-1}^{2}} & = & 200.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "For example, the Hessian when \\(N=5\\) is\n\n\\[\\begin{split}\\mathbf{H}=\\begin{bmatrix} 1200x_{0}^{2}-400x_{1}+2 & -400x_{0} & 0 & 0 & 0\\\\ -400x_{0} & 202+1200x_{1}^{2}-400x_{2} & -400x_{1} & 0 & 0\\\\ 0 & -400x_{1} & 202+1200x_{2}^{2}-400x_{3} & -400x_{2} & 0\\\\ 0 &  & -400x_{2} & 202+1200x_{3}^{2}-400x_{4} & -400x_{3}\\\\ 0 & 0 & 0 & -400x_{3} & 200\\end{bmatrix}.\\end{split}\\]",
            "markdown"
        ],
        [
            "The code which computes this Hessian along with the code to minimize\nthe function using Newton-CG method is shown in the following example:",
            "markdown"
        ],
        [
            "def rosen_hess(x):\n...     x = np.asarray(x)\n...     H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n...     diagonal = np.zeros_like(x)\n...     diagonal[0] = 1200*x[0]**2-400*x[1]+2\n...     diagonal[-1] = 200\n...     diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n...     H = H + np.diag(diagonal)\n...     return H",
            "code"
        ],
        [
            "res = minimize(rosen, x0, method='Newton-CG',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'xtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 19                       # may vary\n         Function evaluations: 22\n         Gradient evaluations: 19\n         Hessian evaluations: 19\n res.x\narray([1.,  1.,  1.,  1.,  1.])",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Newton-Conjugate-Gradient algorithm (method='Newton-CG')->Hessian product example:": [
        [
            "For larger minimization problems, storing the entire Hessian matrix can\nconsume considerable time and memory. The Newton-CG algorithm only needs\nthe product of the Hessian times an arbitrary vector. As a result, the user\ncan supply code to compute this product rather than the full Hessian by\ngiving a hess function which take the minimization vector as the first\nargument and the arbitrary vector as the second argument (along with extra\narguments passed to the function to be minimized). If possible, using\nNewton-CG with the Hessian product option is probably the fastest way to\nminimize the function.",
            "markdown"
        ],
        [
            "In this case, the product of the Rosenbrock Hessian with an arbitrary\nvector is not difficult to compute. If \\(\\mathbf{p}\\) is the arbitrary\nvector, then \\(\\mathbf{H}\\left(\\mathbf{x}\\right)\\mathbf{p}\\) has\nelements:\n\n\\[\\begin{split}\\mathbf{H}\\left(\\mathbf{x}\\right)\\mathbf{p}=\\begin{bmatrix} \\left(1200x_{0}^{2}-400x_{1}+2\\right)p_{0}-400x_{0}p_{1}\\\\ \\vdots\\\\ -400x_{i-1}p_{i-1}+\\left(202+1200x_{i}^{2}-400x_{i+1}\\right)p_{i}-400x_{i}p_{i+1}\\\\ \\vdots\\\\ -400x_{N-2}p_{N-2}+200p_{N-1}\\end{bmatrix}.\\end{split}\\]",
            "markdown"
        ],
        [
            "Code which makes use of this Hessian product to minimize the\nRosenbrock function using minimize follows:",
            "markdown"
        ],
        [
            "def rosen_hess_p(x, p):\n...     x = np.asarray(x)\n...     Hp = np.zeros_like(x)\n...     Hp[0] = (1200*x[0]**2 - 400*x[1] + 2)*p[0] - 400*x[0]*p[1]\n...     Hp[1:-1] = -400*x[:-2]*p[:-2]+(202+1200*x[1:-1]**2-400*x[2:])*p[1:-1] \\\n...                -400*x[1:-1]*p[2:]\n...     Hp[-1] = -400*x[-2]*p[-2] + 200*p[-1]\n...     return Hp",
            "code"
        ],
        [
            "res = minimize(rosen, x0, method='Newton-CG',\n...                jac=rosen_der, hessp=rosen_hess_p,\n...                options={'xtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 20                    # may vary\n         Function evaluations: 23\n         Gradient evaluations: 20\n         Hessian evaluations: 44\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ],
        [
            "According to [NW] p. 170 the Newton-CG algorithm can be inefficient\nwhen the Hessian is ill-conditioned because of the poor quality search directions\nprovided by the method in those situations. The method trust-ncg,\naccording to the authors, deals more effectively with this problematic situation\nand will be described next.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')": [
        [
            "The Newton-CG method is a line search method: it finds a direction\nof search minimizing a quadratic approximation of the function and then uses\na line search algorithm to find the (nearly) optimal step size in that direction.\nAn alternative approach is to, first, fix the step size limit \\(\\Delta\\) and then find the\noptimal step \\(\\mathbf{p}\\) inside the given trust-radius by solving\nthe following quadratic subproblem:\n\n\\begin{eqnarray*}\n   \\min_{\\mathbf{p}} f\\left(\\mathbf{x}_{k}\\right)+\\nabla f\\left(\\mathbf{x}_{k}\\right)\\cdot\\mathbf{p}+\\frac{1}{2}\\mathbf{p}^{T}\\mathbf{H}\\left(\\mathbf{x}_{k}\\right)\\mathbf{p};&\\\\\n   \\text{subject to: } \\|\\mathbf{p}\\|\\le \\Delta.&\n \\end{eqnarray*}",
            "markdown"
        ],
        [
            "The solution is then updated \\(\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\mathbf{p}\\) and\nthe trust-radius \\(\\Delta\\) is adjusted according to the degree of agreement of the quadratic\nmodel with the real function. This family of methods is known as trust-region methods.\nThe trust-ncg algorithm is a trust-region method that uses a conjugate gradient algorithm\nto solve the trust-region subproblem [NW].",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')->Full Hessian example:": [
        [
            "res = minimize(rosen, x0, method='trust-ncg',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 20                    # may vary\n         Function evaluations: 21\n         Gradient evaluations: 20\n         Hessian evaluations: 19\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Newton-Conjugate-Gradient Algorithm (method='trust-ncg')->Hessian product example:": [
        [
            "res = minimize(rosen, x0, method='trust-ncg',\n...                jac=rosen_der, hessp=rosen_hess_p,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 20                    # may vary\n         Function evaluations: 21\n         Gradient evaluations: 20\n         Hessian evaluations: 0\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm (method='trust-krylov')": [
        [
            "Similar to the trust-ncg method, the trust-krylov method is a method\nsuitable for large-scale problems as it uses the hessian only as linear\noperator by means of matrix-vector products.\nIt solves the quadratic subproblem more accurately than the trust-ncg\nmethod.\n\n\\begin{eqnarray*}\n   \\min_{\\mathbf{p}} f\\left(\\mathbf{x}_{k}\\right)+\\nabla f\\left(\\mathbf{x}_{k}\\right)\\cdot\\mathbf{p}+\\frac{1}{2}\\mathbf{p}^{T}\\mathbf{H}\\left(\\mathbf{x}_{k}\\right)\\mathbf{p};&\\\\\n   \\text{subject to: } \\|\\mathbf{p}\\|\\le \\Delta.&\n \\end{eqnarray*}",
            "markdown"
        ],
        [
            "This method wraps the [TRLIB] implementation of the [GLTR] method solving\nexactly a trust-region subproblem restricted to a truncated Krylov subspace.\nFor indefinite problems it is usually better to use this method as it reduces\nthe number of nonlinear iterations at the expense of few more matrix-vector\nproducts per subproblem solve in comparison to the trust-ncg method.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm (method='trust-krylov')->Full Hessian example:": [
        [
            "res = minimize(rosen, x0, method='trust-krylov',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 19                    # may vary\n         Function evaluations: 20\n         Gradient evaluations: 20\n         Hessian evaluations: 18\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Truncated Generalized Lanczos / Conjugate Gradient Algorithm (method='trust-krylov')->Hessian product example:": [
        [
            "res = minimize(rosen, x0, method='trust-krylov',\n...                jac=rosen_der, hessp=rosen_hess_p,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 19                    # may vary\n         Function evaluations: 20\n         Gradient evaluations: 20\n         Hessian evaluations: 0\n res.x\narray([1., 1., 1., 1., 1.])\n\n\n\n\n[TRLIB]",
            "code"
        ],
        [
            "F. Lenders, C. Kirches, A. Potschka: \u00e2\u0080\u009ctrlib: A vector-free\nimplementation of the GLTR method for iterative solution of\nthe trust region problem\u00e2\u0080\u009d, arXiv:1611.04718\n\n\n[GLTR]",
            "markdown"
        ],
        [
            "N. Gould, S. Lucidi, M. Roma, P. Toint: \u00e2\u0080\u009cSolving the\nTrust-Region Subproblem using the Lanczos Method\u00e2\u0080\u009d,\nSIAM J. Optim., 9(2), 504\u00e2\u0080\u0093525, (1999).\nDOI:10.1137/S1052623497322735",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Trust-Region Nearly Exact Algorithm (method='trust-exact')": [
        [
            "All methods Newton-CG, trust-ncg and trust-krylov are suitable for dealing with\nlarge-scale problems (problems with thousands of variables). That is because the conjugate\ngradient algorithm approximately solve the trust-region subproblem (or invert the Hessian)\nby iterations without the explicit Hessian factorization. Since only the product of the Hessian\nwith an arbitrary vector is needed, the algorithm is specially suited for dealing\nwith sparse Hessians, allowing low storage requirements and significant time savings for\nthose sparse problems.",
            "markdown"
        ],
        [
            "For medium-size problems, for which the storage and factorization cost of the Hessian are not critical,\nit is possible to obtain a solution within fewer iteration by solving the trust-region subproblems\nalmost exactly. To achieve that, a certain nonlinear equations is solved iteratively for each quadratic\nsubproblem [CGT]. This solution requires usually 3 or 4 Cholesky factorizations of the\nHessian matrix. As the result, the method converges in fewer number of iterations\nand takes fewer evaluations of the objective function than the other implemented\ntrust-region methods. The Hessian product option is not supported by this algorithm. An\nexample using the Rosenbrock function follows:",
            "markdown"
        ],
        [
            "res = minimize(rosen, x0, method='trust-exact',\n...                jac=rosen_der, hess=rosen_hess,\n...                options={'gtol': 1e-8, 'disp': True})\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 13                    # may vary\n         Function evaluations: 14\n         Gradient evaluations: 13\n         Hessian evaluations: 14\n res.x\narray([1., 1., 1., 1., 1.])\n\n\n\n\n[NW]\n(1,2,3)",
            "code"
        ],
        [
            "J. Nocedal, S.J. Wright \u00e2\u0080\u009cNumerical optimization.\u00e2\u0080\u009d\n2nd edition. Springer Science (2006).\n\n\n[CGT]",
            "markdown"
        ],
        [
            "Conn, A. R., Gould, N. I., & Toint, P. L.\n\u00e2\u0080\u009cTrust region methods\u00e2\u0080\u009d. Siam. (2000). pp. 169-200.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)": [
        [
            "The minimize function provides algorithms for constrained minimization,\nnamely 'trust-constr' ,  'SLSQP' and 'COBYLA'. They require the constraints\nto be defined using slightly different structures. The method 'trust-constr' requires\nthe  constraints to be defined as a sequence of objects LinearConstraint and\nNonlinearConstraint. Methods 'SLSQP' and 'COBYLA', on the other hand,\nrequire constraints to be defined  as a sequence of dictionaries, with keys\ntype, fun and jac.",
            "markdown"
        ],
        [
            "As an example let us consider the constrained minimization of the Rosenbrock function:\n\n  \\begin{eqnarray*} \\min_{x_0, x_1} & ~~100\\left(x_{1}-x_{0}^{2}\\right)^{2}+\\left(1-x_{0}\\right)^{2} &\\\\\n                  \\text{subject to: } & x_0 + 2 x_1 \\leq 1 & \\\\\n                                      & x_0^2 + x_1 \\leq 1  & \\\\\n                                      & x_0^2 - x_1 \\leq 1  & \\\\\n                                      & 2 x_0 + x_1 = 1 & \\\\\n                                      & 0 \\leq  x_0  \\leq 1 & \\\\\n                                      & -0.5 \\leq  x_1  \\leq 2.0. & \\end{eqnarray*}",
            "markdown"
        ],
        [
            "This optimization problem has the unique solution \\([x_0, x_1] = [0.4149,~ 0.1701]\\),\nfor which only the first and fourth constraints are active.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')": [
        [
            "The trust-region constrained method deals with constrained minimization problems of the form:\n\n  \\begin{eqnarray*} \\min_x & f(x) & \\\\\n       \\text{subject to: } & ~~~ c^l  \\leq c(x) \\leq c^u, &\\\\\n        &  x^l  \\leq x \\leq x^u. & \\end{eqnarray*}",
            "markdown"
        ],
        [
            "When \\(c^l_j = c^u_j\\) the method reads the \\(j\\)-th constraint as an\nequality constraint and deals with it accordingly. Besides that, one-sided constraint\ncan be specified by setting the upper or lower bound to np.inf with the appropriate sign.",
            "markdown"
        ],
        [
            "The implementation is based on [EQSQP] for equality-constraint problems and on [TRIP]\nfor problems with inequality constraints. Both are trust-region type algorithms suitable\nfor large-scale problems.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Bounds Constraints:": [
        [
            "The bound constraints  \\(0 \\leq  x_0  \\leq 1\\) and \\(-0.5 \\leq  x_1  \\leq 2.0\\)\nare defined using a Bounds object.",
            "markdown"
        ],
        [
            "from scipy.optimize import Bounds\n bounds = Bounds([0, -0.5], [1.0, 2.0])",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Linear Constraints:": [
        [
            "The constraints \\(x_0 + 2 x_1 \\leq 1\\)\nand \\(2 x_0 + x_1 = 1\\) can be written in the linear constraint standard format:\n\n  \\begin{equation*} \\begin{bmatrix}-\\infty \\\\1\\end{bmatrix} \\leq\n   \\begin{bmatrix} 1& 2 \\\\ 2& 1\\end{bmatrix}\n    \\begin{bmatrix} x_0 \\\\x_1\\end{bmatrix} \\leq\n     \\begin{bmatrix} 1 \\\\ 1\\end{bmatrix},\\end{equation*}",
            "markdown"
        ],
        [
            "and defined using a LinearConstraint object.",
            "markdown"
        ],
        [
            "from scipy.optimize import LinearConstraint\n linear_constraint = LinearConstraint([[1, 2], [2, 1]], [-np.inf, 1], [1, 1])",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Nonlinear Constraints:": [
        [
            "The nonlinear constraint:\n\n  \\begin{equation*} c(x) =\n  \\begin{bmatrix} x_0^2 + x_1 \\\\ x_0^2 - x_1\\end{bmatrix}\n   \\leq\n   \\begin{bmatrix} 1 \\\\ 1\\end{bmatrix}, \\end{equation*}",
            "markdown"
        ],
        [
            "with Jacobian matrix:\n\n  \\begin{equation*} J(x) =\n  \\begin{bmatrix} 2x_0 & 1 \\\\ 2x_0 & -1\\end{bmatrix},\\end{equation*}",
            "markdown"
        ],
        [
            "and linear combination of the Hessians:\n\n  \\begin{equation*} H(x, v) = \\sum_{i=0}^1 v_i \\nabla^2 c_i(x) =\n  v_0\\begin{bmatrix} 2 & 0 \\\\ 0 & 0\\end{bmatrix} +\n  v_1\\begin{bmatrix} 2 & 0 \\\\ 0 & 0\\end{bmatrix},\n  \\end{equation*}",
            "markdown"
        ],
        [
            "is defined using a NonlinearConstraint object.",
            "markdown"
        ],
        [
            "def cons_f(x):\n...     return [x[0]**2 + x[1], x[0]**2 - x[1]]\n def cons_J(x):\n...     return [[2*x[0], 1], [2*x[0], -1]]\n def cons_H(x, v):\n...     return v[0]*np.array([[2, 0], [0, 0]]) + v[1]*np.array([[2, 0], [0, 0]])\n from scipy.optimize import NonlinearConstraint\n nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess=cons_H)",
            "code"
        ],
        [
            "Alternatively, it is also possible to define the Hessian \\(H(x, v)\\)\nas a sparse matrix,",
            "markdown"
        ],
        [
            "from scipy.sparse import csc_matrix\n def cons_H_sparse(x, v):\n...     return v[0]*csc_matrix([[2, 0], [0, 0]]) + v[1]*csc_matrix([[2, 0], [0, 0]])\n nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1,\n...                                            jac=cons_J, hess=cons_H_sparse)",
            "code"
        ],
        [
            "or as a LinearOperator object.",
            "markdown"
        ],
        [
            "from scipy.sparse.linalg import LinearOperator\n def cons_H_linear_operator(x, v):\n...     def matvec(p):\n...         return np.array([p[0]*2*(v[0]+v[1]), 0])\n...     return LinearOperator((2, 2), matvec=matvec)\n nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1,\n...                                           jac=cons_J, hess=cons_H_linear_operator)",
            "code"
        ],
        [
            "When the evaluation of the Hessian \\(H(x, v)\\)\nis difficult to implement or computationally infeasible, one may use HessianUpdateStrategy.\nCurrently available strategies are BFGS and SR1.",
            "markdown"
        ],
        [
            "from scipy.optimize import BFGS\n nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess=BFGS())",
            "code"
        ],
        [
            "Alternatively, the Hessian may be approximated using finite differences.",
            "markdown"
        ],
        [
            "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess='2-point')",
            "code"
        ],
        [
            "The Jacobian of the constraints can be approximated by finite differences as well. In this case,\nhowever, the Hessian cannot be computed with finite differences and needs to\nbe provided by the user or defined using HessianUpdateStrategy.",
            "markdown"
        ],
        [
            "nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac='2-point', hess=BFGS())",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Solving the Optimization Problem:": [
        [
            "The optimization problem is solved using:",
            "markdown"
        ],
        [
            "x0 = np.array([0.5, 0])\n res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hess=rosen_hess,\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 1.11e-16, execution time: 0.016 s.\n print(res.x)\n[0.41494531 0.17010937]",
            "code"
        ],
        [
            "When needed, the objective function Hessian can be defined using a LinearOperator object,",
            "markdown"
        ],
        [
            "def rosen_hess_linop(x):\n...     def matvec(p):\n...         return rosen_hess_p(x, p)\n...     return LinearOperator((2, 2), matvec=matvec)\n res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hess=rosen_hess_linop,\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 1.11e-16, execution time: 0.018 s.\n print(res.x)\n[0.41494531 0.17010937]",
            "code"
        ],
        [
            "or a Hessian-vector product through the parameter hessp.",
            "markdown"
        ],
        [
            "res = minimize(rosen, x0, method='trust-constr', jac=rosen_der, hessp=rosen_hess_p,\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 8, CG iterations: 7, optimality: 2.99e-09, constraint violation: 1.11e-16, execution time: 0.018 s.\n print(res.x)\n[0.41494531 0.17010937]",
            "code"
        ],
        [
            "Alternatively, the first and second derivatives of the objective function can be approximated.\nFor instance,  the Hessian can be approximated with SR1 quasi-Newton approximation\nand the gradient with finite differences.",
            "markdown"
        ],
        [
            "from scipy.optimize import SR1\n res = minimize(rosen, x0, method='trust-constr',  jac=\"2-point\", hess=SR1(),\n...                constraints=[linear_constraint, nonlinear_constraint],\n...                options={'verbose': 1}, bounds=bounds)\n# may vary\n`gtol` termination condition is satisfied.\nNumber of iterations: 12, function evaluations: 24, CG iterations: 7, optimality: 4.48e-09, constraint violation: 0.00e+00, execution time: 0.016 s.\n print(res.x)\n[0.41494531 0.17010937]\n\n\n\n\n[TRIP]",
            "code"
        ],
        [
            "Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\nAn interior point algorithm for large-scale nonlinear  programming.\nSIAM Journal on Optimization 9.4: 877-900.\n\n\n[EQSQP]",
            "markdown"
        ],
        [
            "Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\nimplementation of an algorithm for large-scale equality constrained\noptimization. SIAM Journal on Optimization 8.3: 682-706.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Sequential Least SQuares Programming (SLSQP) Algorithm (method='SLSQP')": [
        [
            "The SLSQP method deals with constrained minimization problems of the form:\n\n  \\begin{eqnarray*} \\min_x & f(x) \\\\\n       \\text{subject to: } & c_j(x) =  0  ,  &j \\in \\mathcal{E}\\\\\n         & c_j(x) \\geq 0  ,  &j \\in \\mathcal{I}\\\\\n        &  \\text{lb}_i  \\leq x_i \\leq \\text{ub}_i , &i = 1,...,N. \\end{eqnarray*}",
            "markdown"
        ],
        [
            "Where \\(\\mathcal{E}\\) or \\(\\mathcal{I}\\) are sets of indices\ncontaining equality and inequality constraints.",
            "markdown"
        ],
        [
            "Both linear and nonlinear constraints are defined as dictionaries with keys type, fun and jac.",
            "markdown"
        ],
        [
            "ineq_cons = {'type': 'ineq',\n...              'fun' : lambda x: np.array([1 - x[0] - 2*x[1],\n...                                          1 - x[0]**2 - x[1],\n...                                          1 - x[0]**2 + x[1]]),\n...              'jac' : lambda x: np.array([[-1.0, -2.0],\n...                                          [-2*x[0], -1.0],\n...                                          [-2*x[0], 1.0]])}\n eq_cons = {'type': 'eq',\n...            'fun' : lambda x: np.array([2*x[0] + x[1] - 1]),\n...            'jac' : lambda x: np.array([2.0, 1.0])}",
            "code"
        ],
        [
            "And the optimization problem is solved with:",
            "markdown"
        ],
        [
            "x0 = np.array([0.5, 0])\n res = minimize(rosen, x0, method='SLSQP', jac=rosen_der,\n...                constraints=[eq_cons, ineq_cons], options={'ftol': 1e-9, 'disp': True},\n...                bounds=bounds)\n# may vary\nOptimization terminated successfully.    (Exit mode 0)\n            Current function value: 0.342717574857755\n            Iterations: 5\n            Function evaluations: 6\n            Gradient evaluations: 5\n print(res.x)\n[0.41494475 0.1701105 ]",
            "code"
        ],
        [
            "Most of the options available for the method 'trust-constr' are not available\nfor 'SLSQP'.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Global optimization": [
        [
            "Global optimization aims to find the global minimum of a function within given\nbounds, in the presence of potentially many local minima. Typically, global\nminimizers efficiently search the parameter space, while using a local\nminimizer (e.g., minimize) under the hood.  SciPy contains a\nnumber of good global optimizers.  Here, we\u00e2\u0080\u0099ll use those on the same objective\nfunction, namely the (aptly named) eggholder function:",
            "markdown"
        ],
        [
            "def eggholder(x):\n...     return (-(x[1] + 47) * np.sin(np.sqrt(abs(x[0]/2 + (x[1]  + 47))))\n...             -x[0] * np.sin(np.sqrt(abs(x[0] - (x[1]  + 47)))))\n\n bounds = [(-512, 512), (-512, 512)]",
            "code"
        ],
        [
            "This function looks like an egg carton:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n from mpl_toolkits.mplot3d import Axes3D\n\n x = np.arange(-512, 513)\n y = np.arange(-512, 513)\n xgrid, ygrid = np.meshgrid(x, y)\n xy = np.stack([xgrid, ygrid])\n\n fig = plt.figure()\n ax = fig.add_subplot(111, projection='3d')\n ax.view_init(45, -45)\n ax.plot_surface(xgrid, ygrid, eggholder(xy), cmap='terrain')\n ax.set_xlabel('x')\n ax.set_ylabel('y')\n ax.set_zlabel('eggholder(x, y)')\n plt.show()\n\n\n<figure class=\"align-center\">\n<img alt=\"&quot;A 3-D plot shown from a three-quarter view. The function is very noisy with dozens of valleys and peaks. There is no clear min or max discernable from this view and it's not possible to see all the local peaks and valleys from this view.&quot;\" class=\"plot-directive\" src=\"../_images/optimize_global_2.png\"/>\n</figure>",
            "code"
        ],
        [
            "We now use the global optimizers to obtain the minimum and the function value\nat the minimum. We\u00e2\u0080\u0099ll store the results in a dictionary so we can compare\ndifferent optimization results later.",
            "markdown"
        ],
        [
            "from scipy import optimize\n results = dict()\n results['shgo'] = optimize.shgo(eggholder, bounds)\n results['shgo']\n     fun: -935.3379515604197  # may vary\n    funl: array([-935.33795156])\n message: 'Optimization terminated successfully.'\n    nfev: 42\n     nit: 2\n   nlfev: 37\n   nlhev: 0\n   nljev: 9\n success: True\n       x: array([439.48096952, 453.97740589])\n      xl: array([[439.48096952, 453.97740589]])",
            "code"
        ],
        [
            "results['DA'] = optimize.dual_annealing(eggholder, bounds)\n results['DA']\n     fun: -956.9182316237413  # may vary\n message: ['Maximum number of iteration reached']\n    nfev: 4091\n    nhev: 0\n     nit: 1000\n    njev: 0\n       x: array([482.35324114, 432.87892901])",
            "code"
        ],
        [
            "All optimizers return an OptimizeResult, which in addition to the solution\ncontains information on the number of function evaluations, whether the\noptimization was successful, and more.  For brevity, we won\u00e2\u0080\u0099t show the full\noutput of the other optimizers:",
            "markdown"
        ],
        [
            "results['DE'] = optimize.differential_evolution(eggholder, bounds)",
            "code"
        ],
        [
            "shgo has a second method, which returns all local minima rather than\nonly what it thinks is the global minimum:",
            "markdown"
        ],
        [
            "results['shgo_sobol'] = optimize.shgo(eggholder, bounds, n=200, iters=5,\n...                                       sampling_method='sobol')",
            "code"
        ],
        [
            "We\u00e2\u0080\u0099ll now plot all found minima on a heatmap of the function:",
            "markdown"
        ],
        [
            "fig = plt.figure()\n ax = fig.add_subplot(111)\n im = ax.imshow(eggholder(xy), interpolation='bilinear', origin='lower',\n...                cmap='gray')\n ax.set_xlabel('x')\n ax.set_ylabel('y')\n\n def plot_point(res, marker='o', color=None):\n...     ax.plot(512+res.x[0], 512+res.x[1], marker=marker, color=color, ms=10)\n\n plot_point(results['DE'], color='c')  # differential_evolution - cyan\n plot_point(results['DA'], color='w')  # dual_annealing.        - white\n\n # SHGO produces multiple minima, plot them all (with a smaller marker size)\n plot_point(results['shgo'], color='r', marker='+')\n plot_point(results['shgo_sobol'], color='r', marker='x')\n for i in range(results['shgo_sobol'].xl.shape[0]):\n...     ax.plot(512 + results['shgo_sobol'].xl[i, 0],\n...             512 + results['shgo_sobol'].xl[i, 1],\n...             'ro', ms=2)\n\n ax.set_xlim([-4, 514*2])\n ax.set_ylim([-4, 514*2])\n plt.show()\n\n\n<figure class=\"align-center\">\n<img alt=\"&quot;This X-Y plot is a heatmap with the Z value denoted with the lowest points as black and the highest values as white. The image resembles a chess board rotated 45 degrees but heavily smoothed. A red dot is located at many of the minima on the grid resulting from the SHGO optimizer. SHGO shows the global minima as a red X in the top right. A local minima found with dual annealing is a white circle marker in the top left. A different local minima found with basinhopping is a yellow marker in the top center. The code is plotting the differential evolution result as a cyan circle, but it is not visible on the plot. At a glance it's not clear which of these valleys is the true global minima.&quot;\" class=\"plot-directive\" src=\"../_images/optimize_global_1.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Least-squares minimization (least_squares)": [
        [
            "SciPy is capable of solving robustified bound-constrained nonlinear\nleast-squares problems:\n\n\\begin{align}\n&\\min_\\mathbf{x} \\frac{1}{2} \\sum_{i = 1}^m \\rho\\left(f_i(\\mathbf{x})^2\\right) \\\\\n&\\text{subject to }\\mathbf{lb} \\leq \\mathbf{x} \\leq \\mathbf{ub}\n\\end{align}",
            "markdown"
        ],
        [
            "Here \\(f_i(\\mathbf{x})\\) are smooth functions from\n\\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\), we refer to them as residuals.\nThe purpose of a scalar-valued function \\(\\rho(\\cdot)\\) is to reduce the\ninfluence of outlier residuals and contribute to robustness of the solution,\nwe refer to it as a loss function. A linear loss function gives a standard\nleast-squares problem. Additionally, constraints in a form of lower and upper\nbounds on some of \\(x_j\\) are allowed.",
            "markdown"
        ],
        [
            "All methods specific to least-squares minimization utilize a \\(m \\times n\\)\nmatrix of partial derivatives called Jacobian and defined as\n\\(J_{ij} = \\partial f_i / \\partial x_j\\). It is highly recommended to\ncompute this matrix analytically and pass it to least_squares,\notherwise, it will be estimated by finite differences, which takes a lot of\nadditional time and can be very inaccurate in hard cases.",
            "markdown"
        ],
        [
            "Function least_squares can be used for fitting a function\n\\(\\varphi(t; \\mathbf{x})\\) to empirical data \\(\\{(t_i, y_i), i = 0, \\ldots, m-1\\}\\).\nTo do this, one should simply precompute residuals as\n\\(f_i(\\mathbf{x}) = w_i (\\varphi(t_i; \\mathbf{x}) - y_i)\\), where \\(w_i\\)\nare weights assigned to each observation.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Least-squares minimization (least_squares)->Example of solving a fitting problem": [
        [
            "Here we consider an enzymatic reaction [1]. There are 11 residuals defined as\n\n\\[f_i(x) = \\frac{x_0 (u_i^2 + u_i x_1)}{u_i^2 + u_i x_2 + x_3} - y_i, \\quad i = 0, \\ldots, 10,\\]",
            "markdown"
        ],
        [
            "where \\(y_i\\) are measurement values and \\(u_i\\) are values of\nthe independent variable. The unknown vector of parameters is\n\\(\\mathbf{x} = (x_0, x_1, x_2, x_3)^T\\). As was said previously, it is\nrecommended to compute Jacobian matrix in a closed form:\n\n \\begin{align}\n &J_{i0} = \\frac{\\partial f_i}{\\partial x_0} = \\frac{u_i^2 + u_i x_1}{u_i^2 + u_i x_2 + x_3} \\\\\n &J_{i1} = \\frac{\\partial f_i}{\\partial x_1} = \\frac{u_i x_0}{u_i^2 + u_i x_2 + x_3} \\\\\n &J_{i2} = \\frac{\\partial f_i}{\\partial x_2} = -\\frac{x_0 (u_i^2 + u_i x_1) u_i}{(u_i^2 + u_i x_2 + x_3)^2} \\\\\n &J_{i3} = \\frac{\\partial f_i}{\\partial x_3} = -\\frac{x_0 (u_i^2 + u_i x_1)}{(u_i^2 + u_i x_2 + x_3)^2}\n \\end{align}",
            "markdown"
        ],
        [
            "We are going to use the \u00e2\u0080\u009chard\u00e2\u0080\u009d starting point defined in [2]. To find a\nphysically meaningful solution, avoid potential division by zero and assure\nconvergence to the global minimum we impose constraints\n\\(0 \\leq x_j \\leq 100, j = 0, 1, 2, 3\\).",
            "markdown"
        ],
        [
            "The code below implements least-squares estimation of \\(\\mathbf{x}\\) and\nfinally plots the original data and the fitted model function:",
            "markdown"
        ],
        [
            "from scipy.optimize import least_squares",
            "code"
        ],
        [
            "def model(x, u):\n...     return x[0] * (u ** 2 + x[1] * u) / (u ** 2 + x[2] * u + x[3])",
            "code"
        ],
        [
            "def fun(x, u, y):\n...     return model(x, u) - y",
            "code"
        ],
        [
            "def jac(x, u, y):\n...     J = np.empty((u.size, x.size))\n...     den = u ** 2 + x[2] * u + x[3]\n...     num = u ** 2 + x[1] * u\n...     J[:, 0] = num / den\n...     J[:, 1] = x[0] * u / den\n...     J[:, 2] = -x[0] * num * u / den ** 2\n...     J[:, 3] = -x[0] * num / den ** 2\n...     return J",
            "code"
        ],
        [
            "u = np.array([4.0, 2.0, 1.0, 5.0e-1, 2.5e-1, 1.67e-1, 1.25e-1, 1.0e-1,\n...               8.33e-2, 7.14e-2, 6.25e-2])\n y = np.array([1.957e-1, 1.947e-1, 1.735e-1, 1.6e-1, 8.44e-2, 6.27e-2,\n...               4.56e-2, 3.42e-2, 3.23e-2, 2.35e-2, 2.46e-2])\n x0 = np.array([2.5, 3.9, 4.15, 3.9])\n res = least_squares(fun, x0, jac=jac, bounds=(0, 100), args=(u, y), verbose=1)\n# may vary\n`ftol` termination condition is satisfied.\nFunction evaluations 130, initial cost 4.4383e+00, final cost 1.5375e-04, first-order optimality 4.92e-08.\n res.x\narray([ 0.19280596,  0.19130423,  0.12306063,  0.13607247])",
            "code"
        ],
        [
            "import matplotlib.pyplot as plt\n u_test = np.linspace(0, 5)\n y_test = model(res.x, u_test)\n plt.plot(u, y, 'o', markersize=4, label='data')\n plt.plot(u_test, y_test, label='fitted model')\n plt.xlabel(\"u\")\n plt.ylabel(\"y\")\n plt.legend(loc='lower right')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code plots an X-Y time-series. The series starts in the lower left at (0, 0) and rapidly trends up to the maximum of 0.2 then flattens out. The fitted model is shown as a smooth orange trace and is well fit to the data.\"' class=\"plot-directive\" src=\"../_images/optimize-1.png\"/>\n</figure>\n<aside class=\"footnote-list brackets\">\n<aside class=\"footnote brackets\" id=\"id15\" role=\"note\">\n[1]",
            "code"
        ],
        [
            "J. Kowalik and J. F. Morrison, \u00e2\u0080\u009cAnalysis of kinetic data for allosteric enzyme reactions as\na nonlinear regression problem\u00e2\u0080\u009d, Math. Biosci., vol. 2, pp. 57-66, 1968.\n</aside>\n<aside class=\"footnote brackets\" id=\"id16\" role=\"note\">\n[2]",
            "markdown"
        ],
        [
            "Averick et al., \u00e2\u0080\u009cThe MINPACK-2 Test Problem Collection\u00e2\u0080\u009d.\n\n\n\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Least-squares minimization (least_squares)->Further examples": [
        [
            "Three interactive examples below illustrate usage of least_squares in\ngreater detail.",
            "markdown"
        ],
        [
            "Large-scale bundle adjustment in scipy\ndemonstrates large-scale capabilities of least_squares and how to\nefficiently compute finite difference approximation of sparse Jacobian.",
            "markdown"
        ],
        [
            "Robust nonlinear regression in scipy\nshows how to handle outliers with a robust loss function in a nonlinear\nregression.",
            "markdown"
        ],
        [
            "Solving a discrete boundary-value problem in scipy\nexamines how to solve a large system of equations and use bounds to achieve\ndesired properties of the solution.",
            "markdown"
        ],
        [
            "For the details about mathematical algorithms behind the implementation refer\nto documentation of least_squares.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Univariate function minimizers (minimize_scalar)": [
        [
            "Often only the minimum of an univariate function (i.e., a function that\ntakes a scalar as input) is needed. In these circumstances, other\noptimization techniques have been developed that can work faster. These are\naccessible from the minimize_scalar function, which proposes several\nalgorithms.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Univariate function minimizers (minimize_scalar)->Unconstrained minimization (method='brent')": [
        [
            "There are, actually, two methods that can be used to minimize an univariate\nfunction: brent and golden, but golden is included only for academic\npurposes and should rarely be used. These can be respectively selected\nthrough the <em class=\"xref py py-obj\">method parameter in minimize_scalar. The brent\nmethod uses Brent\u00e2\u0080\u0099s algorithm for locating a minimum. Optimally, a bracket\n(the bracket parameter) should be given which contains the minimum desired. A\nbracket is a triple \\(\\left( a, b, c \\right)\\) such that \\(f\n\\left( a \\right) > f \\left( b \\right) &lt; f \\left( c \\right)\\) and \\(a &lt;\nb &lt; c\\) . If this is not given, then alternatively two starting points can\nbe chosen and a bracket will be found from these points using a simple\nmarching algorithm. If these two starting points are not provided, <em class=\"xref py py-obj\">0 and\n<em class=\"xref py py-obj\">1 will be used (this may not be the right choice for your function and\nresult in an unexpected minimum being returned).",
            "markdown"
        ],
        [
            "Here is an example:",
            "markdown"
        ],
        [
            "from scipy.optimize import minimize_scalar\n f = lambda x: (x - 2) * (x + 1)**2\n res = minimize_scalar(f, method='brent')\n print(res.x)\n1.0",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Univariate function minimizers (minimize_scalar)->Bounded minimization (method='bounded')": [
        [
            "Very often, there are constraints that can be placed on the solution space\nbefore minimization occurs. The <em class=\"xref py py-obj\">bounded method in minimize_scalar\nis an example of a constrained minimization procedure that provides a\nrudimentary interval constraint for scalar functions. The interval\nconstraint allows the minimization to occur only between two fixed\nendpoints, specified using the mandatory <em class=\"xref py py-obj\">bounds parameter.",
            "markdown"
        ],
        [
            "For example, to find the minimum of \\(J_{1}\\left( x \\right)\\) near\n\\(x=5\\) , minimize_scalar can be called using the interval\n\\(\\left[ 4, 7 \\right]\\) as a constraint. The result is\n\\(x_{\\textrm{min}}=5.3314\\) :",
            "markdown"
        ],
        [
            "from scipy.special import j1\n res = minimize_scalar(j1, bounds=(4, 7), method='bounded')\n res.x\n5.33144184241",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Custom minimizers": [
        [
            "Sometimes, it may be useful to use a custom method as a (multivariate\nor univariate) minimizer, for example, when using some library wrappers\nof minimize (e.g., basinhopping).",
            "markdown"
        ],
        [
            "We can achieve that by, instead of passing a method name, passing\na callable (either a function or an object implementing a <em class=\"xref py py-obj\">__call__\nmethod) as the <em class=\"xref py py-obj\">method parameter.",
            "markdown"
        ],
        [
            "Let us consider an (admittedly rather virtual) need to use a trivial\ncustom multivariate minimization method that will just search the\nneighborhood in each dimension independently with a fixed step size:",
            "markdown"
        ],
        [
            "from scipy.optimize import OptimizeResult\n def custmin(fun, x0, args=(), maxfev=None, stepsize=0.1,\n...         maxiter=100, callback=None, **options):\n...     bestx = x0\n...     besty = fun(x0)\n...     funcalls = 1\n...     niter = 0\n...     improved = True\n...     stop = False\n...\n...     while improved and not stop and niter &lt; maxiter:\n...         improved = False\n...         niter += 1\n...         for dim in range(np.size(x0)):\n...             for s in [bestx[dim] - stepsize, bestx[dim] + stepsize]:\n...                 testx = np.copy(bestx)\n...                 testx[dim] = s\n...                 testy = fun(testx, *args)\n...                 funcalls += 1\n...                 if testy &lt; besty:\n...                     besty = testy\n...                     bestx = testx\n...                     improved = True\n...             if callback is not None:\n...                 callback(bestx)\n...             if maxfev is not None and funcalls = maxfev:\n...                 stop = True\n...                 break\n...\n...     return OptimizeResult(fun=besty, x=bestx, nit=niter,\n...                           nfev=funcalls, success=(niter  1))\n x0 = [1.35, 0.9, 0.8, 1.1, 1.2]\n res = minimize(rosen, x0, method=custmin, options=dict(stepsize=0.05))\n res.x\narray([1., 1., 1., 1., 1.])",
            "code"
        ],
        [
            "This will work just as well in case of univariate optimization:",
            "markdown"
        ],
        [
            "def custmin(fun, bracket, args=(), maxfev=None, stepsize=0.1,\n...         maxiter=100, callback=None, **options):\n...     bestx = (bracket[1] + bracket[0]) / 2.0\n...     besty = fun(bestx)\n...     funcalls = 1\n...     niter = 0\n...     improved = True\n...     stop = False\n...\n...     while improved and not stop and niter &lt; maxiter:\n...         improved = False\n...         niter += 1\n...         for testx in [bestx - stepsize, bestx + stepsize]:\n...             testy = fun(testx, *args)\n...             funcalls += 1\n...             if testy &lt; besty:\n...                 besty = testy\n...                 bestx = testx\n...                 improved = True\n...         if callback is not None:\n...             callback(bestx)\n...         if maxfev is not None and funcalls = maxfev:\n...             stop = True\n...             break\n...\n...     return OptimizeResult(fun=besty, x=bestx, nit=niter,\n...                           nfev=funcalls, success=(niter  1))\n def f(x):\n...    return (x - 2)**2 * (x + 2)**2\n res = minimize_scalar(f, bracket=(-3.5, 0), method=custmin,\n...                       options=dict(stepsize = 0.05))\n res.x\n-2.0",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Root finding->Scalar functions": [
        [
            "If one has a single-variable equation, there are multiple different root\nfinding algorithms that can be tried. Most of these algorithms require the\nendpoints of an interval in which a root is expected (because the function\nchanges signs). In general, brentq is the best choice, but the other\nmethods may be useful in certain circumstances or for academic purposes.\nWhen a bracket is not available, but one or more derivatives are available,\nthen newton (or halley, secant) may be applicable.\nThis is especially the case if the function is defined on a subset of the\ncomplex plane, and the bracketing methods cannot be used.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Root finding->Fixed-point solving": [
        [
            "A problem closely related to finding the zeros of a function is the\nproblem of finding a fixed point of a function. A fixed point of a\nfunction is the point at which evaluation of the function returns the\npoint: \\(g\\left(x\\right)=x.\\) Clearly, the fixed point of \\(g\\)\nis the root of \\(f\\left(x\\right)=g\\left(x\\right)-x.\\)\nEquivalently, the root of \\(f\\) is the fixed point of\n\\(g\\left(x\\right)=f\\left(x\\right)+x.\\) The routine\nfixed_point provides a simple iterative method using Aitkens\nsequence acceleration to estimate the fixed point of \\(g\\) given a\nstarting point.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Root finding->Sets of equations": [
        [
            "Finding a root of a set of non-linear equations can be achieved using the\nroot function. Several methods are available, amongst which hybr\n(the default) and lm, which, respectively, use the hybrid method of Powell\nand the Levenberg-Marquardt method from MINPACK.",
            "markdown"
        ],
        [
            "The following example considers the single-variable transcendental\nequation\n\n\\[x+2\\cos\\left(x\\right)=0,\\]",
            "markdown"
        ],
        [
            "a root of which can be found as follows:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.optimize import root\n def func(x):\n...     return x + 2 * np.cos(x)\n sol = root(func, 0.3)\n sol.x\narray([-1.02986653])\n sol.fun\narray([ -6.66133815e-16])",
            "code"
        ],
        [
            "Consider now a set of non-linear equations\n\n \\begin{eqnarray*}\n x_{0}\\cos\\left(x_{1}\\right) & = & 4,\\\\\n x_{0}x_{1}-x_{1} & = & 5.\n \\end{eqnarray*}",
            "markdown"
        ],
        [
            "We define the objective function so that it also returns the Jacobian and\nindicate this by setting the jac parameter to True. Also, the\nLevenberg-Marquardt solver is used here.",
            "markdown"
        ],
        [
            "def func2(x):\n...     f = [x[0] * np.cos(x[1]) - 4,\n...          x[1]*x[0] - x[1] - 5]\n...     df = np.array([[np.cos(x[1]), -x[0] * np.sin(x[1])],\n...                    [x[1], x[0] - 1]])\n...     return f, df\n sol = root(func2, [1, 1], jac=True, method='lm')\n sol.x\narray([ 6.50409711,  0.90841421])",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Root finding->Root finding for large problems": [
        [
            "Methods hybr and lm in root cannot deal with a very large\nnumber of variables (N), as they need to calculate and invert a dense N\nx N Jacobian matrix on every Newton step. This becomes rather inefficient\nwhen N grows.",
            "markdown"
        ],
        [
            "Consider, for instance, the following problem: we need to solve the\nfollowing integrodifferential equation on the square\n\\([0,1]\\times[0,1]\\):\n\n\\[(\\partial_x^2 + \\partial_y^2) P + 5 \\left(\\int_0^1\\int_0^1\\cosh(P)\\,dx\\,dy\\right)^2 = 0\\]",
            "markdown"
        ],
        [
            "with the boundary condition \\(P(x,1) = 1\\) on the upper edge and\n\\(P=0\\) elsewhere on the boundary of the square. This can be done\nby approximating the continuous function P by its values on a grid,\n\\(P_{n,m}\\approx{}P(n h, m h)\\), with a small grid spacing\nh. The derivatives and integrals can then be approximated; for\ninstance \\(\\partial_x^2 P(x,y)\\approx{}(P(x+h,y) - 2 P(x,y) +\nP(x-h,y))/h^2\\). The problem is then equivalent to finding the root of\nsome function residual(P), where P is a vector of length\n\\(N_x N_y\\).",
            "markdown"
        ],
        [
            "Now, because \\(N_x N_y\\) can be large, methods hybr or lm in\nroot will take a long time to solve this problem. The solution can,\nhowever, be found using one of the large-scale solvers, for example\nkrylov, broyden2, or anderson. These use what is known as the\ninexact Newton method, which instead of computing the Jacobian matrix\nexactly, forms an approximation for it.",
            "markdown"
        ],
        [
            "The problem we have can now be solved as follows:",
            "markdown"
        ],
        [
            "import numpy as np\nfrom scipy.optimize import root\nfrom numpy import cosh, zeros_like, mgrid, zeros\n\n# parameters\nnx, ny = 75, 75\nhx, hy = 1./(nx-1), 1./(ny-1)\n\nP_left, P_right = 0, 0\nP_top, P_bottom = 1, 0\n\ndef residual(P):\n   d2x = zeros_like(P)\n   d2y = zeros_like(P)\n\n   d2x[1:-1] = (P[2:]   - 2*P[1:-1] + P[:-2]) / hx/hx\n   d2x[0]    = (P[1]    - 2*P[0]    + P_left)/hx/hx\n   d2x[-1]   = (P_right - 2*P[-1]   + P[-2])/hx/hx\n\n   d2y[:,1:-1] = (P[:,2:] - 2*P[:,1:-1] + P[:,:-2])/hy/hy\n   d2y[:,0]    = (P[:,1]  - 2*P[:,0]    + P_bottom)/hy/hy\n   d2y[:,-1]   = (P_top   - 2*P[:,-1]   + P[:,-2])/hy/hy\n\n   return d2x + d2y + 5*cosh(P).mean()**2\n\n# solve\nguess = zeros((nx, ny), float)\nsol = root(residual, guess, method='krylov', options={'disp': True})\n#sol = root(residual, guess, method='broyden2', options={'disp': True, 'max_rank': 50})\n#sol = root(residual, guess, method='anderson', options={'disp': True, 'M': 10})\nprint('Residual: %g' % abs(residual(sol.x)).max())\n\n# visualize\nimport matplotlib.pyplot as plt\nx, y = mgrid[0:1:(nx*1j), 0:1:(ny*1j)]\nplt.pcolormesh(x, y, sol.x, shading='gouraud')\nplt.colorbar()\nplt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates a 2-D heatmap with Z values from 0 to 1. The graph resembles a smooth, dark blue-green, U shape, with an open yellow top. The right, bottom, and left edges have a value near zero and the top has a value close to 1. The center of the solution space has a value close to 0.8.\"' class=\"plot-directive\" src=\"../_images/optimize-2.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Root finding->Still too slow? Preconditioning.": [
        [
            "When looking for the zero of the functions \\(f_i({\\bf x}) = 0\\),\ni = 1, 2, \u00e2\u0080\u00a6, N, the krylov solver spends most of the\ntime inverting the Jacobian matrix,\n\n\\[J_{ij} = \\frac{\\partial f_i}{\\partial x_j} .\\]",
            "markdown"
        ],
        [
            "If you have an approximation for the inverse matrix\n\\(M\\approx{}J^{-1}\\), you can use it for preconditioning the\nlinear-inversion problem. The idea is that instead of solving\n\\(J{\\bf s}={\\bf y}\\) one solves \\(MJ{\\bf s}=M{\\bf y}\\): since\nmatrix \\(MJ\\) is \u00e2\u0080\u009ccloser\u00e2\u0080\u009d to the identity matrix than \\(J\\)\nis, the equation should be easier for the Krylov method to deal with.",
            "markdown"
        ],
        [
            "The matrix M can be passed to root with method krylov as an\noption options['jac_options']['inner_M']. It can be a (sparse) matrix\nor a scipy.sparse.linalg.LinearOperator instance.",
            "markdown"
        ],
        [
            "For the problem in the previous section, we note that the function to\nsolve consists of two parts: the first one is the application of the\nLaplace operator, \\([\\partial_x^2 + \\partial_y^2] P\\), and the second\nis the integral. We can actually easily compute the Jacobian corresponding\nto the Laplace operator part: we know that in 1-D\n\n\\[\\begin{split}\\partial_x^2 \\approx \\frac{1}{h_x^2} \\begin{pmatrix}\n-2 & 1 & 0 & 0 \\cdots \\\\\n1 & -2 & 1 & 0 \\cdots \\\\\n0 & 1 & -2 & 1 \\cdots \\\\\n\\ldots\n\\end{pmatrix}\n= h_x^{-2} L\\end{split}\\]",
            "markdown"
        ],
        [
            "so that the whole 2-D operator is represented by\n\n\\[J_1 = \\partial_x^2 + \\partial_y^2\n\\simeq\nh_x^{-2} L \\otimes I + h_y^{-2} I \\otimes L\\]",
            "markdown"
        ],
        [
            "The matrix \\(J_2\\) of the Jacobian corresponding to the integral\nis more difficult to calculate, and since all of it entries are\nnonzero, it will be difficult to invert. \\(J_1\\) on the other hand\nis a relatively simple matrix, and can be inverted by\nscipy.sparse.linalg.splu (or the inverse can be approximated by\nscipy.sparse.linalg.spilu). So we are content to take\n\\(M\\approx{}J_1^{-1}\\) and hope for the best.",
            "markdown"
        ],
        [
            "In the example below, we use the preconditioner \\(M=J_1^{-1}\\).",
            "markdown"
        ],
        [
            "import numpy as np\nfrom scipy.optimize import root\nfrom scipy.sparse import spdiags, kron\nfrom scipy.sparse.linalg import spilu, LinearOperator\nfrom numpy import cosh, zeros_like, mgrid, zeros, eye\n\n# parameters\nnx, ny = 75, 75\nhx, hy = 1./(nx-1), 1./(ny-1)\n\nP_left, P_right = 0, 0\nP_top, P_bottom = 1, 0\n\ndef get_preconditioner():\n    \"\"\"Compute the preconditioner M\"\"\"\n    diags_x = zeros((3, nx))\n    diags_x[0,:] = 1/hx/hx\n    diags_x[1,:] = -2/hx/hx\n    diags_x[2,:] = 1/hx/hx\n    Lx = spdiags(diags_x, [-1,0,1], nx, nx)\n\n    diags_y = zeros((3, ny))\n    diags_y[0,:] = 1/hy/hy\n    diags_y[1,:] = -2/hy/hy\n    diags_y[2,:] = 1/hy/hy\n    Ly = spdiags(diags_y, [-1,0,1], ny, ny)\n\n    J1 = kron(Lx, eye(ny)) + kron(eye(nx), Ly)\n\n    # Now we have the matrix `J_1`. We need to find its inverse `M` --\n    # however, since an approximate inverse is enough, we can use\n    # the *incomplete LU* decomposition\n\n    J1_ilu = spilu(J1)\n\n    # This returns an object with a method .solve() that evaluates\n    # the corresponding matrix-vector product. We need to wrap it into\n    # a LinearOperator before it can be passed to the Krylov methods:\n\n    M = LinearOperator(shape=(nx*ny, nx*ny), matvec=J1_ilu.solve)\n    return M\n\ndef solve(preconditioning=True):\n    \"\"\"Compute the solution\"\"\"\n    count = [0]\n\n    def residual(P):\n        count[0] += 1\n\n        d2x = zeros_like(P)\n        d2y = zeros_like(P)\n\n        d2x[1:-1] = (P[2:]   - 2*P[1:-1] + P[:-2])/hx/hx\n        d2x[0]    = (P[1]    - 2*P[0]    + P_left)/hx/hx\n        d2x[-1]   = (P_right - 2*P[-1]   + P[-2])/hx/hx\n\n        d2y[:,1:-1] = (P[:,2:] - 2*P[:,1:-1] + P[:,:-2])/hy/hy\n        d2y[:,0]    = (P[:,1]  - 2*P[:,0]    + P_bottom)/hy/hy\n        d2y[:,-1]   = (P_top   - 2*P[:,-1]   + P[:,-2])/hy/hy\n\n        return d2x + d2y + 5*cosh(P).mean()**2\n\n    # preconditioner\n    if preconditioning:\n        M = get_preconditioner()\n    else:\n        M = None\n\n    # solve\n    guess = zeros((nx, ny), float)\n\n    sol = root(residual, guess, method='krylov',\n               options={'disp': True,\n                        'jac_options': {'inner_M': M}})\n    print('Residual', abs(residual(sol.x)).max())\n    print('Evaluations', count[0])\n\n    return sol.x\n\ndef main():\n    sol = solve(preconditioning=True)\n\n    # visualize\n    import matplotlib.pyplot as plt\n    x, y = mgrid[0:1:(nx*1j), 0:1:(ny*1j)]\n    plt.clf()\n    plt.pcolor(x, y, sol)\n    plt.clim(0, 1)\n    plt.colorbar()\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    main()",
            "code"
        ],
        [
            "Resulting run, first without preconditioning:",
            "markdown"
        ],
        [
            "0:  |F(x)| = 803.614; step 1; tol 0.000257947\n1:  |F(x)| = 345.912; step 1; tol 0.166755\n2:  |F(x)| = 139.159; step 1; tol 0.145657\n3:  |F(x)| = 27.3682; step 1; tol 0.0348109\n4:  |F(x)| = 1.03303; step 1; tol 0.00128227\n5:  |F(x)| = 0.0406634; step 1; tol 0.00139451\n6:  |F(x)| = 0.00344341; step 1; tol 0.00645373\n7:  |F(x)| = 0.000153671; step 1; tol 0.00179246\n8:  |F(x)| = 6.7424e-06; step 1; tol 0.00173256\nResidual 3.57078908664e-07\nEvaluations 317",
            "code"
        ],
        [
            "and then with preconditioning:",
            "markdown"
        ],
        [
            "0:  |F(x)| = 136.993; step 1; tol 7.49599e-06\n1:  |F(x)| = 4.80983; step 1; tol 0.00110945\n2:  |F(x)| = 0.195942; step 1; tol 0.00149362\n3:  |F(x)| = 0.000563597; step 1; tol 7.44604e-06\n4:  |F(x)| = 1.00698e-09; step 1; tol 2.87308e-12\nResidual 9.29603061195e-11\nEvaluations 77",
            "code"
        ],
        [
            "Using a preconditioner reduced the number of evaluations of the\nresidual function by a factor of 4. For problems where the\nresidual is expensive to compute, good preconditioning can be crucial\n\u00e2\u0080\u0094 it can even decide whether the problem is solvable in practice or\nnot.",
            "markdown"
        ],
        [
            "Preconditioning is an art, science, and industry. Here, we were lucky\nin making a simple choice that worked reasonably well, but there is a\nlot more depth to this topic than is shown here.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Linear programming (linprog)": [
        [
            "The function linprog can minimize a linear objective function\nsubject to linear equality and inequality constraints. This kind of\nproblem is well known as linear programming. Linear programming solves\nproblems of the following form:\n\n\\[\\begin{split}\\min_x \\ & c^T x \\\\\n\\mbox{such that} \\ & A_{ub} x \\leq b_{ub},\\\\\n& A_{eq} x = b_{eq},\\\\\n& l \\leq x \\leq u ,\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(x\\) is a vector of decision variables; \\(c\\), \\(b_{ub}\\),\n\\(b_{eq}\\), \\(l\\), and \\(u\\) are vectors; and \\(A_{ub}\\) and\n\\(A_{eq}\\) are matrices.",
            "markdown"
        ],
        [
            "In this tutorial, we will try to solve a typical linear programming\nproblem using linprog.",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Linear programming (linprog)->Linear programming example": [
        [
            "Consider the following simple linear programming problem:\n\n\\[\\begin{split}\\max_{x_1, x_2, x_3, x_4} \\ & 29x_1 + 45x_2 \\\\\n\\mbox{such that} \\\n& x_1 -x_2 -3x_3 \\leq 5\\\\\n& 2x_1 -3x_2 -7x_3 + 3x_4 \\geq 10\\\\\n& 2x_1 + 8x_2 + x_3 = 60\\\\\n& 4x_1 + 4x_2 + x_4 = 60\\\\\n& 0 \\leq x_0\\\\\n& 0 \\leq x_1 \\leq 5\\\\\n& x_2 \\leq 0.5\\\\\n& -3 \\leq x_3\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "We need some mathematical manipulations to convert the target problem to the form accepted by linprog.",
            "markdown"
        ],
        [
            "First of all, let\u00e2\u0080\u0099s consider the objective function.\nWe want to maximize the objective\nfunction, but linprog can only accept a minimization problem. This is easily remedied by converting the maximize\n\\(29x_1 + 45x_2\\) to minimizing \\(-29x_1 -45x_2\\). Also, \\(x_3, x_4\\) are not shown in the objective\nfunction. That means the weights corresponding with \\(x_3, x_4\\) are zero. So, the objective function can be\nconverted to:\n\n\\[\\min_{x_1, x_2, x_3, x_4} \\ -29x_1 -45x_2 + 0x_3 + 0x_4\\]",
            "markdown"
        ],
        [
            "If we define the vector of decision variables \\(x = [x_1, x_2, x_3, x_4]^T\\), the objective weights vector \\(c\\) of linprog in this problem\nshould be\n\n\\[c = [-29, -45, 0, 0]^T\\]",
            "markdown"
        ],
        [
            "Next, let\u00e2\u0080\u0099s consider the two inequality constraints. The first one is a \u00e2\u0080\u009cless than\u00e2\u0080\u009d inequality, so it is already in the form accepted by linprog.\nThe second one is a \u00e2\u0080\u009cgreater than\u00e2\u0080\u009d inequality, so we need to multiply both sides by \\(-1\\) to convert it to a \u00e2\u0080\u009cless than\u00e2\u0080\u009d inequality.\nExplicitly showing zero coefficients, we have:\n\n\\[\\begin{split}x_1 -x_2 -3x_3 + 0x_4  &\\leq 5\\\\\n-2x_1 + 3x_2 + 7x_3 - 3x_4 &\\leq -10\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "These equations can be converted to matrix form:\n\n\\[\\begin{split}A_{ub} x \\leq b_{ub}\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "where\n\n \\begin{equation*} A_{ub} =\n \\begin{bmatrix} 1 & -1 & -3 & 0 \\\\\n                 -2 & 3 & 7 & -3\n \\end{bmatrix}\n \\end{equation*}\n \\begin{equation*} b_{ub} =\n \\begin{bmatrix} 5 \\\\\n                 -10\n \\end{bmatrix}\n \\end{equation*}",
            "markdown"
        ],
        [
            "Next, let\u00e2\u0080\u0099s consider the two equality constraints. Showing zero weights explicitly, these are:\n\n\\[\\begin{split}2x_1 + 8x_2 + 1x_3 + 0x_4 &= 60\\\\\n4x_1 + 4x_2 + 0x_3 + 1x_4 &= 60\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "These equations can be converted to matrix form:\n\n\\[\\begin{split}A_{eq} x = b_{eq}\\\\\\end{split}\\]",
            "markdown"
        ],
        [
            "where\n\n \\begin{equation*} A_{eq} =\n \\begin{bmatrix} 2 & 8 & 1 & 0 \\\\\n                 4 & 4 & 0 & 1\n \\end{bmatrix}\n \\end{equation*}\n \\begin{equation*} b_{eq} =\n \\begin{bmatrix} 60 \\\\\n                 60\n \\end{bmatrix}\n \\end{equation*}",
            "markdown"
        ],
        [
            "Lastly, let\u00e2\u0080\u0099s consider the separate inequality constraints on individual decision variables, which are known as\n\u00e2\u0080\u009cbox constraints\u00e2\u0080\u009d or \u00e2\u0080\u009csimple bounds\u00e2\u0080\u009d. These constraints can be applied using the bounds argument of linprog.\nAs noted in the linprog documentation, the default value of bounds is (0, None), meaning that the\nlower bound on each decision variable is 0, and the upper bound on each decision variable is infinity:\nall the decision variables are non-negative. Our bounds are different, so we will need to specify the lower and upper bound on each\ndecision variable as a tuple and group these tuples into a list.",
            "markdown"
        ],
        [
            "Finally, we can solve the transformed problem using linprog.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.optimize import linprog\n c = np.array([-29.0, -45.0, 0.0, 0.0])\n A_ub = np.array([[1.0, -1.0, -3.0, 0.0],\n...                 [-2.0, 3.0, 7.0, -3.0]])\n b_ub = np.array([5.0, -10.0])\n A_eq = np.array([[2.0, 8.0, 1.0, 0.0],\n...                 [4.0, 4.0, 0.0, 1.0]])\n b_eq = np.array([60.0, 60.0])\n x0_bounds = (0, None)\n x1_bounds = (0, 5.0)\n x2_bounds = (-np.inf, 0.5)  # +/- np.inf can be used instead of None\n x3_bounds = (-3.0, None)\n bounds = [x0_bounds, x1_bounds, x2_bounds, x3_bounds]\n result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)\n print(result.message)\nThe problem is infeasible. (HiGHS Status 8: model_status is Infeasible; primal_status is At lower/fixed bound)",
            "code"
        ],
        [
            "The result states that our problem is infeasible, meaning that there is no solution vector that satisfies all the\nconstraints. That doesn\u00e2\u0080\u0099t necessarily mean we did anything wrong; some problems truly are infeasible.\nSuppose, however, that we were to decide that our bound constraint on \\(x_1\\) was too tight and that it could be loosened\nto \\(0 \\leq x_1 \\leq 6\\). After adjusting our code x1_bounds = (0, 6) to reflect the change and executing it again:",
            "markdown"
        ],
        [
            "x1_bounds = (0, 6)\n bounds = [x0_bounds, x1_bounds, x2_bounds, x3_bounds]\n result = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)\n print(result.message)\nOptimization terminated successfully. (HiGHS Status 7: Optimal)",
            "code"
        ],
        [
            "The result shows the optimization was successful.\nWe can check the objective value (result.fun) is same as \\(c^Tx\\):",
            "markdown"
        ],
        [
            "x = np.array(result.x)\n obj = result.fun\n print(c @ x)\n-505.97435889013434  # may vary\n print(obj)\n-505.97435889013434  # may vary",
            "code"
        ],
        [
            "We can also check that all constraints are satisfied within reasonable tolerances:",
            "markdown"
        ],
        [
            "print(b_ub - (A_ub @ x).flatten())  # this is equivalent to result.slack\n[ 6.52747190e-10, -2.26730279e-09]  # may vary\n print(b_eq - (A_eq @ x).flatten())  # this is equivalent to result.con\n[ 9.78840831e-09, 1.04662945e-08]]  # may vary\n print([0 &lt;= result.x[0], 0 &lt;= result.x[1] &lt;= 6.0, result.x[2] &lt;= 0.5, -3.0 &lt;= result.x[3]])\n[True, True, True, True]",
            "code"
        ]
    ],
    "Optimization (scipy.optimize)->Assignment problems->Linear sum assignment problem example": [
        [
            "Consider the problem of selecting students for a swimming medley relay team.\nWe have a table showing times for each swimming style of five students:",
            "markdown"
        ],
        [
            "We need to choose a student for each of the four swimming styles such that\nthe total relay time is minimized.\nThis is a typical linear sum assignment problem. We can use linear_sum_assignment to solve it.",
            "markdown"
        ],
        [
            "The linear sum assignment problem is one of the most famous combinatorial optimization problems.\nGiven a \u00e2\u0080\u009ccost matrix\u00e2\u0080\u009d \\(C\\), the problem is to choose",
            "markdown"
        ],
        [
            "exactly one element from each row",
            "markdown"
        ],
        [
            "without choosing more than one element from any column",
            "markdown"
        ],
        [
            "such that the sum of the chosen elements is minimized",
            "markdown"
        ],
        [
            "In other words, we need to assign each row to one column such that the sum of\nthe corresponding entries is minimized.",
            "markdown"
        ],
        [
            "Formally, let \\(X\\) be a boolean matrix where \\(X[i,j] = 1\\) iff row  \\(i\\) is assigned to column \\(j\\).\nThen the optimal assignment has cost\n\n\\[\\min \\sum_i \\sum_j C_{i,j} X_{i,j}\\]",
            "markdown"
        ],
        [
            "The first step is to define the cost matrix.\nIn this example, we want to assign each swimming style to a student.\nlinear_sum_assignment is able to assign each row of a cost matrix to a column.\nTherefore, to form the cost matrix, the table above needs to be transposed so that the rows\ncorrespond with swimming styles and the columns correspond with students:",
            "markdown"
        ],
        [
            "import numpy as np\n cost = np.array([[43.5, 45.5, 43.4, 46.5, 46.3],\n...                  [47.1, 42.1, 39.1, 44.1, 47.8],\n...                  [48.4, 49.6, 42.1, 44.5, 50.4],\n...                  [38.2, 36.8, 43.2, 41.2, 37.2]])",
            "code"
        ],
        [
            "We can solve the assignment problem with linear_sum_assignment:",
            "markdown"
        ],
        [
            "from scipy.optimize import linear_sum_assignment\n row_ind, col_ind = linear_sum_assignment(cost)",
            "code"
        ],
        [
            "The row_ind and col_ind are optimal assigned matrix indexes of the cost matrix:",
            "markdown"
        ],
        [
            "row_ind\narray([0, 1, 2, 3])\n col_ind\narray([0, 2, 3, 1])",
            "code"
        ],
        [
            "The optimal assignment is:",
            "markdown"
        ],
        [
            "styles = np.array([\"backstroke\", \"breaststroke\", \"butterfly\", \"freestyle\"])[row_ind]\n students = np.array([\"A\", \"B\", \"C\", \"D\", \"E\"])[col_ind]\n dict(zip(styles, students))\n{'backstroke': 'A', 'breaststroke': 'C', 'butterfly': 'D', 'freestyle': 'B'}",
            "code"
        ],
        [
            "The optimal total medley time is:",
            "markdown"
        ],
        [
            "cost[row_ind, col_ind].sum()\n163.89999999999998",
            "code"
        ],
        [
            "Note that this result is not the same as the sum of the minimum times for each swimming style:",
            "markdown"
        ],
        [
            "np.min(cost, axis=1).sum()\n161.39999999999998",
            "code"
        ],
        [
            "because student \u00e2\u0080\u009cC\u00e2\u0080\u009d is the best swimmer in both \u00e2\u0080\u009cbreaststroke\u00e2\u0080\u009d and \u00e2\u0080\u009cbutterfly\u00e2\u0080\u009d style.\nWe cannot assign student \u00e2\u0080\u009cC\u00e2\u0080\u009d to both styles, so we assigned student C to the \u00e2\u0080\u009cbreaststroke\u00e2\u0080\u009d style\nand D to the \u00e2\u0080\u009cbutterfly\u00e2\u0080\u009d style to minimize the total time.",
            "markdown"
        ],
        [
            "References",
            "markdown"
        ],
        [
            "Some further reading and related software, such as Newton-Krylov [KK],\nPETSc [PP], and PyAMG [AMG]:\n\n\n[KK]",
            "markdown"
        ],
        [
            "D.A. Knoll and D.E. Keyes, \u00e2\u0080\u009cJacobian-free Newton-Krylov methods\u00e2\u0080\u009d,\nJ. Comp. Phys. 193, 357 (2004). DOI:10.1016/j.jcp.2003.08.010\n\n\n[PP]",
            "markdown"
        ],
        [
            "PETSc https://www.mcs.anl.gov/petsc/ and its Python bindings\nhttps://bitbucket.org/petsc/petsc4py/\n\n\n[AMG]",
            "markdown"
        ],
        [
            "PyAMG (algebraic multigrid preconditioners/solvers)\nhttps://github.com/pyamg/pyamg/issues",
            "markdown"
        ]
    ],
    "Optimization (scipy.optimize)->Mixed integer linear programming->Knapsack problem example": [
        [
            "The knapsack problem is a well known combinatorial optimization problem.\nGiven a set of items, each with a size and a value, the problem is to choose\nthe items that maximize the total value under the condition that the total size\nis below a certain threshold.",
            "markdown"
        ],
        [
            "Formally, let",
            "markdown"
        ],
        [
            "\\(x_i\\) be a boolean variable that indicates whether item \\(i\\) is\nincluded in the knapsack,",
            "markdown"
        ],
        [
            "\\(n\\) be the total number of items,",
            "markdown"
        ],
        [
            "\\(v_i\\) be the value of item \\(i\\),",
            "markdown"
        ],
        [
            "\\(s_i\\) be the size of item \\(i\\), and",
            "markdown"
        ],
        [
            "\\(C\\) be the capacity of the knapsack.",
            "markdown"
        ],
        [
            "Then the problem is:\n\n\\[\\max \\sum_i^n  v_{i} x_{i}\\]\n\n\\[\\text{subject to} \\sum_i^n s_{i} x_{i} \\leq C,  x_{i} \\in {0, 1}\\]",
            "markdown"
        ],
        [
            "Although the objective function and inequality constraints are linear in the\ndecision variables \\(x_i\\), this differs from a typical linear\nprogramming problem in that the decision variables can only assume integer\nvalues.  Specifically, our decision variables can only be \\(0\\) or\n\\(1\\), so this is known as a binary integer linear program (BILP). Such\na problem falls within the larger class of mixed integer linear programs\n(MILPs), which we we can solve with milp.",
            "markdown"
        ],
        [
            "In our example, there are 8 items to choose from, and the size and value of\neach is specified as follows.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import optimize\n sizes = np.array([21, 11, 15, 9, 34, 25, 41, 52])\n values = np.array([22, 12, 16, 10, 35, 26, 42, 53])",
            "code"
        ],
        [
            "We need to constrain our eight decision variables to be binary. We do so\nby adding a Bounds: constraint to ensure that they lie between\n\\(0\\) and \\(1\\), and we apply \u00e2\u0080\u009cintegrality\u00e2\u0080\u009d constraints to ensure that\nthey are either \\(0\\) or \\(1\\).",
            "markdown"
        ],
        [
            "bounds = optimize.Bounds(0, 1)  # 0 &lt;= x_i &lt;= 1\n integrality = np.full_like(values, True)  # x_i are integers",
            "code"
        ],
        [
            "The knapsack capacity constraint is specified using LinearConstraint.",
            "markdown"
        ],
        [
            "capacity = 100\n constraints = optimize.LinearConstraint(A=sizes, lb=0, ub=capacity)",
            "code"
        ],
        [
            "If we are following the usual rules of linear algebra, the input A should\nbe a  two-dimensional matrix, and the lower and upper bounds lb and ub\nshould be one-dimensional vectors, but LinearConstraint is forgiving\nas long as the inputs can be broadcast to consistent shapes.",
            "markdown"
        ],
        [
            "Using the variables defined above, we can solve the knapsack problem using\nmilp. Note that milp minimizes the objective function, but we\nwant to maximize the total value, so we set <em class=\"xref py py-obj\">c to be negative of the values.",
            "markdown"
        ],
        [
            "from scipy.optimize import milp\n res = milp(c=-values, constraints=constraints,\n...            integrality=integrality, bounds=bounds)",
            "code"
        ],
        [
            "Let\u00e2\u0080\u0099s check the result:",
            "markdown"
        ],
        [
            "res.success\nTrue\n res.x\narray([1., 1., 0., 1., 1., 1., 0., 0.])",
            "code"
        ],
        [
            "This means that we should select the items 1, 2, 4, 5, 6 to optimize the total\nvalue under the size constraint. Note that this is different from we would have\nobtained had we solved the linear programming relaxation (without integrality\nconstraints) and attempted to round the decision variables.",
            "markdown"
        ],
        [
            "from scipy.optimize import milp\n res = milp(c=-values, constraints=constraints,\n...            integrality=False, bounds=bounds)\n res.x\narray([1.        , 1.        , 1.        , 1.        ,\n       0.55882353, 1.        , 0.        , 0.        ])",
            "code"
        ],
        [
            "If we were to round this solution up to\narray([1., 1., 1., 1., 1., 1., 0., 0.]), our knapsack would be over the\ncapacity constraint, whereas if we were to round down to\narray([1., 1., 1., 1., 0., 1., 0., 0.]), we would have a sub-optimal\nsolution.",
            "markdown"
        ],
        [
            "For more MILP tutorials, see the Jupyter notebooks on SciPy Cookbooks:",
            "markdown"
        ],
        [
            "Compressed Sensing l1 program",
            "markdown"
        ],
        [
            "Compressed Sensing l0 program",
            "markdown"
        ]
    ],
    "Interpolation (scipy.interpolate)": [
        [
            "There are several general facilities available in SciPy for interpolation and\nsmoothing for data in 1, 2, and higher dimensions. The choice of a specific\ninterpolation routine depends on the data: whether it is one-dimensional,\nis given on a structured grid, or is unstructured. One other factor is the\ndesired smoothness of the interpolator. In short, routines recommended for\ninterpolation can be summarized as follows:",
            "markdown"
        ],
        [
            "For data smoothing, functions are provided\nfor 1- and 2-D data using cubic splines, based on the FORTRAN library FITPACK.",
            "markdown"
        ],
        [
            "Additionally, routines are provided for interpolation / smoothing using\nradial basis functions with several kernels.",
            "markdown"
        ],
        [
            "Futher details are given in the links below.\n\n\n1-D interpolation\nPiecewise linear interpolation\nCubic splines\nMonotone interpolants\nInterpolation with B-splines\nParametric spline curves\nLegacy interface for 1-D interpolation (interp1d)\nMissing data\n\n\nPiecewise polynomials and splines\nManipulating PPoly objects\nB-splines: knots and coefficients\nB-spline basis elements\nDesign matrices in the B-spline basis\n\n\n\n\nSmoothing splines\nSpline smoothing in 1-D\nProcedural (splrep)\nObject-oriented (UnivariateSpline)\n\n\n2-D smoothing splines\nBivariate spline fitting of scattered data\nBivariate spline fitting of data on a grid\nBivariate spline fitting of data in spherical coordinates\n\n\n\n\nMultivariate data interpolation on a regular grid  (RegularGridInterpolator)\nUniformly spaced data\n\n\nScattered data interpolation (griddata)\nUsing radial basis functions for smoothing/interpolation\n1-D Example\n2-D Example\n\n\nExtrapolation tips and tricks\ninterp1d : replicate numpy.interp left and right fill values\nCubicSpline extend the boundary conditions\nManually implement the asymptotics\nThe setup\nUse the known asymptotics\n\n\nExrapolation in D > 1",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)": [
        [
            "Contents",
            "markdown"
        ],
        [
            "Fourier Transforms (scipy.fft)",
            "markdown"
        ],
        [
            "Fast Fourier transforms",
            "markdown"
        ],
        [
            "1-D discrete Fourier transforms",
            "markdown"
        ],
        [
            "2- and N-D discrete Fourier transforms",
            "markdown"
        ],
        [
            "Discrete Cosine Transforms",
            "markdown"
        ],
        [
            "Type I DCT",
            "markdown"
        ],
        [
            "Type II DCT",
            "markdown"
        ],
        [
            "Type III DCT",
            "markdown"
        ],
        [
            "Type IV DCT",
            "markdown"
        ],
        [
            "DCT and IDCT",
            "markdown"
        ],
        [
            "Example",
            "markdown"
        ],
        [
            "Discrete Sine Transforms",
            "markdown"
        ],
        [
            "Type I DST",
            "markdown"
        ],
        [
            "Type II DST",
            "markdown"
        ],
        [
            "Type III DST",
            "markdown"
        ],
        [
            "Type IV DST",
            "markdown"
        ],
        [
            "DST and IDST",
            "markdown"
        ],
        [
            "Fast Hankel Transform",
            "markdown"
        ],
        [
            "References",
            "markdown"
        ],
        [
            "Fourier analysis is a method for expressing a function as a sum of periodic\ncomponents, and for recovering the signal from those components. When both\nthe function and its Fourier transform are replaced with discretized\ncounterparts, it is called the discrete Fourier transform (DFT). The DFT has\nbecome a mainstay of numerical computing in part because of a very fast\nalgorithm for computing it, called the Fast Fourier Transform (FFT), which was\nknown to Gauss (1805) and was brought to light in its current form by Cooley\nand Tukey [CT65]. Press et al. [NR07] provide an accessible introduction to\nFourier analysis and its applications.",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Fast Fourier transforms->1-D discrete Fourier transforms": [
        [
            "The FFT <em class=\"xref py py-obj\">y[k] of length \\(N\\) of the length-\\(N\\) sequence <em class=\"xref py py-obj\">x[n] is\ndefined as\n\n\\[y[k] = \\sum_{n=0}^{N-1} e^{-2 \\pi j \\frac{k n}{N} } x[n] \\, ,\\]",
            "markdown"
        ],
        [
            "and the inverse transform is defined as follows\n\n\\[x[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} e^{2 \\pi j \\frac{k n}{N} } y[k] \\, .\\]",
            "markdown"
        ],
        [
            "These transforms can be calculated by means of fft and ifft,\nrespectively, as shown in the following example.",
            "markdown"
        ],
        [
            "from scipy.fft import fft, ifft\n import numpy as np\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5])\n y = fft(x)\n y\narray([ 4.5       +0.j        ,  2.08155948-1.65109876j,\n       -1.83155948+1.60822041j, -1.83155948-1.60822041j,\n        2.08155948+1.65109876j])\n yinv = ifft(y)\n yinv\narray([ 1.0+0.j,  2.0+0.j,  1.0+0.j, -1.0+0.j,  1.5+0.j])",
            "code"
        ],
        [
            "From the definition of the FFT it can be seen that\n\n\\[y[0] = \\sum_{n=0}^{N-1} x[n] \\, .\\]",
            "markdown"
        ],
        [
            "In the example",
            "markdown"
        ],
        [
            "np.sum(x)\n4.5",
            "code"
        ],
        [
            "which corresponds to \\(y[0]\\). For N even, the elements\n\\(y[1]...y[N/2-1]\\) contain the positive-frequency terms, and the elements\n\\(y[N/2]...y[N-1]\\) contain the negative-frequency terms, in order of\ndecreasingly negative frequency. For N odd, the elements\n\\(y[1]...y[(N-1)/2]\\) contain the positive-frequency terms, and the\nelements \\(y[(N+1)/2]...y[N-1]\\) contain the negative-frequency terms, in\norder of decreasingly negative frequency.",
            "markdown"
        ],
        [
            "In case the sequence x is real-valued, the values of \\(y[n]\\) for positive\nfrequencies is the conjugate of the values \\(y[n]\\) for negative\nfrequencies (because the spectrum is symmetric). Typically, only the FFT\ncorresponding to positive frequencies is plotted.",
            "markdown"
        ],
        [
            "The example plots the FFT of the sum of two sines.",
            "markdown"
        ],
        [
            "from scipy.fft import fft, fftfreq\n import numpy as np\n # Number of sample points\n N = 600\n # sample spacing\n T = 1.0 / 800.0\n x = np.linspace(0.0, N*T, N, endpoint=False)\n y = np.sin(50.0 * 2.0*np.pi*x) + 0.5*np.sin(80.0 * 2.0*np.pi*x)\n yf = fft(y)\n xf = fftfreq(N, T)[:N//2]\n import matplotlib.pyplot as plt\n plt.plot(xf, 2.0/N * np.abs(yf[0:N//2]))\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot showing amplitude on the Y axis vs frequency on the X axis. A single blue trace has an amplitude of zero all the way across with the exception of two peaks. The taller first peak is at 50 Hz with a second peak at 80 Hz.\"' class=\"plot-directive\" src=\"../_images/fft-1.png\"/>\n</figure>",
            "code"
        ],
        [
            "The FFT input signal is inherently truncated. This truncation can be modeled\nas multiplication of an infinite signal with a rectangular window function. In\nthe spectral domain this multiplication becomes convolution of the signal\nspectrum with the window function spectrum, being of form \\(\\sin(x)/x\\).\nThis convolution is the cause of an effect called spectral leakage (see\n[WPW]). Windowing the signal with a dedicated window function helps mitigate\nspectral leakage. The example below uses a Blackman window from scipy.signal\nand shows the effect of windowing (the zero component of the FFT has been\ntruncated for illustrative purposes).",
            "markdown"
        ],
        [
            "from scipy.fft import fft, fftfreq\n import numpy as np\n # Number of sample points\n N = 600\n # sample spacing\n T = 1.0 / 800.0\n x = np.linspace(0.0, N*T, N, endpoint=False)\n y = np.sin(50.0 * 2.0*np.pi*x) + 0.5*np.sin(80.0 * 2.0*np.pi*x)\n yf = fft(y)\n from scipy.signal import blackman\n w = blackman(N)\n ywf = fft(y*w)\n xf = fftfreq(N, T)[:N//2]\n import matplotlib.pyplot as plt\n plt.semilogy(xf[1:N//2], 2.0/N * np.abs(yf[1:N//2]), '-b')\n plt.semilogy(xf[1:N//2], 2.0/N * np.abs(ywf[1:N//2]), '-r')\n plt.legend(['FFT', 'FFT w. window'])\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y log-linear plot with amplitude on the Y axis vs frequency on the X axis. The first trace is the FFT with two peaks at 50 and 80 Hz and a noise floor around an amplitude of 1e-2. The second trace is the windowed FFT and has the same two peaks but the noise floor is much lower around an amplitude of 1e-7 due to the window function.\"' class=\"plot-directive\" src=\"../_images/fft-2.png\"/>\n</figure>",
            "code"
        ],
        [
            "In case the sequence x is complex-valued, the spectrum is no longer symmetric.\nTo simplify working with the FFT functions, scipy provides the following two\nhelper functions.",
            "markdown"
        ],
        [
            "The function fftfreq returns the FFT sample frequency points.",
            "markdown"
        ],
        [
            "from scipy.fft import fftfreq\n freq = fftfreq(8, 0.125)\n freq\narray([ 0., 1., 2., 3., -4., -3., -2., -1.])",
            "code"
        ],
        [
            "In a similar spirit, the function fftshift allows swapping the lower\nand upper halves of a vector, so that it becomes suitable for display.",
            "markdown"
        ],
        [
            "from scipy.fft import fftshift\n x = np.arange(8)\n fftshift(x)\narray([4, 5, 6, 7, 0, 1, 2, 3])",
            "code"
        ],
        [
            "The example below plots the FFT of two complex exponentials; note the\nasymmetric spectrum.",
            "markdown"
        ],
        [
            "from scipy.fft import fft, fftfreq, fftshift\n import numpy as np\n # number of signal points\n N = 400\n # sample spacing\n T = 1.0 / 800.0\n x = np.linspace(0.0, N*T, N, endpoint=False)\n y = np.exp(50.0 * 1.j * 2.0*np.pi*x) + 0.5*np.exp(-80.0 * 1.j * 2.0*np.pi*x)\n yf = fft(y)\n xf = fftfreq(N, T)\n xf = fftshift(xf)\n yplot = fftshift(yf)\n import matplotlib.pyplot as plt\n plt.plot(xf, 1.0/N * np.abs(yplot))\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with amplitude on the Y axis vs frequency on the X axis. The trace is zero-valued across the plot except for two sharp peaks at -80 and 50 Hz. The 50 Hz peak on the right is twice as tall.\"' class=\"plot-directive\" src=\"../_images/fft-3.png\"/>\n</figure>",
            "code"
        ],
        [
            "The function rfft calculates the FFT of a real sequence and outputs the\ncomplex FFT coefficients \\(y[n]\\) for only half of the frequency range. The\nremaining negative frequency components are implied by the Hermitian symmetry of\nthe FFT for a real input (y[n] = conj(y[-n])). In case of N being even:\n\\([Re(y[0]) + 0j, y[1], ..., Re(y[N/2]) + 0j]\\); in case of N being odd\n\\([Re(y[0]) + 0j, y[1], ..., y[N/2]\\). The terms shown explicitly as\n\\(Re(y[k]) + 0j\\) are restricted to be purely real since, by the hermitian\nproperty, they are their own complex conjugate.",
            "markdown"
        ],
        [
            "The corresponding function irfft calculates the IFFT of the FFT\ncoefficients with this special ordering.",
            "markdown"
        ],
        [
            "from scipy.fft import fft, rfft, irfft\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5, 1.0])\n fft(x)\narray([ 5.5 +0.j        ,  2.25-0.4330127j , -2.75-1.29903811j,\n        1.5 +0.j        , -2.75+1.29903811j,  2.25+0.4330127j ])\n yr = rfft(x)\n yr\narray([ 5.5 +0.j        ,  2.25-0.4330127j , -2.75-1.29903811j,\n        1.5 +0.j        ])\n irfft(yr)\narray([ 1. ,  2. ,  1. , -1. ,  1.5,  1. ])\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5])\n fft(x)\narray([ 4.5       +0.j        ,  2.08155948-1.65109876j,\n       -1.83155948+1.60822041j, -1.83155948-1.60822041j,\n        2.08155948+1.65109876j])\n yr = rfft(x)\n yr\narray([ 4.5       +0.j        ,  2.08155948-1.65109876j,\n        -1.83155948+1.60822041j])",
            "code"
        ],
        [
            "Notice that the rfft of odd and even length signals are of the same shape.\nBy default, irfft assumes the output signal should be of even length. And\nso, for odd signals, it will give the wrong result:",
            "markdown"
        ],
        [
            "irfft(yr)\narray([ 1.70788987,  2.40843925, -0.37366961,  0.75734049])",
            "code"
        ],
        [
            "To recover the original odd-length signal, we <strong>must</strong> pass the output shape by\nthe <em class=\"xref py py-obj\">n parameter.",
            "markdown"
        ],
        [
            "irfft(yr, n=len(x))\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Fast Fourier transforms->2- and N-D discrete Fourier transforms": [
        [
            "The functions fft2 and ifft2 provide 2-D FFT and\nIFFT, respectively. Similarly, fftn and ifftn provide\nN-D FFT, and IFFT, respectively.",
            "markdown"
        ],
        [
            "For real-input signals, similarly to rfft, we have the functions\nrfft2 and irfft2 for 2-D real transforms;\nrfftn and irfftn for N-D real transforms.",
            "markdown"
        ],
        [
            "The example below demonstrates a 2-D IFFT and plots the resulting\n(2-D) time-domain signals.",
            "markdown"
        ],
        [
            "from scipy.fft import ifftn\n import matplotlib.pyplot as plt\n import matplotlib.cm as cm\n import numpy as np\n N = 30\n f, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, sharex='col', sharey='row')\n xf = np.zeros((N,N))\n xf[0, 5] = 1\n xf[0, N-5] = 1\n Z = ifftn(xf)\n ax1.imshow(xf, cmap=cm.Reds)\n ax4.imshow(np.real(Z), cmap=cm.gray)\n xf = np.zeros((N, N))\n xf[5, 0] = 1\n xf[N-5, 0] = 1\n Z = ifftn(xf)\n ax2.imshow(xf, cmap=cm.Reds)\n ax5.imshow(np.real(Z), cmap=cm.gray)\n xf = np.zeros((N, N))\n xf[5, 10] = 1\n xf[N-5, N-10] = 1\n Z = ifftn(xf)\n ax3.imshow(xf, cmap=cm.Reds)\n ax6.imshow(np.real(Z), cmap=cm.gray)\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates six heatmaps arranged in a 2x3 grid. The top row shows mostly blank canvases with the exception of two tiny red peaks on each image. The bottom row shows the real-part of the inverse FFT of each image above it. The first column has two dots arranged horizontally in the top image and in the bottom image a smooth grayscale plot of 5 black vertical stripes representing the 2-D time domain signal. The second column has two dots arranged vertically in the top image and in the bottom image a smooth grayscale plot of 5 horizontal black stripes representing the 2-D time domain signal. In the last column the top image has two dots diagonally located; the corresponding image below has perhaps 20 black stripes at a 60 degree angle.\"' class=\"plot-directive\" src=\"../_images/fft-4.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Cosine Transforms": [
        [
            "SciPy provides a DCT with the function dct and a corresponding IDCT\nwith the function idct. There are 8 types of the DCT [WPC], [Mak];\nhowever, only the first 4 types are implemented in scipy. \u00e2\u0080\u009cThe\u00e2\u0080\u009d DCT generally\nrefers to DCT type 2, and \u00e2\u0080\u009cthe\u00e2\u0080\u009d Inverse DCT generally refers to DCT type 3. In\naddition, the DCT coefficients can be normalized differently (for most types,\nscipy provides None and ortho). Two parameters of the dct/idct\nfunction calls allow setting the DCT type and coefficient normalization.",
            "markdown"
        ],
        [
            "For a single dimension array x, dct(x, norm=\u00e2\u0080\u0099ortho\u00e2\u0080\u0099) is equal to\nMATLAB dct(x).",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Type I DCT": [
        [
            "SciPy uses the following definition of the unnormalized DCT-I\n(norm=None):\n\n\\[y[k] = x_0 + (-1)^k x_{N-1} + 2\\sum_{n=1}^{N-2} x[n]\n\\cos\\left(\\frac{\\pi nk}{N-1}\\right),\n\\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ],
        [
            "Note that the DCT-I is only supported for input size > 1.",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Type II DCT": [
        [
            "SciPy uses the following definition of the unnormalized DCT-II\n(norm=None):\n\n\\[y[k] = 2 \\sum_{n=0}^{N-1} x[n] \\cos \\left({\\pi(2n+1)k \\over 2N} \\right)\n\\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ],
        [
            "In case of the normalized DCT (norm='ortho'), the DCT coefficients\n\\(y[k]\\) are multiplied by a scaling factor <em class=\"xref py py-obj\">f:\n\n\\[\\begin{split}f = \\begin{cases} \\sqrt{1/(4N)}, & \\text{if $k = 0$} \\\\    \\sqrt{1/(2N)},\n& \\text{otherwise} \\end{cases} \\, .\\end{split}\\]",
            "markdown"
        ],
        [
            "In this case, the DCT \u00e2\u0080\u009cbase functions\u00e2\u0080\u009d \\(\\phi_k[n] = 2 f \\cos\n\\left({\\pi(2n+1)k \\over 2N} \\right)\\) become orthonormal:\n\n\\[\\sum_{n=0}^{N-1} \\phi_k[n] \\phi_l[n] = \\delta_{lk}.\\]",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Type III DCT": [
        [
            "SciPy uses the following definition of the unnormalized DCT-III\n(norm=None):\n\n\\[y[k] = x_0 + 2 \\sum_{n=1}^{N-1} x[n] \\cos\\left({\\pi n(2k+1) \\over 2N}\\right)\n\\qquad 0 \\le k &lt; N,\\]",
            "markdown"
        ],
        [
            "or, for norm='ortho':\n\n\\[y[k] = {x_0\\over\\sqrt{N}} + {2\\over\\sqrt{N}} \\sum_{n=1}^{N-1} x[n]\n\\cos\\left({\\pi n(2k+1) \\over 2N}\\right) \\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Type IV DCT": [
        [
            "SciPy uses the following definition of the unnormalized DCT-IV\n(norm=None):\n\n\\[y[k] = 2 \\sum_{n=0}^{N-1} x[n] \\cos\\left({\\pi (2n+1)(2k+1) \\over 4N}\\right)\n\\qquad 0 \\le k &lt; N,\\]",
            "markdown"
        ],
        [
            "or, for norm='ortho':\n\n\\[y[k] = \\sqrt{2\\over N}\\sum_{n=0}^{N-1} x[n] \\cos\\left({\\pi (2n+1)(2k+1) \\over 4N}\\right)\n\\qquad 0 \\le k &lt; N\\]",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->DCT and IDCT": [
        [
            "The (unnormalized) DCT-III is the inverse of the (unnormalized) DCT-II, up to a\nfactor of <em class=\"xref py py-obj\">2N. The orthonormalized DCT-III is exactly the inverse of the\northonormalized DCT- II. The function idct performs the mappings between\nthe DCT and IDCT types, as well as the correct normalization.",
            "markdown"
        ],
        [
            "The following example shows the relation between DCT and IDCT for different\ntypes and normalizations.",
            "markdown"
        ],
        [
            "from scipy.fft import dct, idct\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5])",
            "code"
        ],
        [
            "The DCT-II and DCT-III are each other\u00e2\u0080\u0099s inverses, so for an orthonormal transform\nwe return back to the original signal.",
            "markdown"
        ],
        [
            "dct(dct(x, type=2, norm='ortho'), type=3, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "Doing the same under default normalization, however, we pick up an extra scaling\nfactor of \\(2N=10\\) since the forward transform is unnormalized.",
            "markdown"
        ],
        [
            "dct(dct(x, type=2), type=3)\narray([ 10.,  20.,  10., -10.,  15.])",
            "code"
        ],
        [
            "For this reason, we should use the function idct using the same type for both,\ngiving a correctly normalized result.",
            "markdown"
        ],
        [
            "# Normalized inverse: no scaling factor\n idct(dct(x, type=2), type=2)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "Analogous results can be seen for the DCT-I, which is its own inverse up to a\nfactor of \\(2(N-1)\\).",
            "markdown"
        ],
        [
            "dct(dct(x, type=1, norm='ortho'), type=1, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n # Unnormalized round-trip via DCT-I: scaling factor 2*(N-1) = 8\n dct(dct(x, type=1), type=1)\narray([ 8. ,  16.,  8. , -8. ,  12.])\n # Normalized inverse: no scaling factor\n idct(dct(x, type=1), type=1)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "And for the DCT-IV, which is also its own inverse up to a factor of \\(2N\\).",
            "markdown"
        ],
        [
            "dct(dct(x, type=4, norm='ortho'), type=4, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n # Unnormalized round-trip via DCT-IV: scaling factor 2*N = 10\n dct(dct(x, type=4), type=4)\narray([ 10.,  20.,  10., -10.,  15.])\n # Normalized inverse: no scaling factor\n idct(dct(x, type=4), type=4)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->Example": [
        [
            "The DCT exhibits the \u00e2\u0080\u009cenergy compaction property\u00e2\u0080\u009d, meaning that for many\nsignals only the first few DCT coefficients have significant magnitude.\nZeroing out the other coefficients leads to a small reconstruction error, a\nfact which is exploited in lossy signal compression (e.g. JPEG compression).",
            "markdown"
        ],
        [
            "The example below shows a signal x and two reconstructions (\\(x_{20}\\) and\n\\(x_{15}\\)) from the signal\u00e2\u0080\u0099s DCT coefficients. The signal \\(x_{20}\\)\nis reconstructed from the first 20 DCT coefficients, \\(x_{15}\\) is\nreconstructed from the first 15 DCT coefficients. It can be seen that the\nrelative error of using 20 coefficients is still very small (~0.1%), but\nprovides a five-fold compression rate.",
            "markdown"
        ],
        [
            "from scipy.fft import dct, idct\n import matplotlib.pyplot as plt\n N = 100\n t = np.linspace(0,20,N, endpoint=False)\n x = np.exp(-t/3)*np.cos(2*t)\n y = dct(x, norm='ortho')\n window = np.zeros(N)\n window[:20] = 1\n yr = idct(y*window, norm='ortho')\n sum(abs(x-yr)**2) / sum(abs(x)**2)\n0.0009872817275276098\n plt.plot(t, x, '-bx')\n plt.plot(t, yr, 'ro')\n window = np.zeros(N)\n window[:15] = 1\n yr = idct(y*window, norm='ortho')\n sum(abs(x-yr)**2) / sum(abs(x)**2)\n0.06196643004256714\n plt.plot(t, yr, 'g+')\n plt.legend(['x', '$x_{20}$', '$x_{15}$'])\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot showing amplitude on the Y axis and time on the X axis. The first blue trace is the original signal and starts at amplitude 1 and oscillates down to 0 amplitude over the duration of the plot resembling a frequency chirp. The second red trace is the x_20 reconstruction using the DCT and closely follows the original signal in the high amplitude region but it is unclear to the right side of the plot. The third green trace is the x_15 reconstruction using the DCT and is less precise than the x_20 reconstruction but still similar to x.\"' class=\"plot-directive\" src=\"../_images/fft-5.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Sine Transforms": [
        [
            "SciPy provides a DST [Mak] with the function dst and a corresponding IDST\nwith the function idst.",
            "markdown"
        ],
        [
            "There are, theoretically, 8 types of the DST for different combinations of\neven/odd boundary conditions and boundary offsets [WPS], only the first 4\ntypes are implemented in scipy.",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Sine Transforms->Type I DST": [
        [
            "DST-I assumes the input is odd around n=-1 and n=N. SciPy uses the following\ndefinition of the unnormalized DST-I (norm=None):\n\n\\[y[k] = 2\\sum_{n=0}^{N-1} x[n]  \\sin\\left( \\pi {(n+1) (k+1)}\\over{N+1}\n\\right), \\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ],
        [
            "Note also that the DST-I is only supported for input size > 1. The\n(unnormalized) DST-I is its own inverse, up to a factor of <em class=\"xref py py-obj\">2(N+1).",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Sine Transforms->Type II DST": [
        [
            "DST-II assumes the input is odd around n=-1/2 and even around n=N. SciPy uses\nthe following definition of the unnormalized DST-II (norm=None):\n\n\\[y[k] = 2 \\sum_{n=0}^{N-1} x[n]  \\sin\\left( {\\pi (n+1/2)(k+1)} \\over N\n\\right), \\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Sine Transforms->Type III DST": [
        [
            "DST-III assumes the input is odd around n=-1 and even around n=N-1. SciPy uses\nthe following definition of the unnormalized DST-III (norm=None):\n\n\\[y[k] = (-1)^k x[N-1] + 2 \\sum_{n=0}^{N-2} x[n] \\sin \\left( {\\pi\n(n+1)(k+1/2)} \\over N \\right), \\qquad 0 \\le k &lt; N.\\]",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Sine Transforms->Type IV DST": [
        [
            "SciPy uses the following definition of the unnormalized DST-IV\n(norm=None):\n\n\\[y[k] = 2 \\sum_{n=0}^{N-1} x[n] \\sin\\left({\\pi (2n+1)(2k+1) \\over 4N}\\right)\n\\qquad 0 \\le k &lt; N,\\]",
            "markdown"
        ],
        [
            "or, for norm='ortho':\n\n\\[y[k] = \\sqrt{2\\over N}\\sum_{n=0}^{N-1} x[n] \\sin\\left({\\pi (2n+1)(2k+1) \\over 4N}\\right)\n\\qquad 0 \\le k &lt; N,\\]",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Discrete Sine Transforms->DST and IDST": [
        [
            "The following example shows the relation between DST and IDST for\ndifferent types and normalizations.",
            "markdown"
        ],
        [
            "from scipy.fft import dst, idst\n x = np.array([1.0, 2.0, 1.0, -1.0, 1.5])",
            "code"
        ],
        [
            "The DST-II and DST-III are each other\u00e2\u0080\u0099s inverses, so for an orthonormal transform\nwe return back to the original signal.",
            "markdown"
        ],
        [
            "dst(dst(x, type=2, norm='ortho'), type=3, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "Doing the same under default normalization, however, we pick up an extra scaling\nfactor of \\(2N=10\\) since the forward transform is unnormalized.",
            "markdown"
        ],
        [
            "dst(dst(x, type=2), type=3)\narray([ 10.,  20.,  10., -10.,  15.])",
            "code"
        ],
        [
            "For this reason, we should use the function idst using the same type for both,\ngiving a correctly normalized result.",
            "markdown"
        ],
        [
            "idst(dst(x, type=2), type=2)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "Analogous results can be seen for the DST-I, which is its own inverse up to a\nfactor of \\(2(N-1)\\).",
            "markdown"
        ],
        [
            "dst(dst(x, type=1, norm='ortho'), type=1, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n  # scaling factor 2*(N+1) = 12\n dst(dst(x, type=1), type=1)\narray([ 12.,  24.,  12., -12.,  18.])\n  # no scaling factor\n idst(dst(x, type=1), type=1)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ],
        [
            "And for the DST-IV, which is also its own inverse up to a factor of \\(2N\\).",
            "markdown"
        ],
        [
            "dst(dst(x, type=4, norm='ortho'), type=4, norm='ortho')\narray([ 1. ,  2. ,  1. , -1. ,  1.5])\n  # scaling factor 2*N = 10\n dst(dst(x, type=4), type=4)\narray([ 10.,  20.,  10., -10.,  15.])\n  # no scaling factor\n idst(dst(x, type=4), type=4)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "code"
        ]
    ],
    "Fourier Transforms (scipy.fft)->Fast Hankel Transform": [
        [
            "SciPy provides the functions fht and ifht to perform the Fast\nHankel Transform (FHT) and its inverse (IFHT) on logarithmically-spaced input\narrays.",
            "markdown"
        ],
        [
            "The FHT is the discretised version of the continuous Hankel transform defined\nby [Ham00]\n\n\\[A(k) = \\int_{0}^{\\infty} \\! a(r) \\, J_{\\mu}(kr) \\, k \\, dr \\;,\\]",
            "markdown"
        ],
        [
            "with \\(J_{\\mu}\\) the Bessel function of order \\(\\mu\\). Under a change\nof variables \\(r \\to \\log r\\), \\(k \\to \\log k\\), this becomes\n\n\\[A(e^{\\log k})\n= \\int_{0}^{\\infty} \\! a(e^{\\log r}) \\, J_{\\mu}(e^{\\log k + \\log r})\n                                    \\, e^{\\log k + \\log r} \\, d{\\log r}\\]",
            "markdown"
        ],
        [
            "which is a convolution in logarithmic space. The FHT algorithm uses the FFT\nto perform this convolution on discrete input data.",
            "markdown"
        ],
        [
            "Care must be taken to minimise numerical ringing due to the circular nature\nof FFT convolution. To ensure that the low-ringing condition [Ham00] holds,\nthe output array can be slightly shifted by an offset computed using the\nfhtoffset function.",
            "markdown"
        ]
    ],
    "Fourier Transforms (scipy.fft)->References": [
        [
            "Cooley, James W., and John W. Tukey, 1965, \u00e2\u0080\u009cAn algorithm for the\nmachine calculation of complex Fourier series,\u00e2\u0080\u009d Math. Comput.\n19: 297-301.\n\n\n[NR07]",
            "markdown"
        ],
        [
            "Press, W., Teukolsky, S., Vetterline, W.T., and Flannery, B.P.,\n2007, Numerical Recipes: The Art of Scientific Computing, ch.\n12-13.  Cambridge Univ. Press, Cambridge, UK.\n\n\n[Mak]\n(1,2)",
            "markdown"
        ],
        [
            "J. Makhoul, 1980, \u00e2\u0080\u0098A Fast Cosine Transform in One and Two Dimensions\u00e2\u0080\u0099,\n<em class=\"xref py py-obj\">IEEE Transactions on acoustics, speech and signal processing\nvol. 28(1), pp. 27-34, DOI:10.1109/TASSP.1980.1163351\n\n\n[Ham00]\n(1,2)",
            "markdown"
        ],
        [
            "A. J. S. Hamilton, 2000, \u00e2\u0080\u009cUncorrelated modes of the non-linear power\nspectrum\u00e2\u0080\u009d, MNRAS, 312, 257. DOI:10.1046/j.1365-8711.2000.03071.x\n\n\n[WPW]",
            "markdown"
        ],
        [
            "https://en.wikipedia.org/wiki/Window_function\n\n\n[WPC]",
            "markdown"
        ],
        [
            "https://en.wikipedia.org/wiki/Discrete_cosine_transform\n\n\n[WPS]",
            "markdown"
        ],
        [
            "https://en.wikipedia.org/wiki/Discrete_sine_transform",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)": [
        [
            "The signal processing toolbox currently contains some filtering\nfunctions, a limited set of filter design tools, and a few B-spline\ninterpolation algorithms for 1- and 2-D data. While the\nB-spline algorithms could technically be placed under the\ninterpolation category, they are included here because they only work\nwith equally-spaced data and make heavy use of filter-theory and\ntransfer-function formalism to provide a fast B-spline transform. To\nunderstand this section, you will need to understand that a signal in\nSciPy is an array of real or complex numbers.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->B-splines": [
        [
            "A B-spline is an approximation of a continuous function over a finite-\ndomain in terms of B-spline coefficients and knot points. If the knot-\npoints are equally spaced with spacing \\(\\Delta x\\), then the B-spline\napproximation to a 1-D function is the finite-basis expansion.\n\n\\[y\\left(x\\right)\\approx\\sum_{j}c_{j}\\beta^{o}\\left(\\frac{x}{\\Delta x}-j\\right).\\]",
            "markdown"
        ],
        [
            "In two dimensions with knot-spacing \\(\\Delta x\\) and \\(\\Delta y\\), the\nfunction representation is\n\n\\[z\\left(x,y\\right)\\approx\\sum_{j}\\sum_{k}c_{jk}\\beta^{o}\\left(\\frac{x}{\\Delta x}-j\\right)\\beta^{o}\\left(\\frac{y}{\\Delta y}-k\\right).\\]",
            "markdown"
        ],
        [
            "In these expressions, \\(\\beta^{o}\\left(\\cdot\\right)\\) is the space-limited\nB-spline basis function of order \\(o\\). The requirement of equally-spaced\nknot-points and equally-spaced data points, allows the development of fast\n(inverse-filtering) algorithms for determining the coefficients, \\(c_{j}\\),\nfrom sample-values, \\(y_{n}\\). Unlike the general spline interpolation\nalgorithms, these algorithms can quickly find the spline coefficients for large\nimages.",
            "markdown"
        ],
        [
            "The advantage of representing a set of samples via B-spline basis\nfunctions is that continuous-domain operators (derivatives, re-\nsampling, integral, etc.), which assume that the data samples are drawn\nfrom an underlying continuous function, can be computed with relative\nease from the spline coefficients. For example, the second derivative\nof a spline is\n\n\\[y{}^{\\prime\\prime}\\left(x\\right)=\\frac{1}{\\Delta x^{2}}\\sum_{j}c_{j}\\beta^{o\\prime\\prime}\\left(\\frac{x}{\\Delta x}-j\\right).\\]",
            "markdown"
        ],
        [
            "Using the property of B-splines that\n\n\\[\\frac{d^{2}\\beta^{o}\\left(w\\right)}{dw^{2}}=\\beta^{o-2}\\left(w+1\\right)-2\\beta^{o-2}\\left(w\\right)+\\beta^{o-2}\\left(w-1\\right),\\]",
            "markdown"
        ],
        [
            "it can be seen that\n\n\\[y^{\\prime\\prime}\\left(x\\right)=\\frac{1}{\\Delta x^{2}}\\sum_{j}c_{j}\\left[\\beta^{o-2}\\left(\\frac{x}{\\Delta x}-j+1\\right)-2\\beta^{o-2}\\left(\\frac{x}{\\Delta x}-j\\right)+\\beta^{o-2}\\left(\\frac{x}{\\Delta x}-j-1\\right)\\right].\\]",
            "markdown"
        ],
        [
            "If \\(o=3\\), then at the sample points:\n\n \\begin{eqnarray*} \\Delta x^{2}\\left.y^{\\prime}\\left(x\\right)\\right|_{x=n\\Delta x} & = & \\sum_{j}c_{j}\\delta_{n-j+1}-2c_{j}\\delta_{n-j}+c_{j}\\delta_{n-j-1},\\\\  & = & c_{n+1}-2c_{n}+c_{n-1}.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "Thus, the second-derivative signal can be easily calculated from the spline\nfit. If desired, smoothing splines can be found to make the second derivative\nless sensitive to random errors.",
            "markdown"
        ],
        [
            "The savvy reader will have already noticed that the data samples are related\nto the knot coefficients via a convolution operator, so that simple\nconvolution with the sampled B-spline function recovers the original data from\nthe spline coefficients. The output of convolutions can change depending on\nhow the boundaries are handled (this becomes increasingly more important as the\nnumber of dimensions in the dataset increases). The algorithms relating to\nB-splines in the signal-processing subpackage assume mirror-symmetric\nboundary conditions. Thus, spline coefficients are computed based on that\nassumption, and data-samples can be recovered exactly from the spline\ncoefficients by assuming them to be mirror-symmetric also.",
            "markdown"
        ],
        [
            "Currently the package provides functions for determining second- and third-\norder cubic spline coefficients from equally-spaced samples in one and two\ndimensions (qspline1d, qspline2d, cspline1d,\ncspline2d). The package also supplies a function ( bspline )\nfor evaluating the B-spline basis function, \\(\\beta^{o}\\left(x\\right)\\) for\narbitrary order and \\(x.\\) For large \\(o\\), the B-spline basis\nfunction can be approximated well by a zero-mean Gaussian function with\nstandard-deviation equal to \\(\\sigma_{o}=\\left(o+1\\right)/12\\) :\n\n\\[\\beta^{o}\\left(x\\right)\\approx\\frac{1}{\\sqrt{2\\pi\\sigma_{o}^{2}}}\\exp\\left(-\\frac{x^{2}}{2\\sigma_{o}}\\right).\\]",
            "markdown"
        ],
        [
            "A function to compute this Gaussian for arbitrary \\(x\\) and \\(o\\) is\nalso available ( gauss_spline ). The following code and figure use\nspline-filtering to compute an edge-image (the second derivative of a smoothed\nspline) of a raccoon\u00e2\u0080\u0099s face, which is an array returned by the command scipy.datasets.face.\nThe command sepfir2d was used to apply a separable 2-D FIR\nfilter with mirror-symmetric boundary conditions to the spline coefficients.\nThis function is ideally-suited for reconstructing samples from spline\ncoefficients and is faster than convolve2d, which convolves arbitrary\n2-D filters and allows for choosing mirror-symmetric boundary\nconditions.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "image = datasets.face(gray=True).astype(np.float32)\n derfilt = np.array([1.0, -2, 1.0], dtype=np.float32)\n ck = signal.cspline2d(image, 8.0)\n deriv = (signal.sepfir2d(ck, derfilt, [1]) +\n...          signal.sepfir2d(ck, [1], derfilt))",
            "code"
        ],
        [
            "Alternatively, we could have done:",
            "markdown"
        ],
        [
            "laplacian = np.array([[0,1,0], [1,-4,1], [0,1,0]], dtype=np.float32)\nderiv2 = signal.convolve2d(ck,laplacian,mode='same',boundary='symm')",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image)\n plt.gray()\n plt.title('Original image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is a normal grayscale photo of a raccoon climbing on a palm plant. The second plot has the 2-D spline filter applied to the photo and is completely grey except the edges of the photo have been emphasized, especially on the raccoon fur and palm fronds.\"' class=\"plot-directive\" src=\"../_images/signal-1_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(deriv)\n plt.gray()\n plt.title('Output of spline edge filter')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is a normal grayscale photo of a raccoon climbing on a palm plant. The second plot has the 2-D spline filter applied to the photo and is completely grey except the edges of the photo have been emphasized, especially on the raccoon fur and palm fronds.\"' class=\"plot-directive\" src=\"../_images/signal-1_01_00.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering": [
        [
            "Filtering is a generic name for any system that modifies an input\nsignal in some way. In SciPy, a signal can be thought of as a NumPy\narray. There are different kinds of filters for different kinds of\noperations. There are two broad kinds of filtering operations: linear\nand non-linear. Linear filters can always be reduced to multiplication\nof the flattened NumPy array by an appropriate matrix resulting in\nanother flattened NumPy array. Of course, this is not usually the best\nway to compute the filter, as the matrices and vectors involved may be\nhuge. For example, filtering a \\(512 \\times 512\\) image with this\nmethod would require multiplication of a \\(512^2 \\times 512^2\\)\nmatrix with a \\(512^2\\) vector. Just trying to store the\n\\(512^2 \\times 512^2\\) matrix using a standard NumPy array would\nrequire \\(68,719,476,736\\) elements. At 4 bytes per element this\nwould require \\(256\\textrm{GB}\\) of memory. In most applications,\nmost of the elements of this matrix are zero and a different method\nfor computing the output of the filter is employed.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Convolution/Correlation": [
        [
            "Many linear filters also have the property of shift-invariance. This\nmeans that the filtering operation is the same at different locations\nin the signal and it implies that the filtering matrix can be\nconstructed from knowledge of one row (or column) of the matrix alone.\nIn this case, the matrix multiplication can be accomplished using\nFourier transforms.",
            "markdown"
        ],
        [
            "Let \\(x\\left[n\\right]\\) define a 1-D signal indexed by the\ninteger \\(n.\\) Full convolution of two 1-D signals can be\nexpressed as\n\n\\[y\\left[n\\right]=\\sum_{k=-\\infty}^{\\infty}x\\left[k\\right]h\\left[n-k\\right].\\]",
            "markdown"
        ],
        [
            "This equation can only be implemented directly if we limit the\nsequences to finite-support sequences that can be stored in a\ncomputer, choose \\(n=0\\) to be the starting point of both\nsequences, let \\(K+1\\) be that value for which\n\\(x\\left[n\\right]=0\\) for all \\(n\\geq K+1\\) and \\(M+1\\) be\nthat value for which \\(h\\left[n\\right]=0\\) for all \\(n\\geq M+1\\),\nthen the discrete convolution expression is\n\n\\[y\\left[n\\right]=\\sum_{k=\\max\\left(n-M,0\\right)}^{\\min\\left(n,K\\right)}x\\left[k\\right]h\\left[n-k\\right].\\]",
            "markdown"
        ],
        [
            "For convenience, assume \\(K\\geq M.\\) Then, more explicitly, the output of\nthis operation is\n\n \\begin{eqnarray*} y\\left[0\\right] & = & x\\left[0\\right]h\\left[0\\right]\\\\ y\\left[1\\right] & = & x\\left[0\\right]h\\left[1\\right]+x\\left[1\\right]h\\left[0\\right]\\\\ y\\left[2\\right] & = & x\\left[0\\right]h\\left[2\\right]+x\\left[1\\right]h\\left[1\\right]+x\\left[2\\right]h\\left[0\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ y\\left[M\\right] & = & x\\left[0\\right]h\\left[M\\right]+x\\left[1\\right]h\\left[M-1\\right]+\\cdots+x\\left[M\\right]h\\left[0\\right]\\\\ y\\left[M+1\\right] & = & x\\left[1\\right]h\\left[M\\right]+x\\left[2\\right]h\\left[M-1\\right]+\\cdots+x\\left[M+1\\right]h\\left[0\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ y\\left[K\\right] & = & x\\left[K-M\\right]h\\left[M\\right]+\\cdots+x\\left[K\\right]h\\left[0\\right]\\\\ y\\left[K+1\\right] & = & x\\left[K+1-M\\right]h\\left[M\\right]+\\cdots+x\\left[K\\right]h\\left[1\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ y\\left[K+M-1\\right] & = & x\\left[K-1\\right]h\\left[M\\right]+x\\left[K\\right]h\\left[M-1\\right]\\\\ y\\left[K+M\\right] & = & x\\left[K\\right]h\\left[M\\right].\\end{eqnarray*}",
            "markdown"
        ],
        [
            "Thus, the full discrete convolution of two finite sequences of lengths\n\\(K+1\\) and \\(M+1\\), respectively, results in a finite sequence of length\n\\(K+M+1=\\left(K+1\\right)+\\left(M+1\\right)-1.\\)",
            "markdown"
        ],
        [
            "1-D convolution is implemented in SciPy with the function\nconvolve. This function takes as inputs the signals \\(x,\\)\n\\(h\\), and two optional flags \u00e2\u0080\u0098mode\u00e2\u0080\u0099 and \u00e2\u0080\u0098method\u00e2\u0080\u0099, and returns the signal\n\\(y.\\)",
            "markdown"
        ],
        [
            "The first optional flag, \u00e2\u0080\u0098mode\u00e2\u0080\u0099, allows for the specification of which part of the\noutput signal to return. The default value of \u00e2\u0080\u0098full\u00e2\u0080\u0099 returns the entire signal.\nIf the flag has a value of \u00e2\u0080\u0098same\u00e2\u0080\u0099, then only the middle \\(K\\) values are\nreturned, starting at \\(y\\left[\\left\\lfloor \\frac{M-1}{2}\\right\\rfloor\n\\right]\\), so that the output has the same length as the first input. If the flag\nhas a value of \u00e2\u0080\u0098valid\u00e2\u0080\u0099, then only the middle\n\\(K-M+1=\\left(K+1\\right)-\\left(M+1\\right)+1\\) output values are returned,\nwhere \\(z\\) depends on all of the values of the smallest input from\n\\(h\\left[0\\right]\\) to \\(h\\left[M\\right].\\) In other words, only the\nvalues \\(y\\left[M\\right]\\) to \\(y\\left[K\\right]\\) inclusive are\nreturned.",
            "markdown"
        ],
        [
            "The second optional flag, \u00e2\u0080\u0098method\u00e2\u0080\u0099, determines how the convolution is computed,\neither through the Fourier transform approach with fftconvolve or\nthrough the direct method. By default, it selects the expected faster method.\nThe Fourier transform method has order \\(O(N\\log N)\\), while the direct\nmethod has order \\(O(N^2)\\). Depending on the big O constant and the value\nof \\(N\\), one of these two methods may be faster. The default value, \u00e2\u0080\u0098auto\u00e2\u0080\u0099,\nperforms a rough calculation and chooses the expected faster method, while the\nvalues \u00e2\u0080\u0098direct\u00e2\u0080\u0099 and \u00e2\u0080\u0098fft\u00e2\u0080\u0099 force computation with the other two methods.",
            "markdown"
        ],
        [
            "The code below shows a simple example for convolution of 2 sequences:",
            "markdown"
        ],
        [
            "x = np.array([1.0, 2.0, 3.0])\n h = np.array([0.0, 1.0, 0.0, 0.0, 0.0])\n signal.convolve(x, h)\narray([ 0.,  1.,  2.,  3.,  0.,  0.,  0.])\n signal.convolve(x, h, 'same')\narray([ 2.,  3.,  0.])",
            "code"
        ],
        [
            "This same function convolve can actually take N-D\narrays as inputs and will return the N-D convolution of the\ntwo arrays, as is shown in the code example below. The same input flags are\navailable for that case as well.",
            "markdown"
        ],
        [
            "x = np.array([[1., 1., 0., 0.], [1., 1., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]])\n h = np.array([[1., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 0.]])\n signal.convolve(x, h)\narray([[ 1.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 1.,  1.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.]])",
            "code"
        ],
        [
            "Correlation is very similar to convolution except that the minus sign\nbecomes a plus sign. Thus,\n\n\\[w\\left[n\\right]=\\sum_{k=-\\infty}^{\\infty}y\\left[k\\right]x\\left[n+k\\right],\\]",
            "markdown"
        ],
        [
            "is the (cross) correlation of the signals \\(y\\) and \\(x.\\) For\nfinite-length signals with \\(y\\left[n\\right]=0\\) outside of the range\n\\(\\left[0,K\\right]\\) and \\(x\\left[n\\right]=0\\) outside of the range\n\\(\\left[0,M\\right],\\) the summation can simplify to\n\n\\[w\\left[n\\right]=\\sum_{k=\\max\\left(0,-n\\right)}^{\\min\\left(K,M-n\\right)}y\\left[k\\right]x\\left[n+k\\right].\\]",
            "markdown"
        ],
        [
            "Assuming again that \\(K\\geq M\\), this is\n\n \\begin{eqnarray*} w\\left[-K\\right] & = & y\\left[K\\right]x\\left[0\\right]\\\\ w\\left[-K+1\\right] & = & y\\left[K-1\\right]x\\left[0\\right]+y\\left[K\\right]x\\left[1\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ w\\left[M-K\\right] & = & y\\left[K-M\\right]x\\left[0\\right]+y\\left[K-M+1\\right]x\\left[1\\right]+\\cdots+y\\left[K\\right]x\\left[M\\right]\\\\ w\\left[M-K+1\\right] & = & y\\left[K-M-1\\right]x\\left[0\\right]+\\cdots+y\\left[K-1\\right]x\\left[M\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ w\\left[-1\\right] & = & y\\left[1\\right]x\\left[0\\right]+y\\left[2\\right]x\\left[1\\right]+\\cdots+y\\left[M+1\\right]x\\left[M\\right]\\\\ w\\left[0\\right] & = & y\\left[0\\right]x\\left[0\\right]+y\\left[1\\right]x\\left[1\\right]+\\cdots+y\\left[M\\right]x\\left[M\\right]\\\\ w\\left[1\\right] & = & y\\left[0\\right]x\\left[1\\right]+y\\left[1\\right]x\\left[2\\right]+\\cdots+y\\left[M-1\\right]x\\left[M\\right]\\\\ w\\left[2\\right] & = & y\\left[0\\right]x\\left[2\\right]+y\\left[1\\right]x\\left[3\\right]+\\cdots+y\\left[M-2\\right]x\\left[M\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ w\\left[M-1\\right] & = & y\\left[0\\right]x\\left[M-1\\right]+y\\left[1\\right]x\\left[M\\right]\\\\ w\\left[M\\right] & = & y\\left[0\\right]x\\left[M\\right].\\end{eqnarray*}",
            "markdown"
        ],
        [
            "The SciPy function correlate implements this operation. Equivalent\nflags are available for this operation to return the full \\(K+M+1\\) length\nsequence (\u00e2\u0080\u0098full\u00e2\u0080\u0099) or a sequence with the same size as the largest sequence\nstarting at \\(w\\left[-K+\\left\\lfloor \\frac{M-1}{2}\\right\\rfloor \\right]\\)\n(\u00e2\u0080\u0098same\u00e2\u0080\u0099) or a sequence where the values depend on all the values of the\nsmallest sequence (\u00e2\u0080\u0098valid\u00e2\u0080\u0099). This final option returns the \\(K-M+1\\)\nvalues \\(w\\left[M-K\\right]\\) to \\(w\\left[0\\right]\\) inclusive.",
            "markdown"
        ],
        [
            "The function correlate can also take arbitrary N-D arrays as input\nand return the N-D convolution of the two arrays on output.",
            "markdown"
        ],
        [
            "When \\(N=2,\\) correlate and/or convolve can be used\nto construct arbitrary image filters to perform actions such as blurring,\nenhancing, and edge-detection for an image.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "image = datasets.face(gray=True)\n w = np.zeros((50, 50))\n w[0][0] = 1.0\n w[49][25] = 1.0\n image_new = signal.fftconvolve(image, w)",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image)\n plt.gray()\n plt.title('Original image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is the familiar photo of a raccoon climbing on a palm. The second plot has the FIR filter applied and has the two copies of the photo superimposed due to the twin peaks manually set in the filter kernel definition.\"' class=\"plot-directive\" src=\"../_images/signal-2_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image_new)\n plt.gray()\n plt.title('Filtered image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is the familiar photo of a raccoon climbing on a palm. The second plot has the FIR filter applied and has the two copies of the photo superimposed due to the twin peaks manually set in the filter kernel definition.\"' class=\"plot-directive\" src=\"../_images/signal-2_01_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "Calculating the convolution in the time domain as above is mainly used for\nfiltering when one of the signals is much smaller than the other ( \\(K\\gg\nM\\) ), otherwise linear filtering is more efficiently calculated in the\nfrequency domain provided by the function fftconvolve. By default,\nconvolve estimates the fastest method using choose_conv_method.",
            "markdown"
        ],
        [
            "If the filter function \\(w[n,m]\\) can be factored according to\n\n\\[h[n, m] = h_1[n] h_2[m],\\]",
            "markdown"
        ],
        [
            "convolution can be calculated by means of the function sepfir2d. As an\nexample, we consider a Gaussian filter gaussian\n\n\\[h[n, m] \\propto e^{-x^2-y^2} = e^{-x^2} e^{-y^2},\\]",
            "markdown"
        ],
        [
            "which is often used for blurring.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "image = np.asarray(datasets.ascent(), np.float64)\n w = signal.windows.gaussian(51, 10.0)\n image_new = signal.sepfir2d(image, w, w)",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image)\n plt.gray()\n plt.title('Original image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt=\"&quot;This code displays two plots. The first plot is a grayscale photo of two people climbing a wooden staircase taken from below. The second plot has the 2-D gaussian FIR window applied and appears very blurry. You can still tell it's a photo but the subject is ambiguous.&quot;\" class=\"plot-directive\" src=\"../_images/signal-3_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "plt.figure()\n plt.imshow(image_new)\n plt.gray()\n plt.title('Filtered image')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt=\"&quot;This code displays two plots. The first plot is a grayscale photo of two people climbing a wooden staircase taken from below. The second plot has the 2-D gaussian FIR window applied and appears very blurry. You can still tell it's a photo but the subject is ambiguous.&quot;\" class=\"plot-directive\" src=\"../_images/signal-3_01_00.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Difference-equation filtering": [
        [
            "A general class of linear 1-D filters (that includes convolution\nfilters) are filters described by the difference equation\n\n\\[\\sum_{k=0}^{N}a_{k}y\\left[n-k\\right]=\\sum_{k=0}^{M}b_{k}x\\left[n-k\\right],\\]",
            "markdown"
        ],
        [
            "where \\(x\\left[n\\right]\\) is the input sequence and\n\\(y\\left[n\\right]\\) is the output sequence. If we assume initial rest so\nthat \\(y\\left[n\\right]=0\\) for \\(n&lt;0\\), then this kind of filter can\nbe implemented using convolution. However, the convolution filter sequence\n\\(h\\left[n\\right]\\) could be infinite if \\(a_{k}\\neq0\\) for\n\\(k\\geq1.\\) In addition, this general class of linear filter allows\ninitial conditions to be placed on \\(y\\left[n\\right]\\) for \\(n&lt;0\\)\nresulting in a filter that cannot be expressed using convolution.",
            "markdown"
        ],
        [
            "The difference equation filter can be thought of as finding\n\\(y\\left[n\\right]\\) recursively in terms of its previous values\n\n\\[a_{0}y\\left[n\\right]=-a_{1}y\\left[n-1\\right]-\\cdots-a_{N}y\\left[n-N\\right]+\\cdots+b_{0}x\\left[n\\right]+\\cdots+b_{M}x\\left[n-M\\right].\\]",
            "markdown"
        ],
        [
            "Often, \\(a_{0}=1\\) is chosen for normalization. The implementation in SciPy\nof this general difference equation filter is a little more complicated than\nwould be implied by the previous equation. It is implemented so that only one\nsignal needs to be delayed. The actual implementation equations are (assuming\n\\(a_{0}=1\\) ):\n\n \\begin{eqnarray*} y\\left[n\\right] & = & b_{0}x\\left[n\\right]+z_{0}\\left[n-1\\right]\\\\ z_{0}\\left[n\\right] & = & b_{1}x\\left[n\\right]+z_{1}\\left[n-1\\right]-a_{1}y\\left[n\\right]\\\\ z_{1}\\left[n\\right] & = & b_{2}x\\left[n\\right]+z_{2}\\left[n-1\\right]-a_{2}y\\left[n\\right]\\\\ \\vdots & \\vdots & \\vdots\\\\ z_{K-2}\\left[n\\right] & = & b_{K-1}x\\left[n\\right]+z_{K-1}\\left[n-1\\right]-a_{K-1}y\\left[n\\right]\\\\ z_{K-1}\\left[n\\right] & = & b_{K}x\\left[n\\right]-a_{K}y\\left[n\\right],\\end{eqnarray*}",
            "markdown"
        ],
        [
            "where \\(K=\\max\\left(N,M\\right).\\) Note that \\(b_{K}=0\\) if \\(K>M\\)\nand \\(a_{K}=0\\) if \\(K>N.\\) In this way, the output at time \\(n\\)\ndepends only on the input at time \\(n\\) and the value of \\(z_{0}\\) at\nthe previous time. This can always be calculated as long as the \\(K\\)\nvalues \\(z_{0}\\left[n-1\\right]\\ldots z_{K-1}\\left[n-1\\right]\\) are\ncomputed and stored at each time step.",
            "markdown"
        ],
        [
            "The difference-equation filter is called using the command lfilter in\nSciPy. This command takes as inputs the vector \\(b,\\) the vector,\n\\(a,\\) a signal \\(x\\) and returns the vector \\(y\\) (the same\nlength as \\(x\\) ) computed using the equation given above. If \\(x\\) is\nN-D, then the filter is computed along the axis provided.\nIf desired, initial conditions providing the values of\n\\(z_{0}\\left[-1\\right]\\) to \\(z_{K-1}\\left[-1\\right]\\) can be provided\nor else it will be assumed that they are all zero. If initial conditions are\nprovided, then the final conditions on the intermediate variables are also\nreturned. These could be used, for example, to restart the calculation in the\nsame state.",
            "markdown"
        ],
        [
            "Sometimes, it is more convenient to express the initial conditions in terms of\nthe signals \\(x\\left[n\\right]\\) and \\(y\\left[n\\right].\\) In other\nwords, perhaps you have the values of \\(x\\left[-M\\right]\\) to\n\\(x\\left[-1\\right]\\) and the values of \\(y\\left[-N\\right]\\) to\n\\(y\\left[-1\\right]\\) and would like to determine what values of\n\\(z_{m}\\left[-1\\right]\\) should be delivered as initial conditions to the\ndifference-equation filter. It is not difficult to show that, for \\(0\\leq\nm&lt;K,\\)\n\n\\[z_{m}\\left[n\\right]=\\sum_{p=0}^{K-m-1}\\left(b_{m+p+1}x\\left[n-p\\right]-a_{m+p+1}y\\left[n-p\\right]\\right).\\]",
            "markdown"
        ],
        [
            "Using this formula, we can find the initial-condition vector\n\\(z_{0}\\left[-1\\right]\\) to \\(z_{K-1}\\left[-1\\right]\\) given initial\nconditions on \\(y\\) (and \\(x\\) ). The command lfiltic performs\nthis function.",
            "markdown"
        ],
        [
            "As an example, consider the following system:\n\n\\[y[n] = \\frac{1}{2} x[n] + \\frac{1}{4} x[n-1] + \\frac{1}{3} y[n-1]\\]",
            "markdown"
        ],
        [
            "The code calculates the signal \\(y[n]\\) for a given signal \\(x[n]\\);\nfirst for initial conditions \\(y[-1] = 0\\) (default case), then for\n\\(y[-1] = 2\\) by means of lfiltic.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import signal",
            "code"
        ],
        [
            "x = np.array([1., 0., 0., 0.])\n b = np.array([1.0/2, 1.0/4])\n a = np.array([1.0, -1.0/3])\n signal.lfilter(b, a, x)\narray([0.5, 0.41666667, 0.13888889, 0.0462963])\n zi = signal.lfiltic(b, a, y=[2.])\n signal.lfilter(b, a, x, zi=zi)\n(array([ 1.16666667,  0.63888889,  0.21296296,  0.07098765]), array([0.02366]))",
            "code"
        ],
        [
            "Note that the output signal \\(y[n]\\) has the same length as the length as\nthe input signal \\(x[n]\\).",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Difference-equation filtering->Analysis of Linear Systems": [
        [
            "Linear system described a linear-difference equation can be fully described by\nthe coefficient vectors \\(a\\) and \\(b\\) as was done above; an alternative\nrepresentation is to provide a factor \\(k\\), \\(N_z\\) zeros \\(z_k\\)\nand \\(N_p\\) poles \\(p_k\\), respectively, to describe the system by\nmeans of its transfer function \\(H(z)\\), according to\n\n\\[H(z) = k \\frac{ (z-z_1)(z-z_2)...(z-z_{N_z})}{ (z-p_1)(z-p_2)...(z-p_{N_p})}.\\]",
            "markdown"
        ],
        [
            "This alternative representation can be obtained with the scipy function\ntf2zpk; the inverse is provided by zpk2tf.",
            "markdown"
        ],
        [
            "For the above example we have",
            "markdown"
        ],
        [
            "b = np.array([1.0/2, 1.0/4])\n a = np.array([1.0, -1.0/3])\n signal.tf2zpk(b, a)\n(array([-0.5]), array([ 0.33333333]), 0.5)",
            "code"
        ],
        [
            "i.e., the system has a zero at \\(z=-1/2\\) and a pole at \\(z=1/3\\).",
            "markdown"
        ],
        [
            "The scipy function freqz allows calculation of the frequency response\nof a system described by the coefficients \\(a_k\\) and \\(b_k\\). See the\nhelp of the freqz function for a comprehensive example.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Filter Design": [
        [
            "Time-discrete filters can be classified into finite response (FIR) filters and\ninfinite response (IIR) filters. FIR filters can provide a linear phase\nresponse, whereas IIR filters cannot. SciPy provides functions\nfor designing both types of filters.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter": [
        [
            "The function firwin designs filters according to the window method.\nDepending on the provided arguments, the function returns different filter\ntypes (e.g., low-pass, band-pass\u00e2\u0080\u00a6).",
            "markdown"
        ],
        [
            "The example below designs a low-pass and a band-stop filter, respectively.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "b1 = signal.firwin(40, 0.5)\n b2 = signal.firwin(41, [0.3, 0.8])\n w1, h1 = signal.freqz(b1)\n w2, h2 = signal.freqz(b2)",
            "code"
        ],
        [
            "plt.title('Digital filter frequency response')\n plt.plot(w1, 20*np.log10(np.abs(h1)), 'b')\n plt.plot(w2, 20*np.log10(np.abs(h2)), 'r')\n plt.ylabel('Amplitude Response (dB)')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays an X-Y plot with the amplitude response on the Y axis vs frequency on the X axis. The first (low-pass) trace in blue starts with a pass-band at 0 dB and curves down around halfway through with some ripple in the stop-band about 80 dB down. The second (band-stop) trace in red starts and ends at 0 dB, but the middle third is down about 60 dB from the peak with some ripple where the filter would supress a signal.\"' class=\"plot-directive\" src=\"../_images/signal-4.png\"/>\n</figure>",
            "code"
        ],
        [
            "Note that firwin uses, per default, a normalized frequency defined such\nthat the value \\(1\\) corresponds to the Nyquist frequency, whereas the\nfunction freqz is defined such that the value \\(\\pi\\) corresponds\nto the Nyquist frequency.",
            "markdown"
        ],
        [
            "The function firwin2 allows design of almost arbitrary frequency\nresponses by specifying an array of corner frequencies and corresponding\ngains, respectively.",
            "markdown"
        ],
        [
            "The example below designs a filter with such an arbitrary amplitude response.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "b = signal.firwin2(150, [0.0, 0.3, 0.6, 1.0], [1.0, 2.0, 0.5, 0.0])\n w, h = signal.freqz(b)",
            "code"
        ],
        [
            "plt.title('Digital filter frequency response')\n plt.plot(w, np.abs(h))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. A single trace forms a shape similar to a heartbeat signal.\"' class=\"plot-directive\" src=\"../_images/signal-5.png\"/>\n</figure>",
            "code"
        ],
        [
            "Note the linear scaling of the y-axis and the different definition of the\nNyquist frequency in firwin2 and freqz (as explained above).",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter": [
        [
            "SciPy provides two functions to directly design IIR iirdesign and\niirfilter, where the filter type (e.g., elliptic) is passed as an\nargument and several more filter design functions for specific filter types,\ne.g., ellip.",
            "markdown"
        ],
        [
            "The example below designs an elliptic low-pass filter with defined pass-band\nand stop-band ripple, respectively. Note the much lower filter order (order 4)\ncompared with the FIR filters from the examples above in order to reach the same\nstop-band attenuation of \\(\\approx 60\\) dB.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "b, a = signal.iirfilter(4, Wn=0.2, rp=5, rs=60, btype='lowpass', ftype='ellip')\n w, h = signal.freqz(b, a)",
            "code"
        ],
        [
            "plt.title('Digital filter frequency response')\n plt.plot(w, 20*np.log10(np.abs(h)))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response [dB]')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with amplitude response on the Y axis vs Frequency on the X axis. A single trace shows a smooth low-pass filter with the left third passband near 0 dB. The right two-thirds are about 60 dB down with two sharp narrow valleys dipping down to -100 dB.\"' class=\"plot-directive\" src=\"../_images/signal-6.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients": [
        [
            "Filter coefficients can be stored in several different formats:",
            "markdown"
        ],
        [
            "\u00e2\u0080\u0098ba\u00e2\u0080\u0099 or \u00e2\u0080\u0098tf\u00e2\u0080\u0099 = transfer function coefficients",
            "markdown"
        ],
        [
            "\u00e2\u0080\u0098zpk\u00e2\u0080\u0099 = zeros, poles, and overall gain",
            "markdown"
        ],
        [
            "\u00e2\u0080\u0098ss\u00e2\u0080\u0099 = state-space system representation",
            "markdown"
        ],
        [
            "\u00e2\u0080\u0098sos\u00e2\u0080\u0099 = transfer function coefficients of second-order sections",
            "markdown"
        ],
        [
            "Functions, such as tf2zpk and zpk2ss, can convert between them.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients->Transfer function representation": [
        [
            "The ba or tf format is a 2-tuple (b, a) representing a transfer\nfunction, where <em class=\"xref py py-obj\">b is a length M+1 array of coefficients of the <em class=\"xref py py-obj\">M-order\nnumerator polynomial, and <em class=\"xref py py-obj\">a is a length N+1 array of coefficients of the\n<em class=\"xref py py-obj\">N-order denominator, as positive, descending powers of the transfer function\nvariable. So the tuple of \\(b = [b_0, b_1, ..., b_M]\\) and\n\\(a =[a_0, a_1, ..., a_N]\\) can represent an analog filter of the form:\n\n\\[H(s) = \\frac\n{b_0 s^M + b_1 s^{(M-1)} + \\cdots + b_M}\n{a_0 s^N + a_1 s^{(N-1)} + \\cdots + a_N}\n= \\frac\n{\\sum_{i=0}^M b_i s^{(M-i)}}\n{\\sum_{i=0}^N a_i s^{(N-i)}}\\]",
            "markdown"
        ],
        [
            "or a discrete-time filter of the form:\n\n\\[H(z) = \\frac\n{b_0 z^M + b_1 z^{(M-1)} + \\cdots + b_M}\n{a_0 z^N + a_1 z^{(N-1)} + \\cdots + a_N}\n= \\frac\n{\\sum_{i=0}^M b_i z^{(M-i)}}\n{\\sum_{i=0}^N a_i z^{(N-i)}}.\\]",
            "markdown"
        ],
        [
            "This \u00e2\u0080\u009cpositive powers\u00e2\u0080\u009d form is found more commonly in controls\nengineering.  If <em class=\"xref py py-obj\">M and <em class=\"xref py py-obj\">N are equal (which is true for all filters\ngenerated by the bilinear transform), then this happens to be equivalent\nto the \u00e2\u0080\u009cnegative powers\u00e2\u0080\u009d discrete-time form preferred in DSP:\n\n\\[H(z) = \\frac\n{b_0 + b_1 z^{-1} + \\cdots + b_M z^{-M}}\n{a_0 + a_1 z^{-1} + \\cdots + a_N z^{-N}}\n= \\frac\n{\\sum_{i=0}^M b_i z^{-i}}\n{\\sum_{i=0}^N a_i z^{-i}}.\\]",
            "markdown"
        ],
        [
            "Although this is true for common filters, remember that this is not true\nin the general case. If <em class=\"xref py py-obj\">M and <em class=\"xref py py-obj\">N are not equal, the discrete-time\ntransfer function coefficients must first be converted to the \u00e2\u0080\u009cpositive\npowers\u00e2\u0080\u009d form before finding the poles and zeros.",
            "markdown"
        ],
        [
            "This representation suffers from numerical error at higher orders, so other\nformats are preferred when possible.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients->Zeros and poles representation": [
        [
            "The zpk format is a 3-tuple (z, p, k), where <em class=\"xref py py-obj\">z is an <em class=\"xref py py-obj\">M-length\narray of the complex zeros of the transfer function\n\\(z = [z_0, z_1, ..., z_{M-1}]\\), <em class=\"xref py py-obj\">p is an <em class=\"xref py py-obj\">N-length array of the\ncomplex poles of the transfer function \\(p = [p_0, p_1, ..., p_{N-1}]\\),\nand <em class=\"xref py py-obj\">k is a scalar gain.  These represent the digital transfer function:\n\n\\[H(z) = k \\cdot \\frac\n{(z - z_0) (z - z_1) \\cdots (z - z_{(M-1)})}\n{(z - p_0) (z - p_1) \\cdots (z - p_{(N-1)})}\n= k \\frac\n{\\prod_{i=0}^{M-1} (z - z_i)}\n{\\prod_{i=0}^{N-1} (z - p_i)}\\]",
            "markdown"
        ],
        [
            "or the analog transfer function:\n\n\\[H(s) = k \\cdot \\frac\n{(s - z_0) (s - z_1) \\cdots (s - z_{(M-1)})}\n{(s - p_0) (s - p_1) \\cdots (s - p_{(N-1)})}\n= k \\frac\n{\\prod_{i=0}^{M-1} (s - z_i)}\n{\\prod_{i=0}^{N-1} (s - p_i)}.\\]",
            "markdown"
        ],
        [
            "Although the sets of roots are stored as ordered NumPy arrays, their ordering\ndoes not matter: ([-1, -2], [-3, -4], 1) is the same filter as\n([-2, -1], [-4, -3], 1).",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients->State-space system representation": [
        [
            "The ss format is a 4-tuple of arrays (A, B, C, D) representing the\nstate-space of an <em class=\"xref py py-obj\">N-order digital/discrete-time system of the form:\n\n\\[\\begin{split}\\mathbf{x}[k+1] = A \\mathbf{x}[k] + B \\mathbf{u}[k]\\\\\n\\mathbf{y}[k] = C \\mathbf{x}[k] + D \\mathbf{u}[k]\\end{split}\\]",
            "markdown"
        ],
        [
            "or a continuous/analog system of the form:\n\n\\[\\begin{split}\\dot{\\mathbf{x}}(t) = A \\mathbf{x}(t) + B \\mathbf{u}(t)\\\\\n\\mathbf{y}(t) = C \\mathbf{x}(t) + D \\mathbf{u}(t),\\end{split}\\]",
            "markdown"
        ],
        [
            "with <em class=\"xref py py-obj\">P inputs, <em class=\"xref py py-obj\">Q outputs and <em class=\"xref py py-obj\">N state variables, where:",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">x is the state vector",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">y is the output vector of length <em class=\"xref py py-obj\">Q",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">u is the input vector of length <em class=\"xref py py-obj\">P",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">A is the state matrix, with shape (N, N)",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">B is the input matrix with shape (N, P)",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">C is the output matrix with shape (Q, N)",
            "markdown"
        ],
        [
            "<em class=\"xref py py-obj\">D is the feedthrough or feedforward matrix with shape (Q, P).  (In\ncases where the system does not have a direct feedthrough, all values in\n<em class=\"xref py py-obj\">D are zero.)",
            "markdown"
        ],
        [
            "State-space is the most general representation and the only one that allows\nfor multiple-input, multiple-output (MIMO) systems. There are multiple\nstate-space representations for a given transfer function. Specifically, the\n\u00e2\u0080\u009ccontrollable canonical form\u00e2\u0080\u009d and \u00e2\u0080\u009cobservable canonical form\u00e2\u0080\u009d have the same\ncoefficients as the tf representation, and, therefore, suffer from the same\nnumerical errors.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Filter Design->Filter Coefficients->Second-order sections representation": [
        [
            "The sos format is a single 2-D array of shape (n_sections, 6),\nrepresenting a sequence of second-order transfer functions which, when\ncascaded in series, realize a higher-order filter with minimal numerical\nerror. Each row corresponds to a second-order tf representation, with\nthe first three columns providing the numerator coefficients and the last\nthree providing the denominator coefficients:\n\n\\[[b_0, b_1, b_2, a_0, a_1, a_2]\\]",
            "markdown"
        ],
        [
            "The coefficients are typically normalized, such that \\(a_0\\) is always 1.\nThe section order is usually not important with floating-point computation;\nthe filter output will be the same, regardless of the order.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Filter Design->Filter transformations": [
        [
            "The IIR filter design functions first generate a prototype analog low-pass filter\nwith a normalized cutoff frequency of 1 rad/sec. This is then transformed into\nother frequencies and band types using the following substitutions:",
            "markdown"
        ],
        [
            "Here, \\(\\omega_0\\) is the new cutoff or center frequency, and\n\\(\\mathrm{BW}\\) is the bandwidth.  These preserve symmetry on a logarithmic\nfrequency axis.",
            "markdown"
        ],
        [
            "To convert the transformed analog filter into a digital filter, the\nbilinear transform is used, which makes the following substitution:\n\n\\[s \\rightarrow \\frac{2}{T} \\frac{z - 1}{z + 1},\\]",
            "markdown"
        ],
        [
            "where T is the sampling time (the inverse of the sampling frequency).",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Other filters": [
        [
            "The signal processing package provides many more filters as well.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Other filters->Median Filter": [
        [
            "A median filter is commonly applied when noise is markedly non-Gaussian or\nwhen it is desired to preserve edges. The median filter works by sorting all\nof the array pixel values in a rectangular region surrounding the point of\ninterest. The sample median of this list of neighborhood pixel values is used\nas the value for the output array. The sample median is the middle-array value\nin a sorted list of neighborhood values. If there are an even number of\nelements in the neighborhood, then the average of the middle two values is\nused as the median. A general purpose median filter that works on\nN-D arrays is medfilt. A specialized version that works\nonly for 2-D arrays is available as medfilt2d.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Other filters->Order Filter": [
        [
            "A median filter is a specific example of a more general class of filters\ncalled order filters. To compute the output at a particular pixel, all order\nfilters use the array values in a region surrounding that pixel. These array\nvalues are sorted and then one of them is selected as the output value. For\nthe median filter, the sample median of the list of array values is used as\nthe output. A general-order filter allows the user to select which of the\nsorted values will be used as the output. So, for example, one could choose to\npick the maximum in the list or the minimum. The order filter takes an\nadditional argument besides the input array and the region mask that specifies\nwhich of the elements in the sorted list of neighbor array values should be\nused as the output. The command to perform an order filter is\norder_filter.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Other filters->Wiener filter": [
        [
            "The Wiener filter is a simple deblurring filter for denoising images. This is\nnot the Wiener filter commonly described in image-reconstruction problems but,\ninstead, it is a simple, local-mean filter. Let \\(x\\) be the input signal,\nthen the output is\n\n\\[\\begin{split}y=\\left\\{ \\begin{array}{cc} \\frac{\\sigma^{2}}{\\sigma_{x}^{2}}m_{x}+\\left(1-\\frac{\\sigma^{2}}{\\sigma_{x}^{2}}\\right)x & \\sigma_{x}^{2}\\geq\\sigma^{2},\\\\ m_{x} & \\sigma_{x}^{2}&lt;\\sigma^{2},\\end{array}\\right.\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(m_{x}\\) is the local estimate of the mean and\n\\(\\sigma_{x}^{2}\\) is the local estimate of the variance. The window for\nthese estimates is an optional input parameter (default is \\(3\\times3\\) ).\nThe parameter \\(\\sigma^{2}\\) is a threshold noise parameter. If\n\\(\\sigma\\) is not given, then it is estimated as the average of the local\nvariances.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Other filters->Hilbert filter": [
        [
            "The Hilbert transform constructs the complex-valued analytic signal\nfrom a real signal. For example, if \\(x=\\cos\\omega n\\), then\n\\(y=\\textrm{hilbert}\\left(x\\right)\\) would return (except near the\nedges) \\(y=\\exp\\left(j\\omega n\\right).\\) In the frequency domain,\nthe hilbert transform performs\n\n\\[Y=X\\cdot H,\\]",
            "markdown"
        ],
        [
            "where \\(H\\) is \\(2\\) for positive frequencies, \\(0\\) for negative\nfrequencies, and \\(1\\) for zero-frequencies.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Filtering->Analog Filter Design": [
        [
            "The functions iirdesign, iirfilter, and the filter design\nfunctions for specific filter types (e.g., ellip) all have a flag\n<em class=\"xref py py-obj\">analog, which allows the design of analog filters as well.",
            "markdown"
        ],
        [
            "The example below designs an analog (IIR) filter, obtains via tf2zpk\nthe poles and zeros and plots them in the complex s-plane. The zeros at\n\\(\\omega \\approx 150\\) and \\(\\omega \\approx 300\\) can be clearly seen\nin the amplitude response.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "b, a = signal.iirdesign(wp=100, ws=200, gpass=2.0, gstop=40., analog=True)\n w, h = signal.freqs(b, a)",
            "code"
        ],
        [
            "plt.title('Analog filter frequency response')\n plt.plot(w, 20*np.log10(np.abs(h)))\n plt.ylabel('Amplitude Response [dB]')\n plt.xlabel('Frequency')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is an IIR filter response as an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. The low-pass filter shown has a passband from 0 to 100 Hz with 0 dB response and a stop-band from about 175 Hz to 1 KHz about 40 dB down. There are two sharp discontinuities in the filter near 175 Hz and 300 Hz. The second plot is an X-Y showing the transfer function in the complex plane. The Y axis is real-valued an the X axis is complex-valued. The filter has four zeros near [300+0j, 175+0j, -175+0j, -300+0j] shown as blue X markers. The filter also has four poles near [50-30j, -50-30j, 100-8j, -100-8j] shown as red dots.\"' class=\"plot-directive\" src=\"../_images/signal-7_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "z, p, k = signal.tf2zpk(b, a)",
            "code"
        ],
        [
            "plt.plot(np.real(z), np.imag(z), 'ob', markerfacecolor='none')\n plt.plot(np.real(p), np.imag(p), 'xr')\n plt.legend(['Zeros', 'Poles'], loc=2)",
            "code"
        ],
        [
            "plt.title('Pole / Zero Plot')\n plt.xlabel('Real')\n plt.ylabel('Imaginary')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is an IIR filter response as an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. The low-pass filter shown has a passband from 0 to 100 Hz with 0 dB response and a stop-band from about 175 Hz to 1 KHz about 40 dB down. There are two sharp discontinuities in the filter near 175 Hz and 300 Hz. The second plot is an X-Y showing the transfer function in the complex plane. The Y axis is real-valued an the X axis is complex-valued. The filter has four zeros near [300+0j, 175+0j, -175+0j, -300+0j] shown as blue X markers. The filter also has four poles near [50-30j, -50-30j, 100-8j, -100-8j] shown as red dots.\"' class=\"plot-directive\" src=\"../_images/signal-7_01_00.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements": [
        [
            "The scipy function periodogram provides a method to estimate the\nspectral density using the periodogram method.",
            "markdown"
        ],
        [
            "The example below calculates the periodogram of a sine signal in white\nGaussian noise.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "fs = 10e3\n N = 1e5\n amp = 2*np.sqrt(2)\n freq = 1270.0\n noise_power = 0.001 * fs / 2\n time = np.arange(N) / fs\n x = amp*np.sin(2*np.pi*freq*time)\n x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)",
            "code"
        ],
        [
            "f, Pper_spec = signal.periodogram(x, fs, 'flattop', scaling='spectrum')",
            "code"
        ],
        [
            "plt.semilogy(f, Pper_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a noise floor with a power level of 1e-3 with a single peak at 1270 Hz up to a power of 1. The noise floor measurements appear noisy and oscillate down to 1e-7.\"' class=\"plot-directive\" src=\"../_images/signal-8.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Signal Processing (scipy.signal)->Spectral Analysis->Spectral Analysis using Welch\u00e2\u0080\u0099s Method": [
        [
            "An improved method, especially with respect to noise immunity, is Welch\u00e2\u0080\u0099s\nmethod, which is implemented by the scipy function welch.",
            "markdown"
        ],
        [
            "The example below estimates the spectrum using Welch\u00e2\u0080\u0099s method and uses the\nsame parameters as the example above. Note the much smoother noise floor of\nthe spectrogram.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "fs = 10e3\n N = 1e5\n amp = 2*np.sqrt(2)\n freq = 1270.0\n noise_power = 0.001 * fs / 2\n time = np.arange(N) / fs\n x = amp*np.sin(2*np.pi*freq*time)\n x += np.random.normal(scale=np.sqrt(noise_power), size=time.shape)",
            "code"
        ],
        [
            "f, Pwelch_spec = signal.welch(x, fs, scaling='spectrum')",
            "code"
        ],
        [
            "plt.semilogy(f, Pwelch_spec)\n plt.xlabel('frequency [Hz]')\n plt.ylabel('PSD')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays a single X-Y log-linear plot with the power spectral density on the Y axis vs frequency on the X axis. A single blue trace shows a smooth noise floor at a power level of 6e-2 with a single peak up to a power level of 2 at 1270 Hz.\"' class=\"plot-directive\" src=\"../_images/signal-9.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Signal Processing (scipy.signal)->Spectral Analysis->Lomb-Scargle Periodograms (lombscargle)": [
        [
            "Least-squares spectral analysis (LSSA) [1] [2] is a method of estimating a frequency\nspectrum, based on a least-squares fit of sinusoids to data samples, similar\nto Fourier analysis. Fourier analysis, the most used spectral method in\nscience, generally boosts long-periodic noise in long-gapped records; LSSA\nmitigates such problems.",
            "markdown"
        ],
        [
            "The Lomb-Scargle method performs spectral analysis on unevenly-sampled data and\nis known to be a powerful way to find, and test the significance of, weak\nperiodic signals.",
            "markdown"
        ],
        [
            "For a time series comprising \\(N_{t}\\) measurements \\(X_{j}\\equiv\nX(t_{j})\\) sampled at times \\(t_{j}\\), where \\((j = 1, \\ldots, N_{t})\\),\nassumed to have been scaled and shifted, such that its mean is zero and its\nvariance is unity, the normalized Lomb-Scargle periodogram at frequency\n\\(f\\) is\n\n\\[P_{n}(f) \\frac{1}{2}\\left\\{\\frac{\\left[\\sum_{j}^{N_{t}}X_{j}\\cos\\omega(t_{j}-\\tau)\\right]^{2}}{\\sum_{j}^{N_{t}}\\cos^{2}\\omega(t_{j}-\\tau)}+\\frac{\\left[\\sum_{j}^{N_{t}}X_{j}\\sin\\omega(t_{j}-\\tau)\\right]^{2}}{\\sum_{j}^{N_{t}}\\sin^{2}\\omega(t_{j}-\\tau)}\\right\\}.\\]",
            "markdown"
        ],
        [
            "Here, \\(\\omega \\equiv 2\\pi f\\) is the angular frequency. The frequency-dependent\ntime offset \\(\\tau\\) is given by\n\n\\[\\tan 2\\omega\\tau = \\frac{\\sum_{j}^{N_{t}}\\sin 2\\omega t_{j}}{\\sum_{j}^{N_{t}}\\cos 2\\omega t_{j}}.\\]",
            "markdown"
        ],
        [
            "The lombscargle function calculates the periodogram using a slightly\nmodified algorithm due to Townsend [3], which allows the periodogram to be\ncalculated using only a single pass through the input arrays for each\nfrequency.",
            "markdown"
        ],
        [
            "The equation is refactored as:\n\n\\[P_{n}(f) = \\frac{1}{2}\\left[\\frac{(c_{\\tau}XC + s_{\\tau}XS)^{2}}{c_{\\tau}^{2}CC + 2c_{\\tau}s_{\\tau}CS + s_{\\tau}^{2}SS} + \\frac{(c_{\\tau}XS - s_{\\tau}XC)^{2}}{c_{\\tau}^{2}SS - 2c_{\\tau}s_{\\tau}CS + s_{\\tau}^{2}CC}\\right]\\]",
            "markdown"
        ],
        [
            "and\n\n\\[\\tan 2\\omega\\tau = \\frac{2CS}{CC-SS}.\\]",
            "markdown"
        ],
        [
            "Here,\n\n\\[c_{\\tau} = \\cos\\omega\\tau,\\qquad s_{\\tau} = \\sin\\omega\\tau,\\]",
            "markdown"
        ],
        [
            "while the sums are\n\n\\[\\begin{split}XC &= \\sum_{j}^{N_{t}} X_{j}\\cos\\omega t_{j}\\\\\nXS &= \\sum_{j}^{N_{t}} X_{j}\\sin\\omega t_{j}\\\\\nCC &= \\sum_{j}^{N_{t}} \\cos^{2}\\omega t_{j}\\\\\nSS &= \\sum_{j}^{N_{t}} \\sin^{2}\\omega t_{j}\\\\\nCS &= \\sum_{j}^{N_{t}} \\cos\\omega t_{j}\\sin\\omega t_{j}.\\end{split}\\]",
            "markdown"
        ],
        [
            "This requires \\(N_{f}(2N_{t}+3)\\) trigonometric function evaluations\ngiving a factor of \\(\\sim 2\\) speed increase over the straightforward\nimplementation.",
            "markdown"
        ]
    ],
    "Signal Processing (scipy.signal)->Detrend": [
        [
            "SciPy provides the function detrend to remove a constant or linear\ntrend in a data series in order to see effect of higher order.",
            "markdown"
        ],
        [
            "The example below removes the constant and linear trend of a second-order\npolynomial time series and plots the remaining signal components.",
            "markdown"
        ],
        [
            "import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "t = np.linspace(-10, 10, 20)\n y = 1 + t + 0.01*t**2\n yconst = signal.detrend(y, type='constant')\n ylin = signal.detrend(y, type='linear')",
            "code"
        ],
        [
            "plt.plot(t, y, '-rx')\n plt.plot(t, yconst, '-bo')\n plt.plot(t, ylin, '-k+')\n plt.grid()\n plt.legend(['signal', 'const. detrend', 'linear detrend'])\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with no units. A red trace corresponding to the original signal curves from the bottom left to the top right. A blue trace has the constant detrend applied and is below the red trace with zero Y offset. The last black trace has the linear detrend applied and is almost flat from left to right highlighting the curve of the original signal. This last trace has an average slope of zero and looks very different.\"' class=\"plot-directive\" src=\"../_images/signal-10.png\"/>\n</figure>",
            "code"
        ],
        [
            "References",
            "markdown"
        ],
        [
            "Some further reading and related software:\n<aside class=\"footnote-list brackets\">\n<aside class=\"footnote brackets\" id=\"id4\" role=\"note\">\n[1]",
            "markdown"
        ],
        [
            "N.R. Lomb \u00e2\u0080\u009cLeast-squares frequency analysis of unequally spaced\ndata\u00e2\u0080\u009d, Astrophysics and Space Science, vol 39, pp. 447-462, 1976\n</aside>\n<aside class=\"footnote brackets\" id=\"id5\" role=\"note\">\n[2]",
            "markdown"
        ],
        [
            "J.D. Scargle \u00e2\u0080\u009cStudies in astronomical time series analysis. II -\nStatistical aspects of spectral analysis of unevenly spaced data\u00e2\u0080\u009d,\nThe Astrophysical Journal, vol 263, pp. 835-853, 1982\n</aside>\n<aside class=\"footnote brackets\" id=\"id6\" role=\"note\">\n[3]",
            "markdown"
        ],
        [
            "R.H.D. Townsend, \u00e2\u0080\u009cFast calculation of the Lomb-Scargle\nperiodogram using graphics processing units.\u00e2\u0080\u009d, The Astrophysical\nJournal Supplement Series, vol 191, pp. 247-253, 2010\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)": [
        [
            "When SciPy is built using the optimized ATLAS LAPACK and BLAS\nlibraries, it has very fast linear algebra capabilities. If you dig\ndeep enough, all of the raw LAPACK and BLAS libraries are available\nfor your use for even more speed. In this section, some easier-to-use\ninterfaces to these routines are described.",
            "markdown"
        ],
        [
            "All of these linear algebra routines expect an object that can be\nconverted into a 2-D array. The output of these routines is\nalso a 2-D array.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->scipy.linalg vs numpy.linalg": [
        [
            "scipy.linalg contains all the functions in\nnumpy.linalg.\nplus some other more advanced ones not contained in numpy.linalg.",
            "markdown"
        ],
        [
            "Another advantage of using scipy.linalg over numpy.linalg is that\nit is always compiled with BLAS/LAPACK support, while for numpy this is\noptional. Therefore, the scipy version might be faster depending on how\nnumpy was installed.",
            "markdown"
        ],
        [
            "Therefore, unless you don\u00e2\u0080\u0099t want to add scipy as a dependency to\nyour numpy program, use scipy.linalg instead of numpy.linalg.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->numpy.matrix vs 2-D numpy.ndarray": [
        [
            "The classes that represent matrices, and basic operations, such as\nmatrix multiplications and transpose are a part of numpy.\nFor convenience, we summarize the differences between numpy.matrix\nand numpy.ndarray here.",
            "markdown"
        ],
        [
            "numpy.matrix is matrix class that has a more convenient interface\nthan numpy.ndarray for matrix operations. This class supports, for\nexample, MATLAB-like creation syntax via the semicolon, has matrix\nmultiplication as default for the * operator, and contains I\nand T members that serve as shortcuts for inverse and transpose:",
            "markdown"
        ],
        [
            "import numpy as np\n A = np.mat('[1 2;3 4]')\n A\nmatrix([[1, 2],\n        [3, 4]])\n A.I\nmatrix([[-2. ,  1. ],\n        [ 1.5, -0.5]])\n b = np.mat('[5 6]')\n b\nmatrix([[5, 6]])\n b.T\nmatrix([[5],\n        [6]])\n A*b.T\nmatrix([[17],\n        [39]])",
            "code"
        ],
        [
            "Despite its convenience, the use of the numpy.matrix class is\ndiscouraged, since it adds nothing that cannot be accomplished\nwith 2-D numpy.ndarray objects, and may lead to a confusion of which class\nis being used. For example, the above code can be rewritten as:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1,2],[3,4]])\n A\narray([[1, 2],\n      [3, 4]])\n linalg.inv(A)\narray([[-2. ,  1. ],\n      [ 1.5, -0.5]])\n b = np.array([[5,6]]) #2D array\n b\narray([[5, 6]])\n b.T\narray([[5],\n      [6]])\n A*b #not matrix multiplication!\narray([[ 5, 12],\n      [15, 24]])\n A.dot(b.T) #matrix multiplication\narray([[17],\n      [39]])\n b = np.array([5,6]) #1D array\n b\narray([5, 6])\n b.T  #not matrix transpose!\narray([5, 6])\n A.dot(b)  #does not matter for multiplication\narray([17, 39])",
            "code"
        ],
        [
            "scipy.linalg operations can be applied equally to\nnumpy.matrix or to 2D numpy.ndarray objects.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Basic routines->Finding the inverse": [
        [
            "The inverse of a matrix \\(\\mathbf{A}\\) is the matrix\n\\(\\mathbf{B}\\), such that \\(\\mathbf{AB}=\\mathbf{I}\\), where\n\\(\\mathbf{I}\\) is the identity matrix consisting of ones down the\nmain diagonal.  Usually, \\(\\mathbf{B}\\) is denoted\n\\(\\mathbf{B}=\\mathbf{A}^{-1}\\) . In SciPy, the matrix inverse of\nthe NumPy array, A, is obtained using linalg.inv (A), or\nusing A.I if A is a Matrix. For example, let\n\n\\[\\begin{split}\\mathbf{A} = \\left[\\begin{array}{ccc} 1 & 3 & 5\\\\ 2 & 5 & 1\\\\ 2 & 3 & 8\\end{array}\\right],\\end{split}\\]",
            "markdown"
        ],
        [
            "then\n\n\\[\\begin{split}\\mathbf{A^{-1}} = \\frac{1}{25}\n    \\left[\\begin{array}{ccc} -37 & 9 & 22 \\\\\n                              14 & 2 & -9 \\\\\n                              4 & -3 & 1\n          \\end{array}\\right] = %\n     \\left[\\begin{array}{ccc} -1.48 & 0.36 & 0.88  \\\\\n                               0.56 & 0.08 & -0.36 \\\\\n                               0.16 & -0.12 & 0.04\n           \\end{array}\\right].\\end{split}\\]",
            "markdown"
        ],
        [
            "The following example demonstrates this computation in SciPy",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1,3,5],[2,5,1],[2,3,8]])\n A\narray([[1, 3, 5],\n      [2, 5, 1],\n      [2, 3, 8]])\n linalg.inv(A)\narray([[-1.48,  0.36,  0.88],\n      [ 0.56,  0.08, -0.36],\n      [ 0.16, -0.12,  0.04]])\n A.dot(linalg.inv(A)) #double check\narray([[  1.00000000e+00,  -1.11022302e-16,  -5.55111512e-17],\n      [  3.05311332e-16,   1.00000000e+00,   1.87350135e-16],\n      [  2.22044605e-16,  -1.11022302e-16,   1.00000000e+00]])",
            "code"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Basic routines->Solving a linear system": [
        [
            "Solving linear systems of equations is straightforward using the scipy\ncommand linalg.solve. This command expects an input matrix and\na right-hand side vector. The solution vector is then computed. An\noption for entering a symmetric matrix is offered, which can speed up\nthe processing when applicable. As an example, suppose it is desired\nto solve the following simultaneous equations:\n\n \\begin{eqnarray*} x + 3y + 5z & = & 10 \\\\\n                   2x + 5y + z & = & 8  \\\\\n                   2x + 3y + 8z & = & 3\n \\end{eqnarray*}",
            "markdown"
        ],
        [
            "We could find the solution vector using a matrix inverse:\n\n\\[\\begin{split}\\left[\\begin{array}{c} x\\\\ y\\\\ z\\end{array}\\right]=\\left[\\begin{array}{ccc} 1 & 3 & 5\\\\ 2 & 5 & 1\\\\ 2 & 3 & 8\\end{array}\\right]^{-1}\\left[\\begin{array}{c} 10\\\\ 8\\\\ 3\\end{array}\\right]=\\frac{1}{25}\\left[\\begin{array}{c} -232\\\\ 129\\\\ 19\\end{array}\\right]=\\left[\\begin{array}{c} -9.28\\\\ 5.16\\\\ 0.76\\end{array}\\right].\\end{split}\\]",
            "markdown"
        ],
        [
            "However, it is better to use the linalg.solve command, which can be\nfaster and more numerically stable. In this case, it, however, gives the\nsame answer as shown in the following example:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1, 2], [3, 4]])\n A\narray([[1, 2],\n      [3, 4]])\n b = np.array([[5], [6]])\n b\narray([[5],\n      [6]])\n linalg.inv(A).dot(b)  # slow\narray([[-4. ],\n      [ 4.5]])\n A.dot(linalg.inv(A).dot(b)) - b  # check\narray([[  8.88178420e-16],\n      [  2.66453526e-15]])\n np.linalg.solve(A, b)  # fast\narray([[-4. ],\n      [ 4.5]])\n A.dot(np.linalg.solve(A, b)) - b  # check\narray([[ 0.],\n      [ 0.]])",
            "code"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Basic routines->Finding the determinant": [
        [
            "The determinant of a square matrix \\(\\mathbf{A}\\) is often denoted\n\\(\\left|\\mathbf{A}\\right|\\) and is a quantity often used in linear\nalgebra. Suppose \\(a_{ij}\\) are the elements of the matrix\n\\(\\mathbf{A}\\) and let \\(M_{ij}=\\left|\\mathbf{A}_{ij}\\right|\\)\nbe the determinant of the matrix left by removing the\n\\(i^{\\textrm{th}}\\) row and \\(j^{\\textrm{th}}\\) column from\n\\(\\mathbf{A}\\) . Then, for any row \\(i,\\)\n\n\\[\\left|\\mathbf{A}\\right|=\\sum_{j}\\left(-1\\right)^{i+j}a_{ij}M_{ij}.\\]",
            "markdown"
        ],
        [
            "This is a recursive way to define the determinant, where the base case\nis defined by accepting that the determinant of a \\(1\\times1\\) matrix is the only matrix element. In SciPy the determinant can be\ncalculated with linalg.det. For example, the determinant of\n\n\\[\\begin{split}\\mathbf{A=}\\left[\\begin{array}{ccc} 1 & 3 & 5\\\\ 2 & 5 & 1\\\\ 2 & 3 & 8\\end{array}\\right]\\end{split}\\]",
            "markdown"
        ],
        [
            "is\n\n \\begin{eqnarray*} \\left|\\mathbf{A}\\right| & = & 1\\left|\\begin{array}{cc} 5 & 1\\\\ 3 & 8\\end{array}\\right|-3\\left|\\begin{array}{cc} 2 & 1\\\\ 2 & 8\\end{array}\\right|+5\\left|\\begin{array}{cc} 2 & 5\\\\ 2 & 3\\end{array}\\right|\\\\  & = & 1\\left(5\\cdot8-3\\cdot1\\right)-3\\left(2\\cdot8-2\\cdot1\\right)+5\\left(2\\cdot3-2\\cdot5\\right)=-25.\\end{eqnarray*}.",
            "markdown"
        ],
        [
            "In SciPy, this is computed as shown in this example:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1,2],[3,4]])\n A\narray([[1, 2],\n      [3, 4]])\n linalg.det(A)\n-2.0",
            "code"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Basic routines->Computing norms": [
        [
            "Matrix and vector norms can also be computed with SciPy. A wide range\nof norm definitions are available using different parameters to the\norder argument of linalg.norm. This function takes a rank-1\n(vectors) or a rank-2 (matrices) array and an optional order argument\n(default is 2). Based on these inputs, a vector or matrix norm of the\nrequested order is computed.",
            "markdown"
        ],
        [
            "For vector x, the order parameter can be any real number including\ninf or -inf. The computed norm is\n\n\\[\\begin{split}\\left\\Vert \\mathbf{x}\\right\\Vert =\\left\\{ \\begin{array}{cc} \\max\\left|x_{i}\\right| & \\textrm{ord}=\\textrm{inf}\\\\ \\min\\left|x_{i}\\right| & \\textrm{ord}=-\\textrm{inf}\\\\ \\left(\\sum_{i}\\left|x_{i}\\right|^{\\textrm{ord}}\\right)^{1/\\textrm{ord}} & \\left|\\textrm{ord}\\right|&lt;\\infty.\\end{array}\\right.\\end{split}\\]",
            "markdown"
        ],
        [
            "For matrix \\(\\mathbf{A}\\), the only valid values for norm are \\(\\pm2,\\pm1,\\) \\(\\pm\\) inf, and \u00e2\u0080\u0098fro\u00e2\u0080\u0099 (or \u00e2\u0080\u0098f\u00e2\u0080\u0099) Thus,\n\n\\[\\begin{split}\\left\\Vert \\mathbf{A}\\right\\Vert =\\left\\{ \\begin{array}{cc} \\max_{i}\\sum_{j}\\left|a_{ij}\\right| & \\textrm{ord}=\\textrm{inf}\\\\ \\min_{i}\\sum_{j}\\left|a_{ij}\\right| & \\textrm{ord}=-\\textrm{inf}\\\\ \\max_{j}\\sum_{i}\\left|a_{ij}\\right| & \\textrm{ord}=1\\\\ \\min_{j}\\sum_{i}\\left|a_{ij}\\right| & \\textrm{ord}=-1\\\\ \\max\\sigma_{i} & \\textrm{ord}=2\\\\ \\min\\sigma_{i} & \\textrm{ord}=-2\\\\ \\sqrt{\\textrm{trace}\\left(\\mathbf{A}^{H}\\mathbf{A}\\right)} & \\textrm{ord}=\\textrm{'fro'}\\end{array}\\right.\\end{split}\\]",
            "markdown"
        ],
        [
            "where \\(\\sigma_{i}\\) are the singular values of \\(\\mathbf{A}\\).",
            "markdown"
        ],
        [
            "Examples:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A=np.array([[1,2],[3,4]])\n A\narray([[1, 2],\n      [3, 4]])\n linalg.norm(A)\n5.4772255750516612\n linalg.norm(A,'fro') # frobenius norm is the default\n5.4772255750516612\n linalg.norm(A,1) # L1 norm (max column sum)\n6\n linalg.norm(A,-1)\n4\n linalg.norm(A,np.inf) # L inf norm (max row sum)\n7",
            "code"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Basic routines->Solving linear least-squares problems and pseudo-inverses": [
        [
            "Linear least-squares problems occur in many branches of applied\nmathematics. In this problem, a set of linear scaling coefficients is\nsought that allows a model to fit the data. In particular, it is assumed\nthat data \\(y_{i}\\) is related to data \\(\\mathbf{x}_{i}\\)\nthrough a set of coefficients \\(c_{j}\\) and model functions\n\\(f_{j}\\left(\\mathbf{x}_{i}\\right)\\) via the model\n\n\\[y_{i}=\\sum_{j}c_{j}f_{j}\\left(\\mathbf{x}_{i}\\right)+\\epsilon_{i},\\]",
            "markdown"
        ],
        [
            "where \\(\\epsilon_{i}\\) represents uncertainty in the data. The\nstrategy of least squares is to pick the coefficients \\(c_{j}\\) to\nminimize\n\n\\[J\\left(\\mathbf{c}\\right)=\\sum_{i}\\left|y_{i}-\\sum_{j}c_{j}f_{j}\\left(x_{i}\\right)\\right|^{2}.\\]",
            "markdown"
        ],
        [
            "Theoretically, a global minimum will occur when\n\n\\[\\frac{\\partial J}{\\partial c_{n}^{*}}=0=\\sum_{i}\\left(y_{i}-\\sum_{j}c_{j}f_{j}\\left(x_{i}\\right)\\right)\\left(-f_{n}^{*}\\left(x_{i}\\right)\\right)\\]",
            "markdown"
        ],
        [
            "or\n\n \\begin{eqnarray*} \\sum_{j}c_{j}\\sum_{i}f_{j}\\left(x_{i}\\right)f_{n}^{*}\\left(x_{i}\\right) & = & \\sum_{i}y_{i}f_{n}^{*}\\left(x_{i}\\right)\\\\ \\mathbf{A}^{H}\\mathbf{Ac} & = & \\mathbf{A}^{H}\\mathbf{y}\\end{eqnarray*},",
            "markdown"
        ],
        [
            "where\n\n\\[\\left\\{ \\mathbf{A}\\right\\} _{ij}=f_{j}\\left(x_{i}\\right).\\]",
            "markdown"
        ],
        [
            "When \\(\\mathbf{A^{H}A}\\) is invertible, then\n\n\\[\\mathbf{c}=\\left(\\mathbf{A}^{H}\\mathbf{A}\\right)^{-1}\\mathbf{A}^{H}\\mathbf{y}=\\mathbf{A}^{\\dagger}\\mathbf{y},\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{A}^{\\dagger}\\) is called the pseudo-inverse of\n\\(\\mathbf{A}.\\) Notice that using this definition of\n\\(\\mathbf{A}\\) the model can be written\n\n\\[\\mathbf{y}=\\mathbf{Ac}+\\boldsymbol{\\epsilon}.\\]",
            "markdown"
        ],
        [
            "The command linalg.lstsq will solve the linear least-squares\nproblem for \\(\\mathbf{c}\\) given \\(\\mathbf{A}\\) and\n\\(\\mathbf{y}\\) . In addition, linalg.pinv will find\n\\(\\mathbf{A}^{\\dagger}\\) given \\(\\mathbf{A}.\\)",
            "markdown"
        ],
        [
            "The following example and figure demonstrate the use of\nlinalg.lstsq and linalg.pinv for solving a data-fitting\nproblem. The data shown below were generated using the model:\n\n\\[y_{i}=c_{1}e^{-x_{i}}+c_{2}x_{i},\\]",
            "markdown"
        ],
        [
            "where \\(x_{i}=0.1i\\) for \\(i=1\\ldots10\\) , \\(c_{1}=5\\),\nand \\(c_{2}=4.\\) Noise is added to \\(y_{i}\\) and the\ncoefficients \\(c_{1}\\) and \\(c_{2}\\) are estimated using\nlinear least squares.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n import matplotlib.pyplot as plt\n rng = np.random.default_rng()",
            "code"
        ],
        [
            "c1, c2 = 5.0, 2.0\n i = np.r_[1:11]\n xi = 0.1*i\n yi = c1*np.exp(-xi) + c2*xi\n zi = yi + 0.05 * np.max(yi) * rng.standard_normal(len(yi))",
            "code"
        ],
        [
            "A = np.c_[np.exp(-xi)[:, np.newaxis], xi[:, np.newaxis]]\n c, resid, rank, sigma = linalg.lstsq(A, zi)",
            "code"
        ],
        [
            "xi2 = np.r_[0.1:1.0:100j]\n yi2 = c[0]*np.exp(-xi2) + c[1]*xi2",
            "code"
        ],
        [
            "plt.plot(xi,zi,'x',xi2,yi2)\n plt.axis([0,1.1,3.0,5.5])\n plt.xlabel('$x_i$')\n plt.title('Data fitting with linalg.lstsq')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/linalg-1.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Basic routines->Generalized inverse": [
        [
            "The generalized inverse is calculated using the command\nlinalg.pinv. Let \\(\\mathbf{A}\\) be an\n\\(M\\times N\\) matrix, then if \\(M>N\\), the generalized\ninverse is\n\n\\[\\mathbf{A}^{\\dagger}=\\left(\\mathbf{A}^{H}\\mathbf{A}\\right)^{-1}\\mathbf{A}^{H},\\]",
            "markdown"
        ],
        [
            "while if \\(M&lt;N\\) matrix, the generalized inverse is\n\n\\[\\mathbf{A}^{\\#}=\\mathbf{A}^{H}\\left(\\mathbf{A}\\mathbf{A}^{H}\\right)^{-1}.\\]",
            "markdown"
        ],
        [
            "In the case that \\(M=N\\), then\n\n\\[\\mathbf{A}^{\\dagger}=\\mathbf{A}^{\\#}=\\mathbf{A}^{-1},\\]",
            "markdown"
        ],
        [
            "as long as \\(\\mathbf{A}\\) is invertible.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Decompositions": [
        [
            "In many applications, it is useful to decompose a matrix using other\nrepresentations. There are several decompositions supported by SciPy.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Decompositions->Eigenvalues and eigenvectors": [
        [
            "The eigenvalue-eigenvector problem is one of the most commonly\nemployed linear algebra operations. In one popular form, the\neigenvalue-eigenvector problem is to find for some square matrix\n\\(\\mathbf{A}\\) scalars \\(\\lambda\\) and corresponding vectors\n\\(\\mathbf{v}\\), such that\n\n\\[\\mathbf{Av}=\\lambda\\mathbf{v}.\\]",
            "markdown"
        ],
        [
            "For an \\(N\\times N\\) matrix, there are \\(N\\) (not necessarily\ndistinct) eigenvalues \u00e2\u0080\u0094 roots of the (characteristic) polynomial\n\n\\[\\left|\\mathbf{A}-\\lambda\\mathbf{I}\\right|=0.\\]",
            "markdown"
        ],
        [
            "The eigenvectors, \\(\\mathbf{v}\\), are also sometimes called right\neigenvectors to distinguish them from another set of left eigenvectors\nthat satisfy\n\n\\[\\mathbf{v}_{L}^{H}\\mathbf{A}=\\lambda\\mathbf{v}_{L}^{H}\\]",
            "markdown"
        ],
        [
            "or\n\n\\[\\mathbf{A}^{H}\\mathbf{v}_{L}=\\lambda^{*}\\mathbf{v}_{L}.\\]",
            "markdown"
        ],
        [
            "With its default optional arguments, the command linalg.eig\nreturns \\(\\lambda\\) and \\(\\mathbf{v}.\\) However, it can also\nreturn \\(\\mathbf{v}_{L}\\) and just \\(\\lambda\\) by itself (\nlinalg.eigvals returns just \\(\\lambda\\) as well).",
            "markdown"
        ],
        [
            "In addition, linalg.eig can also solve the more general eigenvalue problem\n\n \\begin{eqnarray*} \\mathbf{Av} & = & \\lambda\\mathbf{Bv}\\\\ \\mathbf{A}^{H}\\mathbf{v}_{L} & = & \\lambda^{*}\\mathbf{B}^{H}\\mathbf{v}_{L}\\end{eqnarray*}",
            "markdown"
        ],
        [
            "for square matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}.\\) The\nstandard eigenvalue problem is an example of the general eigenvalue\nproblem for \\(\\mathbf{B}=\\mathbf{I}.\\) When a generalized\neigenvalue problem can be solved, it provides a decomposition of\n\\(\\mathbf{A}\\) as\n\n\\[\\mathbf{A}=\\mathbf{BV}\\boldsymbol{\\Lambda}\\mathbf{V}^{-1},\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{V}\\) is the collection of eigenvectors into\ncolumns and \\(\\boldsymbol{\\Lambda}\\) is a diagonal matrix of\neigenvalues.",
            "markdown"
        ],
        [
            "By definition, eigenvectors are only defined up to a constant scale\nfactor. In SciPy, the scaling factor for the eigenvectors is chosen so\nthat \\(\\left\\Vert \\mathbf{v}\\right\\Vert\n^{2}=\\sum_{i}v_{i}^{2}=1.\\)",
            "markdown"
        ],
        [
            "As an example, consider finding the eigenvalues and eigenvectors of\nthe matrix\n\n\\[\\begin{split}\\mathbf{A}=\\left[\\begin{array}{ccc} 1 & 5 & 2\\\\ 2 & 4 & 1\\\\ 3 & 6 & 2\\end{array}\\right].\\end{split}\\]",
            "markdown"
        ],
        [
            "The characteristic polynomial is\n\n \\begin{eqnarray*} \\left|\\mathbf{A}-\\lambda\\mathbf{I}\\right| & = & \\left(1-\\lambda\\right)\\left[\\left(4-\\lambda\\right)\\left(2-\\lambda\\right)-6\\right]-\\\\  &  & 5\\left[2\\left(2-\\lambda\\right)-3\\right]+2\\left[12-3\\left(4-\\lambda\\right)\\right]\\\\  & = & -\\lambda^{3}+7\\lambda^{2}+8\\lambda-3.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "The roots of this polynomial are the eigenvalues of \\(\\mathbf{A}\\):\n\n \\begin{eqnarray*} \\lambda_{1} & = & 7.9579\\\\ \\lambda_{2} & = & -1.2577\\\\ \\lambda_{3} & = & 0.2997.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "The eigenvectors corresponding to each eigenvalue can be found using\nthe original equation. The eigenvectors associated with these\neigenvalues can then be found.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1, 2], [3, 4]])\n la, v = linalg.eig(A)\n l1, l2 = la\n print(l1, l2)   # eigenvalues\n(-0.3722813232690143+0j) (5.372281323269014+0j)\n print(v[:, 0])   # first eigenvector\n[-0.82456484  0.56576746]\n print(v[:, 1])   # second eigenvector\n[-0.41597356 -0.90937671]\n print(np.sum(abs(v**2), axis=0))  # eigenvectors are unitary\n[1. 1.]\n v1 = np.array(v[:, 0]).T\n print(linalg.norm(A.dot(v1) - l1*v1))  # check the computation\n3.23682852457e-16",
            "code"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Decompositions->Singular value decomposition": [
        [
            "Singular value decomposition (SVD) can be thought of as an extension of\nthe eigenvalue problem to matrices that are not square. Let\n\\(\\mathbf{A}\\) be an \\(M\\times N\\) matrix with \\(M\\) and\n\\(N\\) arbitrary. The matrices \\(\\mathbf{A}^{H}\\mathbf{A}\\) and\n\\(\\mathbf{A}\\mathbf{A}^{H}\\) are square hermitian matrices [1] of\nsize \\(N\\times N\\) and \\(M\\times M\\), respectively. It is known\nthat the eigenvalues of square hermitian matrices are real and\nnon-negative. In addition, there are at most\n\\(\\min\\left(M,N\\right)\\) identical non-zero eigenvalues of\n\\(\\mathbf{A}^{H}\\mathbf{A}\\) and \\(\\mathbf{A}\\mathbf{A}^{H}.\\)\nDefine these positive eigenvalues as \\(\\sigma_{i}^{2}.\\) The\nsquare-root of these are called singular values of \\(\\mathbf{A}.\\)\nThe eigenvectors of \\(\\mathbf{A}^{H}\\mathbf{A}\\) are collected by\ncolumns into an \\(N\\times N\\) unitary [2] matrix\n\\(\\mathbf{V}\\), while the eigenvectors of\n\\(\\mathbf{A}\\mathbf{A}^{H}\\) are collected by columns in the\nunitary matrix \\(\\mathbf{U}\\), the singular values are collected\nin an \\(M\\times N\\) zero matrix\n\\(\\mathbf{\\boldsymbol{\\Sigma}}\\) with main diagonal entries set to\nthe singular values. Then\n\n\\[\\mathbf{A=U}\\boldsymbol{\\Sigma}\\mathbf{V}^{H}\\]",
            "markdown"
        ],
        [
            "is the singular value decomposition of \\(\\mathbf{A}.\\) Every\nmatrix has a singular value decomposition. Sometimes, the singular\nvalues are called the spectrum of \\(\\mathbf{A}.\\) The command\nlinalg.svd will return \\(\\mathbf{U}\\) ,\n\\(\\mathbf{V}^{H}\\), and \\(\\sigma_{i}\\) as an array of the\nsingular values. To obtain the matrix \\(\\boldsymbol{\\Sigma}\\), use\nlinalg.diagsvd. The following example illustrates the use of\nlinalg.svd:",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import linalg\n A = np.array([[1,2,3],[4,5,6]])\n A\narray([[1, 2, 3],\n      [4, 5, 6]])\n M,N = A.shape\n U,s,Vh = linalg.svd(A)\n Sig = linalg.diagsvd(s,M,N)\n U, Vh = U, Vh\n U\narray([[-0.3863177 , -0.92236578],\n      [-0.92236578,  0.3863177 ]])\n Sig\narray([[ 9.508032  ,  0.        ,  0.        ],\n      [ 0.        ,  0.77286964,  0.        ]])\n Vh\narray([[-0.42866713, -0.56630692, -0.7039467 ],\n      [ 0.80596391,  0.11238241, -0.58119908],\n      [ 0.40824829, -0.81649658,  0.40824829]])\n U.dot(Sig.dot(Vh)) #check computation\narray([[ 1.,  2.,  3.],\n      [ 4.,  5.,  6.]])\n\n\n<aside class=\"footnote-list brackets\">\n<aside class=\"footnote brackets\" id=\"id3\" role=\"note\">\n[1]",
            "code"
        ],
        [
            "A hermitian matrix \\(\\mathbf{D}\\) satisfies \\(\\mathbf{D}^{H}=\\mathbf{D}.\\)\n</aside>\n<aside class=\"footnote brackets\" id=\"id4\" role=\"note\">\n[2]",
            "markdown"
        ],
        [
            "A unitary matrix \\(\\mathbf{D}\\) satisfies \\(\\mathbf{D}^{H}\\mathbf{D}=\\mathbf{I}=\\mathbf{D}\\mathbf{D}^{H}\\) so that \\(\\mathbf{D}^{-1}=\\mathbf{D}^{H}.\\)\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Decompositions->LU decomposition": [
        [
            "The LU decomposition finds a representation for the \\(M\\times N\\)\nmatrix \\(\\mathbf{A}\\) as\n\n\\[\\mathbf{A}=\\mathbf{P}\\,\\mathbf{L}\\,\\mathbf{U},\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{P}\\) is an \\(M\\times M\\) permutation matrix (a\npermutation of the rows of the identity matrix), \\(\\mathbf{L}\\) is\nin \\(M\\times K\\) lower triangular or trapezoidal matrix (\n\\(K=\\min\\left(M,N\\right)\\)) with unit-diagonal, and\n\\(\\mathbf{U}\\) is an upper triangular or trapezoidal matrix. The\nSciPy command for this decomposition is linalg.lu.",
            "markdown"
        ],
        [
            "Such a decomposition is often useful for solving many simultaneous\nequations where the left-hand side does not change but the right-hand\nside does. For example, suppose we are going to solve\n\n\\[\\mathbf{A}\\mathbf{x}_{i}=\\mathbf{b}_{i}\\]",
            "markdown"
        ],
        [
            "for many different \\(\\mathbf{b}_{i}\\). The LU decomposition allows this to be written as\n\n\\[\\mathbf{PLUx}_{i}=\\mathbf{b}_{i}.\\]",
            "markdown"
        ],
        [
            "Because \\(\\mathbf{L}\\) is lower-triangular, the equation can be\nsolved for \\(\\mathbf{U}\\mathbf{x}_{i}\\) and, finally,\n\\(\\mathbf{x}_{i}\\) very rapidly using forward- and\nback-substitution. An initial time spent factoring \\(\\mathbf{A}\\)\nallows for very rapid solution of similar systems of equations in the\nfuture. If the intent for performing LU decomposition is for solving\nlinear systems, then the command linalg.lu_factor should be used\nfollowed by repeated applications of the command\nlinalg.lu_solve to solve the system for each new\nright-hand side.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Decompositions->Cholesky decomposition": [
        [
            "Cholesky decomposition is a special case of LU decomposition\napplicable to Hermitian positive definite matrices. When\n\\(\\mathbf{A}=\\mathbf{A}^{H}\\) and\n\\(\\mathbf{x}^{H}\\mathbf{Ax}\\geq0\\) for all \\(\\mathbf{x}\\),\nthen decompositions of \\(\\mathbf{A}\\) can be found so that\n\n \\begin{eqnarray*} \\mathbf{A} & = & \\mathbf{U}^{H}\\mathbf{U}\\\\ \\mathbf{A} & = & \\mathbf{L}\\mathbf{L}^{H}\\end{eqnarray*},",
            "markdown"
        ],
        [
            "where \\(\\mathbf{L}\\) is lower triangular and \\(\\mathbf{U}\\) is\nupper triangular. Notice that \\(\\mathbf{L}=\\mathbf{U}^{H}.\\) The\ncommand linalg.cholesky computes the Cholesky\nfactorization. For using the Cholesky factorization to solve systems of\nequations, there are also linalg.cho_factor and\nlinalg.cho_solve routines that work similarly to their LU\ndecomposition counterparts.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Decompositions->QR decomposition": [
        [
            "The QR decomposition (sometimes called a polar decomposition) works\nfor any \\(M\\times N\\) array and finds an \\(M\\times M\\) unitary\nmatrix \\(\\mathbf{Q}\\) and an \\(M\\times N\\) upper-trapezoidal\nmatrix \\(\\mathbf{R}\\), such that\n\n\\[\\mathbf{A=QR}.\\]",
            "markdown"
        ],
        [
            "Notice that if the SVD of \\(\\mathbf{A}\\) is known, then the QR decomposition can be found.\n\n\\[\\mathbf{A}=\\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{H}=\\mathbf{QR}\\]",
            "markdown"
        ],
        [
            "implies that \\(\\mathbf{Q}=\\mathbf{U}\\) and\n\\(\\mathbf{R}=\\boldsymbol{\\Sigma}\\mathbf{V}^{H}.\\) Note, however,\nthat in SciPy independent algorithms are used to find QR and SVD\ndecompositions. The command for QR decomposition is linalg.qr.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Decompositions->Schur decomposition": [
        [
            "For a square \\(N\\times N\\) matrix, \\(\\mathbf{A}\\), the Schur\ndecomposition finds (not necessarily unique) matrices\n\\(\\mathbf{T}\\) and \\(\\mathbf{Z}\\), such that\n\n\\[\\mathbf{A}=\\mathbf{ZT}\\mathbf{Z}^{H},\\]",
            "markdown"
        ],
        [
            "where \\(\\mathbf{Z}\\) is a unitary matrix and \\(\\mathbf{T}\\) is\neither upper triangular or quasi upper triangular, depending on whether\nor not a real Schur form or complex Schur form is requested.  For a\nreal Schur form both \\(\\mathbf{T}\\) and \\(\\mathbf{Z}\\) are\nreal-valued when \\(\\mathbf{A}\\) is real-valued. When\n\\(\\mathbf{A}\\) is a real-valued matrix, the real Schur form is only\nquasi upper triangular because \\(2\\times2\\) blocks extrude from\nthe main diagonal corresponding to any complex-valued\neigenvalues. The command linalg.schur finds the Schur\ndecomposition, while the command linalg.rsf2csf converts\n\\(\\mathbf{T}\\) and \\(\\mathbf{Z}\\) from a real Schur form to a\ncomplex Schur form. The Schur form is especially useful in calculating\nfunctions of matrices.",
            "markdown"
        ],
        [
            "The following example illustrates the Schur decomposition:",
            "markdown"
        ],
        [
            "from scipy import linalg\n A = np.mat('[1 3 2; 1 4 5; 2 3 6]')\n T, Z = linalg.schur(A)\n T1, Z1 = linalg.schur(A, 'complex')\n T2, Z2 = linalg.rsf2csf(T, Z)\n T\narray([[ 9.90012467,  1.78947961, -0.65498528],\n       [ 0.        ,  0.54993766, -1.57754789],\n       [ 0.        ,  0.51260928,  0.54993766]])\n T2\narray([[ 9.90012467+0.00000000e+00j, -0.32436598+1.55463542e+00j,\n        -0.88619748+5.69027615e-01j],\n       [ 0.        +0.00000000e+00j,  0.54993766+8.99258408e-01j,\n         1.06493862+3.05311332e-16j],\n       [ 0.        +0.00000000e+00j,  0.        +0.00000000e+00j,\n         0.54993766-8.99258408e-01j]])\n abs(T1 - T2) # different\narray([[  1.06604538e-14,   2.06969555e+00,   1.69375747e+00],  # may vary\n       [  0.00000000e+00,   1.33688556e-15,   4.74146496e-01],\n       [  0.00000000e+00,   0.00000000e+00,   1.13220977e-15]])\n abs(Z1 - Z2) # different\narray([[ 0.06833781,  0.88091091,  0.79568503],    # may vary\n       [ 0.11857169,  0.44491892,  0.99594171],\n       [ 0.12624999,  0.60264117,  0.77257633]])\n T, Z, T1, Z1, T2, Z2 = map(np.mat,(T,Z,T1,Z1,T2,Z2))\n abs(A - Z*T*Z.H)  # same\nmatrix([[  5.55111512e-16,   1.77635684e-15,   2.22044605e-15],\n        [  0.00000000e+00,   3.99680289e-15,   8.88178420e-16],\n        [  1.11022302e-15,   4.44089210e-16,   3.55271368e-15]])\n abs(A - Z1*T1*Z1.H)  # same\nmatrix([[  4.26993904e-15,   6.21793362e-15,   8.00007092e-15],\n        [  5.77945386e-15,   6.21798014e-15,   1.06653681e-14],\n        [  7.16681444e-15,   8.90271058e-15,   1.77635764e-14]])\n abs(A - Z2*T2*Z2.H)  # same\nmatrix([[  6.02594127e-16,   1.77648931e-15,   2.22506907e-15],\n        [  2.46275555e-16,   3.99684548e-15,   8.91642616e-16],\n        [  8.88225111e-16,   8.88312432e-16,   4.44104848e-15]])",
            "code"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Decompositions->Interpolative decomposition": [
        [
            "scipy.linalg.interpolative contains routines for computing the\ninterpolative decomposition (ID) of a matrix. For a matrix \\(A\n\\in \\mathbb{C}^{m \\times n}\\) of rank \\(k \\leq \\min \\{ m, n \\}\\)\nthis is a factorization\n\n\\[A \\Pi =\n\\begin{bmatrix}\n A \\Pi_{1} & A \\Pi_{2}\n\\end{bmatrix} =\nA \\Pi_{1}\n\\begin{bmatrix}\n I & T\n\\end{bmatrix},\\]",
            "markdown"
        ],
        [
            "where \\(\\Pi = [\\Pi_{1}, \\Pi_{2}]\\) is a permutation matrix with\n\\(\\Pi_{1} \\in \\{ 0, 1 \\}^{n \\times k}\\), i.e., \\(A \\Pi_{2} =\nA \\Pi_{1} T\\). This can equivalently be written as \\(A = BP\\),\nwhere \\(B = A \\Pi_{1}\\) and \\(P = [I, T] \\Pi^{\\mathsf{T}}\\)\nare the skeleton and interpolation matrices, respectively.",
            "markdown"
        ],
        [
            "See also",
            "markdown"
        ],
        [
            "scipy.linalg.interpolative \u00e2\u0080\u0094 for more information.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Matrix functions": [
        [
            "Consider the function \\(f\\left(x\\right)\\) with Taylor series expansion\n\n\\[f\\left(x\\right)=\\sum_{k=0}^{\\infty}\\frac{f^{\\left(k\\right)}\\left(0\\right)}{k!}x^{k}.\\]",
            "markdown"
        ],
        [
            "A matrix function can be defined using this Taylor series for the\nsquare matrix \\(\\mathbf{A}\\) as\n\n\\[f\\left(\\mathbf{A}\\right)=\\sum_{k=0}^{\\infty}\\frac{f^{\\left(k\\right)}\\left(0\\right)}{k!}\\mathbf{A}^{k}.\\]",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "While this serves as a useful representation of a matrix function, it is\nrarely the best way to calculate a matrix function. In particular, if the\nmatrix is not diagonalizable, results may be innacurate.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Matrix functions->Exponential and logarithm functions": [
        [
            "The matrix exponential is one of the more common matrix functions.\nThe preferred method for implementing the matrix exponential is to use\nscaling and a Pad\u00c3\u00a9 approximation for \\(e^{x}\\). This algorithm is\nimplemented as linalg.expm.",
            "markdown"
        ],
        [
            "The inverse of the matrix exponential is the matrix logarithm defined\nas the inverse of the matrix exponential:\n\n\\[\\mathbf{A}\\equiv\\exp\\left(\\log\\left(\\mathbf{A}\\right)\\right).\\]",
            "markdown"
        ],
        [
            "The matrix logarithm can be obtained with linalg.logm.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Matrix functions->Trigonometric functions": [
        [
            "The trigonometric functions, \\(\\sin\\), \\(\\cos\\), and\n\\(\\tan\\), are implemented for matrices in linalg.sinm,\nlinalg.cosm, and linalg.tanm, respectively. The matrix\nsine and cosine can be defined using Euler\u00e2\u0080\u0099s identity as\n\n \\begin{eqnarray*} \\sin\\left(\\mathbf{A}\\right) & = & \\frac{e^{j\\mathbf{A}}-e^{-j\\mathbf{A}}}{2j}\\\\ \\cos\\left(\\mathbf{A}\\right) & = & \\frac{e^{j\\mathbf{A}}+e^{-j\\mathbf{A}}}{2}.\\end{eqnarray*}",
            "markdown"
        ],
        [
            "The tangent is\n\n\\[\\tan\\left(x\\right)=\\frac{\\sin\\left(x\\right)}{\\cos\\left(x\\right)}=\\left[\\cos\\left(x\\right)\\right]^{-1}\\sin\\left(x\\right)\\]",
            "markdown"
        ],
        [
            "and so the matrix tangent is defined as\n\n\\[\\left[\\cos\\left(\\mathbf{A}\\right)\\right]^{-1}\\sin\\left(\\mathbf{A}\\right).\\]",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Matrix functions->Hyperbolic trigonometric functions": [
        [
            "The hyperbolic trigonometric functions, \\(\\sinh\\), \\(\\cosh\\),\nand \\(\\tanh\\), can also be defined for matrices using the familiar\ndefinitions:\n\n \\begin{eqnarray*} \\sinh\\left(\\mathbf{A}\\right) & = & \\frac{e^{\\mathbf{A}}-e^{-\\mathbf{A}}}{2}\\\\ \\cosh\\left(\\mathbf{A}\\right) & = & \\frac{e^{\\mathbf{A}}+e^{-\\mathbf{A}}}{2}\\\\ \\tanh\\left(\\mathbf{A}\\right) & = & \\left[\\cosh\\left(\\mathbf{A}\\right)\\right]^{-1}\\sinh\\left(\\mathbf{A}\\right).\\end{eqnarray*}",
            "markdown"
        ],
        [
            "These matrix functions can be found using linalg.sinhm,\nlinalg.coshm, and linalg.tanhm.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Matrix functions->Arbitrary function": [
        [
            "Finally, any arbitrary function that takes one complex number and\nreturns a complex number can be called as a matrix function using the\ncommand linalg.funm. This command takes the matrix and an\narbitrary Python function. It then implements an algorithm from Golub\nand Van Loan\u00e2\u0080\u0099s book \u00e2\u0080\u009cMatrix Computations\u00e2\u0080\u009d to compute the function applied\nto the matrix using a Schur decomposition.  Note that the function\nneeds to accept complex numbers as input in order to work with this\nalgorithm. For example, the following code computes the zeroth-order\nBessel function applied to a matrix.",
            "markdown"
        ],
        [
            "from scipy import special, linalg\n rng = np.random.default_rng()\n A = rng.random((3, 3))\n B = linalg.funm(A, lambda x: special.jv(0, x))\n A\narray([[0.06369197, 0.90647174, 0.98024544],\n       [0.68752227, 0.5604377 , 0.49142032],\n       [0.86754578, 0.9746787 , 0.37932682]])\n B\narray([[ 0.6929219 , -0.29728805, -0.15930896],\n       [-0.16226043,  0.71967826, -0.22709386],\n       [-0.19945564, -0.33379957,  0.70259022]])\n linalg.eigvals(A)\narray([ 1.94835336+0.j, -0.72219681+0.j, -0.22270006+0.j])\n special.jv(0, linalg.eigvals(A))\narray([0.25375345+0.j, 0.87379738+0.j, 0.98763955+0.j])\n linalg.eigvals(B)\narray([0.25375345+0.j, 0.87379738+0.j, 0.98763955+0.j])",
            "code"
        ],
        [
            "Note how, by virtue of how matrix analytic functions are defined,\nthe Bessel function has acted on the matrix eigenvalues.",
            "markdown"
        ]
    ],
    "Linear Algebra (scipy.linalg)->Special matrices": [
        [
            "SciPy and NumPy provide several functions for creating special matrices\nthat are frequently used in engineering and science.",
            "markdown"
        ],
        [
            "For examples of the use of these functions, see their respective docstrings.",
            "markdown"
        ]
    ],
    "Sparse eigenvalue problems with ARPACK->Introduction": [
        [
            "ARPACK [1] is a Fortran package which provides routines for quickly finding a few\neigenvalues/eigenvectors of large sparse matrices. In order to find these\nsolutions, it requires only left-multiplication by the matrix in question.\nThis operation is performed through a reverse-communication interface. The\nresult of this structure is that ARPACK is able to find eigenvalues and\neigenvectors of any linear function mapping a vector to a vector.",
            "markdown"
        ],
        [
            "All of the functionality provided in ARPACK is contained within the two\nhigh-level interfaces scipy.sparse.linalg.eigs and\nscipy.sparse.linalg.eigsh. eigs\nprovides interfaces for finding the\neigenvalues/vectors of real or complex nonsymmetric square matrices, while\neigsh provides interfaces for real-symmetric or complex-hermitian\nmatrices.",
            "markdown"
        ]
    ],
    "Sparse eigenvalue problems with ARPACK->Basic functionality": [
        [
            "ARPACK can solve either standard eigenvalue problems of the form\n\n\\[A \\mathbf{x} = \\lambda \\mathbf{x}\\]",
            "markdown"
        ],
        [
            "or general eigenvalue problems of the form\n\n\\[A \\mathbf{x} = \\lambda M \\mathbf{x}.\\]",
            "markdown"
        ],
        [
            "The power of ARPACK is that it can compute only a specified subset of\neigenvalue/eigenvector pairs. This is accomplished through the keyword\nwhich. The following values of which are available:",
            "markdown"
        ],
        [
            "which = 'LM' : Eigenvalues with largest magnitude (eigs, eigsh),\nthat is, largest eigenvalues in the euclidean norm of complex numbers.",
            "markdown"
        ],
        [
            "which = 'SM' : Eigenvalues with smallest magnitude (eigs, eigsh),\nthat is, smallest eigenvalues in the euclidean norm of complex numbers.",
            "markdown"
        ],
        [
            "which = 'LR' : Eigenvalues with largest real part (eigs).",
            "markdown"
        ],
        [
            "which = 'SR' : Eigenvalues with smallest real part (eigs).",
            "markdown"
        ],
        [
            "which = 'LI' : Eigenvalues with largest imaginary part (eigs).",
            "markdown"
        ],
        [
            "which = 'SI' : Eigenvalues with smallest imaginary part (eigs).",
            "markdown"
        ],
        [
            "which = 'LA' : Eigenvalues with largest algebraic value (eigsh),\nthat is, largest eigenvalues inclusive of any negative sign.",
            "markdown"
        ],
        [
            "which = 'SA' : Eigenvalues with smallest algebraic value (eigsh),\nthat is, smallest eigenvalues inclusive of any negative sign.",
            "markdown"
        ],
        [
            "which = 'BE' : Eigenvalues from both ends of the spectrum (eigsh).",
            "markdown"
        ],
        [
            "Note that ARPACK is generally better at finding extremal eigenvalues, that\nis, eigenvalues with large magnitudes. In particular, using which = 'SM'\nmay lead to slow execution time and/or anomalous results. A better approach\nis to use shift-invert mode.",
            "markdown"
        ]
    ],
    "Sparse eigenvalue problems with ARPACK->Shift-invert mode": [
        [
            "Shift-invert mode relies on the following observation. For the generalized\neigenvalue problem\n\n\\[A \\mathbf{x} = \\lambda M \\mathbf{x},\\]",
            "markdown"
        ],
        [
            "it can be shown that\n\n\\[(A - \\sigma M)^{-1} M \\mathbf{x} = \\nu \\mathbf{x},\\]",
            "markdown"
        ],
        [
            "where\n\n\\[\\nu = \\frac{1}{\\lambda - \\sigma}.\\]",
            "markdown"
        ]
    ],
    "Sparse eigenvalue problems with ARPACK->Examples": [
        [
            "Imagine you\u00e2\u0080\u0099d like to find the smallest and largest eigenvalues and the\ncorresponding eigenvectors for a large matrix. ARPACK can handle many\nforms of input: dense matrices ,such as numpy.ndarray instances, sparse\nmatrices, such as scipy.sparse.csr_matrix, or a general linear operator\nderived from scipy.sparse.linalg.LinearOperator. For this example, for\nsimplicity, we\u00e2\u0080\u0099ll construct a symmetric, positive-definite matrix.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.linalg import eig, eigh\n from scipy.sparse.linalg import eigs, eigsh\n np.set_printoptions(suppress=True)\n rng = np.random.default_rng()\n\n X = rng.random((100, 100)) - 0.5\n X = np.dot(X, X.T)  # create a symmetric matrix",
            "code"
        ],
        [
            "We now have a symmetric matrix X, with which to test the routines. First,\ncompute a standard eigenvalue decomposition using eigh:",
            "markdown"
        ],
        [
            "evals_all, evecs_all = eigh(X)",
            "code"
        ],
        [
            "As the dimension of X grows, this routine becomes very slow. Especially,\nif only a few eigenvectors and eigenvalues are needed, ARPACK can be a\nbetter option. First let\u00e2\u0080\u0099s compute the largest eigenvalues (which = 'LM')\nof X and compare them to the known results:",
            "markdown"
        ],
        [
            "evals_large, evecs_large = eigsh(X, 3, which='LM')\n print(evals_all[-3:])\n[29.22435321 30.05590784 30.58591252]\n print(evals_large)\n[29.22435321 30.05590784 30.58591252]\n print(np.dot(evecs_large.T, evecs_all[:,-3:]))\narray([[-1.  0.  0.],       # may vary (signs)\n       [ 0.  1.  0.],\n       [-0.  0. -1.]])",
            "code"
        ],
        [
            "The results are as expected. ARPACK recovers the desired eigenvalues and they\nmatch the previously known results. Furthermore, the eigenvectors are\northogonal, as we\u00e2\u0080\u0099d expect. Now, let\u00e2\u0080\u0099s attempt to solve for the eigenvalues\nwith smallest magnitude:",
            "markdown"
        ],
        [
            "evals_small, evecs_small = eigsh(X, 3, which='SM')\nTraceback (most recent call last):       # may vary (convergence)\n...\nscipy.sparse.linalg._eigen.arpack.arpack.ArpackNoConvergence:\nARPACK error -1: No convergence (1001 iterations, 0/3 eigenvectors converged)",
            "code"
        ],
        [
            "Oops. We see that, as mentioned above, ARPACK is not quite as adept at\nfinding small eigenvalues. There are a few ways this problem can be\naddressed. We could increase the tolerance (tol) to lead to faster\nconvergence:",
            "markdown"
        ],
        [
            "evals_small, evecs_small = eigsh(X, 3, which='SM', tol=1E-2)\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 0.99999999  0.00000024 -0.00000049],    # may vary (signs)\n       [-0.00000023  0.99999999  0.00000056],\n       [ 0.00000031 -0.00000037  0.99999852]])",
            "code"
        ],
        [
            "This works, but we lose the precision in the results. Another option is\nto increase the maximum number of iterations (maxiter) from 1000 to 5000:",
            "markdown"
        ],
        [
            "evals_small, evecs_small = eigsh(X, 3, which='SM', maxiter=5000)\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 1.  0.  0.],           # may vary (signs)\n       [-0.  1.  0.],\n       [ 0.  0. -1.]])",
            "code"
        ],
        [
            "We get the results we\u00e2\u0080\u0099d hoped for, but the computation time is much longer.\nFortunately, ARPACK contains a mode that allows a quick determination of\nnon-external eigenvalues: shift-invert mode. As mentioned above, this\nmode involves transforming the eigenvalue problem to an equivalent problem\nwith different eigenvalues. In this case, we hope to find eigenvalues near\nzero, so we\u00e2\u0080\u0099ll choose sigma = 0. The transformed eigenvalues will\nthen satisfy \\(\\nu = 1/(\\lambda - \\sigma) = 1/\\lambda\\), so our\nsmall eigenvalues \\(\\lambda\\) become large eigenvalues \\(\\nu\\).",
            "markdown"
        ],
        [
            "evals_small, evecs_small = eigsh(X, 3, sigma=0, which='LM')\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 1.  0.  0.],    # may vary (signs)\n       [ 0. -1. -0.],\n       [-0. -0.  1.]])",
            "code"
        ],
        [
            "We get the results we were hoping for, with much less computational time.\nNote that the transformation from \\(\\nu \\to \\lambda\\) takes place\nentirely in the background. The user need not worry about the details.",
            "markdown"
        ],
        [
            "The shift-invert mode provides more than just a fast way to obtain a few\nsmall eigenvalues. Say, you\ndesire to find internal eigenvalues and eigenvectors, e.g., those nearest to\n\\(\\lambda = 1\\). Simply set sigma = 1 and ARPACK will take care of\nthe rest:",
            "markdown"
        ],
        [
            "evals_mid, evecs_mid = eigsh(X, 3, sigma=1, which='LM')\n i_sort = np.argsort(abs(1. / (1 - evals_all)))[-3:]\n evals_all[i_sort]\narray([0.94164107, 1.05464515, 0.99090277])\n evals_mid\narray([0.94164107, 0.99090277, 1.05464515])\n print(np.dot(evecs_mid.T, evecs_all[:,i_sort]))\narray([[-0.  1.  0.],     # may vary (signs)\n       [-0. -0.  1.],\n       [ 1.  0.  0.]]",
            "code"
        ],
        [
            "The eigenvalues come out in a different order, but they\u00e2\u0080\u0099re all there.\nNote that the shift-invert mode requires the internal solution of a matrix\ninverse. This is taken care of automatically by eigsh and eigs,\nbut the operation can also be specified by the user. See the docstring of\nscipy.sparse.linalg.eigsh and\nscipy.sparse.linalg.eigs for details.",
            "markdown"
        ]
    ],
    "Sparse eigenvalue problems with ARPACK->Use of LinearOperator": [
        [
            "We consider now the case where you\u00e2\u0080\u0099d like to avoid creating a dense matrix\nand use scipy.sparse.linalg.LinearOperator instead. Our first\nlinear operator applies element-wise multiplication between the input vector\nand a vector \\(\\mathbf{d}\\) provided by the user to the operator itself.\nThis operator mimics a diagonal matrix with the elements of \\(\\mathbf{d}\\)\nalong the main diagonal and it has the main benefit that the forward and\nadjoint operations are simple element-wise multiplications other\nthan matrix-vector multiplications. For a diagonal matrix, we expect the\neigenvalues to be equal to the elements along the main diagonal, in this case\n\\(\\mathbf{d}\\). The eigenvalues and eigenvectors obtained with eigsh\nare compared to those obtained by using eigh when applied to\nthe dense matrix:",
            "markdown"
        ],
        [
            "from scipy.sparse.linalg import LinearOperator\n class Diagonal(LinearOperator):\n...     def __init__(self, diag, dtype='float32'):\n...         self.diag = diag\n...         self.shape = (len(self.diag), len(self.diag))\n...         self.dtype = np.dtype(dtype)\n...     def _matvec(self, x):\n...         return self.diag*x\n...     def _rmatvec(self, x):\n...         return self.diag*x",
            "code"
        ],
        [
            "N = 100\n rng = np.random.default_rng()\n d = rng.normal(0, 1, N).astype(np.float64)\n D = np.diag(d)\n Dop = Diagonal(d, dtype=np.float64)",
            "code"
        ],
        [
            "evals_all, evecs_all = eigh(D)\n evals_large, evecs_large = eigsh(Dop, 3, which='LA', maxiter=1e3)\n evals_all[-3:]\narray([1.53092498, 1.77243671, 2.00582508])\n evals_large\narray([1.53092498, 1.77243671, 2.00582508])\n print(np.dot(evecs_large.T, evecs_all[:,-3:]))\narray([[-1.  0.  0.],     # may vary (signs)\n       [-0. -1.  0.],\n       [ 0.  0. -1.]]",
            "code"
        ],
        [
            "In this case, we have created a quick and easy Diagonal operator.\nThe external library PyLops provides\nsimilar capabilities in the Diagonal operator,\nas well as several other operators.",
            "markdown"
        ],
        [
            "Finally, we consider a linear operator that mimics the application of a\nfirst-derivative stencil. In this case, the operator is equivalent to a real\nnonsymmetric matrix. Once again, we compare the estimated eigenvalues\nand eigenvectors with those from a dense matrix that applies the\nsame first derivative to an input signal:",
            "markdown"
        ],
        [
            "class FirstDerivative(LinearOperator):\n...     def __init__(self, N, dtype='float32'):\n...         self.N = N\n...         self.shape = (self.N, self.N)\n...         self.dtype = np.dtype(dtype)\n...     def _matvec(self, x):\n...         y = np.zeros(self.N, self.dtype)\n...         y[1:-1] = (0.5*x[2:]-0.5*x[0:-2])\n...         return y\n...     def _rmatvec(self, x):\n...         y = np.zeros(self.N, self.dtype)\n...         y[0:-2] = y[0:-2] - (0.5*x[1:-1])\n...         y[2:] = y[2:] + (0.5*x[1:-1])\n...         return y",
            "code"
        ],
        [
            "N = 21\n D = np.diag(0.5*np.ones(N-1), k=1) - np.diag(0.5*np.ones(N-1), k=-1)\n D[0] = D[-1] = 0 # take away edge effects\n Dop = FirstDerivative(N, dtype=np.float64)",
            "code"
        ],
        [
            "evals_all, evecs_all = eig(D)\n evals_large, evecs_large = eigs(Dop, 4, which='LI')\n evals_all_imag = evals_all.imag\n isort_imag = np.argsort(np.abs(evals_all_imag))\n evals_all_imag = evals_all_imag[isort_imag]\n evals_large_imag = evals_large.imag\n isort_imag = np.argsort(np.abs(evals_large_imag))\n evals_large_imag = evals_large_imag[isort_imag]\n evals_all_imag[-4:]\narray([-0.95105652, 0.95105652, -0.98768834, 0.98768834])\n evals_large_imag\narray([0.95105652, -0.95105652, 0.98768834, -0.98768834])",
            "code"
        ],
        [
            "Note that the eigenvalues of this operator are all imaginary. Moreover,\nthe keyword which='LI' of scipy.sparse.linalg.eigs produces\nthe eigenvalues with largest absolute imaginary part (both\npositive and negative). Again, a more advanced implementation of the\nfirst-derivative operator is available in the\nPyLops library under the name of\nFirstDerivative\noperator.",
            "markdown"
        ]
    ],
    "Sparse eigenvalue problems with ARPACK->References": [
        [
            "http://www.caam.rice.edu/software/ARPACK/\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders": [
        [
            "A Word Ladder is a word game\ninvented by Lewis Carroll, in which players find paths between words by\nswitching one letter at a time. For example, one can link \u00e2\u0080\u009cape\u00e2\u0080\u009d and \u00e2\u0080\u009cman\u00e2\u0080\u009d\nin the following way:\n\n\\[{\\rm ape \\to apt \\to ait \\to bit \\to big \\to bag \\to mag \\to man}\\]",
            "markdown"
        ],
        [
            "Note that each step involves changing just one letter of the word. This is\njust one possible path from \u00e2\u0080\u009cape\u00e2\u0080\u009d to \u00e2\u0080\u009cman\u00e2\u0080\u009d, but is it the shortest possible\npath? If we desire to find the shortest word-ladder path between two given\nwords, the sparse graph submodule can help.",
            "markdown"
        ],
        [
            "First, we need a list of valid words. Many operating systems have such a list\nbuilt in. For example, on linux, a word list can often be found at one of the\nfollowing locations:",
            "markdown"
        ],
        [
            "/usr/share/dict\n/var/lib/dict",
            "code"
        ],
        [
            "Another easy source for words are the Scrabble word lists available at various\nsites around the internet (search with your favorite search engine). We\u00e2\u0080\u0099ll\nfirst create this list. The system word lists consist of a file with one\nword per line. The following should be modified to use the particular word\nlist you have available:",
            "markdown"
        ],
        [
            "word_list = open('/usr/share/dict/words').readlines()\n word_list = map(str.strip, word_list)",
            "code"
        ],
        [
            "We want to look at words of length 3, so let\u00e2\u0080\u0099s select just those words of the\ncorrect length. We\u00e2\u0080\u0099ll also eliminate words which start with upper-case\n(proper nouns) or contain non-alphanumeric characters, like apostrophes and\nhyphens. Finally, we\u00e2\u0080\u0099ll make sure everything is lower-case for comparison\nlater:",
            "markdown"
        ],
        [
            "word_list = [word for word in word_list if len(word) == 3]\n word_list = [word for word in word_list if word[0].islower()]\n word_list = [word for word in word_list if word.isalpha()]\n word_list = list(map(str.lower, word_list))\n len(word_list)\n586    # may vary",
            "code"
        ],
        [
            "Now we have a list of 586 valid three-letter words (the exact number may\nchange depending on the particular list used). Each of these words will\nbecome a node in our graph, and we will create edges connecting the nodes\nassociated with each pair of words which differs by only one letter.",
            "markdown"
        ],
        [
            "There are efficient ways to do this, and inefficient ways to do this. To\ndo this as efficiently as possible, we\u00e2\u0080\u0099re going to use some sophisticated\nnumpy array manipulation:",
            "markdown"
        ],
        [
            "import numpy as np\n word_list = np.asarray(word_list)\n word_list.dtype   # these are unicode characters in Python 3\ndtype('&lt;U3')\n word_list.sort()  # sort for quick searching later",
            "code"
        ],
        [
            "We have an array where each entry is three unicode characters long. We\u00e2\u0080\u0099d like\nto find all pairs where exactly one character is different. We\u00e2\u0080\u0099ll start by\nconverting each word to a 3-D vector:",
            "markdown"
        ],
        [
            "word_bytes = np.ndarray((word_list.size, word_list.itemsize),\n...                         dtype='uint8',\n...                         buffer=word_list.data)\n # each unicode character is four bytes long. We only need first byte\n # we know that there are three characters in each word\n word_bytes = word_bytes[:, ::word_list.itemsize//3]\n word_bytes.shape\n(586, 3)    # may vary",
            "code"
        ],
        [
            "Now, we\u00e2\u0080\u0099ll use the\nHamming distance\nbetween each point to determine which pairs of words are connected.\nThe Hamming distance measures the fraction of entries between two vectors\nwhich differ: any two words with a Hamming distance equal to \\(1/N\\),\nwhere \\(N\\) is the number of letters, are connected in the word ladder:",
            "markdown"
        ],
        [
            "from scipy.spatial.distance import pdist, squareform\n from scipy.sparse import csr_matrix\n hamming_dist = pdist(word_bytes, metric='hamming')\n # there are three characters in each word\n graph = csr_matrix(squareform(hamming_dist &lt; 1.5 / 3))",
            "code"
        ],
        [
            "When comparing the distances, we don\u00e2\u0080\u0099t use an equality because this can be\nunstable for floating point values. The inequality produces the desired\nresult, as long as no two entries of the word list are identical. Now, that our\ngraph is set up, we\u00e2\u0080\u0099ll use a shortest path search to find the path between\nany two words in the graph:",
            "markdown"
        ],
        [
            "i1 = word_list.searchsorted('ape')\n i2 = word_list.searchsorted('man')\n word_list[i1]\n'ape'\n word_list[i2]\n'man'",
            "code"
        ],
        [
            "We need to check that these match, because if the words are not in the list,\nthat will not be the case. Now, all we need is to find the shortest path\nbetween these two indices in the graph. We\u00e2\u0080\u0099ll use\nDijkstra\u00e2\u0080\u0099s algorithm,\nbecause it allows us to find the path for just one node:",
            "markdown"
        ],
        [
            "from scipy.sparse.csgraph import dijkstra\n distances, predecessors = dijkstra(graph, indices=i1,\n...                                    return_predecessors=True)\n print(distances[i2])\n5.0    # may vary",
            "code"
        ],
        [
            "So we see that the shortest path between \u00e2\u0080\u009cape\u00e2\u0080\u009d and \u00e2\u0080\u009cman\u00e2\u0080\u009d contains only\nfive steps. We can use the predecessors returned by the algorithm to\nreconstruct this path:",
            "markdown"
        ],
        [
            "path = []\n i = i2\n while i != i1:\n...     path.append(word_list[i])\n...     i = predecessors[i]\n path.append(word_list[i1])\n print(path[::-1])\n['ape', 'apt', 'opt', 'oat', 'mat', 'man']    # may vary",
            "code"
        ],
        [
            "This is three fewer links than our initial example: the path from \u00e2\u0080\u009cape\u00e2\u0080\u009d to \u00e2\u0080\u009cman\u00e2\u0080\u009d\nis only five steps.",
            "markdown"
        ],
        [
            "Using other tools in the module, we can answer other questions. For example,\nare there three-letter words which are not linked in a word ladder? This\nis a question of connected components in the graph:",
            "markdown"
        ],
        [
            "from scipy.sparse.csgraph import connected_components\n N_components, component_list = connected_components(graph)\n print(N_components)\n15    # may vary",
            "code"
        ],
        [
            "In this particular sample of three-letter words, there are 15 connected\ncomponents: that is, 15 distinct sets of words with no paths between the\nsets. How many words are there in each of these sets? We can learn this from\nthe list of components:",
            "markdown"
        ],
        [
            "[np.sum(component_list == i) for i in range(N_components)]\n[571, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]    # may vary",
            "code"
        ],
        [
            "There is one large connected set and 14 smaller ones. Let\u00e2\u0080\u0099s look at the\nwords in the smaller ones:",
            "markdown"
        ],
        [
            "[list(word_list[np.nonzero(component_list == i)]) for i in range(1, N_components)]\n[['aha'],    # may vary\n ['chi'],\n ['ebb'],\n ['ems', 'emu'],\n ['gnu'],\n ['ism'],\n ['khz'],\n ['nth'],\n ['ova'],\n ['qua'],\n ['ugh'],\n ['ups'],\n ['urn'],\n ['use']]",
            "code"
        ],
        [
            "These are all the three-letter words which do not connect to others via a word\nladder.",
            "markdown"
        ],
        [
            "We might also be curious about which words are maximally separated. Which\ntwo words take the most links to connect? We can determine this by computing\nthe matrix of all shortest paths. Note that, by convention, the\ndistance between two non-connected points is reported to be infinity, so\nwe\u00e2\u0080\u0099ll need to remove these before finding the maximum:",
            "markdown"
        ],
        [
            "distances, predecessors = dijkstra(graph, return_predecessors=True)\n max_distance = np.max(distances[~np.isinf(distances)])\n print(max_distance)\n13.0    # may vary",
            "code"
        ],
        [
            "So, there is at least one pair of words which takes 13 steps to get from one\nto the other! Let\u00e2\u0080\u0099s determine which these are:",
            "markdown"
        ],
        [
            "i1, i2 = np.nonzero(distances == max_distance)\n list(zip(word_list[i1], word_list[i2]))\n[('imp', 'ohm'),    # may vary\n ('imp', 'ohs'),\n ('ohm', 'imp'),\n ('ohm', 'ump'),\n ('ohs', 'imp'),\n ('ohs', 'ump'),\n ('ump', 'ohm'),\n ('ump', 'ohs')]",
            "code"
        ],
        [
            "We see that there are two pairs of words which are maximally separated from\neach other: \u00e2\u0080\u0098imp\u00e2\u0080\u0099 and \u00e2\u0080\u0098ump\u00e2\u0080\u0099 on the one hand, and \u00e2\u0080\u0098ohm\u00e2\u0080\u0099 and \u00e2\u0080\u0098ohs\u00e2\u0080\u0099 on the other.\nWe can find the connecting list in the same way as above:",
            "markdown"
        ],
        [
            "path = []\n i = i2[0]\n while i != i1[0]:\n...     path.append(word_list[i])\n...     i = predecessors[i1[0], i]\n path.append(word_list[i1[0]])\n print(path[::-1])\n['imp', 'amp', 'asp', 'ass', 'ads', 'add', 'aid', 'mid', 'mod', 'moo', 'too', 'tho', 'oho', 'ohm']    # may vary",
            "code"
        ],
        [
            "This gives us the path we desired to see.",
            "markdown"
        ],
        [
            "Word ladders are just one potential application of scipy\u00e2\u0080\u0099s fast graph\nalgorithms for sparse matrices. Graph theory makes appearances in many\nareas of mathematics, data analysis, and machine learning. The sparse graph\ntools are flexible enough to handle many of these situations.",
            "markdown"
        ]
    ],
    "Spatial data structures and algorithms (scipy.spatial)": [
        [
            "scipy.spatial can compute triangulations, Voronoi diagrams, and\nconvex hulls of a set of points, by leveraging the Qhull library.",
            "markdown"
        ],
        [
            "Moreover, it contains KDTree implementations for nearest-neighbor point\nqueries, and utilities for distance computations in various metrics.",
            "markdown"
        ]
    ],
    "Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations": [
        [
            "The Delaunay triangulation is a subdivision of a set of points into a\nnon-overlapping set of triangles, such that no point is inside the\ncircumcircle of any triangle. In practice, such triangulations tend to\navoid triangles with small angles.",
            "markdown"
        ],
        [
            "Delaunay triangulation can be computed using scipy.spatial as follows:",
            "markdown"
        ],
        [
            "from scipy.spatial import Delaunay\n import numpy as np\n points = np.array([[0, 0], [0, 1.1], [1, 0], [1, 1]])\n tri = Delaunay(points)",
            "code"
        ],
        [
            "We can visualize it:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n plt.triplot(points[:,0], points[:,1], tri.simplices)\n plt.plot(points[:,0], points[:,1], 'o')",
            "code"
        ],
        [
            "And add some further decorations:",
            "markdown"
        ],
        [
            "for j, p in enumerate(points):\n...     plt.text(p[0]-0.03, p[1]+0.03, j, ha='right') # label the points\n for j, s in enumerate(tri.simplices):\n...     p = points[s].mean(axis=0)\n...     plt.text(p[0], p[1], '#%d' % j, ha='center') # label triangles\n plt.xlim(-0.5, 1.5); plt.ylim(-0.5, 1.5)\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with four green points annotated 0 through 3 roughly in the shape of a box. The box is outlined with a diagonal line between points 0 and 3 forming two adjacent triangles. The top triangle is annotated as #1 and the bottom triangle is annotated as #0.\"' class=\"plot-directive\" src=\"../_images/spatial-1.png\"/>\n</figure>",
            "code"
        ],
        [
            "The structure of the triangulation is encoded in the following way:\nthe simplices attribute contains the indices of the points in the\npoints array that make up the triangle. For instance:",
            "markdown"
        ],
        [
            "i = 1\n tri.simplices[i,:]\narray([3, 1, 0], dtype=int32)\n points[tri.simplices[i,:]]\narray([[ 1. ,  1. ],\n       [ 0. ,  1.1],\n       [ 0. ,  0. ]])",
            "code"
        ],
        [
            "Moreover, neighboring triangles can also be found:",
            "markdown"
        ],
        [
            "tri.neighbors[i]\narray([-1,  0, -1], dtype=int32)",
            "code"
        ],
        [
            "What this tells us is that this triangle has triangle #0 as a neighbor,\nbut no other neighbors. Moreover, it tells us that neighbor 0 is\nopposite the vertex 1 of the triangle:",
            "markdown"
        ],
        [
            "points[tri.simplices[i, 1]]\narray([ 0. ,  1.1])",
            "code"
        ],
        [
            "Indeed, from the figure, we see that this is the case.",
            "markdown"
        ],
        [
            "Qhull can also perform tessellations to simplices for\nhigher-dimensional point sets (for instance, subdivision into\ntetrahedra in 3-D).",
            "markdown"
        ]
    ],
    "Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations->Coplanar points": [
        [
            "It is important to note that not all points necessarily appear as\nvertices of the triangulation, due to numerical precision issues in\nforming the triangulation. Consider the above with a duplicated\npoint:",
            "markdown"
        ],
        [
            "points = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n tri = Delaunay(points)\n np.unique(tri.simplices.ravel())\narray([0, 1, 2, 3], dtype=int32)",
            "code"
        ],
        [
            "Observe that point #4, which is a duplicate, does not occur as a\nvertex of the triangulation. That this happened is recorded:",
            "markdown"
        ],
        [
            "tri.coplanar\narray([[4, 0, 3]], dtype=int32)",
            "code"
        ],
        [
            "This means that point 4 resides near triangle 0 and vertex 3, but is\nnot included in the triangulation.",
            "markdown"
        ],
        [
            "Note that such degeneracies can occur not only because of duplicated\npoints, but also for more complicated geometrical reasons, even in\npoint sets that at first sight seem well-behaved.",
            "markdown"
        ],
        [
            "However, Qhull has the \u00e2\u0080\u009cQJ\u00e2\u0080\u009d option, which instructs it to perturb the\ninput data randomly until degeneracies are resolved:",
            "markdown"
        ],
        [
            "tri = Delaunay(points, qhull_options=\"QJ Pp\")\n points[tri.simplices]\narray([[[1, 0],\n        [1, 1],\n        [0, 0]],\n       [[1, 1],\n        [1, 1],\n        [1, 0]],\n       [[1, 1],\n        [0, 1],\n        [0, 0]],\n       [[0, 1],\n        [1, 1],\n        [1, 1]]])",
            "code"
        ],
        [
            "Two new triangles appeared. However, we see that they are degenerate\nand have zero area.",
            "markdown"
        ]
    ],
    "Spatial data structures and algorithms (scipy.spatial)->Convex hulls": [
        [
            "A convex hull is the smallest convex object containing all points in a\ngiven point set.",
            "markdown"
        ],
        [
            "These can be computed via the Qhull wrappers in scipy.spatial as\nfollows:",
            "markdown"
        ],
        [
            "from scipy.spatial import ConvexHull\n rng = np.random.default_rng()\n points = rng.random((30, 2))   # 30 random points in 2-D\n hull = ConvexHull(points)",
            "code"
        ],
        [
            "The convex hull is represented as a set of N 1-D simplices,\nwhich in 2-D means line segments. The storage scheme is exactly the\nsame as for the simplices in the Delaunay triangulation discussed\nabove.",
            "markdown"
        ],
        [
            "We can illustrate the above result:",
            "markdown"
        ],
        [
            "import matplotlib.pyplot as plt\n plt.plot(points[:,0], points[:,1], 'o')\n for simplex in hull.simplices:\n...     plt.plot(points[simplex,0], points[simplex,1], 'k-')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with a few dozen random blue markers randomly distributed throughout. A single black line forms a convex hull around the boundary of the markers.\"' class=\"plot-directive\" src=\"../_images/spatial-2.png\"/>\n</figure>",
            "code"
        ],
        [
            "The same can be achieved with scipy.spatial.convex_hull_plot_2d.",
            "markdown"
        ]
    ],
    "Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams": [
        [
            "A Voronoi diagram is a subdivision of the space into the nearest\nneighborhoods of a given set of points.",
            "markdown"
        ],
        [
            "There are two ways to approach this object using scipy.spatial.\nFirst, one can use the KDTree to answer the question \u00e2\u0080\u009cwhich of the\npoints is closest to this one\u00e2\u0080\u009d, and define the regions that way:",
            "markdown"
        ],
        [
            "from scipy.spatial import KDTree\n points = np.array([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2],\n...                    [2, 0], [2, 1], [2, 2]])\n tree = KDTree(points)\n tree.query([0.1, 0.1])\n(0.14142135623730953, 0)",
            "code"
        ],
        [
            "So the point (0.1, 0.1) belongs to region 0. In color:",
            "markdown"
        ],
        [
            "x = np.linspace(-0.5, 2.5, 31)\n y = np.linspace(-0.5, 2.5, 33)\n xx, yy = np.meshgrid(x, y)\n xy = np.c_[xx.ravel(), yy.ravel()]\n import matplotlib.pyplot as plt\n dx_half, dy_half = np.diff(x[:2])[0] / 2., np.diff(y[:2])[0] / 2.\n x_edges = np.concatenate((x - dx_half, [x[-1] + dx_half]))\n y_edges = np.concatenate((y - dy_half, [y[-1] + dy_half]))\n plt.pcolormesh(x_edges, y_edges, tree.query(xy)[1].reshape(33, 31), shading='flat')\n plt.plot(points[:,0], points[:,1], 'ko')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/spatial-3_00_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "This does not, however, give the Voronoi diagram as a geometrical\nobject.",
            "markdown"
        ],
        [
            "The representation in terms of lines and points can be again\nobtained via the Qhull wrappers in scipy.spatial:",
            "markdown"
        ],
        [
            "from scipy.spatial import Voronoi\n vor = Voronoi(points)\n vor.vertices\narray([[0.5, 0.5],\n       [0.5, 1.5],\n       [1.5, 0.5],\n       [1.5, 1.5]])",
            "code"
        ],
        [
            "The Voronoi vertices denote the set of points forming the polygonal\nedges of the Voronoi regions. In this case, there are 9 different\nregions:",
            "markdown"
        ],
        [
            "vor.regions\n[[], [-1, 0], [-1, 1], [1, -1, 0], [3, -1, 2], [-1, 3], [-1, 2], [0, 1, 3, 2], [2, -1, 0], [3, -1, 1]]",
            "code"
        ],
        [
            "Negative value -1 again indicates a point at infinity. Indeed,\nonly one of the regions, [0, 1, 3, 2], is bounded. Note here that\ndue to similar numerical precision issues as in Delaunay triangulation\nabove, there may be fewer Voronoi regions than input points.",
            "markdown"
        ],
        [
            "The ridges (lines in 2-D) separating the regions are described as a\nsimilar collection of simplices as the convex hull pieces:",
            "markdown"
        ],
        [
            "vor.ridge_vertices\n[[-1, 0], [-1, 0], [-1, 1], [-1, 1], [0, 1], [-1, 3], [-1, 2], [2, 3], [-1, 3], [-1, 2], [1, 3], [0, 2]]",
            "code"
        ],
        [
            "These numbers present the indices of the Voronoi vertices making up the\nline segments. -1 is again a point at infinity \u00e2\u0080\u0094 only 4 of\nthe 12 lines are a bounded line segment, while others extend to\ninfinity.",
            "markdown"
        ],
        [
            "The Voronoi ridges are perpendicular to the lines drawn between the\ninput points. To which two points each ridge corresponds is also\nrecorded:",
            "markdown"
        ],
        [
            "vor.ridge_points\narray([[0, 3],\n       [0, 1],\n       [2, 5],\n       [2, 1],\n       [1, 4],\n       [7, 8],\n       [7, 6],\n       [7, 4],\n       [8, 5],\n       [6, 3],\n       [4, 5],\n       [4, 3]], dtype=int32)",
            "code"
        ],
        [
            "This information, taken together, is enough to construct the full\ndiagram.",
            "markdown"
        ],
        [
            "We can plot it as follows. First, the points and the Voronoi vertices:",
            "markdown"
        ],
        [
            "plt.plot(points[:, 0], points[:, 1], 'o')\n plt.plot(vor.vertices[:, 0], vor.vertices[:, 1], '*')\n plt.xlim(-1, 3); plt.ylim(-1, 3)",
            "code"
        ],
        [
            "Plotting the finite line segments goes as for the convex hull,\nbut now we have to guard for the infinite edges:",
            "markdown"
        ],
        [
            "for simplex in vor.ridge_vertices:\n...     simplex = np.asarray(simplex)\n...     if np.all(simplex = 0):\n...         plt.plot(vor.vertices[simplex, 0], vor.vertices[simplex, 1], 'k-')",
            "code"
        ],
        [
            "The ridges extending to infinity require a bit more care:",
            "markdown"
        ],
        [
            "center = points.mean(axis=0)\n for pointidx, simplex in zip(vor.ridge_points, vor.ridge_vertices):\n...     simplex = np.asarray(simplex)\n...     if np.any(simplex &lt; 0):\n...         i = simplex[simplex = 0][0] # finite end Voronoi vertex\n...         t = points[pointidx[1]] - points[pointidx[0]]  # tangent\n...         t = t / np.linalg.norm(t)\n...         n = np.array([-t[1], t[0]]) # normal\n...         midpoint = points[pointidx].mean(axis=0)\n...         far_point = vor.vertices[i] + np.sign(np.dot(midpoint - center, n)) * n * 100\n...         plt.plot([vor.vertices[i,0], far_point[0]],\n...                  [vor.vertices[i,1], far_point[1]], 'k--')\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/spatial-3_01_00.png\"/>\n</figure>",
            "code"
        ],
        [
            "This plot can also be created using scipy.spatial.voronoi_plot_2d.",
            "markdown"
        ],
        [
            "Voronoi diagrams can be used to create interesting generative art.  Try playing\nwith the settings of this mandala function to create your own!",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy import spatial\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "def mandala(n_iter, n_points, radius):\n...     \"\"\"Creates a mandala figure using Voronoi tesselations.\n...\n...     Parameters\n...     ----------\n...     n_iter : int\n...         Number of iterations, i.e. how many times the equidistant points will\n...         be generated.\n...     n_points : int\n...         Number of points to draw per iteration.\n...     radius : scalar\n...         The radial expansion factor.\n...\n...     Returns\n...     -------\n...     fig : matplotlib.Figure instance\n...\n...     Notes\n...     -----\n...     This code is adapted from the work of Audrey Roy Greenfeld [1]_ and Carlos\n...     Focil-Espinosa [2]_, who created beautiful mandalas with Python code.  That\n...     code in turn was based on Antonio S\u00c3\u00a1nchez Chinch\u00c3\u00b3n's R code [3]_.\n...\n...     References\n...     ----------\n...     .. [1] https://www.codemakesmehappy.com/2019/09/voronoi-mandalas.html\n...\n...     .. [2] https://github.com/CarlosFocil/mandalapy\n...\n...     .. [3] https://github.com/aschinchon/mandalas\n...\n...     \"\"\"\n...     fig = plt.figure(figsize=(10, 10))\n...     ax = fig.add_subplot(111)\n...     ax.set_axis_off()\n...     ax.set_aspect('equal', adjustable='box')\n...\n...     angles = np.linspace(0, 2*np.pi * (1 - 1/n_points), num=n_points) + np.pi/2\n...     # Starting from a single center point, add points iteratively\n...     xy = np.array([[0, 0]])\n...     for k in range(n_iter):\n...         t1 = np.array([])\n...         t2 = np.array([])\n...         # Add `n_points` new points around each existing point in this iteration\n...         for i in range(xy.shape[0]):\n...             t1 = np.append(t1, xy[i, 0] + radius**k * np.cos(angles))\n...             t2 = np.append(t2, xy[i, 1] + radius**k * np.sin(angles))\n...\n...         xy = np.column_stack((t1, t2))\n...\n...     # Create the Mandala figure via a Voronoi plot\n...     spatial.voronoi_plot_2d(spatial.Voronoi(xy), ax=ax)\n...\n...     return fig",
            "code"
        ],
        [
            "# Modify the following parameters in order to get different figures\n n_iter = 3\n n_points = 6\n radius = 4",
            "code"
        ],
        [
            "fig = mandala(n_iter, n_points, radius)\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/spatial-4.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Introduction": [
        [
            "In this tutorial, we discuss many, but certainly not all, features of\nscipy.stats. The intention here is to provide a user with a\nworking knowledge of this package. We refer to the\nreference manual for further details.",
            "markdown"
        ],
        [
            "Note: This documentation is work in progress.\n\n\nDiscrete Statistical Distributions\nContinuous Statistical Distributions\nUniversal Non-Uniform Random Number Sampling in SciPy\nResampling and Monte Carlo Methods",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Random variables": [
        [
            "There are two general distribution classes that have been implemented\nfor encapsulating continuous random variables and discrete random variables. Over 80 continuous random variables\n(RVs) and 10 discrete random variables have been implemented using\nthese classes. Besides this, new routines and distributions can be\neasily added by the end user. (If you create one, please contribute it.)",
            "markdown"
        ],
        [
            "All of the statistics functions are located in the sub-package\nscipy.stats and a fairly complete listing of these functions\ncan be obtained using info(stats). The list of the random\nvariables available can also be obtained from the docstring for the\nstats sub-package.",
            "markdown"
        ],
        [
            "In the discussion below, we mostly focus on continuous RVs. Nearly everything\nalso applies to discrete variables, but we point out some differences\nhere: Specific points for discrete distributions.",
            "markdown"
        ],
        [
            "In the code samples below, we assume that the scipy.stats package\nis imported as",
            "markdown"
        ],
        [
            "from scipy import stats",
            "code"
        ],
        [
            "and in some cases we assume that individual objects are imported as",
            "markdown"
        ],
        [
            "from scipy.stats import norm",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Getting help": [
        [
            "First of all, all distributions are accompanied with help\nfunctions. To obtain just some basic information, we print the relevant\ndocstring: print(stats.norm.__doc__).",
            "markdown"
        ],
        [
            "To find the support, i.e., upper and lower bounds of the distribution,\ncall:",
            "markdown"
        ],
        [
            "print('bounds of distribution lower: %s, upper: %s' % norm.support())\nbounds of distribution lower: -inf, upper: inf",
            "code"
        ],
        [
            "We can list all methods and properties of the distribution with\ndir(norm). As it turns out, some of the methods are private,\nalthough they are not named as such (their names do not start\nwith a leading underscore), for example veccdf, are only available\nfor internal calculation (those methods will give warnings when one tries to\nuse them, and will be removed at some point).",
            "markdown"
        ],
        [
            "To obtain the real main methods, we list the methods of the frozen\ndistribution. (We explain the meaning of a <em class=\"xref py py-obj\">frozen distribution\nbelow).",
            "markdown"
        ],
        [
            "rv = norm()\n dir(rv)  # reformatted\n['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__',\n '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__',\n '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__',\n '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__',\n '__str__', '__subclasshook__', '__weakref__', 'a', 'args', 'b', 'cdf',\n 'dist', 'entropy', 'expect', 'interval', 'isf', 'kwds', 'logcdf',\n 'logpdf', 'logpmf', 'logsf', 'mean', 'median', 'moment', 'pdf', 'pmf',\n 'ppf', 'random_state', 'rvs', 'sf', 'stats', 'std', 'var']",
            "code"
        ],
        [
            "Finally, we can obtain the list of available distribution through\nintrospection:",
            "markdown"
        ],
        [
            "dist_continu = [d for d in dir(stats) if\n...                 isinstance(getattr(stats, d), stats.rv_continuous)]\n dist_discrete = [d for d in dir(stats) if\n...                  isinstance(getattr(stats, d), stats.rv_discrete)]\n print('number of continuous distributions: %d' % len(dist_continu))\nnumber of continuous distributions: 107\n print('number of discrete distributions:   %d' % len(dist_discrete))\nnumber of discrete distributions:   19",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Common methods": [
        [
            "The main public methods for continuous  RVs are:",
            "markdown"
        ],
        [
            "rvs:   Random Variates",
            "markdown"
        ],
        [
            "pdf:   Probability Density Function",
            "markdown"
        ],
        [
            "cdf:   Cumulative Distribution Function",
            "markdown"
        ],
        [
            "sf:    Survival Function (1-CDF)",
            "markdown"
        ],
        [
            "ppf:   Percent Point Function (Inverse of CDF)",
            "markdown"
        ],
        [
            "isf:   Inverse Survival Function (Inverse of SF)",
            "markdown"
        ],
        [
            "stats: Return mean, variance, (Fisher\u00e2\u0080\u0099s) skew, or (Fisher\u00e2\u0080\u0099s) kurtosis",
            "markdown"
        ],
        [
            "moment: non-central moments of the distribution",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s take a normal RV as an example.",
            "markdown"
        ],
        [
            "norm.cdf(0)\n0.5",
            "code"
        ],
        [
            "To compute the cdf at a number of points, we can pass a list or a numpy array.",
            "markdown"
        ],
        [
            "norm.cdf([-1., 0, 1])\narray([ 0.15865525,  0.5,  0.84134475])\n import numpy as np\n norm.cdf(np.array([-1., 0, 1]))\narray([ 0.15865525,  0.5,  0.84134475])",
            "code"
        ],
        [
            "Thus, the basic methods, such as <em class=\"xref py py-obj\">pdf, <em class=\"xref py py-obj\">cdf, and so on, are vectorized.",
            "markdown"
        ],
        [
            "Other generally useful methods are supported too:",
            "markdown"
        ],
        [
            "norm.mean(), norm.std(), norm.var()\n(0.0, 1.0, 1.0)\n norm.stats(moments=\"mv\")\n(array(0.0), array(1.0))",
            "code"
        ],
        [
            "To find the median of a distribution, we can use the percent point\nfunction ppf, which is the inverse of the cdf:",
            "markdown"
        ],
        [
            "norm.ppf(0.5)\n0.0",
            "code"
        ],
        [
            "To generate a sequence of random variates, use the size keyword\nargument:",
            "markdown"
        ],
        [
            "norm.rvs(size=3)\narray([-0.35687759,  1.34347647, -0.11710531])   # random",
            "code"
        ],
        [
            "Don\u00e2\u0080\u0099t think that norm.rvs(5) generates 5 variates:",
            "markdown"
        ],
        [
            "norm.rvs(5)\n5.471435163732493  # random",
            "code"
        ],
        [
            "Here, 5 with no keyword is being interpreted as the first possible\nkeyword argument, loc, which is the first of a pair of keyword arguments\ntaken by all continuous distributions.\nThis brings us to the topic of the next subsection.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Random number generation": [
        [
            "Drawing random numbers relies on generators from numpy.random package.\nIn the examples above, the specific stream of\nrandom numbers is not reproducible across runs. To achieve reproducibility,\nyou can explicitly seed a random number generator. In NumPy, a generator\nis an instance of numpy.random.Generator. Here is the canonical way to create\na generator:",
            "markdown"
        ],
        [
            "from numpy.random import default_rng\n rng = default_rng()",
            "code"
        ],
        [
            "And fixing the seed can be done like this:",
            "markdown"
        ],
        [
            "# do NOT copy this value\n rng = default_rng(301439351238479871608357552876690613766)",
            "code"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "Do not use this number or common values such as 0. Using just a\nsmall set of seeds to instantiate larger state spaces means that\nthere are some initial states that are impossible to reach. This\ncreates some biases if everyone uses such values. A good way to\nget a seed is to use a numpy.random.SeedSequence:",
            "markdown"
        ],
        [
            "from numpy.random import SeedSequence\n print(SeedSequence().entropy)\n301439351238479871608357552876690613766  # random",
            "code"
        ],
        [
            "The <em class=\"xref py py-obj\">random_state parameter in distributions accepts an instance of\nnumpy.random.Generator class, or an integer, which is then used to\nseed an internal Generator object:",
            "markdown"
        ],
        [
            "norm.rvs(size=5, random_state=rng)\narray([ 0.47143516, -1.19097569,  1.43270697, -0.3126519 , -0.72058873])  # random",
            "code"
        ],
        [
            "For further info, see NumPy\u00e2\u0080\u0099s documentation.",
            "markdown"
        ],
        [
            "To learn more about the random number samplers implemented in SciPy, see\nnon-uniform random number sampling tutorial and quasi monte carlo tutorial",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Shifting and scaling": [
        [
            "All continuous distributions take loc and scale as keyword\nparameters to adjust the location and scale of the distribution,\ne.g., for the standard normal distribution, the location is the mean and\nthe scale is the standard deviation.",
            "markdown"
        ],
        [
            "norm.stats(loc=3, scale=4, moments=\"mv\")\n(array(3.0), array(16.0))",
            "code"
        ],
        [
            "In many cases, the standardized distribution for a random variable X\nis obtained through the transformation (X - loc) / scale. The\ndefault values are loc = 0 and scale = 1.",
            "markdown"
        ],
        [
            "Smart use of loc and scale can help modify the standard\ndistributions in many ways. To illustrate the scaling further, the\ncdf of an exponentially distributed RV with mean \\(1/\\lambda\\)\nis given by\n\n\\[F(x) = 1 - \\exp(-\\lambda x)\\]",
            "markdown"
        ],
        [
            "By applying the scaling rule above, it can be seen that by\ntaking scale\u00a0 = 1./lambda we get the proper scale.",
            "markdown"
        ],
        [
            "from scipy.stats import expon\n expon.mean(scale=3.)\n3.0",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "Distributions that take shape parameters may\nrequire more than simple application of loc and/or\nscale to achieve the desired form. For example, the\ndistribution of 2-D vector lengths given a constant vector\nof length \\(R\\) perturbed by independent N(0, \\(\\sigma^2\\))\ndeviations in each component is\nrice(\\(R/\\sigma\\), scale= \\(\\sigma\\)). The first argument\nis a shape parameter that needs to be scaled along with \\(x\\).",
            "markdown"
        ],
        [
            "The uniform distribution is also interesting:",
            "markdown"
        ],
        [
            "from scipy.stats import uniform\n uniform.cdf([0, 1, 2, 3, 4, 5], loc=1, scale=4)\narray([ 0.  ,  0.  ,  0.25,  0.5 ,  0.75,  1.  ])",
            "code"
        ],
        [
            "Finally, recall from the previous paragraph that we are left with the\nproblem of the meaning of norm.rvs(5). As it turns out, calling a\ndistribution like this, the first argument, i.e., the 5, gets passed\nto set the loc parameter. Let\u00e2\u0080\u0099s see:",
            "markdown"
        ],
        [
            "np.mean(norm.rvs(5, size=500))\n5.0098355106969992  # random",
            "code"
        ],
        [
            "Thus, to explain the output of the example of the last section:\nnorm.rvs(5) generates a single normally distributed random variate with\nmean loc=5, because of the default size=1.",
            "markdown"
        ],
        [
            "We recommend that you set loc and scale parameters explicitly, by\npassing the values as keywords rather than as arguments. Repetition\ncan be minimized when calling more than one method of a given RV by\nusing the technique of Freezing a Distribution, as explained below.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Shape parameters": [
        [
            "While a general continuous random variable can be shifted and scaled\nwith the loc and scale parameters, some distributions require\nadditional shape parameters. For instance, the gamma distribution with density\n\n\\[\\gamma(x, a) = \\frac{\\lambda (\\lambda x)^{a-1}}{\\Gamma(a)} e^{-\\lambda x}\\;,\\]",
            "markdown"
        ],
        [
            "requires the shape parameter \\(a\\). Observe that setting\n\\(\\lambda\\) can be obtained by setting the scale keyword to\n\\(1/\\lambda\\).",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s check the number and name of the shape parameters of the gamma\ndistribution. (We know from the above that this should be 1.)",
            "markdown"
        ],
        [
            "from scipy.stats import gamma\n gamma.numargs\n1\n gamma.shapes\n'a'",
            "code"
        ],
        [
            "Now, we set the value of the shape variable to 1 to obtain the\nexponential distribution, so that we compare easily whether we get the\nresults we expect.",
            "markdown"
        ],
        [
            "gamma(1, scale=2.).stats(moments=\"mv\")\n(array(2.0), array(4.0))",
            "code"
        ],
        [
            "Notice that we can also specify shape parameters as keywords:",
            "markdown"
        ],
        [
            "gamma(a=1, scale=2.).stats(moments=\"mv\")\n(array(2.0), array(4.0))",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Freezing a distribution": [
        [
            "Passing the loc and scale keywords time and again can become\nquite bothersome. The concept of <em class=\"xref py py-obj\">freezing a RV is used to\nsolve such problems.",
            "markdown"
        ],
        [
            "rv = gamma(1, scale=2.)",
            "code"
        ],
        [
            "By using rv we no longer have to include the scale or the shape\nparameters anymore. Thus, distributions can be used in one of two\nways, either by passing all distribution parameters to each method\ncall (such as we did earlier) or by freezing the parameters for the\ninstance of the distribution. Let us check this:",
            "markdown"
        ],
        [
            "rv.mean(), rv.std()\n(2.0, 2.0)",
            "code"
        ],
        [
            "This is, indeed, what we should get.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Broadcasting": [
        [
            "The basic methods pdf, and so on, satisfy the usual numpy broadcasting rules. For\nexample, we can calculate the critical values for the upper tail of\nthe t distribution for different probabilities and degrees of freedom.",
            "markdown"
        ],
        [
            "stats.t.isf([0.1, 0.05, 0.01], [[10], [11]])\narray([[ 1.37218364,  1.81246112,  2.76376946],\n       [ 1.36343032,  1.79588482,  2.71807918]])",
            "code"
        ],
        [
            "Here, the first row contains the critical values for 10 degrees of freedom\nand the second row for 11 degrees of freedom (d.o.f.). Thus, the\nbroadcasting rules give the same result of calling isf twice:",
            "markdown"
        ],
        [
            "stats.t.isf([0.1, 0.05, 0.01], 10)\narray([ 1.37218364,  1.81246112,  2.76376946])\n stats.t.isf([0.1, 0.05, 0.01], 11)\narray([ 1.36343032,  1.79588482,  2.71807918])",
            "code"
        ],
        [
            "If the array with probabilities, i.e., [0.1, 0.05, 0.01] and the\narray of degrees of freedom i.e., [10, 11, 12], have the same\narray shape, then element-wise matching is used. As an example, we can\nobtain the 10% tail for 10 d.o.f., the 5% tail for 11 d.o.f. and the\n1% tail for 12 d.o.f. by calling",
            "markdown"
        ],
        [
            "stats.t.isf([0.1, 0.05, 0.01], [10, 11, 12])\narray([ 1.37218364,  1.79588482,  2.68099799])",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Specific points for discrete distributions": [
        [
            "Discrete distributions have mostly the same basic methods as the\ncontinuous distributions. However pdf is replaced by the probability\nmass function pmf, no estimation methods, such as fit, are\navailable, and scale is not a valid keyword parameter. The\nlocation parameter, keyword loc, can still be used to shift the\ndistribution.",
            "markdown"
        ],
        [
            "The computation of the cdf requires some extra attention. In the case\nof continuous distribution, the cumulative distribution function is, in\nmost standard cases, strictly monotonic increasing in the bounds (a,b)\nand has, therefore, a unique inverse. The cdf of a discrete\ndistribution, however, is a step function, hence the inverse cdf,\ni.e., the percent point function, requires a different definition:",
            "markdown"
        ],
        [
            "ppf(q) = min{x : cdf(x) = q, x integer}",
            "code"
        ],
        [
            "For further info, see the docs here.",
            "markdown"
        ],
        [
            "We can look at the hypergeometric distribution as an example",
            "markdown"
        ],
        [
            "from scipy.stats import hypergeom\n [M, n, N] = [20, 7, 12]",
            "code"
        ],
        [
            "If we use the cdf at some integer points and then evaluate the ppf at those\ncdf values, we get the initial integers back, for example",
            "markdown"
        ],
        [
            "x = np.arange(4) * 2\n x\narray([0, 2, 4, 6])\n prb = hypergeom.cdf(x, M, n, N)\n prb\narray([  1.03199174e-04,   5.21155831e-02,   6.08359133e-01,\n         9.89783282e-01])\n hypergeom.ppf(prb, M, n, N)\narray([ 0.,  2.,  4.,  6.])",
            "code"
        ],
        [
            "If we use values that are not at the kinks of the cdf step function, we get\nthe next higher integer back:",
            "markdown"
        ],
        [
            "hypergeom.ppf(prb + 1e-8, M, n, N)\narray([ 1.,  3.,  5.,  7.])\n hypergeom.ppf(prb - 1e-8, M, n, N)\narray([ 0.,  2.,  4.,  6.])",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Fitting distributions": [
        [
            "The main additional methods of the not frozen distribution are related\nto the estimation of distribution parameters:\n\n\nfit:   maximum likelihood estimation of distribution parameters, including location",
            "markdown"
        ],
        [
            "and scale\n\n</dl>",
            "markdown"
        ],
        [
            "fit_loc_scale: estimation of location and scale when shape parameters are given",
            "markdown"
        ],
        [
            "nnlf:  negative log likelihood function",
            "markdown"
        ],
        [
            "expect: calculate the expectation of a function against the pdf or pmf",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Performance issues and cautionary remarks": [
        [
            "The performance of the individual methods, in terms of speed, varies\nwidely by distribution and method. The results of a method are\nobtained in one of two ways: either by explicit calculation, or by a\ngeneric algorithm that is independent of the specific distribution.",
            "markdown"
        ],
        [
            "Explicit calculation, on the one hand, requires that the method is\ndirectly specified for the given distribution, either through analytic\nformulas or through special functions in scipy.special or\nnumpy.random for rvs. These are usually relatively fast\ncalculations.",
            "markdown"
        ],
        [
            "The generic methods, on the other hand, are used if the distribution\ndoes not specify any explicit calculation. To define a distribution,\nonly one of pdf or cdf is necessary; all other methods can be derived\nusing numeric integration and root finding. However, these indirect\nmethods can be <em class=\"xref py py-obj\">very slow. As an example, rgh =\nstats.gausshyper.rvs(0.5, 2, 2, 2, size=100) creates random\nvariables in a very indirect way and takes about 19 seconds for 100\nrandom variables on my computer, while one million random variables\nfrom the standard normal or from the t distribution take just above\none second.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Random variables->Remaining issues": [
        [
            "The distributions in scipy.stats have recently been corrected and improved\nand gained a considerable test suite; however, a few issues remain:",
            "markdown"
        ],
        [
            "The distributions have been tested over some range of parameters;\nhowever, in some corner ranges, a few incorrect results may remain.",
            "markdown"
        ],
        [
            "The maximum likelihood estimation in <em class=\"xref py py-obj\">fit does not work with\ndefault starting parameters for all distributions and the user\nneeds to supply good starting parameters. Also, for some\ndistribution using a maximum likelihood estimator might\ninherently not be the best choice.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Building specific distributions": [
        [
            "The next examples shows how to build your own distributions. Further\nexamples show the usage of the distributions and some statistical\ntests.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Building specific distributions->Making a continuous distribution, i.e., subclassing rv_continuous": [
        [
            "Making continuous distributions is fairly simple.",
            "markdown"
        ],
        [
            "from scipy import stats\n class deterministic_gen(stats.rv_continuous):\n...     def _cdf(self, x):\n...         return np.where(x &lt; 0, 0., 1.)\n...     def _stats(self):\n...         return 0., 0., 0., 0.",
            "code"
        ],
        [
            "deterministic = deterministic_gen(name=\"deterministic\")\n deterministic.cdf(np.arange(-3, 3, 0.5))\narray([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.])",
            "code"
        ],
        [
            "Interestingly,  the pdf is now computed automatically:",
            "markdown"
        ],
        [
            "deterministic.pdf(np.arange(-3, 3, 0.5))\narray([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n         5.83333333e+04,   4.16333634e-12,   4.16333634e-12,\n         4.16333634e-12,   4.16333634e-12,   4.16333634e-12])",
            "code"
        ],
        [
            "Be aware of the performance issues mentioned in\nPerformance issues and cautionary remarks. The computation of unspecified\ncommon methods can become very slow, since only general methods are\ncalled, which, by their very nature, cannot use any specific\ninformation about the distribution. Thus, as a cautionary example:",
            "markdown"
        ],
        [
            "from scipy.integrate import quad\n quad(deterministic.pdf, -1e-1, 1e-1)\n(4.163336342344337e-13, 0.0)",
            "code"
        ],
        [
            "But this is not correct: the integral over this pdf should be 1. Let\u00e2\u0080\u0099s make the\nintegration interval smaller:",
            "markdown"
        ],
        [
            "quad(deterministic.pdf, -1e-3, 1e-3)  # warning removed\n(1.000076872229173, 0.0010625571718182458)",
            "code"
        ],
        [
            "This looks better. However, the problem originated from the fact that\nthe pdf is not specified in the class definition of the deterministic\ndistribution.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Building specific distributions->Subclassing rv_discrete": [
        [
            "In the following, we use stats.rv_discrete to generate a discrete\ndistribution that has the probabilities of the truncated normal for the\nintervals centered around the integers.",
            "markdown"
        ],
        [
            "<strong>General info</strong>",
            "markdown"
        ],
        [
            "From the docstring of rv_discrete, help(stats.rv_discrete),",
            "markdown"
        ],
        [
            "\u00e2\u0080\u009cYou can construct an arbitrary discrete rv where P{X=xk} = pk by\npassing to the rv_discrete initialization method (through the values=\nkeyword) a tuple of sequences (xk, pk) which describes only those\nvalues of X (xk) that occur with nonzero probability (pk).\u00e2\u0080\u009d\n</blockquote>",
            "markdown"
        ],
        [
            "Next to this, there are some further requirements for this approach to\nwork:",
            "markdown"
        ],
        [
            "The keyword <em class=\"xref py py-obj\">name is required.",
            "markdown"
        ],
        [
            "The support points of the distribution xk have to be integers.",
            "markdown"
        ],
        [
            "The number of significant digits (decimals) needs to be specified.",
            "markdown"
        ],
        [
            "In fact, if the last two requirements are not satisfied, an exception\nmay be raised or the resulting numbers may be incorrect.",
            "markdown"
        ],
        [
            "<strong>An example</strong>",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s do the work. First:",
            "markdown"
        ],
        [
            "npoints = 20   # number of integer support points of the distribution minus 1\n npointsh = npoints // 2\n npointsf = float(npoints)\n nbound = 4   # bounds for the truncated normal\n normbound = (1+1/npointsf) * nbound   # actual bounds of truncated normal\n grid = np.arange(-npointsh, npointsh+2, 1)   # integer grid\n gridlimitsnorm = (grid-0.5) / npointsh * nbound   # bin limits for the truncnorm\n gridlimits = grid - 0.5   # used later in the analysis\n grid = grid[:-1]\n probs = np.diff(stats.truncnorm.cdf(gridlimitsnorm, -normbound, normbound))\n gridint = grid",
            "code"
        ],
        [
            "And, finally, we can subclass rv_discrete:",
            "markdown"
        ],
        [
            "normdiscrete = stats.rv_discrete(values=(gridint,\n...              np.round(probs, decimals=7)), name='normdiscrete')",
            "code"
        ],
        [
            "Now that we have defined the distribution, we have access to all\ncommon methods of discrete distributions.",
            "markdown"
        ],
        [
            "print('mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f' %\n...       normdiscrete.stats(moments='mvsk'))\nmean = -0.0000, variance = 6.3302, skew = 0.0000, kurtosis = -0.0076",
            "code"
        ],
        [
            "nd_std = np.sqrt(normdiscrete.stats(moments='v'))",
            "code"
        ],
        [
            "<strong>Testing the implementation</strong>",
            "markdown"
        ],
        [
            "Let\u00e2\u0080\u0099s generate a random sample and compare observed frequencies with\nthe probabilities.",
            "markdown"
        ],
        [
            "n_sample = 500\n rvs = normdiscrete.rvs(size=n_sample)\n f, l = np.histogram(rvs, bins=gridlimits)\n sfreq = np.vstack([gridint, f, probs*n_sample]).T\n print(sfreq)\n[[-1.00000000e+01  0.00000000e+00  2.95019349e-02]  # random\n [-9.00000000e+00  0.00000000e+00  1.32294142e-01]\n [-8.00000000e+00  0.00000000e+00  5.06497902e-01]\n [-7.00000000e+00  2.00000000e+00  1.65568919e+00]\n [-6.00000000e+00  1.00000000e+00  4.62125309e+00]\n [-5.00000000e+00  9.00000000e+00  1.10137298e+01]\n [-4.00000000e+00  2.60000000e+01  2.24137683e+01]\n [-3.00000000e+00  3.70000000e+01  3.89503370e+01]\n [-2.00000000e+00  5.10000000e+01  5.78004747e+01]\n [-1.00000000e+00  7.10000000e+01  7.32455414e+01]\n [ 0.00000000e+00  7.40000000e+01  7.92618251e+01]\n [ 1.00000000e+00  8.90000000e+01  7.32455414e+01]\n [ 2.00000000e+00  5.50000000e+01  5.78004747e+01]\n [ 3.00000000e+00  5.00000000e+01  3.89503370e+01]\n [ 4.00000000e+00  1.70000000e+01  2.24137683e+01]\n [ 5.00000000e+00  1.10000000e+01  1.10137298e+01]\n [ 6.00000000e+00  4.00000000e+00  4.62125309e+00]\n [ 7.00000000e+00  3.00000000e+00  1.65568919e+00]\n [ 8.00000000e+00  0.00000000e+00  5.06497902e-01]\n [ 9.00000000e+00  0.00000000e+00  1.32294142e-01]\n [ 1.00000000e+01  0.00000000e+00  2.95019349e-02]]\n\n\n<figure class=\"align-center\">\n<img alt='\"An X-Y histogram plot showing the distribution of random variates. A blue trace shows a normal bell curve. A blue bar chart perfectly approximates the curve showing the true distribution. A red bar chart representing the sample is well described by the blue trace but not exact.\"' class=\"plot-directive\" src=\"../_images/normdiscr_plot1.png\"/>\n</figure>\n<figure class=\"align-center\">\n<img alt='\"An X-Y histogram plot showing the cumulative distribution of random variates. A blue trace shows a CDF for a typical normal distribution. A blue bar chart perfectly approximates the curve showing the true distribution. A red bar chart representing the sample is well described by the blue trace but not exact.\"' class=\"plot-directive\" src=\"../_images/normdiscr_plot2.png\"/>\n</figure>",
            "code"
        ],
        [
            "Next, we can test whether our sample was generated by our norm-discrete\ndistribution. This also verifies whether the random numbers were generated\ncorrectly.",
            "markdown"
        ],
        [
            "The chisquare test requires that there are a minimum number of observations\nin each bin. We combine the tail bins into larger bins so that they contain\nenough observations.",
            "markdown"
        ],
        [
            "f2 = np.hstack([f[:5].sum(), f[5:-5], f[-5:].sum()])\n p2 = np.hstack([probs[:5].sum(), probs[5:-5], probs[-5:].sum()])\n ch2, pval = stats.chisquare(f2, p2*n_sample)",
            "code"
        ],
        [
            "print('chisquare for normdiscrete: chi2 = %6.3f pvalue = %6.4f' % (ch2, pval))\nchisquare for normdiscrete: chi2 = 12.466 pvalue = 0.4090  # random",
            "code"
        ],
        [
            "The pvalue in this case is high, so we can be quite confident that\nour random sample was actually generated by the distribution.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Analysing one sample": [
        [
            "First, we create some random variables. We set a seed so that in each run\nwe get identical results to look at. As an example we take a sample from\nthe Student t distribution:",
            "markdown"
        ],
        [
            "x = stats.t.rvs(10, size=1000)",
            "code"
        ],
        [
            "Here, we set the required shape parameter of the t distribution, which\nin statistics corresponds to the degrees of freedom, to 10. Using size=1000 means\nthat our sample consists of 1000 independently drawn (pseudo) random numbers.\nSince we did not specify the keyword arguments <em class=\"xref py py-obj\">loc and <em class=\"xref py py-obj\">scale, those are\nset to their default values zero and one.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Analysing one sample->Descriptive statistics": [
        [
            "<em class=\"xref py py-obj\">x is a numpy array, and we have direct access to all array methods, e.g.,",
            "markdown"
        ],
        [
            "print(x.min())   # equivalent to np.min(x)\n-3.78975572422  # random\n print(x.max())   # equivalent to np.max(x)\n5.26327732981  # random\n print(x.mean())  # equivalent to np.mean(x)\n0.0140610663985  # random\n print(x.var())   # equivalent to np.var(x))\n1.28899386208  # random",
            "code"
        ],
        [
            "How do the sample properties compare to their theoretical counterparts?",
            "markdown"
        ],
        [
            "m, v, s, k = stats.t.stats(10, moments='mvsk')\n n, (smin, smax), sm, sv, ss, sk = stats.describe(x)",
            "code"
        ],
        [
            "sstr = '%-14s mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f'\n print(sstr % ('distribution:', m, v, s ,k))\ndistribution:  mean = 0.0000, variance = 1.2500, skew = 0.0000, kurtosis = 1.0000  # random\n print(sstr % ('sample:', sm, sv, ss, sk))\nsample:        mean = 0.0141, variance = 1.2903, skew = 0.2165, kurtosis = 1.0556  # random",
            "code"
        ],
        [
            "Note: stats.describe uses the unbiased estimator for the variance, while\nnp.var is the biased estimator.",
            "markdown"
        ],
        [
            "For our sample the sample statistics differ a by a small amount from\ntheir theoretical counterparts.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Analysing one sample->T-test and KS-test": [
        [
            "We can use the t-test to test whether the mean of our sample differs\nin a statistically significant way from the theoretical expectation.",
            "markdown"
        ],
        [
            "print('t-statistic = %6.3f pvalue = %6.4f' %  stats.ttest_1samp(x, m))\nt-statistic =  0.391 pvalue = 0.6955  # random",
            "code"
        ],
        [
            "The pvalue is 0.7, this means that with an alpha error of, for\nexample, 10%, we cannot reject the hypothesis that the sample mean\nis equal to zero, the expectation of the standard t-distribution.",
            "markdown"
        ],
        [
            "As an exercise, we can calculate our ttest also directly without\nusing the provided function, which should give us the same answer,\nand so it does:",
            "markdown"
        ],
        [
            "tt = (sm-m)/np.sqrt(sv/float(n))  # t-statistic for mean\n pval = stats.t.sf(np.abs(tt), n-1)*2  # two-sided pvalue = Prob(abs(t)tt)\n print('t-statistic = %6.3f pvalue = %6.4f' % (tt, pval))\nt-statistic =  0.391 pvalue = 0.6955  # random",
            "code"
        ],
        [
            "The Kolmogorov-Smirnov test can be used to test the hypothesis that\nthe sample comes from the standard t-distribution",
            "markdown"
        ],
        [
            "print('KS-statistic D = %6.3f pvalue = %6.4f' % stats.kstest(x, 't', (10,)))\nKS-statistic D =  0.016 pvalue = 0.9571  # random",
            "code"
        ],
        [
            "Again, the p-value is high enough that we cannot reject the\nhypothesis that the random sample really is distributed according to the\nt-distribution. In real applications, we don\u00e2\u0080\u0099t know what the\nunderlying distribution is. If we perform the Kolmogorov-Smirnov\ntest of our sample against the standard normal distribution, then we\nalso cannot reject the hypothesis that our sample was generated by the\nnormal distribution given that, in this example, the p-value is almost 40%.",
            "markdown"
        ],
        [
            "print('KS-statistic D = %6.3f pvalue = %6.4f' % stats.kstest(x, 'norm'))\nKS-statistic D =  0.028 pvalue = 0.3918  # random",
            "code"
        ],
        [
            "However, the standard normal distribution has a variance of 1, while our\nsample has a variance of 1.29. If we standardize our sample and test it\nagainst the normal distribution, then the p-value is again large enough\nthat we cannot reject the hypothesis that the sample came form the\nnormal distribution.",
            "markdown"
        ],
        [
            "d, pval = stats.kstest((x-x.mean())/x.std(), 'norm')\n print('KS-statistic D = %6.3f pvalue = %6.4f' % (d, pval))\nKS-statistic D =  0.032 pvalue = 0.2397  # random",
            "code"
        ],
        [
            "Note: The Kolmogorov-Smirnov test assumes that we test against a\ndistribution with given parameters, since, in the last case, we\nestimated mean and variance, this assumption is violated and the\ndistribution of the test statistic, on which the p-value is based, is\nnot correct.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Analysing one sample->Tails of the distribution": [
        [
            "Finally, we can check the upper tail of the distribution. We can use\nthe percent point function ppf, which is the inverse of the cdf\nfunction, to obtain the critical values, or, more directly, we can use\nthe inverse of the survival function",
            "markdown"
        ],
        [
            "crit01, crit05, crit10 = stats.t.ppf([1-0.01, 1-0.05, 1-0.10], 10)\n print('critical values from ppf at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' % (crit01, crit05, crit10))\ncritical values from ppf at 1%, 5% and 10%   2.7638   1.8125   1.3722\n print('critical values from isf at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' % tuple(stats.t.isf([0.01,0.05,0.10],10)))\ncritical values from isf at 1%, 5% and 10%   2.7638   1.8125   1.3722",
            "code"
        ],
        [
            "freq01 = np.sum(xcrit01) / float(n) * 100\n freq05 = np.sum(xcrit05) / float(n) * 100\n freq10 = np.sum(xcrit10) / float(n) * 100\n print('sample %%-frequency at 1%%, 5%% and 10%% tail %8.4f %8.4f %8.4f' % (freq01, freq05, freq10))\nsample %-frequency at 1%, 5% and 10% tail   1.4000   5.8000  10.5000  # random",
            "code"
        ],
        [
            "In all three cases, our sample has more weight in the top tail than the\nunderlying distribution.\nWe can briefly check a larger sample to see if we get a closer match. In this\ncase, the empirical frequency is quite close to the theoretical probability,\nbut if we repeat this several times, the fluctuations are still pretty large.",
            "markdown"
        ],
        [
            "freq05l = np.sum(stats.t.rvs(10, size=10000)  crit05) / 10000.0 * 100\n print('larger sample %%-frequency at 5%% tail %8.4f' % freq05l)\nlarger sample %-frequency at 5% tail   4.8000  # random",
            "code"
        ],
        [
            "We can also compare it with the tail of the normal distribution, which\nhas less weight in the tails:",
            "markdown"
        ],
        [
            "print('tail prob. of normal at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' %\n...       tuple(stats.norm.sf([crit01, crit05, crit10])*100))\ntail prob. of normal at 1%, 5% and 10%   0.2857   3.4957   8.5003",
            "code"
        ],
        [
            "The chisquare test can be used to test whether for a finite number of bins,\nthe observed frequencies differ significantly from the probabilities of the\nhypothesized distribution.",
            "markdown"
        ],
        [
            "quantiles = [0.0, 0.01, 0.05, 0.1, 1-0.10, 1-0.05, 1-0.01, 1.0]\n crit = stats.t.ppf(quantiles, 10)\n crit\narray([       -inf, -2.76376946, -1.81246112, -1.37218364,  1.37218364,\n        1.81246112,  2.76376946,         inf])\n n_sample = x.size\n freqcount = np.histogram(x, bins=crit)[0]\n tprob = np.diff(quantiles)\n nprob = np.diff(stats.norm.cdf(crit))\n tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  2.30 pvalue = 0.8901  # random\n print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 64.60 pvalue = 0.0000  # random",
            "code"
        ],
        [
            "We see that the standard normal distribution is clearly rejected, while the\nstandard t-distribution cannot be rejected. Since the variance of our sample\ndiffers from both standard distributions, we can again redo the test taking\nthe estimate for scale and location into account.",
            "markdown"
        ],
        [
            "The fit method of the distributions can be used to estimate the parameters\nof the distribution, and the test is repeated using probabilities of the\nestimated distribution.",
            "markdown"
        ],
        [
            "tdof, tloc, tscale = stats.t.fit(x)\n nloc, nscale = stats.norm.fit(x)\n tprob = np.diff(stats.t.cdf(crit, tdof, loc=tloc, scale=tscale))\n nprob = np.diff(stats.norm.cdf(crit, loc=nloc, scale=nscale))\n tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  1.58 pvalue = 0.9542  # random\n print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 11.08 pvalue = 0.0858  # random",
            "code"
        ],
        [
            "Taking account of the estimated parameters, we can still reject the\nhypothesis that our sample came from a normal distribution (at the 5% level),\nbut again, with a p-value of 0.95, we cannot reject the t-distribution.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Analysing one sample->Special tests for normal distributions": [
        [
            "Since the normal distribution is the most common distribution in statistics,\nthere are several additional functions available to test whether a sample\ncould have been drawn from a normal distribution.",
            "markdown"
        ],
        [
            "First, we can test if skew and kurtosis of our sample differ significantly from\nthose of a normal distribution:",
            "markdown"
        ],
        [
            "print('normal skewtest teststat = %6.3f pvalue = %6.4f' % stats.skewtest(x))\nnormal skewtest teststat =  2.785 pvalue = 0.0054  # random\n print('normal kurtosistest teststat = %6.3f pvalue = %6.4f' % stats.kurtosistest(x))\nnormal kurtosistest teststat =  4.757 pvalue = 0.0000  # random",
            "code"
        ],
        [
            "These two tests are combined in the normality test",
            "markdown"
        ],
        [
            "print('normaltest teststat = %6.3f pvalue = %6.4f' % stats.normaltest(x))\nnormaltest teststat = 30.379 pvalue = 0.0000  # random",
            "code"
        ],
        [
            "In all three tests, the p-values are very low and we can reject the hypothesis\nthat the our sample has skew and kurtosis of the normal distribution.",
            "markdown"
        ],
        [
            "Since skew and kurtosis of our sample are based on central moments, we get\nexactly the same results if we test the standardized sample:",
            "markdown"
        ],
        [
            "print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...       stats.normaltest((x-x.mean())/x.std()))\nnormaltest teststat = 30.379 pvalue = 0.0000  # random",
            "code"
        ],
        [
            "Because normality is rejected so strongly, we can check whether the\nnormaltest gives reasonable results for other cases:",
            "markdown"
        ],
        [
            "print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...       stats.normaltest(stats.t.rvs(10, size=100)))\nnormaltest teststat =  4.698 pvalue = 0.0955  # random\n print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...              stats.normaltest(stats.norm.rvs(size=1000)))\nnormaltest teststat =  0.613 pvalue = 0.7361  # random",
            "code"
        ],
        [
            "When testing for normality of a small sample of t-distributed observations\nand a large sample of normal-distributed observations, then in neither case\ncan we reject the null hypothesis that the sample comes from a normal\ndistribution. In the first case, this is because the test is not powerful\nenough to distinguish a t and a normally distributed random variable in a\nsmall sample.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Comparing two samples": [
        [
            "In the following, we are given two samples, which can come either from the\nsame or from different distribution, and we want to test whether these\nsamples have the same statistical properties.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Comparing two samples->Comparing means": [
        [
            "Test with sample with identical means:",
            "markdown"
        ],
        [
            "rvs1 = stats.norm.rvs(loc=5, scale=10, size=500)\n rvs2 = stats.norm.rvs(loc=5, scale=10, size=500)\n stats.ttest_ind(rvs1, rvs2)\nTtest_indResult(statistic=-0.5489036175088705, pvalue=0.5831943748663959)  # random",
            "code"
        ],
        [
            "Test with sample with different means:",
            "markdown"
        ],
        [
            "rvs3 = stats.norm.rvs(loc=8, scale=10, size=500)\n stats.ttest_ind(rvs1, rvs3)\nTtest_indResult(statistic=-4.533414290175026, pvalue=6.507128186389019e-06)  # random",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp": [
        [
            "For the example, where both samples are drawn from the same distribution,\nwe cannot reject the null hypothesis, since the pvalue is high",
            "markdown"
        ],
        [
            "stats.ks_2samp(rvs1, rvs2)\nKstestResult(statistic=0.026, pvalue=0.9959527565364388)  # random",
            "code"
        ],
        [
            "In the second example, with different location, i.e., means, we can\nreject the null hypothesis, since the pvalue is below 1%",
            "markdown"
        ],
        [
            "stats.ks_2samp(rvs1, rvs3)\nKstestResult(statistic=0.114, pvalue=0.00299005061044668)  # random",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Kernel density estimation": [
        [
            "A common task in statistics is to estimate the probability density function\n(PDF) of a random variable from a set of data samples. This task is called\ndensity estimation. The most well-known tool to do this is the histogram.\nA histogram is a useful tool for visualization (mainly because everyone\nunderstands it), but doesn\u00e2\u0080\u0099t use the available data very efficiently. Kernel\ndensity estimation (KDE) is a more efficient tool for the same task. The\ngaussian_kde estimator can be used to estimate the PDF of univariate as\nwell as multivariate data. It works best if the data is unimodal.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Kernel density estimation->Univariate estimation": [
        [
            "We start with a minimal amount of data in order to see how gaussian_kde\nworks and what the different options for bandwidth selection do. The data\nsampled from the PDF are shown as blue dashes at the bottom of the figure (this\nis called a rug plot):",
            "markdown"
        ],
        [
            "from scipy import stats\n import matplotlib.pyplot as plt",
            "code"
        ],
        [
            "x1 = np.array([-7, -5, 1, 4, 5], dtype=np.float64)\n kde1 = stats.gaussian_kde(x1)\n kde2 = stats.gaussian_kde(x1, bw_method='silverman')",
            "code"
        ],
        [
            "fig = plt.figure()\n ax = fig.add_subplot(111)",
            "code"
        ],
        [
            "ax.plot(x1, np.zeros(x1.shape), 'b+', ms=20)  # rug plot\n x_eval = np.linspace(-10, 10, num=200)\n ax.plot(x_eval, kde1(x_eval), 'k-', label=\"Scott's Rule\")\n ax.plot(x_eval, kde2(x_eval), 'r-', label=\"Silverman's Rule\")",
            "code"
        ],
        [
            "plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/stats-1.png\"/>\n</figure>",
            "code"
        ],
        [
            "We see that there is very little difference between Scott\u00e2\u0080\u0099s Rule and\nSilverman\u00e2\u0080\u0099s Rule, and that the bandwidth selection with a limited amount of\ndata is probably a bit too wide. We can define our own bandwidth function to\nget a less smoothed-out result.",
            "markdown"
        ],
        [
            "def my_kde_bandwidth(obj, fac=1./5):\n...     \"\"\"We use Scott's Rule, multiplied by a constant factor.\"\"\"\n...     return np.power(obj.n, -1./(obj.d+4)) * fac",
            "code"
        ],
        [
            "fig = plt.figure()\n ax = fig.add_subplot(111)",
            "code"
        ],
        [
            "ax.plot(x1, np.zeros(x1.shape), 'b+', ms=20)  # rug plot\n kde3 = stats.gaussian_kde(x1, bw_method=my_kde_bandwidth)\n ax.plot(x_eval, kde3(x_eval), 'g-', label=\"With smaller BW\")",
            "code"
        ],
        [
            "plt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/kde_plot2.png\"/>\n</figure>",
            "code"
        ],
        [
            "We see that if we set bandwidth to be very narrow, the obtained estimate for\nthe probability density function (PDF) is simply the sum of Gaussians around\neach data point.",
            "markdown"
        ],
        [
            "We now take a more realistic example and look at the difference between the\ntwo available bandwidth selection rules. Those rules are known to work well\nfor (close to) normal distributions, but even for unimodal distributions that\nare quite strongly non-normal they work reasonably well. As a non-normal\ndistribution we take a Student\u00e2\u0080\u0099s T distribution with 5 degrees of freedom.",
            "markdown"
        ],
        [
            "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\nrng = np.random.default_rng()\nx1 = rng.normal(size=200)  # random data, normal distribution\nxs = np.linspace(x1.min()-1, x1.max()+1, 200)\n\nkde1 = stats.gaussian_kde(x1)\nkde2 = stats.gaussian_kde(x1, bw_method='silverman')\n\nfig = plt.figure(figsize=(8, 6))\n\nax1 = fig.add_subplot(211)\nax1.plot(x1, np.zeros(x1.shape), 'b+', ms=12)  # rug plot\nax1.plot(xs, kde1(xs), 'k-', label=\"Scott's Rule\")\nax1.plot(xs, kde2(xs), 'b-', label=\"Silverman's Rule\")\nax1.plot(xs, stats.norm.pdf(xs), 'r--', label=\"True PDF\")\n\nax1.set_xlabel('x')\nax1.set_ylabel('Density')\nax1.set_title(\"Normal (top) and Student's T$_{df=5}$ (bottom) distributions\")\nax1.legend(loc=1)\n\nx2 = stats.t.rvs(5, size=200, random_state=rng)  # random data, T distribution\nxs = np.linspace(x2.min() - 1, x2.max() + 1, 200)\n\nkde3 = stats.gaussian_kde(x2)\nkde4 = stats.gaussian_kde(x2, bw_method='silverman')\n\nax2 = fig.add_subplot(212)\nax2.plot(x2, np.zeros(x2.shape), 'b+', ms=12)  # rug plot\nax2.plot(xs, kde3(xs), 'k-', label=\"Scott's Rule\")\nax2.plot(xs, kde4(xs), 'b-', label=\"Silverman's Rule\")\nax2.plot(xs, stats.t.pdf(xs, 5), 'r--', label=\"True PDF\")\n\nax2.set_xlabel('x')\nax2.set_ylabel('Density')\n\nplt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/kde_plot3.png\"/>\n</figure>",
            "code"
        ],
        [
            "We now take a look at a bimodal distribution with one wider and one narrower\nGaussian feature. We expect that this will be a more difficult density to\napproximate, due to the different bandwidths required to accurately resolve\neach feature.",
            "markdown"
        ],
        [
            "from functools import partial",
            "code"
        ],
        [
            "loc1, scale1, size1 = (-2, 1, 175)\n loc2, scale2, size2 = (2, 0.2, 50)\n x2 = np.concatenate([np.random.normal(loc=loc1, scale=scale1, size=size1),\n...                      np.random.normal(loc=loc2, scale=scale2, size=size2)])",
            "code"
        ],
        [
            "x_eval = np.linspace(x2.min() - 1, x2.max() + 1, 500)",
            "code"
        ],
        [
            "kde = stats.gaussian_kde(x2)\n kde2 = stats.gaussian_kde(x2, bw_method='silverman')\n kde3 = stats.gaussian_kde(x2, bw_method=partial(my_kde_bandwidth, fac=0.2))\n kde4 = stats.gaussian_kde(x2, bw_method=partial(my_kde_bandwidth, fac=0.5))",
            "code"
        ],
        [
            "pdf = stats.norm.pdf\n bimodal_pdf = pdf(x_eval, loc=loc1, scale=scale1) * float(size1) / x2.size + \\\n...               pdf(x_eval, loc=loc2, scale=scale2) * float(size2) / x2.size",
            "code"
        ],
        [
            "fig = plt.figure(figsize=(8, 6))\n ax = fig.add_subplot(111)",
            "code"
        ],
        [
            "ax.plot(x2, np.zeros(x2.shape), 'b+', ms=12)\n ax.plot(x_eval, kde(x_eval), 'k-', label=\"Scott's Rule\")\n ax.plot(x_eval, kde2(x_eval), 'b-', label=\"Silverman's Rule\")\n ax.plot(x_eval, kde3(x_eval), 'g-', label=\"Scott * 0.2\")\n ax.plot(x_eval, kde4(x_eval), 'c-', label=\"Scott * 0.5\")\n ax.plot(x_eval, bimodal_pdf, 'r--', label=\"Actual PDF\")",
            "code"
        ],
        [
            "ax.set_xlim([x_eval.min(), x_eval.max()])\n ax.legend(loc=2)\n ax.set_xlabel('x')\n ax.set_ylabel('Density')\n plt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/kde_plot4.png\"/>\n</figure>",
            "code"
        ],
        [
            "As expected, the KDE is not as close to the true PDF as we would like due to\nthe different characteristic size of the two features of the bimodal\ndistribution. By halving the default bandwidth (Scott * 0.5), we can do\nsomewhat better, while using a factor 5 smaller bandwidth than the default\ndoesn\u00e2\u0080\u0099t smooth enough. What we really need, though, in this case, is a\nnon-uniform (adaptive) bandwidth.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Kernel density estimation->Multivariate estimation": [
        [
            "With gaussian_kde we can perform multivariate, as well as univariate\nestimation. We demonstrate the bivariate case. First, we generate some random\ndata with a model in which the two variates are correlated.",
            "markdown"
        ],
        [
            "def measure(n):\n...     \"\"\"Measurement model, return two coupled measurements.\"\"\"\n...     m1 = np.random.normal(size=n)\n...     m2 = np.random.normal(scale=0.5, size=n)\n...     return m1+m2, m1-m2",
            "code"
        ],
        [
            "m1, m2 = measure(2000)\n xmin = m1.min()\n xmax = m1.max()\n ymin = m2.min()\n ymax = m2.max()",
            "code"
        ],
        [
            "Then we apply the KDE to the data:",
            "markdown"
        ],
        [
            "X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n positions = np.vstack([X.ravel(), Y.ravel()])\n values = np.vstack([m1, m2])\n kernel = stats.gaussian_kde(values)\n Z = np.reshape(kernel.evaluate(positions).T, X.shape)",
            "code"
        ],
        [
            "Finally, we plot the estimated bivariate distribution as a colormap and plot\nthe individual data points on top.",
            "markdown"
        ],
        [
            "fig = plt.figure(figsize=(8, 6))\n ax = fig.add_subplot(111)",
            "code"
        ],
        [
            "ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,\n...           extent=[xmin, xmax, ymin, ymax])\n ax.plot(m1, m2, 'k.', markersize=2)",
            "code"
        ],
        [
            "ax.set_xlim([xmin, xmax])\n ax.set_ylim([ymin, ymax])",
            "code"
        ],
        [
            "plt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\"An X-Y plot showing a random scattering of points around a 2-D gaussian. The distribution has a semi-major axis at 45 degrees with a semi-minor axis about half as large. Each point in the plot is highlighted with the outer region in red, then yellow, then green, with the center in blue. \"' class=\"plot-directive\" src=\"../_images/kde_plot5.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Kernel density estimation->Multiscale Graph Correlation (MGC)": [
        [
            "With multiscale_graphcorr, we can test for independence on high\ndimensional and nonlinear data. Before we start, let\u00e2\u0080\u0099s import some useful\npackages:",
            "markdown"
        ],
        [
            "import numpy as np\n import matplotlib.pyplot as plt; plt.style.use('classic')\n from scipy.stats import multiscale_graphcorr",
            "code"
        ],
        [
            "Let\u00e2\u0080\u0099s use a custom plotting function to plot the data relationship:",
            "markdown"
        ],
        [
            "def mgc_plot(x, y, sim_name, mgc_dict=None, only_viz=False,\n...              only_mgc=False):\n...     \"\"\"Plot sim and MGC-plot\"\"\"\n...     if not only_mgc:\n...         # simulation\n...         plt.figure(figsize=(8, 8))\n...         ax = plt.gca()\n...         ax.set_title(sim_name + \" Simulation\", fontsize=20)\n...         ax.scatter(x, y)\n...         ax.set_xlabel('X', fontsize=15)\n...         ax.set_ylabel('Y', fontsize=15)\n...         ax.axis('equal')\n...         ax.tick_params(axis=\"x\", labelsize=15)\n...         ax.tick_params(axis=\"y\", labelsize=15)\n...         plt.show()\n...     if not only_viz:\n...         # local correlation map\n...         plt.figure(figsize=(8,8))\n...         ax = plt.gca()\n...         mgc_map = mgc_dict[\"mgc_map\"]\n...         # draw heatmap\n...         ax.set_title(\"Local Correlation Map\", fontsize=20)\n...         im = ax.imshow(mgc_map, cmap='YlGnBu')\n...         # colorbar\n...         cbar = ax.figure.colorbar(im, ax=ax)\n...         cbar.ax.set_ylabel(\"\", rotation=-90, va=\"bottom\")\n...         ax.invert_yaxis()\n...         # Turn spines off and create white grid.\n...         for edge, spine in ax.spines.items():\n...             spine.set_visible(False)\n...         # optimal scale\n...         opt_scale = mgc_dict[\"opt_scale\"]\n...         ax.scatter(opt_scale[0], opt_scale[1],\n...                    marker='X', s=200, color='red')\n...         # other formatting\n...         ax.tick_params(bottom=\"off\", left=\"off\")\n...         ax.set_xlabel('#Neighbors for X', fontsize=15)\n...         ax.set_ylabel('#Neighbors for Y', fontsize=15)\n...         ax.tick_params(axis=\"x\", labelsize=15)\n...         ax.tick_params(axis=\"y\", labelsize=15)\n...         ax.set_xlim(0, 100)\n...         ax.set_ylim(0, 100)\n...         plt.show()",
            "code"
        ],
        [
            "Let\u00e2\u0080\u0099s look at some linear data first:",
            "markdown"
        ],
        [
            "rng = np.random.default_rng()\n x = np.linspace(-1, 1, num=100)\n y = x + 0.3 * rng.random(x.size)",
            "code"
        ],
        [
            "The simulation relationship can be plotted below:",
            "markdown"
        ],
        [
            "mgc_plot(x, y, \"Linear\", only_viz=True)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/mgc_plot1.png\"/>\n</figure>",
            "code"
        ],
        [
            "Now, we can see the test statistic, p-value, and MGC map visualized below. The\noptimal scale is shown on the map as a red \u00e2\u0080\u009cx\u00e2\u0080\u009d:",
            "markdown"
        ],
        [
            "stat, pvalue, mgc_dict = multiscale_graphcorr(x, y)\n print(\"MGC test statistic: \", round(stat, 1))\nMGC test statistic:  1.0\n print(\"P-value: \", round(pvalue, 1))\nP-value:  0.0\n mgc_plot(x, y, \"Linear\", mgc_dict, only_mgc=True)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/mgc_plot2.png\"/>\n</figure>",
            "code"
        ],
        [
            "It is clear from here, that MGC is able to determine a relationship between the\ninput data matrices because the p-value is very low and the MGC test statistic\nis relatively high. The MGC-map indicates a <strong>strongly linear relationship</strong>.\nIntuitively, this is because having more neighbors will help in identifying a\nlinear relationship between \\(x\\) and \\(y\\). The optimal scale in this\ncase is <strong>equivalent to the global scale</strong>, marked by a red spot on the map.",
            "markdown"
        ],
        [
            "The same can be done for nonlinear data sets. The following \\(x\\) and\n\\(y\\) arrays are derived from a nonlinear simulation:",
            "markdown"
        ],
        [
            "unif = np.array(rng.uniform(0, 5, size=100))\n x = unif * np.cos(np.pi * unif)\n y = unif * np.sin(np.pi * unif) + 0.4 * rng.random(x.size)",
            "code"
        ],
        [
            "The simulation relationship can be plotted below:",
            "markdown"
        ],
        [
            "mgc_plot(x, y, \"Spiral\", only_viz=True)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/mgc_plot3.png\"/>\n</figure>",
            "code"
        ],
        [
            "Now, we can see the test statistic, p-value, and MGC map visualized below. The\noptimal scale is shown on the map as a red \u00e2\u0080\u009cx\u00e2\u0080\u009d:",
            "markdown"
        ],
        [
            "stat, pvalue, mgc_dict = multiscale_graphcorr(x, y)\n print(\"MGC test statistic: \", round(stat, 1))\nMGC test statistic:  0.2  # random\n print(\"P-value: \", round(pvalue, 1))\nP-value:  0.0\n mgc_plot(x, y, \"Spiral\", mgc_dict, only_mgc=True)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/mgc_plot4.png\"/>\n</figure>",
            "code"
        ],
        [
            "It is clear from here, that MGC is able to determine a relationship again\nbecause the p-value is very low and the MGC test statistic is relatively high.\nThe MGC-map indicates a <strong>strongly nonlinear relationship</strong>. The optimal scale\nin this case is <strong>equivalent to the local scale</strong>, marked by a red spot on the\nmap.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Quasi-Monte Carlo": [
        [
            "Before talking about Quasi-Monte Carlo (QMC), a quick introduction about Monte\nCarlo (MC). MC methods, or MC experiments, are a broad class of\ncomputational algorithms that rely on repeated random sampling to obtain\nnumerical results. The underlying concept is to use randomness to solve\nproblems that might be deterministic in principle. They are often used in\nphysical and mathematical problems and are most useful when it is difficult or\nimpossible to use other approaches. MC methods are mainly used in\nthree problem classes: optimization, numerical integration, and generating\ndraws from a probability distribution.",
            "markdown"
        ],
        [
            "Generating random numbers with specific properties is a more complex problem\nthan it sounds. Simple MC methods are designed to sample points to be\nindependent and identically distributed (IID). But generating multiple sets\nof random points can produce radically different results.\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_mc.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "In both cases in the plot above, points are generated randomly without any\nknowledge about previously drawn points. It is clear that some regions of\nthe space are left unexplored - which can cause problems in simulations as a\nparticular set of points might trigger a totally different behaviour.",
            "markdown"
        ],
        [
            "A great benefit of MC is that it has known convergence properties.\nLet\u00e2\u0080\u0099s look at the mean of the squared sum in 5 dimensions:\n\n\\[f(\\mathbf{x}) = \\left( \\sum_{j=1}^{5}x_j \\right)^2,\\]",
            "markdown"
        ],
        [
            "with \\(x_j \\sim \\mathcal{U}(0,1)\\). It has a known mean value,\n\\(\\mu = 5/3+5(5-1)/4\\). Using MC sampling, we\ncan compute that mean numerically, and the approximation error follows a\ntheoretical rate of \\(O(n^{-1/2})\\).\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_conv_mc.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "Although the convergence is ensured, practitioners tend to want to have an\nexploration process which is more deterministic. With normal MC, a seed can be\nused to have a repeatable process. But fixing the seed would break the\nconvergence property: a given seed could work for a given class of problem\nand break for another one.",
            "markdown"
        ],
        [
            "What is commonly done to walk through the space in a deterministic manner, is\nto use a regular grid spanning all parameter dimensions, also called a\nsaturated design. Let\u00e2\u0080\u0099s consider the unit-hypercube, with all bounds ranging\nfrom 0 to 1. Now, having a distance of 0.1 between points, the number of points\nrequired to fill the unit interval would be 10. In a 2-dimensional hypercube\nthe same spacing would require 100, and in 3 dimensions 1,000 points. As the\nnumber of dimensions grows, the number of experiments which is required to fill\nthe space rises exponentially as the dimensionality of the space increases.\nThis exponential growth is called \u00e2\u0080\u009cthe curse of dimensionality\u00e2\u0080\u009d.",
            "markdown"
        ],
        [
            "import numpy as np\n disc = 10\n x1 = np.linspace(0, 1, disc)\n x2 = np.linspace(0, 1, disc)\n x3 = np.linspace(0, 1, disc)\n x1, x2, x3 = np.meshgrid(x1, x2, x3)\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_curse.png\"/>\n</figure>",
            "code"
        ],
        [
            "To mitigate this issue, QMC methods have been designed. They are\ndeterministic, have a good coverage of the space and some of them can be\ncontinued and retain good properties.\nThe main difference with MC methods is that the points are not IID but they\nknow about previous points. Hence, some methods are also referred to as\nsequences.\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_mc_qmc.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "This figure presents 2 sets of 256 points. The design of the left is a plain\nMC whereas the design of the right is a QMC design using the Sobol\u00e2\u0080\u0099 method.\nWe clearly see that the QMC version is more uniform. The points sample better\nnear the boundaries and there are less clusters or gaps.",
            "markdown"
        ],
        [
            "One way to assess the uniformity is to use a measure called the discrepancy.\nHere the discrepancy of Sobol\u00e2\u0080\u0099 points is better than crude MC.",
            "markdown"
        ],
        [
            "Coming back to the computation of the mean, QMC methods also have better rates\nof convergence for the error. They can achieve \\(O(n^{-1})\\) for this\nfunction, and even better rates on very smooth functions. This figure shows\nthat the Sobol\u00e2\u0080\u0099 method has a rate of \\(O(n^{-1})\\):\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_conv_mc_sobol.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "We refer to the documentation of scipy.stats.qmc for\nmore mathematical details.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Quasi-Monte Carlo->Calculate the discrepancy": [
        [
            "Let\u00e2\u0080\u0099s consider two sets of points. From the figure below, it is clear that\nthe design on the left covers more of the space than the design on the right.\nThis can be quantified using a discrepancy measure.\nThe lower the discrepancy, the more uniform a sample is.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.stats import qmc\n space_1 = np.array([[1, 3], [2, 6], [3, 2], [4, 5], [5, 1], [6, 4]])\n space_2 = np.array([[1, 5], [2, 4], [3, 3], [4, 2], [5, 1], [6, 6]])\n l_bounds = [0.5, 0.5]\n u_bounds = [6.5, 6.5]\n space_1 = qmc.scale(space_1, l_bounds, u_bounds, reverse=True)\n space_2 = qmc.scale(space_2, l_bounds, u_bounds, reverse=True)\n qmc.discrepancy(space_1)\n0.008142039609053464\n qmc.discrepancy(space_2)\n0.010456854423869011\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_discrepancy.png\"/>\n</figure>",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Quasi-Monte Carlo->Using a QMC engine": [
        [
            "Several QMC samplers/engines are implemented. Here we look at two of the most\nused QMC methods: Sobol and Halton\nsequences.",
            "markdown"
        ],
        [
            "\"\"\"Sobol' and Halton sequences.\"\"\"\nfrom scipy.stats import qmc\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n\nrng = np.random.default_rng()\n\nn_sample = 256\ndim = 2\n\nsample = {}\n\n# Sobol'\nengine = qmc.Sobol(d=dim, seed=rng)\nsample[\"Sobol'\"] = engine.random(n_sample)\n\n# Halton\nengine = qmc.Halton(d=dim, seed=rng)\nsample[\"Halton\"] = engine.random(n_sample)\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\nfor i, kind in enumerate(sample):\n    axs[i].scatter(sample[kind][:, 0], sample[kind][:, 1])\n\n    axs[i].set_aspect('equal')\n    axs[i].set_xlabel(r'$x_1$')\n    axs[i].set_ylabel(r'$x_2$')\n    axs[i].set_title(f'{kind}\u00e2\u0080\u0094$C^2 = ${qmc.discrepancy(sample[kind]):.2}')\n\nplt.tight_layout()\nplt.show()\n\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/qmc_plot_sobol_halton.png\"/>\n</figure>",
            "code"
        ],
        [
            "Warning",
            "markdown"
        ],
        [
            "QMC methods require particular care and the user must read the\ndocumentation to avoid common pitfalls. Sobol\u00e2\u0080\u0099 for instance requires a\nnumber of points following a power of 2. Also, thinning, burning or other\npoint selection can break the properties of the sequence and result in a\nset of points which would not be better than MC.",
            "markdown"
        ],
        [
            "QMC engines are state-aware. Meaning that you can continue the sequence,\nskip some points, or reset it. Let\u00e2\u0080\u0099s take 5 points from\nHalton. And then ask for a second set of 5 points:",
            "markdown"
        ],
        [
            "from scipy.stats import qmc\n engine = qmc.Halton(d=2)\n engine.random(5)\narray([[0.22166437, 0.07980522],  # random\n       [0.72166437, 0.93165708],\n       [0.47166437, 0.41313856],\n       [0.97166437, 0.19091633],\n       [0.01853937, 0.74647189]])\n engine.random(5)\narray([[0.51853937, 0.52424967],  # random\n       [0.26853937, 0.30202745],\n       [0.76853937, 0.857583  ],\n       [0.14353937, 0.63536078],\n       [0.64353937, 0.01807683]])",
            "code"
        ],
        [
            "Now we reset the sequence. Asking for 5 points leads to the same first 5\npoints:",
            "markdown"
        ],
        [
            "engine.reset()\n engine.random(5)\narray([[0.22166437, 0.07980522],  # random\n       [0.72166437, 0.93165708],\n       [0.47166437, 0.41313856],\n       [0.97166437, 0.19091633],\n       [0.01853937, 0.74647189]])",
            "code"
        ],
        [
            "And here we advance the sequence to get the same second set of 5 points:",
            "markdown"
        ],
        [
            "engine.reset()\n engine.fast_forward(5)\n engine.random(5)\narray([[0.51853937, 0.52424967],  # random\n       [0.26853937, 0.30202745],\n       [0.76853937, 0.857583  ],\n       [0.14353937, 0.63536078],\n       [0.64353937, 0.01807683]])",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "By default, both Sobol and\nHalton are scrambled. The convergence properties are\nbetter, and it prevents the appearance of fringes or noticeable patterns\nof points in high dimensions. There should be no practical reason not to\nuse the scrambled version.",
            "markdown"
        ]
    ],
    "Statistics (scipy.stats)->Quasi-Monte Carlo->Making a QMC engine, i.e., subclassing QMCEngine": [
        [
            "To make your own QMCEngine, a few methods have to be\ndefined. Following is an example wrapping numpy.random.Generator.",
            "markdown"
        ],
        [
            "import numpy as np\n from scipy.stats import qmc\n class RandomEngine(qmc.QMCEngine):\n...     def __init__(self, d, seed=None):\n...         super().__init__(d=d, seed=seed)\n...         self.rng = np.random.default_rng(self.rng_seed)\n...\n...\n...     def _random(self, n=1, *, workers=1):\n...         return self.rng.random((n, self.d))\n...\n...\n...     def reset(self):\n...         self.rng = np.random.default_rng(self.rng_seed)\n...         self.num_generated = 0\n...         return self\n...\n...\n...     def fast_forward(self, n):\n...         self.random(n)\n...         return self",
            "code"
        ],
        [
            "Then we use it as any other QMC engine:",
            "markdown"
        ],
        [
            "engine = RandomEngine(2)\n engine.random(5)\narray([[0.22733602, 0.31675834],  # random\n       [0.79736546, 0.67625467],\n       [0.39110955, 0.33281393],\n       [0.59830875, 0.18673419],\n       [0.67275604, 0.94180287]])\n engine.reset()\n engine.random(5)\narray([[0.22733602, 0.31675834],  # random\n       [0.79736546, 0.67625467],\n       [0.39110955, 0.33281393],\n       [0.59830875, 0.18673419],\n       [0.67275604, 0.94180287]])",
            "code"
        ]
    ],
    "Statistics (scipy.stats)->Quasi-Monte Carlo->Guidelines on using QMC": [
        [
            "QMC has rules! Be sure to read the documentation or you might have no\nbenefit over MC.",
            "markdown"
        ],
        [
            "Use Sobol if you need <strong>exactly</strong> \\(2^m\\) points.",
            "markdown"
        ],
        [
            "Halton allows to sample, or skip, an arbitrary number of\npoints. This is at the cost of a slower rate of convergence than Sobol\u00e2\u0080\u0099.",
            "markdown"
        ],
        [
            "Never remove the first points of the sequence. It will destroy the\nproperties.",
            "markdown"
        ],
        [
            "Scrambling is always better.",
            "markdown"
        ],
        [
            "If you use LHS based methods, you cannot add points without losing the LHS\nproperties. (There are some methods to do so, but this is not implemented.)",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Introduction": [
        [
            "Image processing and analysis are generally seen as operations on\n2-D arrays of values. There are, however, a number of\nfields where images of higher dimensionality must be analyzed. Good\nexamples of these are medical imaging and biological imaging.\nnumpy is suited very well for this type of applications due to\nits inherent multidimensional nature. The scipy.ndimage\npackages provides a number of general image processing and analysis\nfunctions that are designed to operate with arrays of arbitrary\ndimensionality. The packages currently includes: functions for\nlinear and non-linear filtering, binary morphology, B-spline\ninterpolation, and object measurements.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Properties shared by all functions": [
        [
            "All functions share some common properties. Notably, all functions\nallow the specification of an output array with the output\nargument. With this argument, you can specify an array that will be\nchanged in-place with the result with the operation. In this case,\nthe result is not returned. Usually, using the output argument is\nmore efficient, since an existing array is used to store the\nresult.",
            "markdown"
        ],
        [
            "The type of arrays returned is dependent on the type of operation,\nbut it is, in most cases, equal to the type of the input. If,\nhowever, the output argument is used, the type of the result is\nequal to the type of the specified output argument. If no output\nargument is given, it is still possible to specify what the result\nof the output should be. This is done by simply assigning the\ndesired numpy type object to the output argument. For example:",
            "markdown"
        ],
        [
            "from scipy.ndimage import correlate\n import numpy as np\n correlate(np.arange(10), [1, 2.5])\narray([ 0,  2,  6,  9, 13, 16, 20, 23, 27, 30])\n correlate(np.arange(10), [1, 2.5], output=np.float64)\narray([  0. ,   2.5,   6. ,   9.5,  13. ,  16.5,  20. ,  23.5,  27. ,  30.5])",
            "code"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Filter functions": [
        [
            "The functions described in this section all perform some type of spatial\nfiltering of the input array: the elements in the output are some function\nof the values in the neighborhood of the corresponding input element. We refer\nto this neighborhood of elements as the filter kernel, which is often\nrectangular in shape but may also have an arbitrary footprint. Many\nof the functions described below allow you to define the footprint\nof the kernel by passing a mask through the footprint parameter.\nFor example, a cross-shaped kernel can be defined as follows:",
            "markdown"
        ],
        [
            "footprint = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n footprint\narray([[0, 1, 0],\n       [1, 1, 1],\n       [0, 1, 0]])",
            "code"
        ],
        [
            "Usually, the origin of the kernel is at the center calculated by\ndividing the dimensions of the kernel shape by two. For instance,\nthe origin of a 1-D kernel of length three is at the\nsecond element. Take, for example, the correlation of a\n1-D array with a filter of length 3 consisting of\nones:",
            "markdown"
        ],
        [
            "from scipy.ndimage import correlate1d\n a = [0, 0, 0, 1, 0, 0, 0]\n correlate1d(a, [1, 1, 1])\narray([0, 0, 1, 1, 1, 0, 0])",
            "code"
        ],
        [
            "Sometimes, it is convenient to choose a different origin for the\nkernel. For this reason, most functions support the origin\nparameter, which gives the origin of the filter relative to its\ncenter. For example:",
            "markdown"
        ],
        [
            "a = [0, 0, 0, 1, 0, 0, 0]\n correlate1d(a, [1, 1, 1], origin = -1)\narray([0, 1, 1, 1, 0, 0, 0])",
            "code"
        ],
        [
            "The effect is a shift of the result towards the left. This feature\nwill not be needed very often, but it may be useful, especially for\nfilters that have an even size. A good example is the calculation\nof backward and forward differences:",
            "markdown"
        ],
        [
            "a = [0, 0, 1, 1, 1, 0, 0]\n correlate1d(a, [-1, 1])               # backward difference\narray([ 0,  0,  1,  0,  0, -1,  0])\n correlate1d(a, [-1, 1], origin = -1)  # forward difference\narray([ 0,  1,  0,  0, -1,  0,  0])",
            "code"
        ],
        [
            "We could also have calculated the forward difference as follows:",
            "markdown"
        ],
        [
            "correlate1d(a, [0, -1, 1])\narray([ 0,  1,  0,  0, -1,  0,  0])",
            "code"
        ],
        [
            "However, using the origin parameter instead of a larger kernel is\nmore efficient. For multidimensional kernels, origin can be a\nnumber, in which case the origin is assumed to be equal along all\naxes, or a sequence giving the origin along each axis.",
            "markdown"
        ],
        [
            "Since the output elements are a function of elements in the\nneighborhood of the input elements, the borders of the array need to\nbe dealt with appropriately by providing the values outside the\nborders. This is done by assuming that the arrays are extended beyond\ntheir boundaries according to certain boundary conditions. In the\nfunctions described below, the boundary conditions can be selected\nusing the mode parameter, which must be a string with the name of the\nboundary condition. The following boundary conditions are currently\nsupported:\n\n\n</blockquote>",
            "markdown"
        ],
        [
            "The following synonyms are also supported for consistency with the\ninterpolation routines:\n\n\n</blockquote>",
            "markdown"
        ],
        [
            "* \u00e2\u0080\u009cgrid-constant\u00e2\u0080\u009d and \u00e2\u0080\u009cconstant\u00e2\u0080\u009d are equivalent for filtering operations, but\nhave different behavior in interpolation functions. For API consistency, the\nfiltering functions accept either name.",
            "markdown"
        ],
        [
            "The \u00e2\u0080\u009cconstant\u00e2\u0080\u009d mode is special since it needs an additional parameter to\nspecify the constant value that should be used.",
            "markdown"
        ],
        [
            "Note that modes mirror and reflect differ only in whether the sample at the\nboundary is repeated upon reflection. For mode mirror, the point of symmetry is\nexactly at the final sample, so that value is not repeated. This mode is also\nknown as whole-sample symmetric since the point of symmetry falls on the final\nsample. Similarly, reflect is often referred to as half-sample symmetric as the\npoint of symmetry is half a sample beyond the array boundary.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The easiest way to implement such boundary conditions would be to\ncopy the data to a larger array and extend the data at the borders\naccording to the boundary conditions. For large arrays and large\nfilter kernels, this would be very memory consuming, and the\nfunctions described below, therefore, use a different approach that\ndoes not require allocating large temporary buffers.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Filter functions->Correlation and convolution": [
        [
            "The correlate1d function calculates a 1-D\ncorrelation along the given axis. The lines of the array along the\ngiven axis are correlated with the given weights. The weights\nparameter must be a 1-D sequence of numbers.",
            "markdown"
        ],
        [
            "The function correlate implements multidimensional\ncorrelation of the input array with a given kernel.",
            "markdown"
        ],
        [
            "The convolve1d function calculates a 1-D\nconvolution along the given axis. The lines of the array along the\ngiven axis are convoluted with the given weights. The weights\nparameter must be a 1-D sequence of numbers.",
            "markdown"
        ],
        [
            "The function convolve implements multidimensional\nconvolution of the input array with a given kernel.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "A convolution is essentially a correlation after mirroring the\nkernel. As a result, the origin parameter behaves differently\nthan in the case of a correlation: the results is shifted in the\nopposite direction.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Filter functions->Smoothing filters": [
        [
            "The gaussian_filter1d function implements a 1-D\nGaussian filter. The standard deviation of the Gaussian filter is\npassed through the parameter sigma. Setting order = 0\ncorresponds to convolution with a Gaussian kernel. An order of 1, 2,\nor 3 corresponds to convolution with the first, second, or third\nderivatives of a Gaussian. Higher-order derivatives are not\nimplemented.",
            "markdown"
        ],
        [
            "The gaussian_filter function implements a multidimensional\nGaussian filter. The standard deviations of the Gaussian filter\nalong each axis are passed through the parameter sigma as a\nsequence or numbers. If sigma is not a sequence but a single\nnumber, the standard deviation of the filter is equal along all\ndirections. The order of the filter can be specified separately for\neach axis. An order of 0 corresponds to convolution with a Gaussian\nkernel. An order of 1, 2, or 3 corresponds to convolution with the\nfirst, second, or third derivatives of a Gaussian. Higher-order\nderivatives are not implemented. The order parameter must be a\nnumber, to specify the same order for all axes, or a sequence of\nnumbers to specify a different order for each axis. The example below\nshows the filter applied on test data with different values of sigma.\nThe order parameter is kept at 0.\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/gaussian_filter_plot1.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The multidimensional filter is implemented as a sequence of\n1-D Gaussian filters. The intermediate arrays are\nstored in the same data type as the output. Therefore, for\noutput types with a lower precision, the results may be imprecise\nbecause intermediate results may be stored with insufficient\nprecision. This can be prevented by specifying a more precise\noutput type.",
            "markdown"
        ],
        [
            "The uniform_filter1d function calculates a 1-D\nuniform filter of the given size along the given axis.",
            "markdown"
        ],
        [
            "The uniform_filter implements a multidimensional uniform\nfilter. The sizes of the uniform filter are given for each axis as a\nsequence of integers by the size parameter. If size is not a\nsequence, but a single number, the sizes along all axes are assumed\nto be equal.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The multidimensional filter is implemented as a sequence of\n1-D uniform filters. The intermediate arrays are\nstored in the same data type as the output. Therefore, for output\ntypes with a lower precision, the results may be imprecise\nbecause intermediate results may be stored with insufficient\nprecision. This can be prevented by specifying a more precise\noutput type.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Filter functions->Filters based on order statistics": [
        [
            "The minimum_filter1d function calculates a 1-D\nminimum filter of the given size along the given axis.",
            "markdown"
        ],
        [
            "The maximum_filter1d function calculates a 1-D\nmaximum filter of the given size along the given axis.",
            "markdown"
        ],
        [
            "The minimum_filter function calculates a multidimensional\nminimum filter. Either the sizes of a rectangular kernel or the\nfootprint of the kernel must be provided. The size parameter, if\nprovided, must be a sequence of sizes or a single number, in which\ncase the size of the filter is assumed to be equal along each axis.\nThe footprint, if provided, must be an array that defines the\nshape of the kernel by its non-zero elements.",
            "markdown"
        ],
        [
            "The maximum_filter function calculates a multidimensional\nmaximum filter. Either the sizes of a rectangular kernel or the\nfootprint of the kernel must be provided. The size parameter, if\nprovided, must be a sequence of sizes or a single number, in which\ncase the size of the filter is assumed to be equal along each axis.\nThe footprint, if provided, must be an array that defines the\nshape of the kernel by its non-zero elements.",
            "markdown"
        ],
        [
            "The rank_filter function calculates a multidimensional rank\nfilter. The rank may be less than zero, i.e., rank = -1\nindicates the largest element. Either the sizes of a rectangular\nkernel or the footprint of the kernel must be provided. The size\nparameter, if provided, must be a sequence of sizes or a single\nnumber, in which case the size of the filter is assumed to be equal\nalong each axis. The footprint, if provided, must be an array that\ndefines the shape of the kernel by its non-zero elements.",
            "markdown"
        ],
        [
            "The percentile_filter function calculates a multidimensional\npercentile filter. The percentile may be less than zero, i.e.,\npercentile = -20 equals percentile = 80. Either the sizes of a\nrectangular kernel or the footprint of the kernel must be provided.\nThe size parameter, if provided, must be a sequence of sizes or a\nsingle number, in which case the size of the filter is assumed to be\nequal along each axis. The footprint, if provided, must be an\narray that defines the shape of the kernel by its non-zero elements.",
            "markdown"
        ],
        [
            "The median_filter function calculates a multidimensional\nmedian filter. Either the sizes of a rectangular kernel or the\nfootprint of the kernel must be provided. The size parameter, if\nprovided, must be a sequence of sizes or a single number, in which\ncase the size of the filter is assumed to be equal along each\naxis. The footprint if provided, must be an array that defines the\nshape of the kernel by its non-zero elements.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Filter functions->Derivatives": [
        [
            "Derivative filters can be constructed in several ways. The function\ngaussian_filter1d, described in\nSmoothing filters, can be used to calculate\nderivatives along a given axis using the order parameter. Other\nderivative filters are the Prewitt and Sobel filters:",
            "markdown"
        ],
        [
            "The prewitt function calculates a derivative along the given\naxis.",
            "markdown"
        ],
        [
            "The sobel function calculates a derivative along the given\naxis.",
            "markdown"
        ],
        [
            "The Laplace filter is calculated by the sum of the second derivatives\nalong all axes. Thus, different Laplace filters can be constructed\nusing different second-derivative functions. Therefore, we provide a\ngeneral function that takes a function argument to calculate the\nsecond derivative along a given direction.",
            "markdown"
        ],
        [
            "The function generic_laplace calculates a Laplace filter\nusing the function passed through derivative2 to calculate\nsecond derivatives. The function derivative2 should have the\nfollowing signature",
            "markdown"
        ],
        [
            "derivative2(input, axis, output, mode, cval, *extra_arguments, **extra_keywords)",
            "code"
        ],
        [
            "It should calculate the second derivative along the dimension\naxis. If output is not None, it should use that for the\noutput and return None, otherwise it should return the\nresult. mode, cval have the usual meaning.",
            "markdown"
        ],
        [
            "The extra_arguments and extra_keywords arguments can be used\nto pass a tuple of extra arguments and a dictionary of named\narguments that are passed to derivative2 at each call.",
            "markdown"
        ],
        [
            "For example",
            "markdown"
        ],
        [
            "def d2(input, axis, output, mode, cval):\n...     return correlate1d(input, [1, -2, 1], axis, output, mode, cval, 0)\n...\n a = np.zeros((5, 5))\n a[2, 2] = 1\n from scipy.ndimage import generic_laplace\n generic_laplace(a, d2)\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  1., -4.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])",
            "code"
        ],
        [
            "To demonstrate the use of the extra_arguments argument, we could do",
            "markdown"
        ],
        [
            "def d2(input, axis, output, mode, cval, weights):\n...     return correlate1d(input, weights, axis, output, mode, cval, 0,)\n...\n a = np.zeros((5, 5))\n a[2, 2] = 1\n generic_laplace(a, d2, extra_arguments = ([1, -2, 1],))\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  1., -4.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "generic_laplace(a, d2, extra_keywords = {'weights': [1, -2, 1]})\narray([[ 0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  1., -4.,  1.,  0.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.]])",
            "code"
        ],
        [
            "The following two functions are implemented using\ngeneric_laplace by providing appropriate functions for the\nsecond-derivative function:",
            "markdown"
        ],
        [
            "The function laplace calculates the Laplace using discrete\ndifferentiation for the second derivative (i.e., convolution with\n[1, -2, 1]).",
            "markdown"
        ],
        [
            "The function gaussian_laplace calculates the Laplace filter\nusing gaussian_filter to calculate the second\nderivatives. The standard deviations of the Gaussian filter along\neach axis are passed through the parameter sigma as a sequence or\nnumbers. If sigma is not a sequence but a single number, the\nstandard deviation of the filter is equal along all directions.",
            "markdown"
        ],
        [
            "The gradient magnitude is defined as the square root of the sum of the\nsquares of the gradients in all directions. Similar to the generic\nLaplace function, there is a generic_gradient_magnitude\nfunction that calculates the gradient magnitude of an array.",
            "markdown"
        ],
        [
            "The function generic_gradient_magnitude calculates a\ngradient magnitude using the function passed through\nderivative to calculate first derivatives. The function\nderivative should have the following signature",
            "markdown"
        ],
        [
            "derivative(input, axis, output, mode, cval, *extra_arguments, **extra_keywords)",
            "code"
        ],
        [
            "It should calculate the derivative along the dimension axis. If\noutput is not None, it should use that for the output and return\nNone, otherwise it should return the result. mode, cval have the\nusual meaning.",
            "markdown"
        ],
        [
            "The extra_arguments and extra_keywords arguments can be used to\npass a tuple of extra arguments and a dictionary of named arguments\nthat are passed to derivative at each call.",
            "markdown"
        ],
        [
            "For example, the sobel function fits the required signature",
            "markdown"
        ],
        [
            "a = np.zeros((5, 5))\n a[2, 2] = 1\n from scipy.ndimage import sobel, generic_gradient_magnitude\n generic_gradient_magnitude(a, sobel)\narray([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.41421356,  2.        ,  1.41421356,  0.        ],\n       [ 0.        ,  2.        ,  0.        ,  2.        ,  0.        ],\n       [ 0.        ,  1.41421356,  2.        ,  1.41421356,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])",
            "code"
        ],
        [
            "See the documentation of generic_laplace for examples of\nusing the extra_arguments and extra_keywords arguments.",
            "markdown"
        ],
        [
            "The sobel and prewitt functions fit the required\nsignature and can, therefore, be used directly with\ngeneric_gradient_magnitude.",
            "markdown"
        ],
        [
            "The function gaussian_gradient_magnitude calculates the\ngradient magnitude using gaussian_filter to calculate the\nfirst derivatives. The standard deviations of the Gaussian filter\nalong each axis are passed through the parameter sigma as a\nsequence or numbers. If sigma is not a sequence but a single\nnumber, the standard deviation of the filter is equal along all\ndirections.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Filter functions->Generic filter functions": [
        [
            "To implement filter functions, generic functions can be used that\naccept a callable object that implements the filtering operation. The\niteration over the input and output arrays is handled by these generic\nfunctions, along with such details as the implementation of the\nboundary conditions. Only a callable object implementing a callback\nfunction that does the actual filtering work must be provided. The\ncallback function can also be written in C and passed using a\nPyCapsule (see Extending scipy.ndimage in C for more\ninformation).",
            "markdown"
        ],
        [
            "The generic_filter1d function implements a generic\n1-D filter function, where the actual filtering\noperation must be supplied as a python function (or other callable\nobject). The generic_filter1d function iterates over the\nlines of an array and calls function at each line. The\narguments that are passed to function are 1-D\narrays of the numpy.float64 type. The first contains the values\nof the current line. It is extended at the beginning and the end,\naccording to the filter_size and origin arguments. The second\narray should be modified in-place to provide the output values of\nthe line. For example, consider a correlation along one dimension:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(3,4)\n correlate1d(a, [1, 2, 3])\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ],
        [
            "The same operation can be implemented using generic_filter1d,\nas follows:",
            "markdown"
        ],
        [
            "def fnc(iline, oline):\n...     oline[...] = iline[:-2] + 2 * iline[1:-1] + 3 * iline[2:]\n...\n from scipy.ndimage import generic_filter1d\n generic_filter1d(a, fnc, 3)\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ],
        [
            "Here, the origin of the kernel was (by default) assumed to be in the\nmiddle of the filter of length 3. Therefore, each input line had been\nextended by one value at the beginning and at the end, before the\nfunction was called.",
            "markdown"
        ],
        [
            "Optionally, extra arguments can be defined and passed to the filter\nfunction. The extra_arguments and extra_keywords arguments can\nbe used to pass a tuple of extra arguments and/or a dictionary of\nnamed arguments that are passed to derivative at each call. For\nexample, we can pass the parameters of our filter as an argument",
            "markdown"
        ],
        [
            "def fnc(iline, oline, a, b):\n...     oline[...] = iline[:-2] + a * iline[1:-1] + b * iline[2:]\n...\n generic_filter1d(a, fnc, 3, extra_arguments = (2, 3))\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "generic_filter1d(a, fnc, 3, extra_keywords = {'a':2, 'b':3})\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ],
        [
            "The generic_filter function implements a generic filter\nfunction, where the actual filtering operation must be supplied as a\npython function (or other callable object). The\ngeneric_filter function iterates over the array and calls\nfunction at each element. The argument of function\nis a 1-D array of the numpy.float64 type that\ncontains the values around the current element that are within the\nfootprint of the filter. The function should return a single value\nthat can be converted to a double precision number. For example,\nconsider a correlation:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(3,4)\n correlate(a, [[1, 0], [0, 3]])\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "The same operation can be implemented using generic_filter, as\nfollows:",
            "markdown"
        ],
        [
            "def fnc(buffer):\n...     return (buffer * np.array([1, 3])).sum()\n...\n from scipy.ndimage import generic_filter\n generic_filter(a, fnc, footprint = [[1, 0], [0, 1]])\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "Here, a kernel footprint was specified that contains only two\nelements. Therefore, the filter function receives a buffer of length\nequal to two, which was multiplied with the proper weights and the\nresult summed.",
            "markdown"
        ],
        [
            "When calling generic_filter, either the sizes of a\nrectangular kernel or the footprint of the kernel must be\nprovided. The size parameter, if provided, must be a sequence of\nsizes or a single number, in which case the size of the filter is\nassumed to be equal along each axis. The footprint, if provided,\nmust be an array that defines the shape of the kernel by its\nnon-zero elements.",
            "markdown"
        ],
        [
            "Optionally, extra arguments can be defined and passed to the filter\nfunction. The extra_arguments and extra_keywords arguments can\nbe used to pass a tuple of extra arguments and/or a dictionary of\nnamed arguments that are passed to derivative at each call. For\nexample, we can pass the parameters of our filter as an argument",
            "markdown"
        ],
        [
            "def fnc(buffer, weights):\n...     weights = np.asarray(weights)\n...     return (buffer * weights).sum()\n...\n generic_filter(a, fnc, footprint = [[1, 0], [0, 1]], extra_arguments = ([1, 3],))\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "generic_filter(a, fnc, footprint = [[1, 0], [0, 1]], extra_keywords= {'weights': [1, 3]})\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "These functions iterate over the lines or elements starting at the\nlast axis, i.e., the last index changes the fastest. This order of\niteration is guaranteed for the case that it is important to adapt the\nfilter depending on spatial location. Here is an example of using a\nclass that implements the filter and keeps track of the current\ncoordinates while iterating. It performs the same filter operation as\ndescribed above for generic_filter, but additionally prints\nthe current coordinates:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(3,4)\n\n class fnc_class:\n...     def __init__(self, shape):\n...         # store the shape:\n...         self.shape = shape\n...         # initialize the coordinates:\n...         self.coordinates = [0] * len(shape)\n...\n...     def filter(self, buffer):\n...         result = (buffer * np.array([1, 3])).sum()\n...         print(self.coordinates)\n...         # calculate the next coordinates:\n...         axes = list(range(len(self.shape)))\n...         axes.reverse()\n...         for jj in axes:\n...             if self.coordinates[jj] &lt; self.shape[jj] - 1:\n...                 self.coordinates[jj] += 1\n...                 break\n...             else:\n...                 self.coordinates[jj] = 0\n...         return result\n...\n fnc = fnc_class(shape = (3,4))\n generic_filter(a, fnc.filter, footprint = [[1, 0], [0, 1]])\n[0, 0]\n[0, 1]\n[0, 2]\n[0, 3]\n[1, 0]\n[1, 1]\n[1, 2]\n[1, 3]\n[2, 0]\n[2, 1]\n[2, 2]\n[2, 3]\narray([[ 0,  3,  7, 11],\n       [12, 15, 19, 23],\n       [28, 31, 35, 39]])",
            "code"
        ],
        [
            "For the generic_filter1d function, the same approach works,\nexcept that this function does not iterate over the axis that is being\nfiltered. The example for generic_filter1d then becomes this:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(3,4)\n\n class fnc1d_class:\n...     def __init__(self, shape, axis = -1):\n...         # store the filter axis:\n...         self.axis = axis\n...         # store the shape:\n...         self.shape = shape\n...         # initialize the coordinates:\n...         self.coordinates = [0] * len(shape)\n...\n...     def filter(self, iline, oline):\n...         oline[...] = iline[:-2] + 2 * iline[1:-1] + 3 * iline[2:]\n...         print(self.coordinates)\n...         # calculate the next coordinates:\n...         axes = list(range(len(self.shape)))\n...         # skip the filter axis:\n...         del axes[self.axis]\n...         axes.reverse()\n...         for jj in axes:\n...             if self.coordinates[jj] &lt; self.shape[jj] - 1:\n...                 self.coordinates[jj] += 1\n...                 break\n...             else:\n...                 self.coordinates[jj] = 0\n...\n fnc = fnc1d_class(shape = (3,4))\n generic_filter1d(a, fnc.filter, 3)\n[0, 0]\n[1, 0]\n[2, 0]\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "code"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Filter functions->Fourier domain filters": [
        [
            "The functions described in this section perform filtering\noperations in the Fourier domain. Thus, the input array of such a\nfunction should be compatible with an inverse Fourier transform\nfunction, such as the functions from the numpy.fft module. We,\ntherefore, have to deal with arrays that may be the result of a real\nor a complex Fourier transform. In the case of a real Fourier\ntransform, only half of the of the symmetric complex transform is\nstored. Additionally, it needs to be known what the length of the\naxis was that was transformed by the real fft. The functions\ndescribed here provide a parameter n that, in the case of a real\ntransform, must be equal to the length of the real transform axis\nbefore transformation. If this parameter is less than zero, it is\nassumed that the input array was the result of a complex Fourier\ntransform. The parameter axis can be used to indicate along which\naxis the real transform was executed.",
            "markdown"
        ],
        [
            "The fourier_shift function multiplies the input array with\nthe multidimensional Fourier transform of a shift operation for the\ngiven shift. The shift parameter is a sequence of shifts for each\ndimension or a single value for all dimensions.",
            "markdown"
        ],
        [
            "The fourier_gaussian function multiplies the input array\nwith the multidimensional Fourier transform of a Gaussian filter\nwith given standard deviations sigma. The sigma parameter is a\nsequence of values for each dimension or a single value for all\ndimensions.",
            "markdown"
        ],
        [
            "The fourier_uniform function multiplies the input array with\nthe multidimensional Fourier transform of a uniform filter with\ngiven sizes size. The size parameter is a sequence of values\nfor each dimension or a single value for all dimensions.",
            "markdown"
        ],
        [
            "The fourier_ellipsoid function multiplies the input array\nwith the multidimensional Fourier transform of an elliptically-shaped\nfilter with given sizes size. The size parameter is a sequence\nof values for each dimension or a single value for all dimensions.\nThis function is only implemented for dimensions 1, 2, and 3.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Interpolation functions": [
        [
            "This section describes various interpolation functions that are based\non B-spline theory. A good introduction to B-splines can be found\nin [1] with detailed algorithms for image interpolation given in [5].",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Interpolation functions->Spline pre-filters": [
        [
            "Interpolation using splines of an order larger than 1 requires a\npre-filtering step. The interpolation functions described in section\nInterpolation functions apply pre-filtering by calling\nspline_filter, but they can be instructed not to do this by\nsetting the prefilter keyword equal to False. This is useful if more\nthan one interpolation operation is done on the same array. In this\ncase, it is more efficient to do the pre-filtering only once and use a\npre-filtered array as the input of the interpolation functions. The\nfollowing two functions implement the pre-filtering:",
            "markdown"
        ],
        [
            "The spline_filter1d function calculates a 1-D\nspline filter along the given axis. An output array can optionally\nbe provided. The order of the spline must be larger than 1 and less\nthan 6.",
            "markdown"
        ],
        [
            "The spline_filter function calculates a multidimensional\nspline filter.",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The multidimensional filter is implemented as a sequence of\n1-D spline filters. The intermediate arrays are\nstored in the same data type as the output. Therefore, if an\noutput with a limited precision is requested, the results may be\nimprecise because intermediate results may be stored with\ninsufficient precision. This can be prevented by specifying a\noutput type of high precision.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Interpolation functions->Interpolation boundary handling": [
        [
            "The interpolation functions all employ spline interpolation to effect some\ntype of geometric transformation of the input array. This requires a\nmapping of the output coordinates to the input coordinates, and\ntherefore, the possibility arises that input values outside the\nboundaries may be needed. This problem is solved in the same way as\ndescribed in Filter functions for the multidimensional\nfilter functions. Therefore, these functions all support a mode\nparameter that determines how the boundaries are handled, and a cval\nparameter that gives a constant value in case that the \u00e2\u0080\u0098constant\u00e2\u0080\u0099 mode\nis used. The behavior of all modes, including at non-integer locations is\nillustrated below. Note the boundaries are not handled the same for all modes;\n<em class=\"xref py py-obj\">reflect (aka <em class=\"xref py py-obj\">grid-mirror) and <em class=\"xref py py-obj\">grid-wrap involve symmetry or repetition\nabout a point that is half way between image samples (dashed vertical lines)\nwhile modes <em class=\"xref py py-obj\">mirror and <em class=\"xref py py-obj\">wrap treat the image as if it\u00e2\u0080\u0099s extent ends exactly\nat the first and last sample point rather than 0.5 samples past it.\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/plot_boundary_modes.png\"/>\n</figure>",
            "markdown"
        ],
        [
            "The coordinates of image samples fall on integer sampling locations\nin the range from 0 to shape[i] - 1 along each axis, i. The figure\nbelow illustrates the interpolation of a point at location (3.7, 3.3)\nwithin an image of shape (7, 7). For an interpolation of order n,\nn + 1 samples are involved along each axis. The filled circles\nillustrate the sampling locations involved in the interpolation of the value at\nthe location of the red x.\n<figure class=\"align-default\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/plot_interp_grid.png\"/>\n</figure>",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Interpolation functions->Interpolation functions": [
        [
            "The geometric_transform function applies an arbitrary\ngeometric transform to the input. The given mapping function is\ncalled at each point in the output to find the corresponding\ncoordinates in the input. mapping must be a callable object that\naccepts a tuple of length equal to the output array rank and returns\nthe corresponding input coordinates as a tuple of length equal to\nthe input array rank. The output shape and output type can\noptionally be provided. If not given, they are equal to the input\nshape and type.",
            "markdown"
        ],
        [
            "For example:",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(4,3).astype(np.float64)\n def shift_func(output_coordinates):\n...     return (output_coordinates[0] - 0.5, output_coordinates[1] - 0.5)\n...\n from scipy.ndimage import geometric_transform\n geometric_transform(a, shift_func)\narray([[ 0.    ,  0.    ,  0.    ],\n       [ 0.    ,  1.3625,  2.7375],\n       [ 0.    ,  4.8125,  6.1875],\n       [ 0.    ,  8.2625,  9.6375]])",
            "code"
        ],
        [
            "Optionally, extra arguments can be defined and passed to the filter\nfunction. The extra_arguments and extra_keywords arguments can\nbe used to pass a tuple of extra arguments and/or a dictionary of\nnamed arguments that are passed to derivative at each call. For\nexample, we can pass the shifts in our example as arguments",
            "markdown"
        ],
        [
            "def shift_func(output_coordinates, s0, s1):\n...     return (output_coordinates[0] - s0, output_coordinates[1] - s1)\n...\n geometric_transform(a, shift_func, extra_arguments = (0.5, 0.5))\narray([[ 0.    ,  0.    ,  0.    ],\n       [ 0.    ,  1.3625,  2.7375],\n       [ 0.    ,  4.8125,  6.1875],\n       [ 0.    ,  8.2625,  9.6375]])",
            "code"
        ],
        [
            "or",
            "markdown"
        ],
        [
            "geometric_transform(a, shift_func, extra_keywords = {'s0': 0.5, 's1': 0.5})\narray([[ 0.    ,  0.    ,  0.    ],\n       [ 0.    ,  1.3625,  2.7375],\n       [ 0.    ,  4.8125,  6.1875],\n       [ 0.    ,  8.2625,  9.6375]])",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The mapping function can also be written in C and passed using a\nscipy.LowLevelCallable. See Extending scipy.ndimage in C for more\ninformation.",
            "markdown"
        ],
        [
            "The function map_coordinates applies an arbitrary coordinate\ntransformation using the given array of coordinates. The shape of\nthe output is derived from that of the coordinate array by dropping\nthe first axis. The parameter coordinates is used to find for each\npoint in the output the corresponding coordinates in the input. The\nvalues of coordinates along the first axis are the coordinates in\nthe input array at which the output value is found. (See also the\nnumarray <em class=\"xref py py-obj\">coordinates function.) Since the coordinates may be non-\ninteger coordinates, the value of the input at these coordinates is\ndetermined by spline interpolation of the requested order.",
            "markdown"
        ],
        [
            "Here is an example that interpolates a 2D array at (0.5, 0.5) and\n(1, 2):",
            "markdown"
        ],
        [
            "a = np.arange(12).reshape(4,3).astype(np.float64)\n a\narray([[  0.,   1.,   2.],\n       [  3.,   4.,   5.],\n       [  6.,   7.,   8.],\n       [  9.,  10.,  11.]])\n from scipy.ndimage import map_coordinates\n map_coordinates(a, [[0.5, 2], [0.5, 1]])\narray([ 1.3625,  7.])",
            "code"
        ],
        [
            "The affine_transform function applies an affine\ntransformation to the input array. The given transformation matrix\nand offset are used to find for each point in the output the\ncorresponding coordinates in the input. The value of the input at\nthe calculated coordinates is determined by spline interpolation of\nthe requested order. The transformation matrix must be\n2-D or can also be given as a 1-D sequence\nor array. In the latter case, it is assumed that the matrix is\ndiagonal. A more efficient interpolation algorithm is then applied\nthat exploits the separability of the problem. The output shape and\noutput type can optionally be provided. If not given, they are equal\nto the input shape and type.",
            "markdown"
        ],
        [
            "The shift function returns a shifted version of the input,\nusing spline interpolation of the requested order.",
            "markdown"
        ],
        [
            "The zoom function returns a rescaled version of the input,\nusing spline interpolation of the requested order.",
            "markdown"
        ],
        [
            "The rotate function returns the input array rotated in the\nplane defined by the two axes given by the parameter axes, using\nspline interpolation of the requested order. The angle must be\ngiven in degrees. If reshape is true, then the size of the output\narray is adapted to contain the rotated input.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Morphology->Binary morphology": [
        [
            "The generate_binary_structure functions generates a binary\nstructuring element for use in binary morphology operations. The\nrank of the structure must be provided. The size of the structure\nthat is returned is equal to three in each direction. The value of\neach element is equal to one if the square of the Euclidean distance\nfrom the element to the center is less than or equal to\nconnectivity. For instance, 2-D 4-connected and\n8-connected structures are generated as follows:",
            "markdown"
        ],
        [
            "from scipy.ndimage import generate_binary_structure\n generate_binary_structure(2, 1)\narray([[False,  True, False],\n       [ True,  True,  True],\n       [False,  True, False]], dtype=bool)\n generate_binary_structure(2, 2)\narray([[ True,  True,  True],\n       [ True,  True,  True],\n       [ True,  True,  True]], dtype=bool)",
            "code"
        ],
        [
            "This is a viusal presentation of generate_binary_structure in 3D:\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/3D_binary_structure.png\"/>\n</figure>\n</blockquote>",
            "markdown"
        ],
        [
            "Most binary morphology functions can be expressed in terms of the\nbasic operations erosion and dilation, which can be seen here:\n\n<figure class=\"align-center\">\n<img alt='\" \"' class=\"plot-directive\" src=\"../_images/morphology_binary_dilation_erosion.png\"/>\n</figure>\n</blockquote>",
            "markdown"
        ],
        [
            "The binary_erosion function implements binary erosion of\narrays of arbitrary rank with the given structuring element. The\norigin parameter controls the placement of the structuring element,\nas described in Filter functions. If no structuring\nelement is provided, an element with connectivity equal to one is\ngenerated using generate_binary_structure. The\nborder_value parameter gives the value of the array outside\nboundaries. The erosion is repeated iterations times. If\niterations is less than one, the erosion is repeated until the\nresult does not change anymore. If a mask array is given, only\nthose elements with a true value at the corresponding mask element\nare modified at each iteration.",
            "markdown"
        ],
        [
            "The binary_dilation function implements binary dilation of\narrays of arbitrary rank with the given structuring element. The\norigin parameter controls the placement of the structuring element,\nas described in Filter functions. If no structuring\nelement is provided, an element with connectivity equal to one is\ngenerated using generate_binary_structure. The\nborder_value parameter gives the value of the array outside\nboundaries. The dilation is repeated iterations times. If\niterations is less than one, the dilation is repeated until the\nresult does not change anymore. If a mask array is given, only\nthose elements with a true value at the corresponding mask element\nare modified at each iteration.",
            "markdown"
        ],
        [
            "Here is an example of using binary_dilation to find all elements\nthat touch the border, by repeatedly dilating an empty array from\nthe border using the data array as the mask:",
            "markdown"
        ],
        [
            "struct = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n a = np.array([[1,0,0,0,0], [1,1,0,1,0], [0,0,1,1,0], [0,0,0,0,0]])\n a\narray([[1, 0, 0, 0, 0],\n       [1, 1, 0, 1, 0],\n       [0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 0]])\n from scipy.ndimage import binary_dilation\n binary_dilation(np.zeros(a.shape), struct, -1, a, border_value=1)\narray([[ True, False, False, False, False],\n       [ True,  True, False, False, False],\n       [False, False, False, False, False],\n       [False, False, False, False, False]], dtype=bool)",
            "code"
        ],
        [
            "The binary_erosion and binary_dilation functions both\nhave an iterations parameter, which allows the erosion or dilation to\nbe repeated a number of times. Repeating an erosion or a dilation with\na given structure n times is equivalent to an erosion or a dilation\nwith a structure that is n-1 times dilated with itself. A function\nis provided that allows the calculation of a structure that is dilated\na number of times with itself:",
            "markdown"
        ],
        [
            "The iterate_structure function returns a structure by dilation\nof the input structure iteration - 1 times with itself.",
            "markdown"
        ],
        [
            "For instance:",
            "markdown"
        ],
        [
            "struct = generate_binary_structure(2, 1)\n struct\narray([[False,  True, False],\n       [ True,  True,  True],\n       [False,  True, False]], dtype=bool)\n from scipy.ndimage import iterate_structure\n iterate_structure(struct, 2)\narray([[False, False,  True, False, False],\n       [False,  True,  True,  True, False],\n       [ True,  True,  True,  True,  True],\n       [False,  True,  True,  True, False],\n       [False, False,  True, False, False]], dtype=bool)\n\nIf the origin of the original structure is equal to 0, then it is\nalso equal to 0 for the iterated structure. If not, the origin\nmust also be adapted if the equivalent of the *iterations*\nerosions or dilations must be achieved with the iterated\nstructure. The adapted origin is simply obtained by multiplying\nwith the number of iterations. For convenience, the\n:func:`iterate_structure` also returns the adapted origin if the\n*origin* parameter is not ``None``:\n\n.. code:: python\n\n    iterate_structure(struct, 2, -1)\n   (array([[False, False,  True, False, False],\n           [False,  True,  True,  True, False],\n           [ True,  True,  True,  True,  True],\n           [False,  True,  True,  True, False],\n           [False, False,  True, False, False]], dtype=bool), [-2, -2])",
            "code"
        ],
        [
            "Other morphology operations can be defined in terms of erosion and\ndilation. The following functions provide a few of these operations\nfor convenience:",
            "markdown"
        ],
        [
            "The binary_opening function implements binary opening of\narrays of arbitrary rank with the given structuring element. Binary\nopening is equivalent to a binary erosion followed by a binary\ndilation with the same structuring element. The origin parameter\ncontrols the placement of the structuring element, as described in\nFilter functions. If no structuring element is\nprovided, an element with connectivity equal to one is generated\nusing generate_binary_structure. The iterations parameter\ngives the number of erosions that is performed followed by the same\nnumber of dilations.",
            "markdown"
        ],
        [
            "The binary_closing function implements binary closing of\narrays of arbitrary rank with the given structuring element. Binary\nclosing is equivalent to a binary dilation followed by a binary\nerosion with the same structuring element. The origin parameter\ncontrols the placement of the structuring element, as described in\nFilter functions. If no structuring element is\nprovided, an element with connectivity equal to one is generated\nusing generate_binary_structure. The iterations parameter\ngives the number of dilations that is performed followed by the same\nnumber of erosions.",
            "markdown"
        ],
        [
            "The binary_fill_holes function is used to close holes in\nobjects in a binary image, where the structure defines the\nconnectivity of the holes. The origin parameter controls the\nplacement of the structuring element, as described in\nFilter functions. If no structuring element is\nprovided, an element with connectivity equal to one is generated\nusing generate_binary_structure.",
            "markdown"
        ],
        [
            "The binary_hit_or_miss function implements a binary\nhit-or-miss transform of arrays of arbitrary rank with the given\nstructuring elements. The hit-or-miss transform is calculated by\nerosion of the input with the first structure, erosion of the\nlogical not of the input with the second structure, followed by\nthe logical and of these two erosions. The origin parameters\ncontrol the placement of the structuring elements, as described in\nFilter functions. If origin2 equals None, it is set\nequal to the origin1 parameter. If the first structuring element\nis not provided, a structuring element with connectivity equal to\none is generated using generate_binary_structure. If\nstructure2 is not provided, it is set equal to the logical not\nof structure1.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Morphology->Grey-scale morphology": [
        [
            "Grey-scale morphology operations are the equivalents of binary\nmorphology operations that operate on arrays with arbitrary values.\nBelow, we describe the grey-scale equivalents of erosion, dilation,\nopening and closing. These operations are implemented in a similar\nfashion as the filters described in Filter functions,\nand we refer to this section for the description of filter kernels and\nfootprints, and the handling of array borders. The grey-scale\nmorphology operations optionally take a structure parameter that\ngives the values of the structuring element. If this parameter is not\ngiven, the structuring element is assumed to be flat with a value equal\nto zero. The shape of the structure can optionally be defined by the\nfootprint parameter. If this parameter is not given, the structure\nis assumed to be rectangular, with sizes equal to the dimensions of\nthe structure array, or by the size parameter if structure is\nnot given. The size parameter is only used if both structure and\nfootprint are not given, in which case the structuring element is\nassumed to be rectangular and flat with the dimensions given by\nsize. The size parameter, if provided, must be a sequence of sizes\nor a single number in which case the size of the filter is assumed to\nbe equal along each axis. The footprint parameter, if provided, must\nbe an array that defines the shape of the kernel by its non-zero\nelements.",
            "markdown"
        ],
        [
            "Similarly to binary erosion and dilation, there are operations for\ngrey-scale erosion and dilation:",
            "markdown"
        ],
        [
            "The grey_erosion function calculates a multidimensional\ngrey-scale erosion.",
            "markdown"
        ],
        [
            "The grey_dilation function calculates a multidimensional\ngrey-scale dilation.",
            "markdown"
        ],
        [
            "Grey-scale opening and closing operations can be defined similarly to\ntheir binary counterparts:",
            "markdown"
        ],
        [
            "The grey_opening function implements grey-scale opening of\narrays of arbitrary rank. Grey-scale opening is equivalent to a\ngrey-scale erosion followed by a grey-scale dilation.",
            "markdown"
        ],
        [
            "The grey_closing function implements grey-scale closing of\narrays of arbitrary rank. Grey-scale opening is equivalent to a\ngrey-scale dilation followed by a grey-scale erosion.",
            "markdown"
        ],
        [
            "The morphological_gradient function implements a grey-scale\nmorphological gradient of arrays of arbitrary rank. The grey-scale\nmorphological gradient is equal to the difference of a grey-scale\ndilation and a grey-scale erosion.",
            "markdown"
        ],
        [
            "The morphological_laplace function implements a grey-scale\nmorphological laplace of arrays of arbitrary rank. The grey-scale\nmorphological laplace is equal to the sum of a grey-scale dilation\nand a grey-scale erosion minus twice the input.",
            "markdown"
        ],
        [
            "The white_tophat function implements a white top-hat filter\nof arrays of arbitrary rank. The white top-hat is equal to the\ndifference of the input and a grey-scale opening.",
            "markdown"
        ],
        [
            "The black_tophat function implements a black top-hat filter\nof arrays of arbitrary rank. The black top-hat is equal to the\ndifference of a grey-scale closing and the input.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Distance transforms": [
        [
            "Distance transforms are used to calculate the minimum distance from\neach element of an object to the background. The following functions\nimplement distance transforms for three different distance metrics:\nEuclidean, city block, and chessboard distances.",
            "markdown"
        ],
        [
            "The function distance_transform_cdt uses a chamfer type\nalgorithm to calculate the distance transform of the input, by\nreplacing each object element (defined by values larger than zero)\nwith the shortest distance to the background (all non-object\nelements). The structure determines the type of chamfering that is\ndone. If the structure is equal to \u00e2\u0080\u0098cityblock\u00e2\u0080\u0099, a structure is\ngenerated using generate_binary_structure with a squared\ndistance equal to 1. If the structure is equal to \u00e2\u0080\u0098chessboard\u00e2\u0080\u0099, a\nstructure is generated using generate_binary_structure with\na squared distance equal to the rank of the array. These choices\ncorrespond to the common interpretations of the city block and the\nchessboard distance metrics in two dimensions.",
            "markdown"
        ],
        [
            "In addition to the distance transform, the feature transform can be\ncalculated. In this case, the index of the closest background element\nis returned along the first axis of the result. The\nreturn_distances, and return_indices flags can be used to\nindicate if the distance transform, the feature transform, or both\nmust be returned.",
            "markdown"
        ],
        [
            "The distances and indices arguments can be used to give optional\noutput arrays that must be of the correct size and type (both\nnumpy.int32). The basics of the algorithm used to implement this\nfunction are described in [2].",
            "markdown"
        ],
        [
            "The function distance_transform_edt calculates the exact\nEuclidean distance transform of the input, by replacing each object\nelement (defined by values larger than zero) with the shortest\nEuclidean distance to the background (all non-object elements).",
            "markdown"
        ],
        [
            "In addition to the distance transform, the feature transform can be\ncalculated. In this case, the index of the closest background element\nis returned along the first axis of the result. The\nreturn_distances and return_indices flags can be used to\nindicate if the distance transform, the feature transform, or both\nmust be returned.",
            "markdown"
        ],
        [
            "Optionally, the sampling along each axis can be given by the\nsampling parameter, which should be a sequence of length equal to\nthe input rank, or a single number in which the sampling is assumed\nto be equal along all axes.",
            "markdown"
        ],
        [
            "The distances and indices arguments can be used to give optional\noutput arrays that must be of the correct size and type\n(numpy.float64 and numpy.int32).The algorithm used to\nimplement this function is described in [3].",
            "markdown"
        ],
        [
            "The function distance_transform_bf uses a brute-force\nalgorithm to calculate the distance transform of the input, by\nreplacing each object element (defined by values larger than zero)\nwith the shortest distance to the background (all non-object\nelements). The metric must be one of \u00e2\u0080\u009ceuclidean\u00e2\u0080\u009d, \u00e2\u0080\u009ccityblock\u00e2\u0080\u009d, or\n\u00e2\u0080\u009cchessboard\u00e2\u0080\u009d.",
            "markdown"
        ],
        [
            "In addition to the distance transform, the feature transform can be\ncalculated. In this case, the index of the closest background element\nis returned along the first axis of the result. The\nreturn_distances and return_indices flags can be used to\nindicate if the distance transform, the feature transform, or both\nmust be returned.",
            "markdown"
        ],
        [
            "Optionally, the sampling along each axis can be given by the\nsampling parameter, which should be a sequence of length equal to\nthe input rank, or a single number in which the sampling is assumed\nto be equal along all axes. This parameter is only used in the case\nof the Euclidean distance transform.",
            "markdown"
        ],
        [
            "The distances and indices arguments can be used to give optional\noutput arrays that must be of the correct size and type\n(numpy.float64 and numpy.int32).",
            "markdown"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "This function uses a slow brute-force algorithm, the function\ndistance_transform_cdt can be used to more efficiently\ncalculate city block and chessboard distance transforms. The\nfunction distance_transform_edt can be used to more\nefficiently calculate the exact Euclidean distance transform.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Segmentation and labeling": [
        [
            "Segmentation is the process of separating objects of interest from\nthe background. The most simple approach is, probably, intensity\nthresholding, which is easily done with numpy functions:",
            "markdown"
        ],
        [
            "a = np.array([[1,2,2,1,1,0],\n...               [0,2,3,1,2,0],\n...               [1,1,1,3,3,2],\n...               [1,1,1,1,2,1]])\n np.where(a  1, 1, 0)\narray([[0, 1, 1, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1],\n       [0, 0, 0, 0, 1, 0]])",
            "code"
        ],
        [
            "The result is a binary image, in which the individual objects still\nneed to be identified and labeled. The function label\ngenerates an array where each object is assigned a unique number:",
            "markdown"
        ],
        [
            "The label function generates an array where the objects in\nthe input are labeled with an integer index. It returns a tuple\nconsisting of the array of object labels and the number of objects\nfound, unless the output parameter is given, in which case only\nthe number of objects is returned. The connectivity of the objects\nis defined by a structuring element. For instance, in 2D\nusing a 4-connected structuring element gives:",
            "markdown"
        ],
        [
            "a = np.array([[0,1,1,0,0,0],[0,1,1,0,1,0],[0,0,0,1,1,1],[0,0,0,0,1,0]])\n s = [[0, 1, 0], [1,1,1], [0,1,0]]\n from scipy.ndimage import label\n label(a, s)\n(array([[0, 1, 1, 0, 0, 0],\n        [0, 1, 1, 0, 2, 0],\n        [0, 0, 0, 2, 2, 2],\n        [0, 0, 0, 0, 2, 0]]), 2)",
            "code"
        ],
        [
            "These two objects are not connected because there is no way in which\nwe can place the structuring element, such that it overlaps with both\nobjects. However, an 8-connected structuring element results in only\na single object:",
            "markdown"
        ],
        [
            "a = np.array([[0,1,1,0,0,0],[0,1,1,0,1,0],[0,0,0,1,1,1],[0,0,0,0,1,0]])\n s = [[1,1,1], [1,1,1], [1,1,1]]\n label(a, s)[0]\narray([[0, 1, 1, 0, 0, 0],\n       [0, 1, 1, 0, 1, 0],\n       [0, 0, 0, 1, 1, 1],\n       [0, 0, 0, 0, 1, 0]])",
            "code"
        ],
        [
            "If no structuring element is provided, one is generated by calling\ngenerate_binary_structure (see\nBinary morphology) using a connectivity of one (which\nin 2D is the 4-connected structure of the first example). The input\ncan be of any type, any value not equal to zero is taken to be part\nof an object. This is useful if you need to \u00e2\u0080\u0098re-label\u00e2\u0080\u0099 an array of\nobject indices, for instance, after removing unwanted objects. Just\napply the label function again to the index array. For instance:",
            "markdown"
        ],
        [
            "l, n = label([1, 0, 1, 0, 1])\n l\narray([1, 0, 2, 0, 3])\n l = np.where(l != 2, l, 0)\n l\narray([1, 0, 0, 0, 3])\n label(l)[0]\narray([1, 0, 0, 0, 2])",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The structuring element used by label is assumed to be\nsymmetric.",
            "markdown"
        ],
        [
            "There is a large number of other approaches for segmentation, for\ninstance, from an estimation of the borders of the objects that can be\nobtained by derivative filters. One such approach is\nwatershed segmentation. The function watershed_ift generates\nan array where each object is assigned a unique label, from an array\nthat localizes the object borders, generated, for instance, by a\ngradient magnitude filter. It uses an array containing initial markers\nfor the objects:",
            "markdown"
        ],
        [
            "The watershed_ift function applies a watershed from markers\nalgorithm, using Image Foresting Transform, as described in\n[4].",
            "markdown"
        ],
        [
            "The inputs of this function are the array to which the transform is\napplied, and an array of markers that designate the objects by a\nunique label, where any non-zero value is a marker. For instance:",
            "markdown"
        ],
        [
            "input = np.array([[0, 0, 0, 0, 0, 0, 0],\n...                   [0, 1, 1, 1, 1, 1, 0],\n...                   [0, 1, 0, 0, 0, 1, 0],\n...                   [0, 1, 0, 0, 0, 1, 0],\n...                   [0, 1, 0, 0, 0, 1, 0],\n...                   [0, 1, 1, 1, 1, 1, 0],\n...                   [0, 0, 0, 0, 0, 0, 0]], np.uint8)\n markers = np.array([[1, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 2, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0]], np.int8)\n from scipy.ndimage import watershed_ift\n watershed_ift(input, markers)\narray([[1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 2, 2, 2, 2, 2, 1],\n       [1, 2, 2, 2, 2, 2, 1],\n       [1, 2, 2, 2, 2, 2, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1]], dtype=int8)",
            "code"
        ],
        [
            "Here, two markers were used to designate an object (marker = 2) and\nthe background (marker = 1). The order in which these are\nprocessed is arbitrary: moving the marker for the background to the\nlower-right corner of the array yields a different result:",
            "markdown"
        ],
        [
            "markers = np.array([[0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 2, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 1]], np.int8)\n watershed_ift(input, markers)\narray([[1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 2, 2, 2, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1]], dtype=int8)",
            "code"
        ],
        [
            "The result is that the object (marker = 2) is smaller because the\nsecond marker was processed earlier. This may not be the desired\neffect if the first marker was supposed to designate a background\nobject. Therefore, watershed_ift treats markers with a\nnegative value explicitly as background markers and processes them\nafter the normal markers. For instance, replacing the first marker\nby a negative marker gives a result similar to the first example:",
            "markdown"
        ],
        [
            "markers = np.array([[0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 2, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, 0],\n...                     [0, 0, 0, 0, 0, 0, -1]], np.int8)\n watershed_ift(input, markers)\narray([[-1, -1, -1, -1, -1, -1, -1],\n       [-1, -1,  2,  2,  2, -1, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1, -1,  2,  2,  2, -1, -1],\n       [-1, -1, -1, -1, -1, -1, -1]], dtype=int8)",
            "code"
        ],
        [
            "The connectivity of the objects is defined by a structuring\nelement. If no structuring element is provided, one is generated by\ncalling generate_binary_structure (see\nBinary morphology) using a connectivity of one (which\nin 2D is a 4-connected structure.) For example, using an 8-connected\nstructure with the last example yields a different object:",
            "markdown"
        ],
        [
            "watershed_ift(input, markers,\n...               structure = [[1,1,1], [1,1,1], [1,1,1]])\narray([[-1, -1, -1, -1, -1, -1, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1,  2,  2,  2,  2,  2, -1],\n       [-1, -1, -1, -1, -1, -1, -1]], dtype=int8)",
            "code"
        ],
        [
            "Note",
            "markdown"
        ],
        [
            "The implementation of watershed_ift limits the data types\nof the input to numpy.uint8 and numpy.uint16.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Object measurements": [
        [
            "Given an array of labeled objects, the properties of the individual\nobjects can be measured. The find_objects function can be used\nto generate a list of slices that for each object, give the\nsmallest sub-array that fully contains the object:",
            "markdown"
        ],
        [
            "The find_objects function finds all objects in a labeled\narray and returns a list of slices that correspond to the smallest\nregions in the array that contains the object.",
            "markdown"
        ],
        [
            "For instance:",
            "markdown"
        ],
        [
            "a = np.array([[0,1,1,0,0,0],[0,1,1,0,1,0],[0,0,0,1,1,1],[0,0,0,0,1,0]])\n l, n = label(a)\n from scipy.ndimage import find_objects\n f = find_objects(l)\n a[f[0]]\narray([[1, 1],\n       [1, 1]])\n a[f[1]]\narray([[0, 1, 0],\n       [1, 1, 1],\n       [0, 1, 0]])",
            "code"
        ],
        [
            "The function find_objects returns slices for all objects,\nunless the max_label parameter is larger then zero, in which case\nonly the first max_label objects are returned. If an index is\nmissing in the label array, None is return instead of a\nslice. For example:",
            "markdown"
        ],
        [
            "from scipy.ndimage import find_objects\n find_objects([1, 0, 3, 4], max_label = 3)\n[(slice(0, 1, None),), None, (slice(2, 3, None),)]",
            "code"
        ],
        [
            "The list of slices generated by find_objects is useful to find\nthe position and dimensions of the objects in the array, but can also\nbe used to perform measurements on the individual objects. Say, we want\nto find the sum of the intensities of an object in image:",
            "markdown"
        ],
        [
            "image = np.arange(4 * 6).reshape(4, 6)\n mask = np.array([[0,1,1,0,0,0],[0,1,1,0,1,0],[0,0,0,1,1,1],[0,0,0,0,1,0]])\n labels = label(mask)[0]\n slices = find_objects(labels)",
            "code"
        ],
        [
            "Then we can calculate the sum of the elements in the second object:",
            "markdown"
        ],
        [
            "np.where(labels[slices[1]] == 2, image[slices[1]], 0).sum()\n80",
            "code"
        ],
        [
            "That is, however, not particularly efficient and may also be more\ncomplicated for other types of measurements. Therefore, a few\nmeasurements functions are defined that accept the array of object\nlabels and the index of the object to be measured. For instance,\ncalculating the sum of the intensities can be done by:",
            "markdown"
        ],
        [
            "from scipy.ndimage import sum as ndi_sum\n ndi_sum(image, labels, 2)\n80",
            "code"
        ],
        [
            "For large arrays and small objects, it is more efficient to call the\nmeasurement functions after slicing the array:",
            "markdown"
        ],
        [
            "ndi_sum(image[slices[1]], labels[slices[1]], 2)\n80",
            "code"
        ],
        [
            "Alternatively, we can do the measurements for a number of labels with\na single function call, returning a list of results. For instance, to\nmeasure the sum of the values of the background and the second object\nin our example, we give a list of labels:",
            "markdown"
        ],
        [
            "ndi_sum(image, labels, [0, 2])\narray([178.0, 80.0])",
            "code"
        ],
        [
            "The measurement functions described below all support the index\nparameter to indicate which object(s) should be measured. The default\nvalue of index is None. This indicates that all elements where the\nlabel is larger than zero should be treated as a single object and\nmeasured. Thus, in this case the labels array is treated as a mask\ndefined by the elements that are larger than zero. If index is a\nnumber or a sequence of numbers it gives the labels of the objects\nthat are measured. If index is a sequence, a list of the results is\nreturned. Functions that return more than one result return their\nresult as a tuple if index is a single number, or as a tuple of\nlists if index is a sequence.",
            "markdown"
        ],
        [
            "The sum function calculates the sum of the elements of the\nobject with label(s) given by index, using the labels array for\nthe object labels. If index is None, all elements with a\nnon-zero label value are treated as a single object. If label is\nNone, all elements of input are used in the calculation.",
            "markdown"
        ],
        [
            "The mean function calculates the mean of the elements of the\nobject with label(s) given by index, using the labels array for\nthe object labels. If index is None, all elements with a\nnon-zero label value are treated as a single object. If label is\nNone, all elements of input are used in the calculation.",
            "markdown"
        ],
        [
            "The variance function calculates the variance of the\nelements of the object with label(s) given by index, using the\nlabels array for the object labels. If index is None, all\nelements with a non-zero label value are treated as a single\nobject. If label is None, all elements of input are used in\nthe calculation.",
            "markdown"
        ],
        [
            "The standard_deviation function calculates the standard\ndeviation of the elements of the object with label(s) given by\nindex, using the labels array for the object labels. If index\nis None, all elements with a non-zero label value are treated as\na single object. If label is None, all elements of input are\nused in the calculation.",
            "markdown"
        ],
        [
            "The minimum function calculates the minimum of the elements\nof the object with label(s) given by index, using the labels\narray for the object labels. If index is None, all elements\nwith a non-zero label value are treated as a single object. If\nlabel is None, all elements of input are used in the\ncalculation.",
            "markdown"
        ],
        [
            "The maximum function calculates the maximum of the elements\nof the object with label(s) given by index, using the labels\narray for the object labels. If index is None, all elements\nwith a non-zero label value are treated as a single object. If\nlabel is None, all elements of input are used in the\ncalculation.",
            "markdown"
        ],
        [
            "The minimum_position function calculates the position of the\nminimum of the elements of the object with label(s) given by\nindex, using the labels array for the object labels. If index\nis None, all elements with a non-zero label value are treated as\na single object. If label is None, all elements of input are\nused in the calculation.",
            "markdown"
        ],
        [
            "The maximum_position function calculates the position of the\nmaximum of the elements of the object with label(s) given by\nindex, using the labels array for the object labels. If index\nis None, all elements with a non-zero label value are treated as\na single object. If label is None, all elements of input are\nused in the calculation.",
            "markdown"
        ],
        [
            "The extrema function calculates the minimum, the maximum,\nand their positions, of the elements of the object with label(s)\ngiven by index, using the labels array for the object labels. If\nindex is None, all elements with a non-zero label value are\ntreated as a single object. If label is None, all elements of\ninput are used in the calculation. The result is a tuple giving\nthe minimum, the maximum, the position of the minimum, and the\nposition of the maximum. The result is the same as a tuple formed by\nthe results of the functions minimum, maximum,\nminimum_position, and maximum_position that are described above.",
            "markdown"
        ],
        [
            "The center_of_mass function calculates the center of mass of\nthe object with label(s) given by index, using the labels\narray for the object labels. If index is None, all elements\nwith a non-zero label value are treated as a single object. If\nlabel is None, all elements of input are used in the\ncalculation.",
            "markdown"
        ],
        [
            "The histogram function calculates a histogram of the\nobject with label(s) given by index, using the labels array for\nthe object labels. If index is None, all elements with a\nnon-zero label value are treated as a single object. If label is\nNone, all elements of input are used in the calculation.\nHistograms are defined by their minimum (min), maximum (max), and\nthe number of bins (bins). They are returned as 1-D\narrays of type numpy.int32.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->Extending scipy.ndimage in C": [
        [
            "A few functions in scipy.ndimage take a callback argument. This\ncan be either a python function or a scipy.LowLevelCallable containing a\npointer to a C function. Using a C function will generally be more\nefficient, since it avoids the overhead of calling a python function on\nmany elements of an array. To use a C function, you must write a C\nextension that contains the callback function and a Python function\nthat returns a scipy.LowLevelCallable containing a pointer to the\ncallback.",
            "markdown"
        ],
        [
            "An example of a function that supports callbacks is\ngeometric_transform, which accepts a callback function that\ndefines a mapping from all output coordinates to corresponding\ncoordinates in the input array. Consider the following python example,\nwhich uses geometric_transform to implement a shift function.",
            "markdown"
        ],
        [
            "from scipy import ndimage\n\ndef transform(output_coordinates, shift):\n    input_coordinates = output_coordinates[0] - shift, output_coordinates[1] - shift\n    return input_coordinates\n\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nshift = 0.5\nprint(ndimage.geometric_transform(im, transform, extra_arguments=(shift,)))",
            "code"
        ],
        [
            "We can also implement the callback function with the following C code:",
            "markdown"
        ],
        [
            "/* example.c */\n\n#include &lt;Python.h\n#include &lt;numpy/npy_common.h\n\nstatic int\n_transform(npy_intp *output_coordinates, double *input_coordinates,\n           int output_rank, int input_rank, void *user_data)\n{\n    npy_intp i;\n    double shift = *(double *)user_data;\n\n    for (i = 0; i &lt; input_rank; i++) {\n        input_coordinates[i] = output_coordinates[i] - shift;\n    }\n    return 1;\n}\n\nstatic char *transform_signature = \"int (npy_intp *, double *, int, int, void *)\";\n\nstatic PyObject *\npy_get_transform(PyObject *obj, PyObject *args)\n{\n    if (!PyArg_ParseTuple(args, \"\")) return NULL;\n    return PyCapsule_New(_transform, transform_signature, NULL);\n}\n\nstatic PyMethodDef ExampleMethods[] = {\n    {\"get_transform\", (PyCFunction)py_get_transform, METH_VARARGS, \"\"},\n    {NULL, NULL, 0, NULL}\n};\n\n/* Initialize the module */\nstatic struct PyModuleDef example = {\n    PyModuleDef_HEAD_INIT,\n    \"example\",\n    NULL,\n    -1,\n    ExampleMethods,\n    NULL,\n    NULL,\n    NULL,\n    NULL\n};\n\nPyMODINIT_FUNC\nPyInit_example(void)\n{\n    return PyModule_Create(&example);\n}",
            "code"
        ],
        [
            "More information on writing Python extension modules can be found\nhere. If the C code is in the file example.c, then it can be\ncompiled with the following setup.py,",
            "markdown"
        ],
        [
            "from distutils.core import setup, Extension\nimport numpy\n\nshift = Extension('example',\n                  ['example.c'],\n                  include_dirs=[numpy.get_include()]\n)\n\nsetup(name='example',\n      ext_modules=[shift]\n)",
            "code"
        ],
        [
            "and now running the script",
            "markdown"
        ],
        [
            "import ctypes\nimport numpy as np\nfrom scipy import ndimage, LowLevelCallable\n\nfrom example import get_transform\n\nshift = 0.5\n\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\ncallback = LowLevelCallable(get_transform(), ptr)\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "produces the same result as the original python script.",
            "markdown"
        ],
        [
            "In the C version, _transform is the callback function and the\nparameters output_coordinates and input_coordinates play the\nsame role as they do in the python version, while output_rank and\ninput_rank provide the equivalents of len(output_coordinates)\nand len(input_coordinates). The variable shift is passed\nthrough user_data instead of\nextra_arguments. Finally, the C callback function returns an integer\nstatus, which is one upon success and zero otherwise.",
            "markdown"
        ],
        [
            "The function py_transform wraps the callback function in a\nPyCapsule. The main steps are:",
            "markdown"
        ],
        [
            "Initialize a PyCapsule. The first argument is a pointer to\nthe callback function.",
            "markdown"
        ],
        [
            "The second argument is the function signature, which must match exactly\nthe one expected by ndimage.",
            "markdown"
        ],
        [
            "Above, we used  scipy.LowLevelCallable to specify user_data\nthat we generated with ctypes.",
            "markdown"
        ],
        [
            "A different approach would be to supply the data in the capsule context,\nthat can be set by <em class=\"xref py py-obj\">PyCapsule_SetContext and omit specifying\nuser_data in scipy.LowLevelCallable. However, in this approach we would\nneed to deal with allocation/freeing of the data \u00e2\u0080\u0094 freeing the data\nafter the capsule has been destroyed can be done by specifying a non-NULL\ncallback function in the third argument of <em class=\"xref py py-obj\">PyCapsule_New.",
            "markdown"
        ],
        [
            "C callback functions for ndimage all follow this scheme. The\nnext section lists the ndimage functions that accept a C\ncallback function and gives the prototype of the function.",
            "markdown"
        ],
        [
            "See also",
            "markdown"
        ],
        [
            "The functions that support low-level callback arguments are:",
            "markdown"
        ],
        [
            "generic_filter, generic_filter1d, geometric_transform",
            "markdown"
        ],
        [
            "Below, we show alternative ways to write the code, using Numba, Cython,\nctypes, or cffi instead of writing wrapper code in C.",
            "markdown"
        ],
        [
            "Numba",
            "markdown"
        ],
        [
            "Numba provides a way to write low-level functions easily in Python.\nWe can write the above using Numba as:",
            "markdown"
        ],
        [
            "# example.py\nimport numpy as np\nimport ctypes\nfrom scipy import ndimage, LowLevelCallable\nfrom numba import cfunc, types, carray\n\n@cfunc(types.intc(types.CPointer(types.intp),\n                  types.CPointer(types.double),\n                  types.intc,\n                  types.intc,\n                  types.voidptr))\ndef transform(output_coordinates_ptr, input_coordinates_ptr,\n              output_rank, input_rank, user_data):\n    input_coordinates = carray(input_coordinates_ptr, (input_rank,))\n    output_coordinates = carray(output_coordinates_ptr, (output_rank,))\n    shift = carray(user_data, (1,), types.double)[0]\n\n    for i in range(input_rank):\n        input_coordinates[i] = output_coordinates[i] - shift\n\n    return 1\n\nshift = 0.5\n\n# Then call the function\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\ncallback = LowLevelCallable(transform.ctypes, ptr)\n\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "Cython",
            "markdown"
        ],
        [
            "Functionally the same code as above can be written in Cython with\nsomewhat less boilerplate as follows:",
            "markdown"
        ],
        [
            "# example.pyx\n\nfrom numpy cimport npy_intp as intp\n\ncdef api int transform(intp *output_coordinates, double *input_coordinates,\n                       int output_rank, int input_rank, void *user_data):\n    cdef intp i\n    cdef double shift = (&lt;double *user_data)[0]\n\n    for i in range(input_rank):\n        input_coordinates[i] = output_coordinates[i] - shift\n    return 1",
            "code"
        ],
        [
            "# script.py\n\nimport ctypes\nimport numpy as np\nfrom scipy import ndimage, LowLevelCallable\n\nimport example\n\nshift = 0.5\n\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\ncallback = LowLevelCallable.from_cython(example, \"transform\", ptr)\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "cffi",
            "markdown"
        ],
        [
            "With cffi, you can interface with a C function residing in a shared\nlibrary (DLL). First, we need to write the shared library, which we do\nin C \u00e2\u0080\u0094 this example is for Linux/OSX:",
            "markdown"
        ],
        [
            "/*\n  example.c\n  Needs to be compiled with \"gcc -std=c99 -shared -fPIC -o example.so example.c\"\n  or similar\n */\n\n#include &lt;stdint.h\n\nint\n_transform(intptr_t *output_coordinates, double *input_coordinates,\n           int output_rank, int input_rank, void *user_data)\n{\n    int i;\n    double shift = *(double *)user_data;\n\n    for (i = 0; i &lt; input_rank; i++) {\n        input_coordinates[i] = output_coordinates[i] - shift;\n    }\n    return 1;\n}",
            "code"
        ],
        [
            "The Python code calling the library is:",
            "markdown"
        ],
        [
            "import os\nimport numpy as np\nfrom scipy import ndimage, LowLevelCallable\nimport cffi\n\n# Construct the FFI object, and copypaste the function declaration\nffi = cffi.FFI()\nffi.cdef(\"\"\"\nint _transform(intptr_t *output_coordinates, double *input_coordinates,\n               int output_rank, int input_rank, void *user_data);\n\"\"\")\n\n# Open library\nlib = ffi.dlopen(os.path.abspath(\"example.so\"))\n\n# Do the function call\nuser_data = ffi.new('double *', 0.5)\ncallback = LowLevelCallable(lib._transform, user_data)\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "You can find more information in the cffi documentation.",
            "markdown"
        ],
        [
            "ctypes",
            "markdown"
        ],
        [
            "With ctypes, the C code and the compilation of the so/DLL is as for\ncffi above.  The Python code is different:",
            "markdown"
        ],
        [
            "# script.py\n\nimport os\nimport ctypes\nimport numpy as np\nfrom scipy import ndimage, LowLevelCallable\n\nlib = ctypes.CDLL(os.path.abspath('example.so'))\n\nshift = 0.5\n\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\n\n# Ctypes has no built-in intptr type, so override the signature\n# instead of trying to get it via ctypes\ncallback = LowLevelCallable(lib._transform, ptr,\n    \"int _transform(intptr_t *, double *, int, int, void *)\")\n\n# Perform the call\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "code"
        ],
        [
            "You can find more information in the ctypes documentation.",
            "markdown"
        ]
    ],
    "Multidimensional image processing (scipy.ndimage)->References": [
        [
            "M. Unser, \u00e2\u0080\u009cSplines: A Perfect Fit for Signal and Image\nProcessing,\u00e2\u0080\u009d IEEE Signal Processing Magazine, vol. 16, no. 6, pp.\n22-38, November 1999.\n</aside>\n<aside class=\"footnote brackets\" id=\"id9\" role=\"note\">\n[2]",
            "markdown"
        ],
        [
            "G. Borgefors, \u00e2\u0080\u009cDistance transformations in arbitrary\ndimensions.\u00e2\u0080\u009d, Computer Vision, Graphics, and Image Processing,\n27:321-345, 1984.\n</aside>\n<aside class=\"footnote brackets\" id=\"id10\" role=\"note\">\n[3]",
            "markdown"
        ],
        [
            "C. R. Maurer, Jr., R. Qi, and V. Raghavan, \u00e2\u0080\u009cA linear time\nalgorithm for computing exact euclidean distance transforms of\nbinary images in arbitrary dimensions.\u00e2\u0080\u009d IEEE Trans. PAMI 25,\n265-270, 2003.\n</aside>\n<aside class=\"footnote brackets\" id=\"id11\" role=\"note\">\n[4]",
            "markdown"
        ],
        [
            "A. X. Falc\u00c3\u00a3o, J. Stolfi, and R. A. Lotufo. \u00e2\u0080\u009cThe image foresting\ntransform: Theory, algorithms, and applications.\u00e2\u0080\u009d IEEE Trans.\nPAMI 26, 19-29. 2004.\n</aside>\n<aside class=\"footnote brackets\" id=\"id12\" role=\"note\">\n[5]",
            "markdown"
        ],
        [
            "T. Briand and P. Monasse, \u00e2\u0080\u009cTheory and Practice of Image B-Spline\nInterpolation\u00e2\u0080\u009d, Image Processing On Line, 8, pp. 99\u00e2\u0080\u0093141, 2018.\nhttps://doi.org/10.5201/ipol.2018.221\n</aside>\n</aside>",
            "markdown"
        ]
    ],
    "File IO (scipy.io)": [
        [
            "See also",
            "markdown"
        ],
        [
            "NumPy IO routines",
            "markdown"
        ],
        [
            "IDL files#",
            "markdown"
        ],
        [
            "Matrix Market files#",
            "markdown"
        ],
        [
            "Wav sound files (scipy.io.wavfile)#",
            "markdown"
        ],
        [
            "Arff files (scipy.io.arff)#",
            "markdown"
        ]
    ],
    "File IO (scipy.io)->MATLAB files->The basic functions": [
        [
            "We\u00e2\u0080\u0099ll start by importing scipy.io and calling it sio for\nconvenience:",
            "markdown"
        ],
        [
            "import scipy.io as sio",
            "code"
        ],
        [
            "If you are using IPython, try tab-completing on sio. Among the many\noptions, you will find:",
            "markdown"
        ],
        [
            "sio.loadmat\nsio.savemat\nsio.whosmat",
            "code"
        ],
        [
            "These are the high-level functions you will most likely use when working\nwith MATLAB files. You\u00e2\u0080\u0099ll also find:",
            "markdown"
        ],
        [
            "sio.matlab",
            "code"
        ],
        [
            "This is the package from which loadmat, savemat, and whosmat\nare imported. Within sio.matlab, you will find the mio module\nThis module contains the machinery that loadmat and savemat use.\nFrom time to time you may find yourself re-using this machinery.",
            "markdown"
        ]
    ],
    "File IO (scipy.io)->MATLAB files->How do I start?": [
        [
            "You may have a .mat file that you want to read into SciPy. Or, you\nwant to pass some variables from SciPy / NumPy into MATLAB.",
            "markdown"
        ],
        [
            "To save us using a MATLAB license, let\u00e2\u0080\u0099s start in Octave. Octave has\nMATLAB-compatible save and load functions. Start Octave (octave at\nthe command line for me):",
            "markdown"
        ],
        [
            "octave:1 a = 1:12\na =\n\n   1   2   3   4   5   6   7   8   9  10  11  12\n\noctave:2 a = reshape(a, [1 3 4])\na =\n\nans(:,:,1) =\n\n   1   2   3\n\nans(:,:,2) =\n\n   4   5   6\n\nans(:,:,3) =\n\n   7   8   9\n\nans(:,:,4) =\n\n   10   11   12\n\noctave:3 save -6 octave_a.mat a % MATLAB 6 compatible\noctave:4 ls octave_a.mat\noctave_a.mat",
            "code"
        ],
        [
            "Now, to Python:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_a.mat')\n mat_contents\n{'a': array([[[  1.,   4.,   7.,  10.],\n        [  2.,   5.,   8.,  11.],\n        [  3.,   6.,   9.,  12.]]]),\n '__version__': '1.0',\n '__header__': 'MATLAB 5.0 MAT-file, written by\n Octave 3.6.3, 2013-02-17 21:02:11 UTC',\n '__globals__': []}\n oct_a = mat_contents['a']\n oct_a\narray([[[  1.,   4.,   7.,  10.],\n        [  2.,   5.,   8.,  11.],\n        [  3.,   6.,   9.,  12.]]])\n oct_a.shape\n(1, 3, 4)",
            "code"
        ],
        [
            "Now let\u00e2\u0080\u0099s try the other way round:",
            "markdown"
        ],
        [
            "import numpy as np\n vect = np.arange(10)\n vect.shape\n(10,)\n sio.savemat('np_vector.mat', {'vect':vect})",
            "code"
        ],
        [
            "Then back to Octave:",
            "markdown"
        ],
        [
            "octave:8 load np_vector.mat\noctave:9 vect\nvect =\n\n  0  1  2  3  4  5  6  7  8  9\n\noctave:10 size(vect)\nans =\n\n    1   10",
            "code"
        ],
        [
            "If you want to inspect the contents of a MATLAB file without reading the\ndata into memory, use the whosmat command:",
            "markdown"
        ],
        [
            "sio.whosmat('octave_a.mat')\n[('a', (1, 3, 4), 'double')]",
            "code"
        ],
        [
            "whosmat returns a list of tuples, one for each array (or other object)\nin the file. Each tuple contains the name, shape and data type of the\narray.",
            "markdown"
        ]
    ],
    "File IO (scipy.io)->MATLAB files->MATLAB structs": [
        [
            "MATLAB structs are a little bit like Python dicts, except the field\nnames must be strings. Any MATLAB object can be a value of a field. As\nfor all objects in MATLAB, structs are, in fact, arrays of structs, where\na single struct is an array of shape (1, 1).",
            "markdown"
        ],
        [
            "octave:11 my_struct = struct('field1', 1, 'field2', 2)\nmy_struct =\n{\n  field1 =  1\n  field2 =  2\n}\n\noctave:12 save -6 octave_struct.mat my_struct",
            "code"
        ],
        [
            "We can load this in Python:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_struct.mat')\n mat_contents\n{'my_struct': array([[([[1.0]], [[2.0]])]],\n      dtype=[('field1', 'O'), ('field2', 'O')]), '__version__': '1.0', '__header__': 'MATLAB 5.0 MAT-file, written by Octave 3.6.3, 2013-02-17 21:23:14 UTC', '__globals__': []}\n oct_struct = mat_contents['my_struct']\n oct_struct.shape\n(1, 1)\n val = oct_struct[0,0]\n val\n([[1.0]], [[2.0]])\n val['field1']\narray([[ 1.]])\n val['field2']\narray([[ 2.]])\n val.dtype\ndtype([('field1', 'O'), ('field2', 'O')])",
            "code"
        ],
        [
            "In the SciPy versions from 0.12.0, MATLAB structs come back as NumPy\nstructured arrays, with fields named for the struct fields. You can see\nthe field names in the dtype output above. Note also:",
            "markdown"
        ],
        [
            "val = oct_struct[0,0]",
            "code"
        ],
        [
            "and:",
            "markdown"
        ],
        [
            "octave:13 size(my_struct)\nans =\n\n   1   1",
            "code"
        ],
        [
            "So, in MATLAB, the struct array must be at least 2-D, and we replicate\nthat when we read into SciPy. If you want all length 1 dimensions\nsqueezed out, try this:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_struct.mat', squeeze_me=True)\n oct_struct = mat_contents['my_struct']\n oct_struct.shape\n()",
            "code"
        ],
        [
            "Sometimes, it\u00e2\u0080\u0099s more convenient to load the MATLAB structs as Python\nobjects rather than NumPy structured arrays - it can make the access\nsyntax in Python a bit more similar to that in MATLAB.  In order to do\nthis, use the struct_as_record=False parameter setting to loadmat.",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_struct.mat', struct_as_record=False)\n oct_struct = mat_contents['my_struct']\n oct_struct[0,0].field1\narray([[ 1.]])",
            "code"
        ],
        [
            "struct_as_record=False works nicely with squeeze_me:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_struct.mat', struct_as_record=False, squeeze_me=True)\n oct_struct = mat_contents['my_struct']\n oct_struct.shape # but no - it's a scalar\nTraceback (most recent call last):\n  File \"&lt;stdin\", line 1, in &lt;module\nAttributeError: 'mat_struct' object has no attribute 'shape'\n type(oct_struct)\n&lt;class 'scipy.io.matlab.mio5_params.mat_struct'\n oct_struct.field1\n1.0",
            "code"
        ],
        [
            "Saving struct arrays can be done in various ways. One simple method is\nto use dicts:",
            "markdown"
        ],
        [
            "a_dict = {'field1': 0.5, 'field2': 'a string'}\n sio.savemat('saved_struct.mat', {'a_dict': a_dict})",
            "code"
        ],
        [
            "loaded as:",
            "markdown"
        ],
        [
            "octave:21 load saved_struct\noctave:22 a_dict\na_dict =\n\n  scalar structure containing the fields:\n\n    field2 = a string\n    field1 =  0.50000",
            "code"
        ],
        [
            "You can also save structs back again to MATLAB (or Octave in our case)\nlike this:",
            "markdown"
        ],
        [
            "dt = [('f1', 'f8'), ('f2', 'S10')]\n arr = np.zeros((2,), dtype=dt)\n arr\narray([(0.0, ''), (0.0, '')],\n      dtype=[('f1', '&lt;f8'), ('f2', 'S10')])\n arr[0]['f1'] = 0.5\n arr[0]['f2'] = 'python'\n arr[1]['f1'] = 99\n arr[1]['f2'] = 'not perl'\n sio.savemat('np_struct_arr.mat', {'arr': arr})",
            "code"
        ]
    ],
    "File IO (scipy.io)->MATLAB files->MATLAB cell arrays": [
        [
            "Cell arrays in MATLAB are rather like Python lists, in the sense that\nthe elements in the arrays can contain any type of MATLAB object. In\nfact, they are most similar to NumPy object arrays, and that is how we\nload them into NumPy.",
            "markdown"
        ],
        [
            "octave:14 my_cells = {1, [2, 3]}\nmy_cells =\n{\n  [1,1] =  1\n  [1,2] =\n\n     2   3\n\n}\n\noctave:15 save -6 octave_cells.mat my_cells",
            "code"
        ],
        [
            "Back to Python:",
            "markdown"
        ],
        [
            "mat_contents = sio.loadmat('octave_cells.mat')\n oct_cells = mat_contents['my_cells']\n print(oct_cells.dtype)\nobject\n val = oct_cells[0,0]\n val\narray([[ 1.]])\n print(val.dtype)\nfloat64",
            "code"
        ],
        [
            "Saving to a MATLAB cell array just involves making a NumPy object array:",
            "markdown"
        ],
        [
            "obj_arr = np.zeros((2,), dtype=np.object)\n obj_arr[0] = 1\n obj_arr[1] = 'a string'\n obj_arr\narray([1, 'a string'], dtype=object)\n sio.savemat('np_cells.mat', {'obj_arr':obj_arr})",
            "code"
        ],
        [
            "octave:16 load np_cells.mat\noctave:17 obj_arr\nobj_arr =\n{\n  [1,1] = 1\n  [2,1] = a string\n}",
            "code"
        ]
    ],
    "File IO (scipy.io)->Netcdf": [
        [
            "Allows reading of  NetCDF files (version of pupynere package)",
            "markdown"
        ]
    ]
}