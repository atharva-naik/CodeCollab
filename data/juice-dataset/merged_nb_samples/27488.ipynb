{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e7059e7",
   "metadata": {},
   "source": [
    "# 0 General functions and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74248972",
   "metadata": {},
   "source": [
    "## 0.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b3f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29600eca",
   "metadata": {},
   "source": [
    "# Camera calibration with chessboard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1503ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs(path):\n",
    "    return [mpimg.imread(e) for e in glob.glob(path)]\n",
    "\n",
    "def show_img_pairs(imgs1, imgs2, title1='', title2='', cmap1=None, cmap2=None):\n",
    "    for img1, img2 in zip(imgs1, imgs2):\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "        f.tight_layout()\n",
    "        \n",
    "        ax1.imshow(img1, cmap=cmap1)\n",
    "        ax1.set_title(title1, fontsize=30)\n",
    "    \n",
    "        ax2.imshow(img2, cmap=cmap2)\n",
    "        ax2.set_title(title2, fontsize=30)\n",
    "        \n",
    "        plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "\n",
    "def get_mapping(chessboard_imgs, chessboard_size):\n",
    "    obj_pts = np.zeros((chessboard_size[0]*chessboard_size[1],3), np.float32)\n",
    "    obj_pts[:,:2] = np.mgrid[0:chessboard_size[0], 0:chessboard_size[1]].T.reshape(-1,2)\n",
    "\n",
    "    objs_pts = []\n",
    "    imgs_pts = []\n",
    "\n",
    "    for idx, img in enumerate(chessboard_imgs):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        ret, corners = cv2.findChessboardCorners(gray, chessboard_size, None)\n",
    "\n",
    "        if ret == True:\n",
    "            objs_pts.append(obj_pts)\n",
    "            imgs_pts.append(corners)\n",
    "            cv2.drawChessboardCorners(img, chessboard_size, corners, ret)\n",
    "            \n",
    "    return objs_pts, imgs_pts\n",
    "\n",
    "chessboard_imgs = load_imgs('camera_cal/calibration*.jpg')\n",
    "chessboard_shape = chessboard_imgs[0].shape[:2]\n",
    "chessboard_size = (9, 6)\n",
    "\n",
    "objs_pts, imgs_pts =  get_mapping(chessboard_imgs, chessboard_size)\n",
    "_, mtx, dist, _, _ = cv2.calibrateCamera(objs_pts, imgs_pts, chessboard_shape, None, None)\n",
    "    \n",
    "def undistort(img):\n",
    "    return cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "undist_imgs = [undistort(img) for img in chessboard_imgs]\n",
    "show_img_pairs(chessboard_imgs, undist_imgs, title1='original', title2='undistroted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f171af",
   "metadata": {},
   "source": [
    "# Undistorting test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d693487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS for image thresholding\n",
    "# Color channel\n",
    "s_thresh_min = 170\n",
    "s_thresh_max = 255\n",
    "\n",
    "# Sobel x\n",
    "sobx_thresh_min = 20\n",
    "sobx_thresh_max = 100\n",
    "sobx_kernel_size = 3\n",
    "\n",
    "# Sobel y\n",
    "soby_thresh_min = 20\n",
    "soby_thresh_max = 100\n",
    "soby_kernel_size = 3\n",
    "\n",
    "# Magnitude gradient\n",
    "mag_thresh_min = 20\n",
    "mag_thresh_max = 100\n",
    "mag_kernel_size = 3\n",
    "\n",
    "# Direction gradient\n",
    "dir_thresh_min = 0.4\n",
    "dir_thresh_max = 0.8\n",
    "dir_kernel_size = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d41251",
   "metadata": {},
   "source": [
    "## 0.2 Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_from_dict(input_dictionary, images_per_line = 5, figw = 15, figh = 15, save = False):\n",
    "    \"\"\"\n",
    "    Plots all images of a dictionary into the jupyter notebook\n",
    "    \n",
    "    Input:\n",
    "    input_dictonary (dict): Input a dictionary with file path as keys and images as values\n",
    "    images_per_line (int): Defines how many images will be displayed per line\n",
    "    figw (int): Defines the width of the overall output figure\n",
    "    figh (int): Defines the hight of the overall output figure\n",
    "    save (bool): Determines whether the images will be saved under the same image path with the filename extension \n",
    "                \"_annotated\" (useful if images were modified)    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Define required number of lines and columns in plot to create subplots\n",
    "    num_images = len(input_dictionary)\n",
    "    images_per_column = int(math.ceil(num_images/images_per_line))\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(images_per_column,images_per_line,figsize = (figw,figh))\n",
    "    \n",
    "    # Remove axis for all subplots\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    # Display all images\n",
    "    for ax, image in zip(axes.flat,sorted(input_dictionary.keys())):\n",
    "        ax.imshow(input_dictionary[image])\n",
    "        ax.set_title(image)\n",
    "        # Save all images if \"save\"-function was activated (to be used if images were modified before)\n",
    "        if save:\n",
    "            img_out_name = \"{}_annotated.png\".format(image[:image.find(\".\")])\n",
    "            plt.imsave(img_out_name,input_dictionary[image].astype(np.uint8))\n",
    "        \n",
    "    # Output plot with all images    \n",
    "    plt.tight_layout()\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_comparison(img_before, img_after, img_name, annotation,cmap_before=None,cmap_after=None):\n",
    "    \"\"\"\n",
    "    Plots a comparison between two images in the jupyter notebook\n",
    "    \n",
    "    Input:\n",
    "    img_before (np.array): Input the initial image before the conversion is applied\n",
    "    img_after (np.array): Input image after the conversion is applied\n",
    "    img_name (string): Name of the image which should be displayed above the image description\n",
    "    annotation (str): Input annotation to the image (conversion method)\n",
    "    \"\"\"\n",
    "    images_per_column = 1\n",
    "    images_per_line = 2\n",
    "    figw = 13\n",
    "    figh = 7\n",
    "    fig, axes = plt.subplots(images_per_column,images_per_line,figsize = (figw,figh))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i == 0:\n",
    "            ax.imshow(img_before,cmap = cmap_before)\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(\"{}\\nImage before {}\".format(img_name, annotation))\n",
    "        if i == 1:\n",
    "            ax.imshow(img_after, cmap = cmap_after)\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(\"{}\\nImage after {}\".format(img_name, annotation))\n",
    "    plt.tight_layout()\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "import string\n",
    "    \n",
    "def save_image_incl_extension(img_after, initial_image_path, img_annotation):\n",
    "    \"\"\"\n",
    "    Saves image after conversion to file\n",
    "    \n",
    "    Input:\n",
    "    img_after (np.array): Image after conversion\n",
    "    initial_image_path (str): Initial path from where the image is sourced\n",
    "    img_annotation (str): Annotation to be added at the end of the initial image path\n",
    "    \"\"\"\n",
    "    img_out_name = \"{}_{}.png\".format(initial_image_path[:initial_image_path.find(\".\")],img_annotation.replace(\" \",\"_\"))\n",
    "    plt.imsave(img_out_name,img_after.astype(np.uint8))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8324a951",
   "metadata": {},
   "source": [
    "# 1 Camera calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff92e9",
   "metadata": {},
   "source": [
    "## 1.1 Load images and fit corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbeb49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "# Define number of chessboard inside corner points\n",
    "chess_corner_x = 9\n",
    "chess_corner_y = 6\n",
    "\n",
    "# Read in and create a list of calibration images\n",
    "calibration_images = glob.glob(\"camera_cal/calibration*.jpg\")\n",
    "\n",
    "# Add container for filenames of images with detected edges and non-detected edges\n",
    "img_corner_det_true = {}\n",
    "img_corner_det_false = {}\n",
    "\n",
    "# Array containers to store object and image points from all images\n",
    "obj_points = [] # 3D points in real world\n",
    "img_points = [] # 2D points in image\n",
    "\n",
    "# Prepare object points like (0,0,0), (1,0,0), (2,0,0), ...., (7,5,0)\n",
    "objp = np.zeros((chess_corner_x * chess_corner_y,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:chess_corner_x,0:chess_corner_y].T.reshape(-1,2) # x, y coordinates  \n",
    "\n",
    "for fname in calibration_images:\n",
    "    # Read in each image\n",
    "    image = mpimg.imread(fname)\n",
    "    \n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Find chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray,(chess_corner_x,chess_corner_y), None)\n",
    "    \n",
    "    # If corner points are found, add object points, image points\n",
    "    if ret == True:\n",
    "        img_points.append(corners)\n",
    "        obj_points.append(objp)\n",
    "        \n",
    "        # Draw and display the corners\n",
    "        image = cv2.drawChessboardCorners(image,(chess_corner_x, chess_corner_y), corners, ret)\n",
    "        \n",
    "        # Append images with corners to dictionary of images\n",
    "        img_corner_det_true[fname] = image        \n",
    "    \n",
    "    else:\n",
    "        # Append images with no corners identified to dictionary of images\n",
    "        img_corner_det_false[fname] = image   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return plot of all images with corners found (and save them to \"..._annotated.png\"-files) \n",
    "# -> To execute second part in parentheses set last function argument to \"True\" \n",
    "# (not activated as computation takes a few seconds) \n",
    "print(\"\\nAll images with identified corners:\")\n",
    "plot_images_from_dict(img_corner_det_true,3,15,20,False)\n",
    "\n",
    "# Return plot of all images for which no corners have been identified (verify whether reason is that not all required corners are on the image)\n",
    "print(\"\\nAll images for which no corners could be identified:\")\n",
    "plot_images_from_dict(img_corner_det_false,3,15,6,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4e19d",
   "metadata": {},
   "source": [
    "## 1.2 Calibrate camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4017a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = load_imgs('test_images/*.jpg')\n",
    "undist_imgs = [undistort(img) for img in test_imgs]\n",
    "show_img_pairs(test_imgs, undist_imgs, title1='original', title2='undistorted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d396c4",
   "metadata": {},
   "source": [
    "# Filters for thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc11f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel_thresh(img, orient='x', ksize=3, thresh=(20, 100)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    dx = 1 if orient == 'x' else 0\n",
    "    dy = 1 if orient == 'y' else 0\n",
    "    sobel = cv2.Sobel(gray, cv2.CV_64F, dx, dy, ksize=ksize)\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    scaled = np.uint8(255 * abs_sobel / np.max(abs_sobel))\n",
    "    mask = np.zeros_like(scaled)\n",
    "    mask[(thresh[0] <= scaled) & (scaled <= thresh[1])] = 1\n",
    "    return mask\n",
    "\n",
    "def mag_thresh(img, ksize=5, thresh=(40, 100)):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=ksize)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=ksize)\n",
    "    mag = np.sqrt(np.square(sobelx) + np.square(sobely))\n",
    "    scaled = np.uint8(255 * mag / np.max(mag))\n",
    "    mask = np.zeros_like(scaled)\n",
    "    mask[(thresh[0] <= scaled) & (scaled <= thresh[1])] = 1\n",
    "    return mask\n",
    "\n",
    "def direct_thresh(rgb, ksize=5, thresh=(0.7, 1.3)):\n",
    "    gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=ksize)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=ksize)\n",
    "    absx = np.abs(sobelx)\n",
    "    absy = np.abs(sobely)\n",
    "    direct = np.arctan2(absy, absx)\n",
    "    mask = np.zeros_like(direct)\n",
    "    mask[(thresh[0] <= direct) & (direct <= thresh[1])] = 1\n",
    "    return mask\n",
    "\n",
    "def hls_thresh(img, channel=2, thresh=(160, 255)):\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    s_ch = hls[:,:,channel]\n",
    "    mask = np.zeros_like(s_ch)\n",
    "    mask[(s_ch >= thresh[0]) & (s_ch <= thresh[1])] = 1\n",
    "    return mask\n",
    "\n",
    "def comb_thresh(img):\n",
    "    sobelx = sobel_thresh(img, 'x', ksize=9, thresh=(20, 100))\n",
    "    s = hls_thresh(img, channel=2, thresh=(160, 255))\n",
    "    direct = direct_thresh(img, ksize=15, thresh=(0.7, 1.3))\n",
    "    combined = np.zeros_like(sobelx)\n",
    "    combined[((sobelx == 1) | (s == 1)) & (direct == 1)] = 1\n",
    "    return combined\n",
    "\n",
    "binary_imgs = [comb_thresh(e) for e in undist_imgs]\n",
    "show_img_pairs(undist_imgs, binary_imgs, cmap2='gray', title1='undistorted', title2='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65950936",
   "metadata": {},
   "source": [
    "# Perspective transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "straight_img = undist_imgs[4]\n",
    "\n",
    "src_pts = np.float32([\n",
    "    [275, 670],\n",
    "    [1028, 670],\n",
    "    [596, 450],\n",
    "    [685, 450]\n",
    "])\n",
    "\n",
    "height, width = straight_img.shape[:2]\n",
    "offset = 250\n",
    "\n",
    "dst_pts = np.float32([\n",
    "    [offset, height - 5],\n",
    "    [width - offset, height - 5],\n",
    "    [offset, 5],\n",
    "    [width - offset, 5]\n",
    "])\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n",
    "\n",
    "def warp(img):\n",
    "    return cv2.warpPerspective(img, M, (img.shape[1], img.shape[0]), flags=cv2.INTER_LINEAR)\n",
    "\n",
    "M_inv = np.linalg.inv(M)\n",
    "\n",
    "def unwarp(img):\n",
    "    height, width = img.shape[:2]\n",
    "    return cv2.warpPerspective(img, M_inv, (width, height))\n",
    "\n",
    "warped_img = warp(straight_img)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 9))\n",
    "fig.tight_layout()\n",
    "\n",
    "ax1.imshow(straight_img)\n",
    "ax1.plot(src_pts[:,0], src_pts[:, 1], 'ro', markersize=10)\n",
    "ax1.set_title('source points', fontsize=30)   \n",
    "\n",
    "ax2.imshow(warped_img)\n",
    "ax2.plot(dst_pts[:,0], dst_pts[:, 1], 'ro', markersize=10)\n",
    "ax2.set_title('destination points', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1359fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_imgs = [warp(img) for img in binary_imgs]\n",
    "show_img_pairs(binary_imgs, warped_imgs, cmap1='gray', cmap2='gray', title1='binary', title2='transformed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f9808",
   "metadata": {},
   "source": [
    "# Pipeline for processing test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbed154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_x(prev_x, offset, margin, convolved_layer):\n",
    "    min_index = max(prev_x + offset - margin, 0)\n",
    "    max_index = min(prev_x + offset + margin, width)\n",
    "    convolved = convolved_layer[min_index:max_index]\n",
    "    \n",
    "    if sum(convolved) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return np.argmax(convolved) + min_index - offset\n",
    "\n",
    "def get_level_y(level, window_height, height):\n",
    "    return height - (level * window_height + int(window_height / 2))\n",
    "\n",
    "def find_points(binary_img, window_size, window_margin):\n",
    "    left_pts = []\n",
    "    right_pts = []\n",
    "    \n",
    "    heigth, width = binary_img.shape\n",
    "    window_width, window_height = window_size\n",
    "    kernel = np.ones(window_width)\n",
    "    \n",
    "    y = get_level_y(0, window_height, height)\n",
    "    \n",
    "    left_sum = np.sum(binary_img[int(3 / 4 * height):, :int(width / 2)], axis=0)\n",
    "    left_x = np.argmax(np.convolve(kernel, left_sum)) - int(window_width / 2)\n",
    "    left_pts.append((left_x, y))\n",
    "    \n",
    "    right_sum = np.sum(binary_img[int(3 / 4 * height):, int(width / 2):], axis=0)\n",
    "    right_x = np.argmax(np.convolve(kernel, right_sum)) - int(window_width / 2) + int(width / 2)\n",
    "    right_pts.append((right_x, y))\n",
    "    \n",
    "    num_levels = (int)(height / window_height)\n",
    "        \n",
    "    for level in range(1, num_levels):\n",
    "        y = get_level_y(level, window_height, height)\n",
    "        layer = np.sum(binary_img[height - (level + 1) * window_height:height - level * window_height,:], axis=0)\n",
    "        convolved = np.convolve(kernel, layer)\n",
    "        offset = int(window_width / 2)\n",
    "    \n",
    "        next_left_x = find_next_x(left_x, offset, window_margin, convolved)\n",
    "        \n",
    "        if next_left_x is not None:\n",
    "            left_x = next_left_x\n",
    "            left_pts.append((left_x, y))\n",
    "        \n",
    "        next_right_x = find_next_x(right_x, offset, window_margin, convolved)\n",
    "        \n",
    "        if next_right_x is not None:\n",
    "            right_x = next_right_x\n",
    "            right_pts.append((right_x, y))\n",
    "    \n",
    "    return (np.array(left_pts), np.array(right_pts))\n",
    "\n",
    "def draw_window(img_size, pt, window_size):\n",
    "    height, width = img_size\n",
    "    window_width, window_height = window_size\n",
    "    \n",
    "    x, y = pt\n",
    "\n",
    "    x_min = max(int(x-window_width/2), 0)\n",
    "    x_max = min(int(x+window_width/2), width)\n",
    "    \n",
    "    y_min = max(int(y-window_height/2), 0)\n",
    "    y_max = min(int(y+window_height/2), height)\n",
    "\n",
    "    output_img = np.zeros(img_size)\n",
    "    output_img[y_min:y_max, x_min:x_max] = 1\n",
    "    return output_img\n",
    "\n",
    "def fit_lane(points):\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "def draw_windows(binary_img, left_pts, right_pts, window_size):\n",
    "    img_size = binary_img.shape\n",
    "    overlay_img = np.zeros(img_size)\n",
    "    \n",
    "    for pt in left_pts:\n",
    "        window_img = draw_window(img_size, pt, window_size)\n",
    "        overlay_img[(window_img == 1) | (overlay_img == 1)] = 1\n",
    "    \n",
    "    for pt in right_pts:\n",
    "        window_img = draw_window(img_size, pt, window_size)\n",
    "        overlay_img[(window_img == 1) | (overlay_img == 1)] = 1\n",
    "    \n",
    "    overlay_img = np.array(overlay_img, np.uint8) * 255\n",
    "    empty_img = np.zeros_like(overlay_img)\n",
    "    overlay_img = np.array(cv2.merge((empty_img, overlay_img, empty_img)), np.uint8)\n",
    "    color_img = np.dstack((binary_img, binary_img, binary_img))*128\n",
    "    return cv2.addWeighted(color_img, 1, overlay_img, 0.5, 0.0)\n",
    "    \n",
    "def draw_fit_overlay(binary, left_points, right_points):\n",
    "    (height, width) = binary.shape\n",
    "    left_fit = np.polyfit(left_points[:,1], left_points[:,0], 2)\n",
    "    right_fit = np.polyfit(right_points[:,1], right_points[:,0], 2)\n",
    "    \n",
    "    plot_y = np.linspace(0, height - 1, height)\n",
    "    left_fit_x = left_fit[0]*plot_y**2+left_fit[1]*plot_y+left_fit[2]\n",
    "    right_fit_x = right_fit[0]*plot_y**2+right_fit[1]*plot_y+right_fit[2]\n",
    "    \n",
    "    overlay = np.zeros_like(binary).astype(np.uint8)\n",
    "    overlay = np.dstack((overlay, overlay, overlay))\n",
    "\n",
    "    left_plot_points = np.array([np.transpose(np.vstack([left_fit_x, plot_y]))])\n",
    "    right_plot_points = np.array([np.flipud(np.transpose(np.vstack([right_fit_x, plot_y])))])\n",
    "    plot_points = np.hstack((left_plot_points, right_plot_points))\n",
    "\n",
    "    cv2.fillPoly(overlay, np.int_([plot_points]), (0, 255, 0))\n",
    "    return overlay\n",
    "\n",
    "def draw_fit(binary, left_points, right_points):\n",
    "    overlay = draw_fit_overlay(binary, left_points, right_points)\n",
    "    color = np.dstack((binary, binary, binary))*255\n",
    "    return cv2.addWeighted(color, 1, overlay, 0.5, 0.0)\n",
    "\n",
    "def get_curvature(points, m_per_pix):\n",
    "    x = points[:,0]*m_per_pix[0]\n",
    "    y = points[:,1]*m_per_pix[1]\n",
    "    y_eval = np.max(y)\n",
    "    fit = np.polyfit(y, x, 2)\n",
    "    return ((1+(2*fit[0]*y_eval+fit[1])**2)**1.5)/np.absolute(2*fit[0])\n",
    "\n",
    "win_size = (50, 80)\n",
    "win_margin = 100\n",
    "m_per_pix= (3.7/700, 30.0/720)\n",
    "\n",
    "window_imgs = []\n",
    "fit_imgs = []\n",
    "poly_imgs = []\n",
    "unwarped_imgs = []\n",
    "final_imgs = []\n",
    "\n",
    "(height, width) = test_imgs[0].shape[:-1]\n",
    "img_size = (width, height)    \n",
    "\n",
    "for i in range(len(test_imgs)):\n",
    "    binary = warped_imgs[i]\n",
    "    left_pts, right_pts = find_points(binary, win_size, win_margin)\n",
    "    left_curvature = get_curvature(left_pts, m_per_pix)\n",
    "    right_curvature = get_curvature(right_pts, m_per_pix)\n",
    "    \n",
    "    window_imgs.append(draw_windows(binary, left_pts, right_pts, win_size))\n",
    "    fit_imgs.append(draw_fit(binary, left_pts, right_pts))\n",
    "    \n",
    "    overlay_img = draw_fit_overlay(binary, left_pts, right_pts)\n",
    "    poly_imgs.append(cv2.addWeighted(np.dstack((binary * 128, binary * 128, binary * 128)), 1, overlay_img, 0.5, 0.0))\n",
    "    \n",
    "    unwarped_img = unwarp(overlay_img)\n",
    "    unwarped_imgs.append(unwarped_img)\n",
    "    \n",
    "    final_img = cv2.addWeighted(test_imgs[i], 1, unwarped_img, 0.5, 0.0)\n",
    "    final_imgs.append(final_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b992ce4",
   "metadata": {},
   "source": [
    "## Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee13ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img_pairs(warped_imgs, window_imgs, cmap1='gray', title1='transformed', title2='windows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aebabb7",
   "metadata": {},
   "source": [
    "## Lane area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_s_thresh = {}\n",
    "annotation = \"S-binary-threshold\"\n",
    "for key in sorted(test_images_s_channel.keys()):\n",
    "    test_images_s_thresh[key] = S_to_thresh(test_images_s_channel[key],s_thresh_min,s_thresh_max)\n",
    "    plot_image_comparison(test_images_s_channel[key],test_images_s_thresh[key],key, annotation,cmap_before=\"gray\",\n",
    "                          cmap_after=\"gray\")\n",
    "plt.imsave(\"output_images/06_s_binary.png\",test_images_s_thresh[\"test_images/test2.jpg\"],cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12988a8c",
   "metadata": {},
   "source": [
    "### 2.2.2 Create Sobel based binary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sobel threshold functions\n",
    "def abs_sobel_thresh(image, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Calculate directional gradient\n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "    if orient == \"x\":\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize = sobel_kernel))\n",
    "    if orient == \"y\":\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize = sobel_kernel))\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    # Apply threshold\n",
    "    grad_binary = np.zeros_like(scaled_sobel)\n",
    "    grad_binary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    return grad_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_abssob_x = {}\n",
    "annotation = \"Sobelx\"\n",
    "for key in sorted(test_images_dst.keys()):\n",
    "    test_images_abssob_x[key] = abs_sobel_thresh(test_images_dst[key], orient='x', sobel_kernel= sobx_kernel_size, \n",
    "                                                 thresh=(sobx_thresh_min, sobx_thresh_max))    \n",
    "    plot_image_comparison(test_images_dst[key],test_images_abssob_x[key],key, annotation,cmap_before=None,\n",
    "                          cmap_after=\"gray\")\n",
    "plt.imsave(\"output_images/07_abssob_x.png\",test_images_abssob_x[\"test_images/test2.jpg\"],cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d04a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_abssob_y = {}\n",
    "annotation = \"Sobely\"\n",
    "for key in sorted(test_images_dst.keys()):\n",
    "    test_images_abssob_y[key] = abs_sobel_thresh(test_images_dst[key], orient='y', sobel_kernel= soby_kernel_size, \n",
    "                                                 thresh=(soby_thresh_min, soby_thresh_max))    \n",
    "    plot_image_comparison(test_images_dst[key],test_images_abssob_y[key],key, annotation,cmap_before=None,\n",
    "                          cmap_after=\"gray\")\n",
    "plt.imsave(\"output_images/08_abssob_y.png\",test_images_abssob_y[\"test_images/test2.jpg\"],cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07660b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mag_thresh(image, sobel_kernel=3, mag_thresh=(0, 255)):\n",
    "    # Calculate gradient magnitude\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F,1,0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F,0,1, ksize = sobel_kernel)\n",
    "    gradmag = np.sqrt(sobelx**2,sobely**2)\n",
    "    scale_factor = np.max(gradmag)/255\n",
    "    gradmag = (gradmag/scale_factor).astype(np.uint8)\n",
    "    # Apply threshold\n",
    "    mag_binary = np.zeros_like(gradmag)\n",
    "    mag_binary[(gradmag >= mag_thresh[0])&(gradmag <= mag_thresh[1])] = 1    \n",
    "    return mag_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d495e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_mag_grad = {}\n",
    "annotation = \"magnitude gradient\"\n",
    "for key in sorted(test_images_dst.keys()):\n",
    "    test_images_mag_grad[key] = mag_thresh(test_images_dst[key], sobel_kernel= mag_kernel_size, \n",
    "                                           mag_thresh=(mag_thresh_min, mag_thresh_max))    \n",
    "    plot_image_comparison(test_images_dst[key],test_images_mag_grad[key],key, annotation,cmap_before=None,\n",
    "                          cmap_after=\"gray\")\n",
    "plt.imsave(\"output_images/09_mag_grad.png\",test_images_mag_grad[\"test_images/test2.jpg\"],cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_threshold(image, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Calculate gradient direction\n",
    "    gray = cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F,1,0, ksize = sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F,0,1, ksize = sobel_kernel)\n",
    "    absgraddir = np.arctan2(np.absolute(sobelx),np.absolute(sobely))\n",
    "    # Apply threshold\n",
    "    dir_binary = np.zeros_like(absgraddir)\n",
    "    dir_binary[(absgraddir >= thresh[0]) & (absgraddir <= thresh[1])] = 1\n",
    "    return dir_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f75e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_dir_grad = {}\n",
    "annotation = \"direction gradient\"\n",
    "for key in sorted(test_images_dst.keys()):\n",
    "    test_images_dir_grad[key] = dir_threshold(test_images_dst[key], sobel_kernel=dir_kernel_size, \n",
    "                                              thresh=(dir_thresh_min, dir_thresh_max))    \n",
    "    plot_image_comparison(test_images_dst[key],test_images_dir_grad[key],key, annotation,cmap_before=None,\n",
    "                          cmap_after=\"gray\")\n",
    "plt.imsave(\"output_images/10_dir_grad.png\",test_images_dir_grad[\"test_images/test2.jpg\"],cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9145a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sob_to_sobcomb(img_gradx, img_grady, img_mag_grad, img_dir_grad):\n",
    "    assert img_gradx.shape == img_grady.shape == img_mag_grad.shape == img_dir_grad.shape, \"not all input images have the same shape\"\n",
    "    combined = np.zeros_like(img_gradx)\n",
    "    combined[((img_gradx == 1) & (img_grady == 1)) | ((img_mag_grad == 1) & (img_dir_grad == 1))] = 1\n",
    "    return combined  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9024807",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_sobcomb = {}\n",
    "annotation = \"combined sobel\"\n",
    "for key in sorted(test_images_dst.keys()):\n",
    "    test_images_sobcomb[key] = sob_to_sobcomb(test_images_abssob_x[key],test_images_abssob_y[key],\n",
    "                                              test_images_mag_grad[key],test_images_dir_grad[key])\n",
    "    plot_image_comparison(test_images_dst[key],test_images_sobcomb[key],key, annotation,cmap_before=None,\n",
    "                          cmap_after=\"gray\")\n",
    "plt.imsave(\"output_images/11_sobcomb.png\",test_images_sobcomb[\"test_images/test2.jpg\"],cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2e605",
   "metadata": {},
   "source": [
    "### 2.2.3 Create overall combined binary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img_pairs(warped_imgs, poly_imgs, cmap1='gray', title1='transformed', title2='lane polygon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad7a15f",
   "metadata": {},
   "source": [
    "# Video pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe796f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_curvature(fit, y):\n",
    "    return ((1+(2*fit[0]*y+fit[1])**2)**1.5)/np.absolute(2*fit[0])\n",
    "\n",
    "def eval_x(fit, y):\n",
    "    return fit[0]*y**2+fit[1]*y+fit[2]\n",
    "\n",
    "def draw_lanes_poly(shape, left_fit, right_fit):\n",
    "    (height, width) = shape\n",
    "    \n",
    "    y = np.linspace(0, height - 1, height)\n",
    "    left_x = eval_x(left_fit, y)\n",
    "    right_x = eval_x(right_fit, y)\n",
    "    \n",
    "    output = np.zeros((height, width, 3)).astype(np.uint8)\n",
    "\n",
    "    left_points = np.array([np.transpose(np.vstack([left_x, y]))])\n",
    "    right_points = np.array([np.flipud(np.transpose(np.vstack([right_x, y])))])\n",
    "    plot_points = np.hstack((left_points, right_points))\n",
    "\n",
    "    cv2.fillPoly(output, np.int_([plot_points]), (0, 255, 0))\n",
    "    return output\n",
    "\n",
    "def draw_text(img, text, position):\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    font_scale = 2\n",
    "    font_color = (0, 255, 0)\n",
    "    line_type = 2\n",
    "    cv2.putText(img, text, position, font, font_scale, font_color, line_type)\n",
    "\n",
    "def find_close_points(binary_img, margin, left_fit, right_fit):\n",
    "    nonzero = binary_img.nonzero()\n",
    "    nonzero_x = np.array(nonzero[1])\n",
    "    nonzero_y = np.array(nonzero[0])\n",
    "    \n",
    "    left_eval_x = eval_x(left_fit, nonzero_y)\n",
    "    left_lane_inds = ((left_eval_x - margin) < nonzero_x) & (nonzero_x < (left_eval_x + margin))\n",
    "\n",
    "    right_eval_x = eval_x(right_fit, nonzero_y)\n",
    "    right_lane_inds = ((right_eval_x - margin) < nonzero_x) & (nonzero_x < (right_eval_x + margin))\n",
    "\n",
    "    left_pts = np.stack([nonzero_x[left_lane_inds], nonzero_y[left_lane_inds]], axis=1)\n",
    "    right_pts = np.stack([nonzero_x[right_lane_inds], nonzero_y[right_lane_inds]], axis=1)\n",
    "    return left_pts, right_pts\n",
    "\n",
    "class Lane():\n",
    "    def __init__(self, img_size, m_per_px, last_n):\n",
    "        self.last_n = last_n\n",
    "        self.m_per_px = m_per_px\n",
    "        self.img_size = img_size \n",
    "        \n",
    "        self.fit = None        \n",
    "        self.curvature = None\n",
    "        self.offset = None\n",
    "        self.slope = None\n",
    "\n",
    "        self.last_fits = []\n",
    "\n",
    "    def update(self, pts):\n",
    "        if len(pts) < 2:\n",
    "            self.fit = None\n",
    "            return\n",
    "\n",
    "        self.fit = np.polyfit(pts[:,1], pts[:,0], 2)\n",
    "        \n",
    "        pts_m = pts*self.m_per_px\n",
    "        fit_m = np.polyfit(pts_m[:,1], pts_m[:,0], 2)\n",
    "        max_y = self.img_size[1]\n",
    "        max_y_m = max_y*self.m_per_px[1]\n",
    "        self.curvature = get_curvature(fit_m, max_y_m)\n",
    "        \n",
    "        mid_x = self.img_size[0] / 2\n",
    "        eval_max_x = eval_x(self.fit, max_y)\n",
    "        self.offset = abs(eval_max_x - mid_x)*self.m_per_px[0]\n",
    "        \n",
    "        eval_min_x = eval_x(self.fit, 0)\n",
    "        self.slope = (eval_max_x - eval_min_x) / max_y\n",
    "\n",
    "    def accept(self):\n",
    "        self.last_fits.insert(0, self.fit)\n",
    "        self.last_fits = self.last_fits[:self.last_n]\n",
    "        \n",
    "    def reject(self):\n",
    "        self.fit = None\n",
    "            \n",
    "    def smooth_fit(self):\n",
    "        if len(self.last_fits):\n",
    "            return np.mean(self.last_fits, axis=0)\n",
    "\n",
    "class LaneFinder():\n",
    "    def __init__(self, img_size, win_size, win_margin, m_per_px, n_last):\n",
    "        self.img_size = img_size\n",
    "        self.win_size = win_size\n",
    "        self.win_margin = win_margin\n",
    "        self.m_per_px = m_per_px\n",
    "        self.n_last = n_last\n",
    "        \n",
    "        self.left_lane = Lane(img_size, m_per_px, n_last)\n",
    "        self.right_lane = Lane(img_size, m_per_px, n_last)\n",
    "        \n",
    "\n",
    "    def process(self, img):\n",
    "        output_img = np.copy(img)\n",
    "        \n",
    "        height, width = img.shape[:2]\n",
    "        undist_img = undistort(img)\n",
    "        binary_img = comb_thresh(undist_img)\n",
    "        warped_img = warp(binary_img)\n",
    "        \n",
    "        if self.left_lane.fit is None or self.right_lane.fit is None:\n",
    "            left_pts, right_pts = find_points(warped_img, self.win_size, self.win_margin)\n",
    "        else: \n",
    "            left_pts, right_pts = find_close_points(warped_img, self.win_margin, self.left_lane.fit, self.right_lane.fit)\n",
    "        \n",
    "        self.left_lane.update(left_pts)\n",
    "        self.right_lane.update(right_pts)\n",
    "        \n",
    "        if self.left_lane.fit is not None and self.right_lane.fit is not None:\n",
    "            curvature_diff = abs(self.left_lane.curvature - self.right_lane.curvature)\n",
    "            curvature_check = self.left_lane.curvature > 800 or self.right_lane.curvature > 800 or curvature_diff < 500\n",
    "            \n",
    "            separation = self.left_lane.offset + self.right_lane.offset\n",
    "            separation_check = 2.6 < separation < 6.4\n",
    "            \n",
    "            slope_diff = abs(self.left_lane.slope - self.right_lane.slope)\n",
    "            slope_check = slope_diff < 0.2\n",
    "            \n",
    "            offset = abs(self.left_lane.offset - self.right_lane.offset)\n",
    "            \n",
    "            curvature_text = 'Curvature left {:.0f}, right {:.0f}'.format(\n",
    "                self.left_lane.curvature, self.right_lane.curvature)\n",
    "            draw_text(output_img, curvature_text, (50, 50))\n",
    "            \n",
    "            offset_text = 'Offset left {:.3f}, right {:.3f}, car {:.3f}'.format(\n",
    "                self.left_lane.offset, self.right_lane.offset, offset)\n",
    "            draw_text(output_img, offset_text, (50, 100))\n",
    "            \n",
    "            slope_text = 'Slope left {:.3f}, right {:.3f}'.format(\n",
    "                self.left_lane.slope, self.right_lane.slope)\n",
    "            draw_text(output_img, slope_text, (50, 150))\n",
    "\n",
    "            diff_text = 'Diff curvature {:.3f}, separation {:.3f}, slope {:.3f}'.format(\n",
    "                curvature_diff, separation, slope_diff)\n",
    "            draw_text(output_img, diff_text, (50, 200))\n",
    "            \n",
    "            check_text = 'Check curvature {}, separation {}, slope {}'.format(curvature_check, separation_check, slope_check)\n",
    "            draw_text(output_img, check_text, (50, 250))\n",
    "        \n",
    "            if curvature_check and separation_check and slope_check:\n",
    "                self.left_lane.accept()\n",
    "                self.right_lane.accept()\n",
    "            else:\n",
    "                self.left_lane.reject()\n",
    "                self.right_lane.reject()\n",
    "\n",
    "        left_fit = self.left_lane.smooth_fit()\n",
    "        right_fit = self.right_lane.smooth_fit()\n",
    "        \n",
    "        if left_fit is not None and right_fit is not None:\n",
    "            poly_img = draw_lanes_poly(warped_img.shape, left_fit, right_fit)\n",
    "            unwarped_img = unwarp(poly_img)\n",
    "            output_img = cv2.addWeighted(output_img, 1, unwarped_img, 0.5, 0.0)\n",
    "        \n",
    "        return output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab59ce9",
   "metadata": {},
   "source": [
    "## Video pipeline can be also used to process images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26955da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    lane_finder = LaneFinder(img_size, win_size, win_margin, m_per_pix, 1)\n",
    "    return lane_finder.process(img)\n",
    "\n",
    "proc_imgs = []\n",
    "\n",
    "for img in test_imgs:\n",
    "    proc_imgs.append(process_image(img))\n",
    "    \n",
    "show_img_pairs(test_imgs, proc_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e462d",
   "metadata": {},
   "source": [
    "## Utility functions for video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c060b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(input_path, output_path):\n",
    "    lane_finder = LaneFinder(img_size, win_size, win_margin, m_per_pix, 5)\n",
    "    test_video = VideoFileClip(input_path)\n",
    "    output_video = test_video.fl_image(lane_finder.process)\n",
    "    %time output_video.write_videofile(output_path, audio=False)\n",
    "    \n",
    "def embed_video(path):\n",
    "    return HTML(\"\"\"\n",
    "        <video width=\"960\" height=\"540\" controls>\n",
    "          <source src=\"{0}\">\n",
    "        </video>\n",
    "    \"\"\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb554f17",
   "metadata": {},
   "source": [
    "## Processing of test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41571284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate camera\n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(obj_points,img_points, gray.shape[::-1],None,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ed0b38",
   "metadata": {},
   "source": [
    "## 1.3 Perform distortion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f527b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for distortion correction on single image\n",
    "def distortion_correction(img,mtx,dist):\n",
    "    return cv2.undistort(img,mtx,dist,None,mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240569d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit an undistorted test image of the chessboard\n",
    "test_distorted = img_corner_det_false[\"camera_cal/calibration01.jpg\"]\n",
    "test_undistorted = distortion_correction(test_distorted,mtx,dist)\n",
    "plot_image_comparison(test_distorted, test_undistorted, \"camera_cal/calibration01.jpg\",\"distortion correction\",cmap_before=None,cmap_after=None)\n",
    "plt.imsave(\"output_images/01_chess_dist.png\",test_distorted)\n",
    "plt.imsave(\"output_images/02_chess_dist.png\",test_undistorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1405fa0",
   "metadata": {},
   "source": [
    "# 2 Pipeline (test images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f0d05",
   "metadata": {},
   "source": [
    "## 2.0 Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "img_paths = glob.glob(\"test_images/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b9e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_path = \"test_images/test1.jpg\"\n",
    "annotation = \"distortion correction\"\n",
    "test_images = {}\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    test_images[img_path] = mpimg.imread(img_path)\n",
    "plt.imsave(\"output_images/03_original.png\",test_images[\"test_images/test2.jpg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b42d5f",
   "metadata": {},
   "source": [
    "## 2.1 Distortion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60e2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_images_dst[\"test_images/straight_lines1.jpg\"])\n",
    "plt.axis(\"off\")\n",
    "plt.plot(255,688,\".\")\n",
    "plt.plot(1051,688,\".\")\n",
    "plt.plot(595,452,\".\")\n",
    "plt.plot(686,452,\".\")\n",
    "plt.savefig(\"output_images/13_src_points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f92912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four source coordinates\n",
    "src = np.float32(\n",
    "    [[255,688],\n",
    "     [1051,688],\n",
    "     [595,452],\n",
    "     [686,452]])\n",
    "\n",
    "# Four desired coordinates\n",
    "dst = np.float32(\n",
    "    [[360,720],\n",
    "     [946,720],\n",
    "     [360,0],\n",
    "     [946,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp(img,src,dst):\n",
    "    \n",
    "    # Define calibration box in source (origingal) and destination (desired or warped) coordinates\n",
    "    img_size = (img.shape[1],img.shape[0])    \n",
    "    \n",
    "    # Compute the perspective transform, M\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    \n",
    "    # Create warped image - uses linear interpolation\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3fd723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that example image is warped correctly\n",
    "example_warped = warp(test_images_dst[\"test_images/straight_lines1.jpg\"],src,dst)\n",
    "plt.imshow(example_warped)\n",
    "plt.axis(\"off\")\n",
    "plt.plot(360,720,\".\")\n",
    "plt.plot(944,720,\".\")\n",
    "plt.plot(360,0,\".\")\n",
    "plt.plot(946,0,\".\")\n",
    "plt.savefig(\"output_images/14_dst_points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d663198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warp test images\n",
    "test_images_warped = {}\n",
    "for key in sorted(test_images_dst.keys()):\n",
    "    test_images_warped[key] = warp(test_images_dst[key],src,dst)    \n",
    "    plot_image_comparison(test_images_dst[key],test_images_warped[key],key, annotation,cmap_before=None,cmap_after=None)\n",
    "plt.imsave(\"output_images/15_warped.png\",test_images_warped[\"test_images/test2.jpg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc28ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warp combined binary\n",
    "test_images_warped_cb = {}\n",
    "for key in sorted(test_images_dst.keys()):\n",
    "    test_images_warped_cb[key] = warp(combined_binary[key],src,dst)    \n",
    "    plot_image_comparison(test_images_warped[key],test_images_warped_cb[key],key, annotation,cmap_before=None,cmap_after=\"gray\")\n",
    "plt.imsave(\"output_images/16_warped_binary.png\", test_images_warped_cb[\"test_images/test2.jpg\"],cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0124cc01",
   "metadata": {},
   "source": [
    "## 2.4 Identify lane-line pixels and fit with polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for lanes if no line fitted so far\n",
    "\n",
    "def identify_lane_line_first(img):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    img (np.array): The warped input image\n",
    "    Output:\n",
    "    out_img (np.array): The output image containing the fitted windows in green,\n",
    "                        the binary points contributing to the left line regression in red,\n",
    "                        the binary points contributing to the right line regression in blue and \n",
    "                        the rest of the binary points in white\n",
    "    left_fit (np.array): Vector of coefficients for fitted second degree polynomial for left line\n",
    "    right_fit (np.array): Vector of coefficients for fitted second degree polynomial for right line\n",
    "    ploty (np.array): A numpy array with the range of x-pixels as values\n",
    "    left_fitx (np.array): Fitted 2nd degree polynomial points for all \"ploty-values\" for left line\n",
    "    right_fitx (np.array): Fitted 2nd degree polynomial points for all \"ploty-values\" for right line\n",
    "    left and right line position of pixels being attributed to respective line:\n",
    "        lefty, righty, leftx, rightx\n",
    "    leftx_dir_marker (bool): Determines curve direction of left lane - if True -> right turn, \n",
    "                        if Fales -> left turn\n",
    "    rightx_dir_marker (bool): Determines curve direction of right lane - if True -> right turn, \n",
    "                        if Fales -> left turn\n",
    "    \"\"\"\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(img[img.shape[0]/2:,:], axis=0)\n",
    "    #plt.plot(histogram)\n",
    "    \n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((img, img, img))*255\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "    \n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 8\n",
    "    \n",
    "    # Set height of windows\n",
    "    window_height = np.int(img.shape[0]/nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 80\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 60\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    # Create dictionary to collect window positions (already hast starting value in it)\n",
    "    leftx_rep = {0:leftx_base}\n",
    "    rightx_rep = {0:rightx_base}\n",
    "    \n",
    "    # Step through the windows one by one (first round) to identify correct window position\n",
    "    # by taking previous image position as starting point and resentering the window position via\n",
    "    # taking the mean of binaries in the initial window\n",
    "    for window in range(nwindows):\n",
    "        \n",
    "        # Identify window boundaries in y direction\n",
    "        win_y_low = img.shape[0] - (window+1) * window_height\n",
    "        win_y_high = img.shape[0] - window * window_height\n",
    "       \n",
    "        # Identify window boundaries in x direction for both left and right and left lanes and \n",
    "        # left and right window side\n",
    "        # Check whether current window position is available and use it (the case for first window \n",
    "        # as position was already defined in the dictionaries leftx_rep and rightx_rep)\n",
    "        try:\n",
    "            win_xleft_low = leftx_rep[window] - margin\n",
    "            win_xleft_high = leftx_rep[window] + margin\n",
    "            win_xright_low = rightx_rep[window] - margin\n",
    "            win_xright_high = rightx_rep[window] + margin\n",
    "        \n",
    "        # Take the previous window as starting point (for all windows except the first one)\n",
    "        except:\n",
    "            win_xleft_low = leftx_rep[window-1] - margin\n",
    "            win_xleft_high = leftx_rep[window-1] + margin\n",
    "            win_xright_low = rightx_rep[window-1] - margin\n",
    "            win_xright_high = rightx_rep[window-1] + margin\n",
    "            \n",
    "\n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the currently identified window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                          (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                           (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        # If > minpix pixels found, position the current left window on their mean \n",
    "        # position in x direction\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_rep[window] = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        # Else position the current left window on the previous window's position in x direction\n",
    "        else:\n",
    "            if (window != 0):\n",
    "                leftx_rep[window] = leftx_rep[window-1]\n",
    "        # If > minpix pixels found, position the current right window on their mean position \n",
    "        # in x direction    \n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_rep[window] = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "        # Else position the current right window on the previous window's position in x direction\n",
    "        else:\n",
    "            if (window != 0):\n",
    "               rightx_rep[window] = rightx_rep[window-1]\n",
    "            \n",
    "    # Reduce the mean of all window positions for left and right lane\n",
    "    # from the mean of the starting window to determine the general direction of the turn\n",
    "    # if negative -> right turn, if positive left turn\n",
    "    \n",
    "    leftx_dir = leftx_rep[0] - np.mean(list(leftx_rep.values()))\n",
    "    rightx_dir = rightx_rep[0] - np.mean(list(rightx_rep.values()))\n",
    "    \n",
    "    # Create direction marker with right turn = True and left turn = False for both left and right line\n",
    "    if leftx_dir < 0:\n",
    "        leftx_dir_marker = True\n",
    "    else:\n",
    "        leftx_dir_marker = False\n",
    "    \n",
    "    if rightx_dir < 0:\n",
    "        rightx_dir_marker = True\n",
    "    else:\n",
    "        rightx_dir_marker = False\n",
    "    \"\"\"\n",
    "    # Not required anymore as lanes with different directions are taken care of in the final pipeline\n",
    "    # If the direction markers for left and right line are pointing in different directions,\n",
    "    # ensure that the direction of the line with a stronger turn will be used for both left and right \n",
    "    # direction marker\n",
    "    if (leftx_dir_marker != rightx_dir_marker):\n",
    "        if abs(leftx_dir) >= abs(rightx_dir):\n",
    "            rightx_dir_marker = leftx_dir_marker\n",
    "            \n",
    "        else:\n",
    "            leftx_dir_marker = rightx_dir_marker\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Step through the windows one by one (second round) to harmonize windows which seem to be biased\n",
    "    # as they do not conform with general turn structure\n",
    "    for window in range(nwindows):\n",
    "        # Take all windows after the first one\n",
    "        if window != 0:\n",
    "            # Perform window adjustments for right turns for left and right line\n",
    "            if rightx_dir_marker:\n",
    "                # If the window position is left of the previous one (probably biased by some \n",
    "                # distortion on the left)\n",
    "                # Right line\n",
    "                if rightx_rep[window] <= rightx_rep[window-1]:\n",
    "                    # Put window in the middle between previous and next window in x direction\n",
    "                    try:\n",
    "                        rightx_rep[window] = int(np.mean([rightx_rep[window - 1],\n",
    "                                                          rightx_rep[window + 1]]))\n",
    "                    # For last window take the position of previous window in x direction\n",
    "                    except:\n",
    "                        rightx_rep[window] = rightx_rep[window - 1]\n",
    "                # Left line\n",
    "                if leftx_rep[window] <= leftx_rep[window-1]:\n",
    "                    # Put window in the middle between previous and next window in x direction\n",
    "                    try:\n",
    "                        leftx_rep[window] = int(np.mean([leftx_rep[window - 1],leftx_rep[window + 1]]))\n",
    "                    # For last window take the position of previous window in x direction\n",
    "                    except:\n",
    "                        leftx_rep[window] = leftx_rep[window - 1]\n",
    "                        \n",
    "            # Perform window adjustments for left turns\n",
    "            else:\n",
    "                # If the window position is right of the previous one (probably biased by some \n",
    "                # distortion on the left)\n",
    "                # Right line\n",
    "                if rightx_rep[window] >= rightx_rep[window-1]:\n",
    "                    # Put window in the middle between previous and next window in x direction\n",
    "                    try:\n",
    "                        rightx_rep[window] = int(np.mean([rightx_rep[window - 1],\n",
    "                                                          rightx_rep[window + 1]]))\n",
    "                    # For last window take the position of previous window in x direction\n",
    "                    except:\n",
    "                        rightx_rep[window] = rightx_rep[window - 1]\n",
    "                if leftx_rep[window] >= leftx_rep[window-1]:\n",
    "                    # Put window in the middle between previous and next window in x direction\n",
    "                    try:\n",
    "                        leftx_rep[window] = int(np.mean([leftx_rep[window - 1],leftx_rep[window + 1]]))\n",
    "                    # For last window take the position of previous window in x direction\n",
    "                    except:\n",
    "                        leftx_rep[window] = leftx_rep[window - 1]\n",
    "                        \n",
    "        # Identify window boundaries in y direction\n",
    "        win_y_low = img.shape[0] - (window+1) * window_height\n",
    "        win_y_high = img.shape[0] - window * window_height      \n",
    "        \n",
    "        \n",
    "        # Identify window boundaries in x direction for both left and right and left lanes and \n",
    "        # left and right window side       \n",
    "        win_xleft_low = leftx_rep[window] - margin\n",
    "        win_xleft_high = leftx_rep[window] + margin\n",
    "        win_xright_low = rightx_rep[window] - margin\n",
    "        win_xright_high = rightx_rep[window] + margin\n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                          (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                           (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 4) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 4)   \n",
    "    \n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)    \n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.array(range(0,img.shape[0]))\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "    \n",
    "    return out_img, left_fit, right_fit, ploty, left_fitx, right_fitx, lefty, righty, leftx, rightx, leftx_dir_marker, rightx_dir_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for lane lines around line found in previous picture\n",
    "def identify_lane_line_cont(img, left_fit, right_fit):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    img (np.array): The warped input image\n",
    "    left_fit (np.array): Vector of coefficients for fitted second degree polynomial for left line \n",
    "                            of previous image\n",
    "    right_fit (np.array): Vector of coefficients for fitted second degree polynomial for right line \n",
    "                            of previous image\n",
    "    Output:\n",
    "    out_img (np.array): The output image containing the fitted windows in green,\n",
    "                        the binary points contributing to the left line regression in red, \n",
    "                        the binary points contributing to the right line regression in blue and \n",
    "                        the rest of the binary points in white\n",
    "    left_fit (np.array): Vector of coefficients for fitted second degree polynomial for left line\n",
    "    right_fit (np.array): Vector of coefficients for fitted second degree polynomial for right line\n",
    "    ploty (np.array): A numpy array with the range of x-pixels as values\n",
    "    left_fitx (np.array): Fitted 2nd degree polynomial points for all \"ploty-values\" for left line \n",
    "                            for current image\n",
    "    right_fitx (np.array): Fitted 2nd degree polynomial points for all \"ploty-values\" for right line \n",
    "                            for current image\n",
    "    leftx_dir_marker (bool): Determines curve direction of left lane - if True -> right turn, \n",
    "                        if Fales -> left turn\n",
    "    rightx_dir_marker (bool): Determines curve direction of right lane - if True -> right turn, \n",
    "                        if Fales -> left turn\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Assume you now have a new warped binary image \n",
    "    # from the next frame of video (also called \"binary_warped\")\n",
    "    # It's now much easier to find line pixels!\n",
    "    nonzero = img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    margin = 80\n",
    "    left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + left_fit[2] - \n",
    "                                   margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + \n",
    "                                                           left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "    right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + right_fit[2] -\n",
    "                                    margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + \n",
    "                                                            right_fit[1]*nonzeroy + right_fit[2] + margin)))  \n",
    "\n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, img.shape[0]-1, img.shape[0])\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    # Determine the direction of the turns\n",
    "    \n",
    "    # For left line\n",
    "    left_line_indicator = left_fitx[0]-left_fitx[len(ploty)-1]\n",
    "    # Right turn\n",
    "    leftx_dir_marker = True\n",
    "    # Left turn\n",
    "    if left_line_indicator < 0:\n",
    "        leftx_dir_marker = False\n",
    "        \n",
    "    # For right lane\n",
    "    right_line_indicator = right_fitx[0]-right_fitx[len(ploty)-1]\n",
    "    # Right turn\n",
    "    rightx_dir_marker = True\n",
    "    # Left turn\n",
    "    if right_line_indicator < 0:\n",
    "        rightx_dir_marker = False\n",
    "        \n",
    "    \n",
    "    # Create an image to draw on and an image to show the selection window\n",
    "    out_img = np.dstack((img, img, img))*255\n",
    "    window_img = np.zeros_like(out_img)\n",
    "    # Color in left and right line pixels\n",
    "    out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "    out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "    \n",
    "    # Generate a polygon to illustrate the search window area\n",
    "    # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "    left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "    left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, ploty])))])\n",
    "    left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "    right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "    right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))])\n",
    "    right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "    cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "    out_img = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "    \n",
    "    return out_img, left_fit, right_fit, ploty, left_fitx, right_fitx, lefty, righty, leftx, rightx, leftx_dir_marker, rightx_dir_marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536148be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot and save an example image\n",
    "def plot_identified_lane_image(out_img,ploty,left_fitx,right_fitx,ending):\n",
    "    plt.imshow(out_img)\n",
    "    plt.plot(left_fitx, ploty, color='yellow')\n",
    "    plt.plot(right_fitx, ploty, color='yellow')\n",
    "    plt.xlim(0, 1280)\n",
    "    plt.ylim(720, 0)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"output_images/{}.png\".format(ending))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of initial lane_line_identification (function identify_lane_line_first)\n",
    "out_img, left_fit, right_fit, ploty, left_fitx, right_fitx, lefty, righty, leftx, rightx,_,_ = identify_lane_line_first(\n",
    "    test_images_warped_cb[\"test_images/test2.jpg\"])\n",
    "\n",
    "plot_identified_lane_image(out_img,ploty,left_fitx,right_fitx,\"19_identified_lanes_first\")\n",
    "\n",
    "# Test of search around line found in previous picture (function identify_lane_line_cont) \n",
    "# -> Test currently performed with same image as initial image\n",
    "out_img, left_fit, right_fit, ploty, left_fitx, right_fitx, lefty, righty, leftx, rightx,_,_ = identify_lane_line_cont(\n",
    "    test_images_warped_cb[\"test_images/test2.jpg\"],left_fit, right_fit)\n",
    "\n",
    "plot_identified_lane_image(out_img,ploty,left_fitx,right_fitx, \"20_identified_lanes_second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec288e3",
   "metadata": {},
   "source": [
    "## 2.5 Calculate radius of curvature and position of vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c291f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_sob_comb(img_sobcomb,img_s_thresh):\n",
    "    assert img_sobcomb.shape == img_s_thresh.shape, \"not all input images have the same shape\"\n",
    "    combined_bin = np.zeros_like(img_sobcomb)\n",
    "    combined_bin[(img_sobcomb == 1) | (img_s_thresh == 1)] = 1\n",
    "    return combined_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d4ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_binary = {}\n",
    "combined_binary = {}\n",
    "\n",
    "for key in sorted(test_images.keys()):\n",
    "\n",
    "    # Stack each channel to view their individual contributions in green and blue respectively\n",
    "    # This returns a stack of the two binary images, whose components you can see as different colors\n",
    "    color_binary[key] = np.dstack(( np.zeros_like(test_images_dir_grad[key]), test_images_sobcomb[key], \n",
    "                                   test_images_s_thresh[key]))\n",
    "\n",
    "    # Combine the two binary thresholds\n",
    "    combined_binary[key] = col_sob_comb(test_images_sobcomb[key],test_images_s_thresh[key])\n",
    "    \n",
    "plt.imsave(\"output_images/12_combined_binary.png\",combined_binary[\"test_images/test2.jpg\"],cmap = \"gray\")\n",
    "    \n",
    "\n",
    "for col,com in zip(sorted(color_binary.keys()),sorted(combined_binary.keys())):\n",
    "    # Plotting thresholded images\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n",
    "    ax1.set_title('Stacked thresholds')\n",
    "    ax1.imshow(color_binary[col])\n",
    "\n",
    "    ax2.set_title('Combined S channel and gradient thresholds')\n",
    "    ax2.imshow(combined_binary[com], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0401f",
   "metadata": {},
   "source": [
    "### 2.2.4 Create overall function for binary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82be7456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating thresholded binary image on single image (function summarizes all previous functions)\n",
    "def img_to_thresh_bin(img,s_thresh_min=170, s_thresh_max=255, sobx_thresh_min=20, sobx_thresh_max=100, \n",
    "                      sobx_kernel_size=3, soby_thresh_min=20, soby_thresh_max = 100, soby_kernel_size = 3, \n",
    "                      mag_thresh_min = 20, mag_thresh_max = 100, mag_kernel_size = 3, dir_thresh_min = 0.4, \n",
    "                      dir_thresh_max = 0.8, dir_kernel_size = 15):\n",
    "\n",
    "    # Create S-Channel binary pipeline\n",
    "    hls = RGB_to_HLS(img)\n",
    "    s = HLS_to_S(hls)\n",
    "    s_thresh = S_to_thresh(s, s_thresh_min, s_thresh_max)\n",
    "    \n",
    "    # Create sobel binary pipeline\n",
    "    sobelx = abs_sobel_thresh(img, orient='x', sobel_kernel= sobx_kernel_size, thresh=(sobx_thresh_min, sobx_thresh_max)) \n",
    "    sobely = abs_sobel_thresh(img, orient='y', sobel_kernel= soby_kernel_size, thresh=(soby_thresh_min, soby_thresh_max))\n",
    "    mag_grad = mag_thresh(img, sobel_kernel= mag_kernel_size, mag_thresh=(mag_thresh_min, mag_thresh_max))\n",
    "    dir_grad = dir_threshold(img, sobel_kernel=dir_kernel_size, thresh=(dir_thresh_min, dir_thresh_max))\n",
    "    sob_comb = sob_to_sobcomb(sobelx,sobely,mag_grad,dir_grad)\n",
    "    \n",
    "    # Return overall combined binary\n",
    "    return col_sob_comb(sob_comb,s_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7f73d",
   "metadata": {},
   "source": [
    "## 2.3 Perform perspective transform"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
