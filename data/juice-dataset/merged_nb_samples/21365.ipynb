{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxes.apply(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ee9a7b",
   "metadata": {},
   "source": [
    "**Note:** only customer has missing values (*NaN*), and the missing information can be found in another instances of the same customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9321e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id = 1654222\n",
    "mv_sw = boxes[(boxes.started_week.notnull())&(boxes.subscription_id == sub_id)].started_week.iloc[0]\n",
    "boxes.loc[boxes.started_week.isna(),'started_week'] = mv_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fadfdf",
   "metadata": {},
   "source": [
    "*Creating feature **time as customer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43509e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "started_week_full = pd.to_datetime(boxes.started_week.apply(conv_weeknumformat_to_datetime))\n",
    "current_ym = pd.to_datetime(boxes.delivery_date.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa5b44",
   "metadata": {},
   "source": [
    "**Note:** We assume *2017-12* as current year-month and thus we predict churn for the near future (i.e. 1/2/3 months)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes['time_as_customer'] = started_week_full.apply(lambda x: abs((x.year - current_ym.year) * 12 + \n",
    "                                                                  (x.month - current_ym.month)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52504843",
   "metadata": {},
   "source": [
    "#### Problems with compensation typer on *Errros dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ab8a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compensation type\n",
    "errors.loc[errors.compensation_amount == 0,'compensation_type'] = 'none'\n",
    "errors.loc[errors.compensation_type == 'full refund','compensation_type'] = 'full_refund'\n",
    "errors.loc[errors.compensation_type == 'sorry','compensation_type'] = 'refund'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f62bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0b99b",
   "metadata": {},
   "source": [
    "#### Featuring extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9a162",
   "metadata": {},
   "source": [
    "*Get customers list*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a7efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = get_subscriptions_list(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7adad21",
   "metadata": {},
   "source": [
    "*List of churners and alives (non-churners)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80044e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary, churn_sub, survive_sub = get_subscriptions_current_status(boxes, cancels, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab33200",
   "metadata": {},
   "source": [
    "*Dataframe for feature extraction*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = pd.DataFrame(list(subscription_id), columns=['subscription_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03121cc7",
   "metadata": {},
   "source": [
    "**Feature: time as customer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcde34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base['time_as_customer'] = boxes[['subscription_id','time_as_customer']].drop_duplicates().time_as_customer.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c34608d",
   "metadata": {},
   "source": [
    "**Feature: number of pauses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee20b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pauses_l6months = pauses.drop_duplicates().groupby(['subscription_id']).subscription_id.count()\n",
    "\n",
    "pp = []\n",
    "for i in subscription_id:\n",
    "    if i in pauses_l6months.index:\n",
    "        pp.append(pauses_l6months[i])\n",
    "    else:\n",
    "        pp.append(0)\n",
    "\n",
    "base['number_pauses'] = pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d50e9da",
   "metadata": {},
   "source": [
    "**Feature: number of different boxes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56724afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = pd.read_csv(\"boxes.csv\", dtype={'subscription_id':int})\n",
    "pauses = pd.read_csv(\"pauses.csv\", dtype={'subscription_id':int})\n",
    "cancels = pd.read_csv(\"cancels.csv\")\n",
    "errors = pd.read_csv(\"errors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac996bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = boxes.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded197a3",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dd98f2",
   "metadata": {},
   "source": [
    "#### Adjusting datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79babc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes.delivery_date = pd.to_datetime(boxes.delivery_date)\n",
    "pauses.pause_start = pd.to_datetime(pauses.pause_start)\n",
    "pauses.pause_end = pd.to_datetime(pauses.pause_end)\n",
    "cancels.event_date = pd.to_datetime(cancels.event_date)\n",
    "errors.reported_date = pd.to_datetime(errors.reported_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f9da9",
   "metadata": {},
   "source": [
    "#### Pause length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b078cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pauses[\"pause_length\"] = pauses.pause_end - pauses.pause_start\n",
    "pauses[\"pause_start_ym\"] = [ t.strftime(\"%Y\") for t in pauses.pause_start]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabc3da8",
   "metadata": {},
   "source": [
    "**Note:** Pause takes 6 days. However, there are few rows presenting *-360 days*. This is clarity an error. Therefore, we could either estimate the pause time period, or remove them. Since there are only few rows, we will remove these instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pauses = pauses[pauses.pause_length == '6 days']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5760264d",
   "metadata": {},
   "source": [
    "#### Missing values :: started_week\n",
    "Uncomment the next cell to verify the missing values per feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1af2c",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = base.iloc[:,1:-1]\n",
    "# Drop 'num_cancellation' to evaluate the performance of models\n",
    "# You will find the explanation soon\n",
    "X = X.drop('num_cancellation', axis=1)\n",
    "\n",
    "y = base.iloc[:,-1]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f268b684",
   "metadata": {},
   "outputs": [],
   "source": [
    "se = StandardScaler().fit(X_train)\n",
    "\n",
    "X_std = se.transform(X)\n",
    "X_train_std = se.transform(X_train)\n",
    "X_test_std = se.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e7715",
   "metadata": {},
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8115fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "orus = RandomOverSampler(random_state=0)\n",
    "oX_resampled, oY_resampled = orus.fit_sample(X, y)\n",
    "\n",
    "# Splitting dataset\n",
    "X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(oX_resampled, oY_resampled, test_size=.20)\n",
    "\n",
    "# Transformation\n",
    "se_os = StandardScaler().fit(X_train_os)\n",
    "X_train_os_std = se_os.transform(X_train_os)\n",
    "X_test_os_std = se_os.transform(X_test_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae98d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
    "           'precision' : make_scorer(precision_score),\n",
    "           'recall' : make_scorer(recall_score), \n",
    "           'f1_score' : make_scorer(f1_score)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c64b5e",
   "metadata": {},
   "source": [
    "**RandomFortest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe976e",
   "metadata": {},
   "source": [
    "*Cross-validation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc73415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm_cv = RandomForestClassifier()\n",
    "\n",
    "stf_fold = StratifiedKFold(10)\n",
    "res = pd.DataFrame.from_dict( cross_validate(rfm_cv, X, y, cv=stf_fold, \n",
    "                                             scoring=scoring, \n",
    "                                             return_train_score=False) )\n",
    "cv_print_evaluation(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a958af",
   "metadata": {},
   "source": [
    "**Note:** The use of **StratifiedKFold** insted of **KFold** is necessary because the dataset suffers from imbalanced dataset problem. Therefore, it is recommended to use stratified sampling to ensure that relative class frequencies is approximately preserved in each train and validation fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cc71c2",
   "metadata": {},
   "source": [
    "*Oversampled data + model and predictions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749c80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = RandomForestClassifier()\n",
    "rfm.fit(X_train_os, y_train_os)\n",
    "\n",
    "rfm_predictions = rfm.predict(X_test_os)\n",
    "rfm_pred_prob = rfm.predict_proba(X_test_os)\n",
    "print_evaluation(y_test_os, rfm_predictions)\n",
    "\n",
    "n_features = 5\n",
    "\n",
    "print(\"\\nImportant features:\")\n",
    "imp_features = get_kimportant_features(rfm.feature_importances_, X.columns, k=n_features, prt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f273e4f",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "\n",
    "1. *num_cancellation*, *time_as_customer*, *num_diff_boxes* and *number_pauses* are really significant for the model.\n",
    "2. Unbalanced data and oversampled data achieve the same performance. However, this is thanks to the *num_cancellation* feature which has the highest importance. If we drop it from the **base dataset**, we notice that the F1-score (CV) was 0.833 and accuracy 0.759. On the other hand, the oversampled data + model approach presented a very similar accuracy (0.973) and F-score (0.973) without *num_cancellation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60024dee",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ba46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv = LogisticRegression()\n",
    "\n",
    "stf_fold = StratifiedKFold(10)\n",
    "res = pd.DataFrame.from_dict( cross_validate(lr_cv, X_std, y, cv=stf_fold, \n",
    "                                             scoring=scoring, \n",
    "                                             return_train_score=False) )\n",
    "cv_print_evaluation(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1482bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_os, y_train_os)\n",
    "\n",
    "lr_predictions = lr.predict(X_test_os)\n",
    "lr_pred_prob = lr.predict_proba(X_test_os)\n",
    "\n",
    "print_evaluation(y_test_os, lr_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa34b2",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "1. If we keep the *num_cancellation* in our **base dataset**, we achive 100% accuracy in all performance metrics, that is, no prediction errors (i.e. confusion matrix shows such an information).\n",
    "2. We also predict the probability of churn to focus on the customer with a high probability of churn.\n",
    "3. Extract more futures and improve the model is possible as well as obtain insights from the model to create actions to prevent the churn.\n",
    "4. Different graphs were used to support the creationg of this model. However, we must understand the customer behavior better. The next graph is very important to verify the peaks of churns based on time subscription."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05986d4",
   "metadata": {},
   "source": [
    "## Information\n",
    "\n",
    "### Task\n",
    "\n",
    "Explore the datasets and develop a model to predict either customer churn over time OR weekly demand for each product type.\n",
    "\n",
    "### Required libraries to run this notebook:\n",
    "\n",
    "Please, install the following libraries before running this jupyter notebook:\n",
    "- Sklearn\n",
    "- Pandas\n",
    "- Imblearn\n",
    "- Plotly\n",
    "- seaborn\n",
    "\n",
    "### My notes\n",
    "\n",
    "#### Size per dataset\n",
    "- Boxes   : (4552066, 6)\n",
    "- pauses  : (5921657, 3)\n",
    "- cancels : (1408824, 3)\n",
    "- errors  : (417689, 5)\n",
    "\n",
    "#### Errors dataset:\n",
    "1. What kind of compensation type *sorry* is?\n",
    "2. 'Sorry' compensation type has only 3 instances. However, they were refunded. Therefore, we should change this type of compensation.\n",
    "3. Mistyping error: *full_refund* and *full refund* are present in the error dataset.\n",
    "4. Credict compensation type with compensation amount equals zero. It is an odd situation that could be either compensation type equals zero, or compensation amount is missing. It also happens with full_refund, partial_refund, and refund.\n",
    "5. *Refund* compensation type is more like a generalization than a specific type of compensation such as full or partial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc7336a",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4215ad",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import operator\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sbn\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "plotly.offline.init_notebook_mode()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c1ad6",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subscriptions_list(b):\n",
    "    return set(b.subscription_id.value_counts().index.tolist())\n",
    "\n",
    "def get_cancel_sub_list(c):\n",
    "    return set(c[c.event_type == 'cancellation'].subscription_id.tolist())\n",
    "\n",
    "def get_subscriptions_current_status(b,c, plot=False):\n",
    "    sublist = get_subscriptions_list(b)\n",
    "    csbc_id = get_cancel_sub_list(c)\n",
    "    \n",
    "    churn_subscription = sublist.intersection(csbc_id)\n",
    "    survive_subscription = sublist.difference(csbc_id)\n",
    "    \n",
    "    n = len(sublist)\n",
    "    ch_pct = len(churn_subscription) * 100.0 / n\n",
    "    su_pct = len(survive_subscription) * 100.0 / n\n",
    "    \n",
    "    summary = pd.DataFrame([[len(churn_subscription), len(survive_subscription)]], \n",
    "                           columns=['qtd_churn','qtd_survive'], index=[\"qtd\"])\n",
    "    \n",
    "    if plot:\n",
    "        gData = [go.Bar(y=summary.qtd_churn, x=summary.index, name=\"Amount of churn\"),\n",
    "                 go.Bar(y=summary.qtd_survive, x=summary.index, name=\"Amount of survive\")]\n",
    "        gLay = go.Layout(title=\"Current status: Amount of churn (%.2f%%) and surive (%.2f%%)\" % (ch_pct, su_pct))\n",
    "        fig = go.Figure(data=gData, layout=gLay)\n",
    "        plotly.offline.iplot(fig)\n",
    "    \n",
    "    return summary, churn_subscription, survive_subscription\n",
    "\n",
    "def conv_weeknumformat_to_datetime(d):\n",
    "    year = int(d[:4])\n",
    "    weeknum = int(d[-2:])\n",
    "    month = int(weeknum/4) # number of weeks\n",
    "    if month < 1:\n",
    "        month = 1\n",
    "    elif month > 12:\n",
    "        month = 12\n",
    "    \n",
    "    mydate = \"%i-%i-01\" % (year,month)\n",
    "    return mydate\n",
    "\n",
    "def missing_values(x):\n",
    "    return sum(x.isna())\n",
    "\n",
    "def print_evaluation(y_true, y_pred):\n",
    "    acc = round(accuracy_score(y_true, y_pred), 3)\n",
    "    pre = round(precision_score(y_true, y_pred), 3)\n",
    "    rec = round(recall_score(y_true, y_pred), 3)\n",
    "    fsc = round(f1_score(y_true, y_pred), 3)\n",
    "    cfm = confusion_matrix(y_true,y_pred)\n",
    "    \n",
    "    print(\"Evaluation:\")\n",
    "    print(\" - Accuracy    = {0}\".format( acc ))\n",
    "    print(\" - Precision  = {0}\".format( pre ))\n",
    "    print(\" - Recall      = {0}\".format( rec ))\n",
    "    print(\" - F-measure   = {0}\".format( fsc ))\n",
    "    \n",
    "    print(\"\\n------------------------------\")\n",
    "    print(\"Confusion Matrix\")\n",
    "    sbn.set(font_scale=1.2)#for label size\n",
    "    sbn.heatmap(cfm, annot=True, fmt=\"d\", annot_kws={\"size\": 16})# font size\n",
    "    print(\"------------------------------\")\n",
    "\n",
    "    \n",
    "def cv_print_evaluation(res):\n",
    "    acc = round(res.test_accuracy.mean(), 3)\n",
    "    pre = round(res.test_precision.mean(), 3)\n",
    "    rec = round(res.test_recall.mean(), 3)\n",
    "    fsc = round(res.test_f1_score.mean(), 3)\n",
    "    \n",
    "    print(\"\"\"--------------------------\n",
    "     Model performance\\n--------------------------\"\"\")\n",
    "    print(\"- Accuracy:   %5s %.3f\\n\" % (\"\", acc))\n",
    "    print(\"- Precision:  %5s %.3f\\n\" % (\"\", pre))\n",
    "    print(\"- Recall:     %5s %.3f\\n\" % (\"\", rec))\n",
    "    print(\"- F1-Measure: %5s %.3f\" % (\"\", fsc))\n",
    "    print(\"--------------------------\")\n",
    "    \n",
    "def get_kimportant_features(importances, features, k, prt=False):\n",
    "    imp_ftres = {}\n",
    "    for c,i in zip(features, importances): \n",
    "        imp_ftres[c] = i\n",
    "    \n",
    "    fimp = sorted(imp_ftres.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sel_features = dict(fimp[:k])\n",
    "    if prt:\n",
    "        for k in sel_features.items():\n",
    "            print(\"%20s : %.3f\" % (k[0],k[1]))\n",
    "    \n",
    "    return sel_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0eade",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ba56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base['num_diff_boxes'] = boxes[['subscription_id',\n",
    "                                'box_id']].drop_duplicates().groupby('subscription_id').box_id.count().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331388d",
   "metadata": {},
   "source": [
    "**Feature: number of different products**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base['num_diff_products'] = boxes[['subscription_id',\n",
    "                                   'product']].drop_duplicates().groupby(['subscription_id']).product.count().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb1f08",
   "metadata": {},
   "source": [
    "**Features: Quantity of cancellation and Quantity of reactivation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a29b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = cancels.drop_duplicates().groupby(['subscription_id','event_type']).event_type.count().unstack()\n",
    "cr.fillna(0, inplace=True)\n",
    "\n",
    "canc = []\n",
    "react = []\n",
    "for i in subscription_id:\n",
    "    if i in cr.index:\n",
    "        v = cr.loc[i]\n",
    "        canc.append(int(v.cancellation))\n",
    "        react.append(int(v.reactivation))\n",
    "    else:\n",
    "        canc.append(0)\n",
    "        react.append(0)\n",
    "\n",
    "base['num_cancellation'] = canc\n",
    "base['num_reactivation'] = react"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4623b19",
   "metadata": {},
   "source": [
    "**Features: compensation types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a57ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ect = errors.drop_duplicates().groupby(['subscription_id',\n",
    "                                        'compensation_type']).compensation_type.count().unstack()\n",
    "\n",
    "ect.fillna(0, inplace=True)\n",
    "\n",
    "credit = []\n",
    "full_refund = []\n",
    "none = []\n",
    "partial_refund = []\n",
    "refund = []\n",
    "for i in subscription_id:\n",
    "        \n",
    "    if i in ect.index:\n",
    "        v = ect.loc[i]\n",
    "\n",
    "        credit.append(v.credit)\n",
    "        full_refund.append(v.full_refund)\n",
    "        none.append(v.none)\n",
    "        partial_refund.append(v.partial_refund)\n",
    "        refund.append(v.refund)\n",
    "    else:\n",
    "        credit.append(0)\n",
    "        full_refund.append(0)\n",
    "        none.append(0)\n",
    "        partial_refund.append(0)\n",
    "        refund.append(0)\n",
    "        \n",
    "base['qtd_credit'] = credit\n",
    "base['qtd_full_refund'] = full_refund\n",
    "base['qtd_none'] = none\n",
    "base['qtd_partial_refund'] = partial_refund\n",
    "base['qtd_refund'] = refund"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3492b8",
   "metadata": {},
   "source": [
    "**Features: number of erros and compensation amount**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tepc = ect.sum(axis=1)\n",
    "amt = errors.drop_duplicates().groupby(['subscription_id']).compensation_amount.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c17f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtd_erros = []\n",
    "amount_comp = []\n",
    "for i in subscription_id:\n",
    "    if i in amt:\n",
    "        ac = amt.loc[i]\n",
    "        amount_comp.append(ac)\n",
    "    else:\n",
    "        amount_comp.append(0)\n",
    "    \n",
    "    if i in tepc.index:\n",
    "        v = tepc.loc[i]        \n",
    "        qtd_erros.append(v)\n",
    "    else:\n",
    "        qtd_erros.append(0)\n",
    "        \n",
    "base['qtd_erros'] = qtd_erros\n",
    "base['amount_comp'] = amount_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d268b8fe",
   "metadata": {},
   "source": [
    "**Feature: status (churn/alive)**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
