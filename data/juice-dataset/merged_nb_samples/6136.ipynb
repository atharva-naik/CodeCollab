{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6034f15",
   "metadata": {},
   "source": [
    "__Chapter 3 - A tour of machine learning classifiers using scikit-learn__\n",
    "\n",
    "1. [First steps with sklearn - training a perceptron](#First-steps-with-sklearn-training-a-perceptron)\n",
    "1. [Modeling class probabilities w/ logistic regression](#Modeling-class-probabilities-w/-logistic-regression)\n",
    "    1. [Intuition: logistic regression](#Intuition-logistic-regression)\n",
    "        1. [Sigmoid function](#Sigmoid-function)\n",
    "        1. [Decision function](#Decision-function)\n",
    "    1. [Training a logistic regression model](#Training-a-logistic-regression-model)\n",
    "1. [Maximum margin classification with SVMs](#Maximum-margin-classification-with-SVMs)\n",
    "    1. [Intuition: SVMs](#SVM-intuition)\n",
    "    1. [Nonlinearly separable cases and slack variables](#Nonlinearly-separable-cases-and-slack-variables)\n",
    "    1. [Logistic regression vs. SVMs](#Logistic-regression-vs-SVMs)\n",
    "    1. [Solving nonlinear problems using a kernel SVM](#Solving-nonlinear-problems-using-a-kernel-SVM)\n",
    "    1. [$\\gamma$ parameter](#gamma-parameter)\n",
    "1. [Decision tree learning](#Decision-tree-learning)\n",
    "    1. [Information gain](#Information-gain)\n",
    "    1. [Combining decisions trees via random forests](#Combining-decisions-trees-via-random-forests)\n",
    "1. [K-nearest neighbors](#K-nearest-neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libary and settings\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import itertools\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "dataPath = os.path.abspath(os.path.join('../../Data'))\n",
    "modulePath = os.path.abspath(os.path.join('../../CustomModules'))\n",
    "sys.path.append(modulePath) if modulePath not in sys.path else None\n",
    "from IPython.core.display import display, HTML; display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "\n",
    "# Data extensions and settings\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold = np.inf, suppress = True)\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.options.display.float_format = '{:,.6f}'.format\n",
    "\n",
    "\n",
    "# Modeling extensions\n",
    "import sklearn.base as base\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.decomposition as decomposition\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.feature_extraction as feature_extraction\n",
    "import sklearn.feature_selection as feature_selection\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.neighbors as neighbors\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.svm as svm\n",
    "import sklearn.tree as tree\n",
    "\n",
    "\n",
    "# Visualization extensions and settings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Custom extensions and settings\n",
    "from quickplot import qp, qpUtil, qpStyle\n",
    "from mlTools import powerGridSearch\n",
    "sns.set(rc = qpStyle.rcGrey)\n",
    "\n",
    "\n",
    "# Magic functions\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276be1be",
   "metadata": {},
   "source": [
    "# First steps with sklearn - training a perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4347e",
   "metadata": {},
   "source": [
    "<a id = 'Choosing-a-classifier'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb25d4",
   "metadata": {},
   "source": [
    "## Choosing a classifier\n",
    "-Five key steps in training an algorithm\n",
    "    1. Select features and collect training data\n",
    "    2. Choose a performance metric\n",
    "    3. Choose a classifier and optimization algorithm\n",
    "    4. Evaluate model performance\n",
    "    5. Tune the algorithm's parameters\n",
    "Evaluate several different classifers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b55c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and inspect class labels\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:,[2,3]]\n",
    "y = iris.target\n",
    "print('Class labels: {0}'.format(np.unique(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00320e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "# Stratify ensures proportional distribution of classes between the train/test data\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = model_selection.train_test_split(X, y\n",
    "                                            ,test_size = 0.3, random_state = 1, stratify = y)\n",
    "\n",
    "print('Label counts in y: {}'.format(np.bincount(y)))\n",
    "print('Label counts in yTrain: {}'.format(np.bincount(yTrain)))\n",
    "print('Label counts in yTest: {}'.format(np.bincount(yTest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "\n",
    "sc = preprocessing.StandardScaler()\n",
    "sc.fit(xTrain)\n",
    "xTrainStd = sc.transform(xTrain)\n",
    "xTestStd = sc.transform(xTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4149b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review how first feature changes after standard scaling\n",
    "\n",
    "print('Original mean: {0}'.format(round(xTrain[:,0].mean(),5)))\n",
    "print('Original standard deviation: {0}'.format(round(xTrain[:,0].std(),5)))\n",
    "print()\n",
    "print('Scaled mean: {0}'.format(round(xTrainStd[:,0].mean(),5)))\n",
    "print('Scaled standard deviation: {0}'.format(round(xTrainStd[:,0].std(),5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cbf77d",
   "metadata": {},
   "source": [
    "> Remarks - Standard scaling uses the mean $\\mu$ and standard deviation $\\sigma$ to alter each feature independently such that each feature has a $\\mu = 0$ and a $\\sigma = 0$\n",
    "\n",
    "> It is important to transform the test dataset using the fit performed on the training set only. First, we want the train and test data to be scaled in the same way. Further, we're assuming we don't know about the test data, and in practice new unseen data will need to be scaled based on the existing scaling operation defintions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Perceptron and fit model\n",
    "\n",
    "ppn = linear_model.Perceptron(n_iter = 40, eta0 = 0.1, random_state = 1)\n",
    "ppn.fit(xTrainStd, yTrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15eca7e",
   "metadata": {},
   "source": [
    "> Remarks - \n",
    "- 'n_iter' limits the number epochs or iterations\n",
    "- 'eta0' controls to learning rate. A value too high will likely overshoot the minimum. A value too low will make the learning process unnecessarily slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c328cb9",
   "metadata": {},
   "source": [
    "# Maximum margin classification with SVMs\n",
    "\n",
    "SVMs are similar to the perceptron in that the goal is determine a boundary between clases. A key difference is that while the perceptron seeks to minimize classification error, SVMs seek to maximize the margin. The margin is the distance between the separating boundary (hyperplane) and the samples closest to that boundary. These samples are referred to as the support vectors.\n",
    "\n",
    "In theory, maximizing margin distance leads to lower generalization error because hyperplanes with narrower distances are close to samples and therefore having higher variance and are more prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f88d80d",
   "metadata": {},
   "source": [
    "<a id = 'SVM-intuition'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f0b195",
   "metadata": {},
   "source": [
    "## Intuition\n",
    "\n",
    "One way to view that boundary is that there are actually three hyperplanes (in a binary classification problem):\n",
    "\n",
    "1. Decision boundary\n",
    "2. Positive hyperplane\n",
    "3. Negative hyperplane\n",
    "\n",
    "The first boundary is the object of the SVM, and the second two hyperplane are parallel to the decision boundary. These positive and negative boundaries are those closest to the positive and negative samples. A simple mathematical expression of these lines:\n",
    "\n",
    "$$\n",
    "w_0 + \\textbf{w}^T\\textbf{x}_{pos} = 1\n",
    "$$\n",
    "$$\n",
    "w_0 + \\textbf{w}^T\\textbf{x}_{neg} = -1\n",
    "$$\n",
    "\n",
    "These linear equations can be subtract from each other, yielding:\n",
    "\n",
    "$$\n",
    "\\textbf{w}^T\\big(\\textbf{x}_{pos} - \\textbf{x}_{neg}\\big) = 2\n",
    "$$\n",
    "\n",
    "Normalizing this equation by the length of the vector \\texfbf{w}:\n",
    "\n",
    "$$\n",
    "\\lVert\\textbf{w}\\rVert = \\sqrt{\\sum\\nolimits_{j=1}^{m}w_j^2}\n",
    "$$\n",
    "\n",
    "Which can be transformed into the following equation:\n",
    "\n",
    "$$\n",
    "\\frac{\\textbf{w}^T\\big(\\textbf{x}_{pos} - \\textbf{x}_{neg}\\big)}{\\lVert\\textbf{w}\\rVert} = \\frac{2}{\\lVert\\textbf{w}\\rVert}\n",
    "$$\n",
    "The LHS of the equation above is the distance between the positive and negative hyperplane. This is the margin that we want to maximize. The objective function of the SVM is to maximize that margin by maximizing the RHS subject the constraint that the samples are correctly classified:\n",
    "\n",
    "$$\n",
    "w_0 + \\textbf{w}^T\\textbf{x}^{(i)} \\geq 1 \\, if \\, y^{(i)} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_0 + \\textbf{w}^T\\textbf{x}^{(i)} \\leq -1 \\, if \\, y^{(i)} = -1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{for} \\, i = 1...N\n",
    "$$\n",
    "\n",
    "where N is the number of samples in the dataset. The equation above essentially say that all positive samples should fall on one side of the positive hyperplane, and all negative samples on one side of the negative hyperplane. Those two key equations can be written compactly as:\n",
    "\n",
    "$$\n",
    "y^i\\big(w_0 + \\textbf{w}^T\\textbf{x}^i\\big) \\geq 1\\forall_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39b40ae",
   "metadata": {},
   "source": [
    "<a id = 'Nonlinearly-separable-cases-and-slack-variables'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f9405",
   "metadata": {},
   "source": [
    "## Nonlinearly separable cases and slack variables\n",
    "\n",
    "To scratch the surface on soft-margin classification, which allows for a certain level of misclassificaiton tolerance through the slack variable $\\xi$. This is helpful in cases where the data is not completely linearly separable. To accomplish this, the slack variable is added to the linear constraints described earlier:\n",
    "\n",
    "$$\n",
    "w_0 + \\textbf{w}^T\\textbf{x}^{(i)} \\geq 1 - \\xi^{(i)} \\, if \\, y^{(i)} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_0 + \\textbf{w}^T\\textbf{x}^{(i)}\\leq - 1 + \\xi^{(i)} \\, if \\, y^{(i)} = -1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{for} \\, i = 1...N\n",
    "$$\n",
    "\n",
    "$N$ is again the number of samples in the dataset. The New objective function to minimize is:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\\lVert\\textbf{w}\\rVert + C \\bigg(\\sum_{i}\\xi^{(i)}\\bigg)\n",
    "$$\n",
    "\n",
    "The variable $C$ controls the penalty for misclassification. Larger values of $C$ correspond to larger error penalties, making the algorithm less forgiving of misclassifications. The model will choose narrower boundaries with higher variance in its efforts to minimize error. Smaller variables of $C$ will be more forgiving of errors and may find a boundary with a wider margin and lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd83a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "sv = svm.SVC(kernel = 'linear', C = 1.0, random_state = 1)\n",
    "sv.fit(xTrainStd, yTrain)\n",
    "\n",
    "p = qp.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas(title = '', xLabel = 'petal length [standardized]', yLabel = 'petal width [standardized]'\n",
    "                  ,yShift = 0.6, position = 111)\n",
    "p.qpDecisionRegion(x = xCombined\n",
    "                   ,y = yCombined\n",
    "                   ,classifier = sv\n",
    "                   ,testIdx = range(105, 150)\n",
    "                   #,bbox = (1.2, 0.9)\n",
    "                   ,ax = ax\n",
    "                  )\n",
    "plt.legend(loc = 'upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e139a",
   "metadata": {},
   "source": [
    "<a id = 'Logistic-regression-vs-SVMs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33283be6",
   "metadata": {},
   "source": [
    "## Logistic regression vs. SVMs\n",
    "\n",
    "Logistic regression and SVMs often produce similar results, but there are a few fundamental differences. \n",
    "- Since SVMs are only interested in finding support vectors (samples closest to the boundary), it is generally  not influenced by outlier. the same cannot be said for logistic regression.\n",
    "- Logistic regression is a simpler model mathematically and can be implemented more easily.\n",
    "- Logistic regression models can be easily updated with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d178b",
   "metadata": {},
   "source": [
    "<a id = 'Solving-nonlinear-problems-using-a-kernel-SVM'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed34f3a",
   "metadata": {},
   "source": [
    "## Solving nonlinear problems using a kernel SVM\n",
    "\n",
    "It is often impossible to separate data points with a line, plane or hyperplane, making logistic regression and SVM incapable of finding a meaningful solution. One potential solution is to use a kernel method to create nonlinear combinations of the original features and project the observations onto a higher-dimensional space via a mapping function $\\phi$ where it becomes linearly separable.\n",
    "\n",
    "$$\n",
    "\\phi(x_1,x_2) = (z_1,z_2,z_3) = (x_1,x_2,x_1^2,x_1^2)\n",
    "$$\n",
    "\n",
    "This enables us to separate two non-linearly separable classes with a linear hyperplane that becomes a nonlinear decision boundary when projected back onto the original feature space.\n",
    "\n",
    "In practice, the training data is mapped to a higher-dimension space using the mapping function $\\phi$, and then unseen data is mapped using the same function to classifiy it using the linear SVM model.\n",
    "\n",
    "The kernel trick is a solution to high computation expense of creating new features, which is especially high when dealing with high-dimensional data. The operation of finding the dot product with $\\textbf{x}^{(i)T}\\textbf{x}^{(j)}$ by wrapping each vector with our function $\\phi$: $\\phi\\big(\\textbf{x}^{(i)T}\\big)\\phi\\big(\\textbf{x}^{(j)}\\big)$. This kernel function avoids the otherwise expensive calculation step of calcuting the dot product of two points explicitely:\n",
    "\n",
    "$$\n",
    "K(\\textbf{x}^{(i)},\\textbf{x}^{(j)}) = \\phi\\big(\\textbf{x}^{(i)T}\\big)\\phi\\big(\\textbf{x}^{(j)}\\big)\n",
    "$$\n",
    "\n",
    "A widely used kernel trick is called the radial basis function (RBF) or the Gaussian kernel:\n",
    "\n",
    "$$\n",
    "K(\\textbf{x}^{(i)},\\textbf{x}^{(j)}) = \\mbox{exp}\\Bigg(-\\frac{\\lVert\\textbf{x}^{(i)} - \\textbf{x}^{(j)}\\rVert}{2\\sigma^2}\\Bigg)\n",
    "$$\n",
    "\n",
    "Which simplifies to:\n",
    "\n",
    "$$\n",
    "K(\\textbf{x}^{(i)},\\textbf{x}^{(j)}) = \\mbox{exp}\\Big(-\\gamma\\lVert\\textbf{x}^{(i)} - \\textbf{x}^{(j)}\\rVert^2\\Big)\n",
    "$$\n",
    "\n",
    "The parameter $\\gamma$ = $\\frac{1}{2\\sigma^2}$ and is tuned udring optimization.\n",
    "\n",
    "A kernel is a similarity function in that it evaluates a pair of samples. The minus sign in front of $\\gamma$ inverts the distance measure into a similarity score (high values, or less negative values, represent more similar observations). The exponential term ensures the similarity score falls between 1 (exactly similar samples) and 0 (very dissimilar samples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c585ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "np.random.seed(1)\n",
    "xXor = np.random.randn(200,2)\n",
    "yXor = np.logical_xor(xXor[:,0] > 0\n",
    "                     ,xXor[:,1] > 1)\n",
    "\n",
    "p = qp.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas()\n",
    "p.qp2dScatterHue(x = xXor[:,0]\n",
    "                  ,y = xXor[:,1]\n",
    "                  ,target = yXor\n",
    "                  ,label = ['1','-1']\n",
    "                  ,xUnits = 'ddd'\n",
    "                  ,yUnits = 'ddd'\n",
    "                  ,ax = ax\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d13344",
   "metadata": {},
   "source": [
    "Remarks - Clearly non-linearly separable class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "sv = svm.SVC(kernel = 'rbf', random_state = 1, gamma = 0.10, C = 10.0)\n",
    "sv.fit(xXor,yXor)\n",
    "\n",
    "p = qp.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "                  ,yShift = 0.8, position = 111)\n",
    "p.qpDecisionRegion(x = xXor\n",
    "                   ,y = yXor\n",
    "                   ,classifier = sv\n",
    "                   ,testIdx = range(105,150)\n",
    "                   ,bbox = (1.2, 0.9)\n",
    "                   ,ax = ax\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d556d9c",
   "metadata": {},
   "source": [
    "<a id = 'gamma-parameter'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e4b63",
   "metadata": {},
   "source": [
    "## $\\gamma$ parameter\n",
    "\n",
    "The parameter gamma $\\gamma$ is a cut-off parameter for the Gaussian sphere. Increasing the value of $\\gamma$ increasing the influece of the training samples, which leads to a tighter and bumpier decision boundary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bce7d1",
   "metadata": {},
   "source": [
    "> Remarks - There is an 81.9% chance sample 0 belongs to class 1, a 77.1% chance sample 2 belongs to class 0, and an 86.6% chance sample 3 belongs to class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b41b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of illustrating to remark above\n",
    "\n",
    "logReg.predict_proba(xTrainStd[:3, :]).argmax(axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31909d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also call the predict method on these samples\n",
    "\n",
    "logReg.predict(xTrainStd[:3, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bda22a",
   "metadata": {},
   "source": [
    "<a id = 'Use-regularization-to-avoid-overfitting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b14cc9e",
   "metadata": {},
   "source": [
    "## Use regularization to avoid overfitting\n",
    "\n",
    "__Weight decay and $\\lambda$__\n",
    "\n",
    "A model that performs well on the training dataset but poorly on the test set is overfitting the data. Another way of saying this is that the model has high variance. These models often have too many parameters, leading to a very complex model.\n",
    "\n",
    "Conversely, a model that underfits the data is not adequately capturing the pattern in the data, resulting in poor accuracy on both the training and test set. Underfit models are said to have high bias.\n",
    "\n",
    "Regularization (also referred to as weight decay) is a method for reducing the complexity of a model. In short, regularization penalizes extreme weight values. A common form of regularization is L2 regularization:\n",
    "\n",
    "$$\n",
    "\\lambda\\sum\\limits_{i=1}^{m}{w_i^2}\n",
    "$$\n",
    "\n",
    "where $\\lambda$ is the regularization parameter that controls the strength of the penalty. Higher values of $\\lambda$ lead to higher levels of regularization, and higher penalties for large weights.\n",
    "\n",
    "__The parameter C__\n",
    "\n",
    "The parameter C is the inverse of $\\lambda$. Consequently, decreasing C increases the regularization strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea853a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight coefficient weights by regularization setting\n",
    "\n",
    "weights, params = [], []\n",
    "for c in np.arange(-5, 5):\n",
    "    logReg = linear_model.LogisticRegression(C = 10.**c, random_state = 1)\n",
    "    logReg.fit(xTrainStd, yTrain)\n",
    "    weights.append(logReg.coef_[1])\n",
    "    params.append(10.**c)\n",
    "\n",
    "p = qp.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas(title = 'L2-regularization', xLabel = 'C', yLabel = 'Weight coefficient', yShift = 0.63)\n",
    "p.qpLine(x = np.array(params)\n",
    "           ,y = np.array(weights)\n",
    "           ,ax = ax\n",
    "           ,label = ['petal length','petal width']\n",
    "           ,yMultiVal = True\n",
    "          )\n",
    "plt.xscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da15785",
   "metadata": {},
   "source": [
    "<a id = '#Maximum-margin-classification-with-SVMs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d396ff",
   "metadata": {},
   "source": [
    "# Modeling class probabilities w/ logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3956577",
   "metadata": {},
   "source": [
    "<a id = 'Intuition-logistic-regression'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae3e5d",
   "metadata": {},
   "source": [
    "## Intuition: logistic regression\n",
    "\n",
    "Unlike actual regression, logistic regression doesn't try to predict the value of a numeric variable given an observation's inputs. Rather, logistic regression is a classification algorithm that returns a probability that an observation belongs to a certain class given an observation's inputs. In other words, logistic regression returns a value between 0 and 1, whereas actual regression returns a value between $-\\infty$ and $\\infty$.\n",
    "\n",
    "The essence of returning a probability $p$ given a set of continuous and categorical attributes begins with the odds ratio, which describes the odds of a particular event occurring and returns a value in the range of 0 to $\\infty$:\n",
    "\n",
    "$$\n",
    "\\mbox{Odds ratio} = \\frac{p}{(p - 1)}\n",
    "$$\n",
    "\n",
    "$p$ is the probability of an event occurring. As an example, the probability that a patient has diabetes given the patient's attributes. \n",
    "\n",
    "This can be further refined by taking the log of the odds ratio (the common convention is to use the natural log):\n",
    "\n",
    "$$\n",
    "logit(p) = log\\frac{p}{(1 - p)}\n",
    "$$\n",
    "\n",
    "The logit function's purpose is to take a probability values between 0 and 1 and transforms them to values that range from $-\\infty$ and $\\infty$. A different way of looking at this is:\n",
    "\n",
    "$$\n",
    "logit\\big(p(y = 1|\\textbf{x})\\big) = w_0x_0 + w_1x_1 + ... w_mx_m = \\textbf{w}^T\\textbf{x}\n",
    "$$\n",
    "\n",
    "Here, the conditional probability that a samples belongs to class 1 given its features $\\textbf{x}$ is converted by the logit function from a number between 0 and 1 to a number between $-\\infty$ and $\\infty$. \n",
    "\n",
    "We can use these values to express a linear relationship between feature values and the log-odds. The RHS of the function, in a 3-dimensional example, would be a plane, which is used for separating classes in a 3-dimensional space. Observations can be very close or far away from the plane (or even on the plane), and that distance can inform the probability of an observation belonging to a certain class. Logistic regression seeks to describe that probability. To get this information, we use the inverse form of the logit function, called the logistic sigmoid function.\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$z$ is equal to $\\textbf{w}^T\\textbf{x}$\n",
    "\n",
    "This technique can be used for binary classification and multi-class classification with a technique referred to as one-versus-rest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33739e15",
   "metadata": {},
   "source": [
    "<a id = 'Sigmoid-function'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd3ee80",
   "metadata": {},
   "source": [
    "### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sigmoid function on the range -7 to 7\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "z = np.arange(-7, 7, 0.1)\n",
    "zPhi = sigmoid(z)\n",
    "\n",
    "\n",
    "p = qp.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas(title = 'Sigmoid function', xLabel = 'z', yLabel = '$\\phi (z)$', yShift = 0.81)\n",
    "p.qpLine(x = z\n",
    "         ,y = zPhi\n",
    "         ,yUnits = 'ff'\n",
    "         ,ax = ax\n",
    "         )\n",
    "plt.axvline(0.0, color = 'black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee8404",
   "metadata": {},
   "source": [
    "By reviewing this S-shaped curve it's clear that as $\\phi(z)$ approaches 1, $z$ is approaching $\\infty$ because $e^{-z}$ becomes very small for large values of $z$. Conversely, as $\\phi(z)$ approaches 0, $z$ is approaching $-\\infty$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65979371",
   "metadata": {},
   "source": [
    "<a id = 'Decision-function'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d382d682",
   "metadata": {},
   "source": [
    "### Decision function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d1f37",
   "metadata": {},
   "source": [
    "\n",
    "The sigmoid functions also illustrates a key point from above: values between $-\\infty$ and $\\infty$ are converted to values between 0 and 1. These values between 0 and 1 are probabilities, and can be interpreted as the probability of a particular sample belonging to class 1 given an attribute input vector \\textbf{x} and weight vector \\textbf{w}. In mathematical notation:\n",
    "\n",
    "$$\n",
    "\\phi(z)= P(y=1|\\textbf{x};\\textbf{w})\n",
    "$$\n",
    "\n",
    "For example, if $\\phi(z)$ = 0.8, then there is an 80% chance that observation belongs to class 1, and a 20% chance of belonging to class 0. This conclusion is based on the decision function:\n",
    "\n",
    "$$\n",
    "\\phi(z) =\n",
    "\\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        1  & \\mbox{if } \\phi(z) >= 0.5 \\\\\n",
    "        0  & \\mbox{otherwise}\n",
    "    \\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e1ef6",
   "metadata": {},
   "source": [
    "<a id = 'Training-a-logistic-regression-model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6b680",
   "metadata": {},
   "source": [
    "## Training a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc08ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with misclassification score\n",
    "\n",
    "yPred = ppn.predict(xTestStd)\n",
    "print('Misclassified samples: {0}'.format((yTest != yPred).sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e45a8",
   "metadata": {},
   "source": [
    ">Remarks - 3 out of 45 samples are incorrectly predicted, yieled an misclassification percent of ~6.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1989d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "logReg = linear_model.LogisticRegression(C = 100.0, random_state = 1)\n",
    "logReg.fit(xTrainStd, yTrain)\n",
    "\n",
    "p = qp.QuickPlot(fig = plt.figure(), chartProp = 15)\n",
    "ax = p.makeCanvas(title = '', xLabel = '', yLabel = ''\n",
    "                  ,yShift = 0.8, position = 111)\n",
    "p.qpDecisionRegion(x = xCombined\n",
    "                   ,y = yCombined\n",
    "                   ,classifier = logReg\n",
    "                   ,testIdx = range(105,150)\n",
    "                   ,bbox = (1.2, 0.9)\n",
    "                   ,ax = ax\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc84dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display probabilities associated with a few samples\n",
    "\n",
    "logReg.predict_proba(xTrainStd[:3, :])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
