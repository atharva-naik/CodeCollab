{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9ebd96f",
   "metadata": {},
   "source": [
    "# Is Software Updated?\n",
    "\n",
    "This notebook looks at references to GitHub URLs in papers available in the OA corpus from EuroPMC,\n",
    "and identifies:\n",
    "\n",
    "  * How many times the GitHub repositories have been updated since paper referencing them was released\n",
    "  \n",
    "Note that at present, we are not distinguishing between URLs referencing software *created* by the paper authors,\n",
    "versus *used* by the authors, nor which software was created as a result of the work in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b8d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from github import Github\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import process_eupmc\n",
    "import process_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6271ab5",
   "metadata": {},
   "source": [
    "## File locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b3e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the data\n",
    "data_dir = '../data'\n",
    "\n",
    "# File containing the list of matching papers\n",
    "matching_papers = data_dir + '/' + 'eupmc_fulltext_html_urls.txt'\n",
    "\n",
    "# File for the output\n",
    "output_jsonfile = data_dir + '/' + 'dict_of_papers.json'\n",
    "\n",
    "# Github Token\n",
    "gh_token = '../secrets/github_token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7abd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gh_token, 'r') as f:\n",
    "    github_token = f.read().rstrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3db378",
   "metadata": {},
   "source": [
    "## Use getpapers to download fulltext of papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a661a0",
   "metadata": {},
   "source": [
    "We currently do this outside of the notebook, and assume that the files are available locally.\n",
    "\n",
    "The command we are using is:\n",
    "\n",
    ">getpapers --query 'github' -x --limit 100 -o data\n",
    "\n",
    "which queries EuPMC for all papers containing the term 'github' and returns the full text of the first 100 papers matching this into the directory 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c92e2",
   "metadata": {},
   "source": [
    "## Textmine each paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f707a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of subdirectories dumped by ContentMine\n",
    "paper_ids = process_eupmc.get_pmcids(matching_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6dd152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the papers and extract all the references to GitHub and Zenodo urls\n",
    "papers_info = process_eupmc.process_papers(paper_ids, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b226f",
   "metadata": {},
   "source": [
    "## Analyse GitHub repos to see frequency of commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ac945",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Github(github_token)\n",
    "number_of_updates = defaultdict(int)\n",
    "frequency_of_updates = defaultdict(int)\n",
    "\n",
    "for p in papers_info:\n",
    "\n",
    "    repos = []\n",
    "    # The following removes references to the main github.com site\n",
    "    # and also treats references to blobs / issues as references to the repo\n",
    "    for gh_url in p.references['github']:\n",
    "        words = gh_url.split('/')\n",
    "        if len(words) > 4: #\n",
    "            reponame = words[3] + '/' + words[4]\n",
    "            if reponame not in repos:\n",
    "                repos.append(reponame)            \n",
    "    \n",
    "    for repo in repos:\n",
    "        print (\"Processing: \", repo)\n",
    "        code = g.get_repo(repo)\n",
    "        # limit to commits since publication date\n",
    "        since = datetime.strptime(p.pub_date, '%Y-%m-%d')\n",
    "        days = (datetime.now() - since).days\n",
    "        commits = code.get_commits()\n",
    "        num_commits = 0\n",
    "        commit_date = commits[num_commits].commit.author.date\n",
    "        while commit_date > since:\n",
    "            num_commits = num_commits + 1\n",
    "            commit_date = commits[num_commits].commit.author.date\n",
    "        print(\"Number of commits since publication: \", num_commits)\n",
    "        commit_freq = num_commits / days\n",
    "        print(\"Commit frequency: \", commit_freq, \"commits/day since publication\")\n",
    "        number_of_updates[num_commits] +=1\n",
    "        # I'm using the magic number 100 until I get a sense of the correct bins to use\n",
    "        frequency_of_updates[int(100 * commit_freq)] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e11546",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "We use a defaultdict so that we can easily zero entries with no data."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
