{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912452c7",
   "metadata": {},
   "source": [
    "# Improving Neural Net Performance\n",
    "**Suggested time to spend on exercise**: 20 minutes\n",
    "\n",
    "In this exercise, we will focus on improving the performance of the NN we trained in the previous exercise. Normalizing the features is particularly important to obtaining good performance. Another way is by trying different optimization algorithms. Note that neither of these methods are specific to neural networks - they are effective means to improve most types of models.\n",
    "\n",
    "First, we'll load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e726470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\"}\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.google as tf\n",
    "from IPython import display\n",
    "from google3.pyglib import gfile\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def preprocess_features(california_housing_dataframe):\n",
    "  \"\"\"This function takes an input dataframe and returns a version of it that has\n",
    "  various features selected and pre-processed.  The input dataframe is expected\n",
    "  to contain data from the california_housing data set.\"\"\"\n",
    "  processed_features = california_housing_dataframe[\n",
    "    [\"latitude\",\n",
    "     \"longitude\",\n",
    "     \"housingMedianAge\",\n",
    "     \"totalRooms\",\n",
    "     \"totalBedrooms\",\n",
    "     \"population\",\n",
    "     \"households\",\n",
    "     \"medianIncome\"]].copy()\n",
    "  processed_features[\"roomsPerPerson\"] = (\n",
    "    california_housing_dataframe[\"totalRooms\"] /\n",
    "    california_housing_dataframe[\"population\"])\n",
    "  # Feel free to add other synthetic features here.\n",
    "  return processed_features\n",
    "\n",
    "\n",
    "def preprocess_targets(california_housing_dataframe):\n",
    "  \"\"\"This function selects and potentially transforms the output target from\n",
    "  a dataframe containing data from the california_housing data set.  Object\n",
    "  returned is a pandas Series.\"\"\"\n",
    "  processed_targets = pd.DataFrame()\n",
    "    # Scale the target to be in units of thousands of dollars.\n",
    "  processed_targets[\"medianHouseValue\"] = (\n",
    "    california_housing_dataframe[\"medianHouseValue\"] / 1000.0)\n",
    "  return processed_targets\n",
    "\n",
    "\n",
    "# Load in the raw data.  Note that there's a separate test data set that we\n",
    "# will leave untouched for now.\n",
    "raw_training_df = pd.read_csv(\n",
    "  gfile.Open(\"/placer/prod/home/ami/mlcc/california_housing/v1/train.csv\"),\n",
    "  sep=\",\")\n",
    "# Randomize the data before selecting train / validation splits.\n",
    "raw_training_df = raw_training_df.reindex(\n",
    "  np.random.permutation(raw_training_df.index))\n",
    "\n",
    "# Choose the first 12000 (out of 17000) examples for training.\n",
    "training_examples = preprocess_features(raw_training_df.head(12000))\n",
    "training_targets = preprocess_targets(raw_training_df.head(12000))\n",
    "\n",
    "# Choose the last 5000 (out of 17000) examples for validation.\n",
    "validation_examples = preprocess_features(raw_training_df.tail(5000))\n",
    "validation_targets = preprocess_targets(raw_training_df.tail(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b492bbfb",
   "metadata": {},
   "source": [
    "Next, we train our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda7ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"output\": \"ignore\", \"timeout\": 180}\n",
    "\n",
    "LEARNING_RATE = 0.0007  # @param\n",
    "STEPS = 5000  # @param\n",
    "BATCH_SIZE = 70  # @param\n",
    "HIDDEN_UNITS = [10, 10]  # @param\n",
    "periods = 10\n",
    "steps_per_period = STEPS / periods\n",
    "\n",
    "# Set up our NN with the desired learning settings.\n",
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(\n",
    "  training_examples)\n",
    "dnn_regressor = tf.contrib.learn.DNNRegressor(\n",
    "  feature_columns=feature_columns,\n",
    "  hidden_units=HIDDEN_UNITS,\n",
    "  optimizer=tf.GradientDescentOptimizer(learning_rate=LEARNING_RATE),\n",
    "  gradient_clip_norm=5.0\n",
    ")\n",
    "\n",
    "print \"Training model...\"\n",
    "print \"RMSE:\"\n",
    "root_mean_squared_errors_training = []\n",
    "root_mean_squared_errors_validation = []\n",
    "for period in range (0, periods):\n",
    "  dnn_regressor.fit(\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    steps=steps_per_period,\n",
    "    batch_size=BATCH_SIZE\n",
    "  )\n",
    "  predictions_validation = dnn_regressor.predict(validation_examples)\n",
    "  predictions_training = dnn_regressor.predict(training_examples)\n",
    "\n",
    "  root_mean_squared_error_validation = math.sqrt(metrics.mean_squared_error(\n",
    "    predictions_validation, validation_targets))\n",
    "  root_mean_squared_error_training = math.sqrt(metrics.mean_squared_error(\n",
    "    predictions_training, training_targets))\n",
    "\n",
    "  root_mean_squared_errors_validation.append(root_mean_squared_error_validation)\n",
    "  root_mean_squared_errors_training.append(root_mean_squared_error_training)\n",
    "\n",
    "  print \"  period %02d : %3.2f\" % (period, root_mean_squared_error_training)\n",
    "\n",
    "# Output a graph of loss metrics over periods.\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xlabel(\"Periods\")\n",
    "plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "plt.plot(root_mean_squared_errors_training, label='training')\n",
    "plt.plot(root_mean_squared_errors_validation, label='validation')\n",
    "plt.legend()\n",
    "\n",
    "# Display some summary information.\n",
    "print \"Final RMSE (on training data):   %0.2f\" % root_mean_squared_error_training\n",
    "print \"Final RMSE (on validation data): %0.2f\" % root_mean_squared_error_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3d2ee",
   "metadata": {},
   "source": [
    "### Task 1: Normalize the features using linear scaling.\n",
    "\n",
    "**Normalize the inputs to the scale -1, 1.  You can use the helper method in the following cell.**\n",
    "\n",
    "**Spend about 5 minutes training and evaluating on the newly normalized data.  How well can you do?**\n",
    "\n",
    "As a rule of thumb, NN's train best when the input features are roughly on the same scale.\n",
    "\n",
    "It can be a good standard practice to normalize the inputs to fall within the range -1, 1. This helps SGD not get stuck taking steps that are too large in one dimension, or too small in another. Fans of numerical optimization may note that there's a connection to the idea of using a preconditioner here.\n",
    "\n",
    "Sanity check your normalized data.  (What would happen if you forgot to normalize one feature?)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
