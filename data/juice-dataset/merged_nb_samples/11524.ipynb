{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ab203",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(1,20,20)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe137c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ele = []\n",
    "y_ele = []\n",
    "\n",
    "for j in range(1,31):\n",
    "    hidden_dim = j\n",
    "    neunet = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "    neunet.fit(X,y,1000,lr=0.001)\n",
    "    cost = neunet.compute_cost(X,y)\n",
    "    x_ele.append(j)\n",
    "    y_ele.append(cost)\n",
    "\n",
    "plt.scatter(x_ele,y_ele)\n",
    "plt.xlabel('number of nodes in a hidden layer')\n",
    "plt.ylabel('cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1ad55",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ele = []\n",
    "y_ele = []\n",
    "\n",
    "for j in range(1,31):\n",
    "    hidden_dim = j\n",
    "    neunet = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "    neunet.fit(X,y,1000,lr=0.001)\n",
    "    \n",
    "    acc = 0\n",
    "    y_pred = neunet.predict(X)\n",
    "    for i in range(len(y_pred)):\n",
    "        con_mat[y_pred[i], y[i]] += 1\n",
    "        if y[i] == y_pred[i]:\n",
    "            acc += 1\n",
    "    acc = acc/len(y_pred)\n",
    "    \n",
    "    x_ele.append(j)\n",
    "    y_ele.append(acc)\n",
    "\n",
    "plt.scatter(x_ele,y_ele)\n",
    "plt.xlabel('number of nodes in a hidden layer')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4869c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "x_ele = []\n",
    "y_ele = []\n",
    "\n",
    "for j in range(1,31):\n",
    "    hidden_dim = j\n",
    "    neunet = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "    start = timeit.default_timer()\n",
    "    neunet.fit(X,y,1000,lr=0.001)\n",
    "    stop = timeit.default_timer()\n",
    "    time = (stop - start) \n",
    "    x_ele.append(j)\n",
    "    y_ele.append(time)\n",
    "\n",
    "plt.scatter(x_ele,y_ele)\n",
    "plt.xlabel('number of nodes in a hidden layer')\n",
    "plt.ylabel('running time for taining model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c415f9",
   "metadata": {},
   "source": [
    "From the first two plots above, I notice that when the number of nodes is less (or equal to) than five the model has relatively higher cost and lower accuracy. However, once the number of nodes is above 5, the cost decreases to a level and sticks around there as number of nodes increases. Similarly, once the number of nodes is above 5, the accuracy increases to a level and sticks around there as number of nodes increases. Therefore, I think I don't need to initialize the number of nodes in hidden layers as 20, maybe 5 or 6 is good enough. Along with the third plot which shows the more nodes I add, the longer it takes to train the model. I become more confident that I don't need to have 20 nodes in the hidden layer. That is too many."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb400580",
   "metadata": {},
   "source": [
    "## Step 6: Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6318dac",
   "metadata": {},
   "source": [
    "The effect of the number of nodes on a neural networks reminds me the problem of overfitting when we do modeling. Overfitting means that we use so many features as inputs that we build a model that fits too closely to the training dataset but it doesn't fit well to the testing dataset, or any other samples. \n",
    "\n",
    "According to the lecutre sildes, there are three ways to reduce overfitting:\n",
    "    1. Use “wrapper” to enumerate models h according to model size (e.g., number of nodes in neural net h). Select model with smallest error.\n",
    "    2. Feature selection: Simplify model by discarding irrelevant attributes (dimensionality reduction).\n",
    "    3. Minimum description length: Select model with smallest number of bits required to encode program and data.\n",
    " \n",
    "According to the Machine Learning course taught by Andrew Ng on Coursera, we can reduce overfitting by:\n",
    "    1. Reducing the number of features:\n",
    "        -manually select whihc features to keep \n",
    "        -use model selection algorithm\n",
    "    2. Regularization:\n",
    "        -keep all features but reduce magnitude/values of each parameters thetas; this works well when we have a lot of features and regularization makes each of them contribute a bit to pedict y. \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76624837",
   "metadata": {},
   "source": [
    "## Step 7: L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc676f88",
   "metadata": {},
   "source": [
    "Now I want to use L2 regulazrization based on my step 3 to see whether it can improve my 3-layer neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0a7fa",
   "metadata": {},
   "source": [
    "To compute L2 regularization, I need to add norm 2 penalizing term in my cost function and also make some change in my fit function under the NeuralNetwork class. So I have the NeuralNetwork_L2 class as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1076690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork_L2:\n",
    "    \"\"\"\n",
    "    This class implements a Logistic Regression Classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, Lambda):\n",
    "        \"\"\"\n",
    "        Initializes the parameters of the logistic regression classifer to \n",
    "        random values.\n",
    "        \n",
    "        args:\n",
    "            input_dim: Number of dimensions of the input data\n",
    "            output_dim: Number of classes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.theta1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "        self.bias1 = np.zeros((1, hidden_dim))\n",
    "        self.theta2 = np.random.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim)\n",
    "        self.bias2 = np.zeros((1, output_dim))\n",
    "        self.Lambda = Lambda\n",
    "        \n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    def compute_cost(self,X, y):\n",
    "        \"\"\"\n",
    "        Computes the total cost on the dataset.\n",
    "        \n",
    "        args:\n",
    "            X: Data array\n",
    "            y: Labels corresponding to input data\n",
    "        \n",
    "        returns:\n",
    "            cost: average cost per data sample\n",
    "        \"\"\"\n",
    "        num_examples = np.shape(X)[0]\n",
    "        z1 = np.dot(X,self.theta1) + self.bias1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = np.dot(a1,self.theta2) + self.bias2\n",
    "        exp_z = np.exp(z2)\n",
    "        softmax_scores = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \n",
    "        one_hot_y = np.zeros((num_examples,np.max(y)+1))\n",
    "        logloss = np.zeros((num_examples,))        \n",
    "        for i in range(np.shape(X)[0]):\n",
    "            one_hot_y[i,y[i]] = 1\n",
    "            logloss[i] = -np.sum(np.log(softmax_scores[i,:]) * one_hot_y[i,:])\n",
    "        data_loss = np.sum(logloss)/np.shape(X)[0] + (self.Lambda/2) * (np.sum(self.theta2**2) + np.sum(self.theta1**2))\n",
    "        return 1./num_examples * data_loss\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    " \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on current model parameters.\n",
    "        \n",
    "        args:\n",
    "            X: Data array\n",
    "            \n",
    "        returns:\n",
    "            predictions: array of predicted labels\n",
    "        \"\"\"\n",
    "        z1 = np.dot(X,self.theta1) + self.bias1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = np.dot(a1,self.theta2) + self.bias2\n",
    "        exp_z = np.exp(z2)\n",
    "        softmax_scores = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        predictions = np.argmax(softmax_scores, axis = 1)\n",
    "        return predictions\n",
    "        \n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    def fit(self,X,y,num_epochs,lr):\n",
    "        \"\"\"\n",
    "        Learns model parameters to fit the data.\n",
    "        \"\"\"  \n",
    "\n",
    "        for epoch in range(0, num_epochs):\n",
    "\n",
    "            # Forward propagation\n",
    "            z1 = np.dot(X,self.theta1) + self.bias1\n",
    "            a1 = np.tanh(z1)\n",
    "            z2 = np.dot(a1,self.theta2) + self.bias2\n",
    "            exp_z = np.exp(z2)\n",
    "            softmax_scores = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "            \n",
    "            # Backpropagation\n",
    "            beta2 = np.zeros_like(softmax_scores)\n",
    "            one_hot_y = np.zeros_like(softmax_scores)\n",
    "            for i in range(X.shape[0]):\n",
    "                one_hot_y[i,y[i]] = 1\n",
    "            beta2 = softmax_scores - one_hot_y\n",
    "            beta1 = np.dot(beta2, self.theta2.T) * (1 - np.power(a1, 2))\n",
    "    \n",
    "            # Compute gradients of model parameters\n",
    "            dtheta2 = np.dot(a1.T, beta2)/np.shape(X)[0] + self.Lambda * self.theta2\n",
    "            dbias2 = np.sum(beta2, axis=0)\n",
    "            dtheta1 = np.dot(X.T, beta1)/np.shape(X)[0] + self.Lambda * self.theta1\n",
    "            dbias1 = np.sum(beta1, axis=0)\n",
    "            \n",
    "            #dtheta1 += self.Lambda * self.theta1\n",
    "            #dtheta2 += self.Lambda * self.theta2\n",
    "            \n",
    "        \n",
    "            # Gradient descent parameter update\n",
    "            self.theta2 -= lr * dtheta2\n",
    "            self.bias2 -= lr * dbias2\n",
    "            self.theta1 -= lr * dtheta1\n",
    "            self.bias1 -= lr * dbias1\n",
    "            \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e001c70b",
   "metadata": {},
   "source": [
    "To better see how 2L regularization reduce overfitting, I split the nonliner dataset into trainning set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649d9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "y_pred = neunet2.predict(Xtest)\n",
    "con_mat = np.zeros((output_dim, output_dim))\n",
    "for i in range(len(y_pred)):\n",
    "    con_mat[y_pred[i], ytest[i]] += 1\n",
    "    if ytest[i] == y_pred[i]:\n",
    "        acc += 1\n",
    "acc = acc/len(y_pred)\n",
    "print ('ACCURACY: ', acc)\n",
    "print ('CONFUSION MATRIX: \\n', con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbcdd7",
   "metadata": {},
   "source": [
    "Compute the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = neunet2.compute_cost(Xtest,ytest)\n",
    "print (cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0410dd",
   "metadata": {},
   "source": [
    "Now we can see that by using L2 regularization, in terms of evaluating how my 3-layer nerual network fitting the testing dataset, the accuracy rising from 0.45 to 0.48, and the cost decreases from 0.92 to 0.00277. Therefore, 2L regularization indeed can reduce overfitting. Furthermore, we can change the value of Lambda to modify the penalizing level for each input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b213c9",
   "metadata": {},
   "source": [
    "## Step 8:  Hand-written Digits Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917fa7c4",
   "metadata": {},
   "source": [
    "In this last step of my project, I chanllenge myself to use the 3-layer neural network I build in previous steps to realize hand-written digits recognition. By having the pixels of the picture of each hand-written digit as inputs, I want my model to compute which number (0, 1, 2,..., 9) each input picture represents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4ff9a",
   "metadata": {},
   "source": [
    "Initialize the NeuralNetwork Class again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0860fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This class implements a Logistic Regression Classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initializes the parameters of the logistic regression classifer to \n",
    "        random values.\n",
    "        \n",
    "        args:\n",
    "            input_dim: Number of dimensions of the input data\n",
    "            output_dim: Number of classes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.theta1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "        self.bias1 = np.zeros((1, hidden_dim))\n",
    "        self.theta2 = np.random.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim)\n",
    "        self.bias2 = np.zeros((1, output_dim))\n",
    "        \n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    def compute_cost(self,X, y):\n",
    "        \"\"\"\n",
    "        Computes the total cost on the dataset.\n",
    "        \n",
    "        args:\n",
    "            X: Data array\n",
    "            y: Labels corresponding to input data\n",
    "        \n",
    "        returns:\n",
    "            cost: average cost per data sample\n",
    "        \"\"\"\n",
    "        num_examples = np.shape(X)[0]\n",
    "        z1 = np.dot(X,self.theta1) + self.bias1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = np.dot(a1,self.theta2) + self.bias2\n",
    "        exp_z = np.exp(z2)\n",
    "        softmax_scores = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \n",
    "        one_hot_y = np.zeros((num_examples,np.max(y)+1))\n",
    "        logloss = np.zeros((num_examples,))        \n",
    "        for i in range(np.shape(X)[0]):\n",
    "            one_hot_y[i,y[i]] = 1\n",
    "            logloss[i] = -np.sum(np.log(softmax_scores[i,:]) * one_hot_y[i,:])\n",
    "        data_loss = np.sum(logloss) \n",
    "        return 1./num_examples * data_loss\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    " \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on current model parameters.\n",
    "        \n",
    "        args:\n",
    "            X: Data array\n",
    "            \n",
    "        returns:\n",
    "            predictions: array of predicted labels\n",
    "        \"\"\"\n",
    "        z1 = np.dot(X,self.theta1) + self.bias1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = np.dot(a1,self.theta2) + self.bias2\n",
    "        exp_z = np.exp(z2)\n",
    "        softmax_scores = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        predictions = np.argmax(softmax_scores, axis = 1)\n",
    "        return predictions\n",
    "        \n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    def fit(self,X,y,num_epochs,lr):\n",
    "        \"\"\"\n",
    "        Learns model parameters to fit the data.\n",
    "        \"\"\"  \n",
    "\n",
    "        for epoch in range(0, num_epochs):\n",
    "\n",
    "            # Forward propagation\n",
    "            z1 = np.dot(X,self.theta1) + self.bias1\n",
    "            a1 = np.tanh(z1)\n",
    "            z2 = np.dot(a1,self.theta2) + self.bias2\n",
    "            exp_z = np.exp(z2)\n",
    "            softmax_scores = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "            \n",
    "            # Backpropagation\n",
    "            beta2 = np.zeros_like(softmax_scores)\n",
    "            one_hot_y = np.zeros_like(softmax_scores)\n",
    "            for i in range(X.shape[0]):\n",
    "                one_hot_y[i,y[i]] = 1\n",
    "            beta2 = softmax_scores - one_hot_y\n",
    "            beta1 = np.dot(beta2, self.theta2.T) * (1 - np.power(a1, 2))\n",
    "    \n",
    "            # Compute gradients of model parameters\n",
    "            dtheta2 = np.dot(a1.T, beta2)\n",
    "            dbias2 = np.sum(beta2, axis=0)\n",
    "            dtheta1 = np.dot(X.T, beta1)\n",
    "            dbias1 = np.sum(beta1, axis=0)\n",
    "        \n",
    "            # Gradient descent parameter update\n",
    "            self.theta2 -= lr * dtheta2\n",
    "            self.bias2 -= lr * dbias2\n",
    "            self.theta1 -= lr * dtheta1\n",
    "            self.bias1 -= lr * dbias1\n",
    "            \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e2981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    \"\"\"\n",
    "    Function to print the decision boundary given by model.\n",
    "    \n",
    "    args:\n",
    "        model: model, whose parameters are used to plot the decision boundary.\n",
    "        X: input data\n",
    "        y: input labels\n",
    "    \"\"\"\n",
    "    \n",
    "    x1_array, x2_array = np.meshgrid(np.arange(-4, 4, 0.01), np.arange(-4, 4, 0.01))\n",
    "    grid_coordinates = np.c_[x1_array.ravel(), x2_array.ravel()]\n",
    "    Z = model.predict(grid_coordinates)\n",
    "    Z = Z.reshape(x1_array.shape)\n",
    "    plt.contourf(x1_array, x2_array, Z, cmap=plt.cm.bwr)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.bwr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168255f",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebfe164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set \n",
    "X_train = np.genfromtxt('DATA/Digit_X_train.csv', delimiter=',')\n",
    "y_train = np.genfromtxt('DATA/Digit_y_train.csv', delimiter=',')\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "#testing set\n",
    "X_test = np.genfromtxt('DATA/Digit_X_test.csv', delimiter=',')\n",
    "y_test = np.genfromtxt('DATA/Digit_y_test.csv', delimiter=',')\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc36e3",
   "metadata": {},
   "source": [
    "Initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e93d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.shape(X_train)[1]\n",
    "output_dim = np.max(y_train) + 1\n",
    "hidden_dim = 20 #set number of hidden nodes\n",
    "neunet = NeuralNetwork(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e314c0",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neunet.fit(X_train,y_train,lr=0.001)\n",
    "y_pred = neunet.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d4d07",
   "metadata": {},
   "source": [
    "Compute accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ea09c",
   "metadata": {},
   "source": [
    "This seems that I can use a straight line to seperate two classes, but I will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f46aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.shape(X)[1]\n",
    "output_dim = np.max(y) + 1\n",
    "logreg = LogisticRegression(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7babd",
   "metadata": {},
   "source": [
    "I initialize the model and check the cost, accuracy, and decision boundary for a first glance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c023dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(logreg, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96605ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = logreg.compute_cost(X,y)\n",
    "print (cost)\n",
    "acc = 0\n",
    "y_pred = logreg.predict(X)\n",
    "con_mat = np.zeros((output_dim, output_dim))\n",
    "for i in range(len(y_pred)):\n",
    "    con_mat[y_pred[i], y[i]] += 1\n",
    "    if y[i] == y_pred[i]:\n",
    "        acc += 1\n",
    "acc = acc/len(y_pred)\n",
    "print ('ACCURACY: ', acc)\n",
    "print ('CONFUSION MATRIX: \\n', con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0d0d6",
   "metadata": {},
   "source": [
    "The decision_boundary shows a very bad model. The cost per sample is 1.91 which is the almost the double of (1-0) and the accuray is 0.128 out of 1 which is very low\n",
    "\n",
    "Now I need to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed04447",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X,y,1000,lr=0.001)\n",
    "plot_decision_boundary(logreg, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "y_pred = logreg.predict(X)\n",
    "con_mat = np.zeros((output_dim, output_dim))\n",
    "for i in range(len(y_pred)):\n",
    "    con_mat[y_pred[i], y[i]] += 1\n",
    "    if y[i] == y_pred[i]:\n",
    "        acc += 1\n",
    "acc = acc/len(y_pred)\n",
    "print ('ACCURACY: ', acc)\n",
    "print ('CONFUSION MATRIX: \\n', con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b1c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = logreg.compute_cost(X,y)\n",
    "print (cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8725678",
   "metadata": {},
   "source": [
    "Now I can see most of the blue dots and red dots are classified seperately in two groups.\n",
    "The accuracy rises to 0.93 and the cost decreases to 0.2. I think I build a very good 2-layer neural network without hiddden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f58a5",
   "metadata": {},
   "source": [
    "## Step 2: Nonlinear data in 2-Layer Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279134b",
   "metadata": {},
   "source": [
    "Now I'm considering whether this kind of 2-layer neural network without hiddden layer can predict nonliner data as well. So we do the same thing above for the nonliner dataset ( DATA/NonlinearX.csv, DATA/NonlinearY.csv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac90495",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = neunet.compute_cost(X,y)\n",
    "print (cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79acd0",
   "metadata": {},
   "source": [
    "By looking at the decision boundary produced by our 3-layer neural network, I find that it's very different with the one generated by the 2-layer neural network. The one produced by my 3-layer neural network has a specific red region to classify the few red dots above the blue dots but my 2-layer neural network doesn't have that. Furthermore, I find that the accuracy rises and the cost decreases. So I think that adding a hidden layer into a neural network can make a better model to do classificaiton. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe790d2e",
   "metadata": {},
   "source": [
    "Then I decide to train the 3-layer neural network for the nonlinear dataset to see whether I can have the same conclusion we get above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ffb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('DATA/NonlinearX.csv', delimiter=',') \n",
    "y = np.genfromtxt('DATA/NonlinearY.csv', delimiter=',').astype(np.int64)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d09fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = np.shape(X)[1]\n",
    "output_dim = np.max(y) + 1\n",
    "hidden_dim = 20 #set number of hidden nodes\n",
    "neunet = NeuralNetwork(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf05839",
   "metadata": {},
   "outputs": [],
   "source": [
    "neunet.fit(X,y,1000,lr=0.001)\n",
    "plot_decision_boundary(neunet, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8061697",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "y_pred = neunet.predict(X)\n",
    "con_mat = np.zeros((output_dim, output_dim))\n",
    "for i in range(len(y_pred)):\n",
    "    con_mat[y_pred[i], y[i]] += 1\n",
    "    if y[i] == y_pred[i]:\n",
    "        acc += 1\n",
    "acc = acc/len(y_pred)\n",
    "print ('ACCURACY: ', acc)\n",
    "print ('CONFUSION MATRIX: \\n', con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e27a92",
   "metadata": {},
   "source": [
    "Compared with the decison boundary produced by our 2-layer neural network in Step 2, our 3-layer neural network computes a nonlinear decison boundary that can better seperate the two classe. The accuracy rises to 0.968 and the cost decreases to 0.08. \n",
    "\n",
    "Therefore, I conclude that a 3-layer neural network with one hidden layer can learn non-linear decision boundary better than a 2-layer neural network without hidden layer, and the former can produce higher accuracy and lower cost than the latter. \n",
    "\n",
    "The main reason is that sometimes a neural network can't derictly understand the inputs well and adding hidden layers can transform original inputs into more useful inputs for different activition funcitons that can produce more accurate output. For example we want to build a picture classifier to determine whether the input picture is a bus, if we have wheel detector (to help tell you it's a vehicle) and a box detector (since the bus is shaped like a big box) and a size detector (to tell you it's too big to be a car) as nodes in a hidden layer, they can help the model better indentify busses.\n",
    "\n",
    "Reference: https://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c45be4",
   "metadata": {},
   "source": [
    "## Step 4: The Effect ot Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674af79d",
   "metadata": {},
   "source": [
    "Now I'm interested in what effect the learning rate has on the 3-layer neural network I trained. I choose to use the nonlinear dataset and I want to see how the cost and the accuracy change by varying the value of learning rate. I will use for loops to assign different learning rate and compute each cost and accuracy the model computs, and then plot them to see the trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e0d230",
   "metadata": {},
   "source": [
    "Load the nonlinear data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04469e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('DATA/NonlinearX.csv', delimiter=',') \n",
    "y = np.genfromtxt('DATA/NonlinearY.csv', delimiter=',').astype(np.int64)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)\n",
    "\n",
    "input_dim = np.shape(Xtrain)[1]\n",
    "output_dim = np.max(ytrain) + 1\n",
    "hidden_dim = 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254bc79",
   "metadata": {},
   "source": [
    "To illustrate how 2L works and reduces overfitting, I will use the unchanged NeuralNetwork class the compute the model's accuracy and cost on testing set. Then I will use the modified NeuralNetwork_L2 class the get the accuracy and cost, and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c0843",
   "metadata": {},
   "source": [
    "Train the model under unchanged NeuralNetwork class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb041b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "neunet = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "neunet.fit(Xtrain,ytrain,1000,lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e7aa3",
   "metadata": {},
   "source": [
    "Compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccdb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "y_pred = neunet.predict(Xtest)\n",
    "con_mat = np.zeros((output_dim, output_dim))\n",
    "for i in range(len(y_pred)):\n",
    "    con_mat[y_pred[i], ytest[i]] += 1\n",
    "    if ytest[i] == y_pred[i]:\n",
    "        acc += 1\n",
    "acc = acc/len(y_pred)\n",
    "print ('ACCURACY: ', acc)\n",
    "print ('CONFUSION MATRIX: \\n', con_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfe24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = neunet.compute_cost(Xtest,ytest)\n",
    "print (cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4cb96",
   "metadata": {},
   "source": [
    "Train the model under modified NeuralNetwork_L2 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298cd1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "con_mat = np.zeros((output_dim, output_dim))\n",
    "for i in range(len(y_pred)):\n",
    "    con_mat[y_pred[i], y_test[i]] += 1\n",
    "    if y_test[i] == y_pred[i]:\n",
    "        acc += 1\n",
    "acc = acc/len(y_pred)\n",
    "\n",
    "print ('ACCURACY: ', acc)\n",
    "print ('CONFUSION MATRIX: \\n', con_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c760709",
   "metadata": {},
   "source": [
    "As the above shows, my model has the accuracy of about 0.95 which is very close to 1, and in the confusion matrix, most of the numbers are lying on the diagonal. Therefore, I can conclude that my 3-layer neural network can work very well for hand-written digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9232be1c",
   "metadata": {},
   "source": [
    "Furthermore, by the following code, I can show the images of first five inputs in testing set and their predicted output and actual output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7c4690",
   "metadata": {},
   "source": [
    "### Q: Give a concise description of current problem. What needs to be solved? Why is the result useful? Do you make any assumptions? What are the anticipated difficulties?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493974b4",
   "metadata": {},
   "source": [
    "After having learned the basic knowledge of Logistic Regression and Neural Network in class, I'm interested in using real datasets to build simple neural networks. Now I get three data sets, the linear one, the nonlinear one, and the hand-written recognition one for challenge. In this project, I will study on how the number of hidden layers, the number of nodes in hidden layers, and the value of learning rate affect the performance of a neural network. This is very important because neural network can greatly help us to build models and make predictions, and I think these three factors are crucial for the accuracy of prediction. \n",
    "\n",
    "Before I start the project, I hypothesize that adding hidden layers into a neural network can improve the accuracy of a prediction, and adding more nodes into one hidden layer and lowering learning rate can also improve the performance of a neural network. My conclusion maybe different with my hypothesis.\n",
    "\n",
    "Furthermore, the anticipated difficulties are how to implement forward propagation, back propagation, cost function, etc., in programming language in Python, and how to display the effects of different parameters on a neural network. Also, I'm worried about the underlying problem of overfitting. I will try to use L2 regularization to deal with it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049a796",
   "metadata": {},
   "source": [
    "# Method and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1496505",
   "metadata": {},
   "source": [
    "### Give a concise description of the implemented method. For example, you might describe the motivation of your idea, the algorithmic steps of your methods, or the mathematical formulation of your method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46e7ab",
   "metadata": {},
   "source": [
    "### Briefly outline the functions you created in your code to carry out the algorithmic steps you described earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334014be",
   "metadata": {},
   "source": [
    "\n",
    "To build a neural network, I need to do the following few main steps:\n",
    "\t1. Load and visualize the data\n",
    "\t2. Initialize the weights of inputs and bias in each layer\n",
    "\t3. Initialize the model \n",
    "\t4. Calculate the cost of the model and visualize the decision boundary\n",
    "    5. Use forward and back propagation under gradient descent to train the model\n",
    "\t6. Visualize the decision boundary again\n",
    "\t7. Compute confusion matrix with accuracy of the model and also the cost again\n",
    "\n",
    "The main functions I need are weight initialization function, cost function, predict function, fit function, and decision boundary function. Furthermore, to add hidden layer into a neural work, I need to make changes on every function we have except the function for plotting decision boundary.\n",
    "\n",
    "To see how the number of nodes in a hidden layer and learning rate affect a model, I decide to build for loops to compute the accuracy and cost of a model at different number of nodes and different values of learning rate for several times and compute each average, then make plots for better visualization. \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebbc75",
   "metadata": {},
   "source": [
    "# Experiments & Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67142103",
   "metadata": {},
   "source": [
    "###  Describe your experiments, including the number of tests that you performed, and the relevant parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd56f89",
   "metadata": {},
   "source": [
    "### Define your evaluation metrics, e.g., detection rates, accuracy, running time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ac4e8",
   "metadata": {},
   "source": [
    "### List your experimental results. Provide examples of input images and output images. If relevant, you may provide images showing any intermediate steps. If your work involves videos, do not submit the videos but only links to them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0288b4",
   "metadata": {},
   "source": [
    "## Step 1: Linear data in 2-Layer Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ccdf9",
   "metadata": {},
   "source": [
    "First, I use linear dataset (DATA/LinearX.csv, DATA/LinearY.csv) to build a 2-layer neural network (no hidden layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325a142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python libraries\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a00c5f",
   "metadata": {},
   "source": [
    "The followings are the main functions I build for the 2-layer neural network without hidden layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f37771",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    This lab implements a Logistic Regression Classifier.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initializes the parameters of the logistic regression classifer to \n",
    "        random values.\n",
    "        \n",
    "        args:\n",
    "            input_dim: Number of dimensions of the input data\n",
    "            output_dim: Number of classes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.theta = np.random.randn(input_dim, output_dim) / np.sqrt(input_dim)       \n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        \n",
    "    #--------------------------------------------------------------------------\n",
    "    \n",
    "    def compute_cost(self,X, y):\n",
    "        \"\"\"\n",
    "        Computes the total cost on the dataset.\n",
    "        \n",
    "        args:\n",
    "            X: Data array\n",
    "            y: Labels corresponding to input data\n",
    "        \n",
    "        returns:\n",
    "            cost: average cost per data sample\n",
    "        \"\"\"\n",
    "        num_examples = np.shape(X)[0]\n",
    "        z = np.dot(X,self.theta) + self.bias\n",
    "        exp_z = np.exp(z)\n",
    "        softmax_scores = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \n",
    "        one_hot_y = np.zeros((num_examples,np.max(y)+1))\n",
    "        logloss = np.zeros((num_examples,))        \n",
    "        for i in range(np.shape(X)[0]):\n",
    "            one_hot_y[i,y[i]] = 1\n",
    "            logloss[i] = -np.sum(np.log(softmax_scores[i,:]) * one_hot_y[i,:])\n",
    "        data_loss = np.sum(logloss)\n",
    "        return 1./num_examples * data_loss\n",
    "\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    " \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Makes a prediction based on current model parameters.\n",
    "        \n",
    "        args:\n",
    "            X: Data array\n",
    "            \n",
    "        returns:\n",
    "            predictions: array of predicted labels\n",
    "        \"\"\"\n",
    "        z = np.dot(X,self.theta) + self.bias\n",
    "        exp_z = np.exp(z)\n",
    "        softmax_scores = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        predictions = np.argmax(softmax_scores, axis = 1)\n",
    "        return predictions\n",
    "        \n",
    "    #--------------------------------------------------------------------------\n",
    "    # TODO: implement logistic regression using gradient descent \n",
    "    #--------------------------------------------------------------------------\n",
    "   \n",
    "    def fit(self,X,y,num_epochs,lr):\n",
    "        for epoch in range(0, num_epochs):\n",
    "\n",
    "            # Forward propagation\n",
    "            z = np.dot(X,self.theta) + self.bias\n",
    "            exp_z = np.exp(z)\n",
    "            softmax_scores = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "        \n",
    "            # Backpropagation\n",
    "            beta = np.zeros_like(softmax_scores)\n",
    "            one_hot_y = np.zeros_like(softmax_scores)\n",
    "            for i in range(X.shape[0]):\n",
    "                one_hot_y[i,y[i]] = 1\n",
    "            beta = softmax_scores - one_hot_y\n",
    "    \n",
    "            # Compute gradients of model parameters\n",
    "            dtheta = np.dot(X.T,beta)\n",
    "            dbias = np.sum(beta, axis=0)\n",
    "    \n",
    "            # Gradient descent parameter update\n",
    "            self.theta -= lr * dtheta\n",
    "            self.bias -= lr * dbias\n",
    "            \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea7cb9f",
   "metadata": {},
   "source": [
    "Load and plot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085b19b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('DATA/linearX.csv', delimiter=',') \n",
    "y = np.genfromtxt('DATA/linearY.csv', delimiter=',').astype(np.int64)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.bwr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('DATA/NonlinearX.csv', delimiter=',') \n",
    "y = np.genfromtxt('DATA/NonlinearY.csv', delimiter=',').astype(np.int64)\n",
    "input_dim = np.shape(X)[1]\n",
    "output_dim = np.max(y) + 1\n",
    "hidden_dim = 20 #set number of hidden nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df88c5",
   "metadata": {},
   "source": [
    "I choose learning rates as 100 numbers from 0.0001 to 0.01 (like 0.0001, 0.0002, ..., 0.0099, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a253254",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0.0001,0.01,100)\n",
    "\n",
    "# record each learning rate and each cost\n",
    "x_ele = []\n",
    "y_ele = []\n",
    "\n",
    "for i in a:\n",
    "    neunet = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "    neunet.fit(X,y,1000,lr=i)\n",
    " \n",
    "    cost = neunet.compute_cost(X,y)\n",
    "    x_ele.append(i)\n",
    "    y_ele.append(cost)\n",
    "\n",
    "plt.scatter(x_ele,y_ele)\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ec6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0.0001,0.01,100)\n",
    "\n",
    "# record each learning rate and each accuracy\n",
    "x_ele = []\n",
    "y_ele = []\n",
    "\n",
    "for j in a:\n",
    "    neunet = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "    neunet.fit(X,y,1000,lr=j)\n",
    "    \n",
    "    acc = 0\n",
    "    y_pred = neunet.predict(X)\n",
    "    for i in range(len(y_pred)):\n",
    "        con_mat[y_pred[i], y[i]] += 1\n",
    "        if y[i] == y_pred[i]:\n",
    "            acc += 1\n",
    "    acc = acc/len(y_pred)\n",
    "    \n",
    "    x_ele.append(j)\n",
    "    y_ele.append(acc)\n",
    "    \n",
    "plt.scatter(x_ele,y_ele)\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "a = np.linspace(0.0001,0.01,100)\n",
    "x_ele = []\n",
    "y_ele = []\n",
    "\n",
    "for j in a:\n",
    "    neunet = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "    start = timeit.default_timer()\n",
    "    neunet.fit(X,y,1000,lr= j)\n",
    "    stop = timeit.default_timer()\n",
    "    time = (stop - start) \n",
    "    x_ele.append(j)\n",
    "    y_ele.append(time)\n",
    "\n",
    "plt.scatter(x_ele,y_ele)\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('running time for taining model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab11f42",
   "metadata": {},
   "source": [
    "From the first two plots above, I find that when the learning rate gets larger, the cost will get larger (seems exponentially) and the accuracy diverges more (higher probability of having a lower accuracy). \n",
    "\n",
    "According to the Machine Learning course taught by Andrew Ng on Coursera, the reason why high learn rate results very high cost and low accuracy is that, when we training a model, gradient descent can overshoot the minimum if high learning rate is too high. It may fail to converge to the minimum or even diverge. Therefore, we should keep rate low enough. However, if the learning rate is too low, the running time for gradient descent to find minimum will be too long (this is also why in the third plot, the running time could be very high as learning rate becomes very close to zero). So we should keep learning rate neither too high nor too low. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42522e0d",
   "metadata": {},
   "source": [
    "## Step 5: The Effect of Number of Nodes in a Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1440f54",
   "metadata": {},
   "source": [
    "To move on, now I want to analyze how the number of nodes in a hidden effect the 3-layer neural network I trained.\n",
    "\n",
    "I still use the nonlinear dataset and use for loops to compute neural networks with different numbers of node, then see how the decision boundary, accuracy, cost, and running time change. "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
