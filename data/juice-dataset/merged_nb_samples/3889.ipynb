{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8709c643",
   "metadata": {},
   "source": [
    "## Luke Waninger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ef8b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source import *\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "exp   = np.exp\n",
    "ident = np.identity\n",
    "na    = np.newaxis\n",
    "norm  = np.linalg.norm\n",
    "np.random.seed(42)\n",
    "\n",
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(f'cores to utilize:   {cpu_count} of {multiprocessing.cpu_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24423159",
   "metadata": {},
   "source": [
    "### The gradient $\\triangledown F(\\alpha)$ of $F$\n",
    "for a kernel based support vector machine can be shown to be..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10a7a62",
   "metadata": {},
   "source": [
    "$\\triangledown F(\\alpha) = 1/n \\sum_{i=1}^n \\triangledown l(y_i, (K\\alpha)_i) + 2\\lambda K \\alpha$\n",
    "\n",
    "\n",
    "\n",
    "$\n",
    "\\triangledown l(y,t) =\n",
    "\t\\begin{cases}\n",
    "        0 \\hspace{55pt}  yt > 1 + h \\\\\n",
    "        -y_i x_i \\frac{1+h-yt}{2h} \\hspace{18pt}  |1-yt| \\le h \\\\\n",
    "        -y_i x_i \\hspace{40pt}  yt < 1-h\n",
    "\t\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7dc721",
   "metadata": {},
   "source": [
    "### Computing the graham and kernel matrices for a Gaussian (RBF) and polynomial kernel\n",
    "I decided to encapsulate these functions into one class each for radial and polynomial kernels for readability and code reuse. The $\\texttt{compute}$ function calculates and returns the requested kernel for any set of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd87870",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel(object):\n",
    "    \"\"\"tiny base class for SVM kernels\"\"\"\n",
    "    def compute(self):\n",
    "        yield\n",
    "\n",
    "\n",
    "class k_radialrbf(Kernel):\n",
    "    def __init__(self, sigma):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'rbf({self.sigma})'\n",
    "\n",
    "    def compute(self, x, xp=None):\n",
    "        sigma = self.sigma\n",
    "        xp = x if xp is None else xp\n",
    "\n",
    "        def norm(mat):\n",
    "            return np.linalg.norm(mat, axis=1)\n",
    "\n",
    "        return exp(-1/(2*sigma**2) * ((norm(x)** 2)[:, na] + (norm(xp)**2)[na, :]-2*(x @ xp.T)))\n",
    "\n",
    "\n",
    "class k_polynomial(Kernel):\n",
    "    def __init__(self, degree, b=1.):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.b = b\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'polynomial({self.degree})'\n",
    "\n",
    "    def compute(self, x, xp=None):\n",
    "        xp = x if xp is None else xp\n",
    "\n",
    "        return (x @ xp.T + self.b)**self.degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be9eac5",
   "metadata": {},
   "source": [
    "### Using scikit learn's builtin digits dataset, download and standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f729f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "scalar  = StandardScaler().fit(x_train)\n",
    "x_train = scalar.transform(x_train)\n",
    "x_test  = scalar.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adece61a",
   "metadata": {},
   "source": [
    "### The support vector machine: $\\texttt{mysvm}$\n",
    "Again, I chose to implement this using OOP so I could encapsulate the SVM, and other helper functions in a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1794a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySVM(Estimator):\n",
    "    \"\"\"kernel based Support Vector Machine\"\"\"\n",
    "    \n",
    "    def __init__(self, kernel):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kernel (Kernel): to use for transforming the input space\n",
    "\n",
    "        \"\"\"\n",
    "        self.kernel = kernel\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'SVM(kernel={self.kernel})'\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(k, y, beta, l, h=0.5):\n",
    "        \"\"\"compute the gradient\n",
    "        \n",
    "        Args:\n",
    "            k (nXn ndarray): computed kernel matrix\n",
    "            y (1Xn ndarray): true labels\n",
    "            beta (1Xn ndarray): weight coefficients\n",
    "            l (float): regularization coefficient\n",
    "            h (float): [optional, 0 < h < 1] hyperparameter\n",
    "            \n",
    "        Returns:\n",
    "            1Xn ndarray gradient to apply\n",
    "        \"\"\"\n",
    "        n, d = k.shape\n",
    "        lg = np.zeros([n, d])\n",
    "\n",
    "        yk = y *(k @ beta)\n",
    "        mask = np.abs(1 - yk)\n",
    "\n",
    "        lg[mask <= h] = ((1/(2*h)) * ((1 + h-yk)[:, na]) * (-y[:, na] * k))[mask <= h]\n",
    "        lg[yk < 1-h]  = (-y[:, na] * k)[yk < 1-h]\n",
    "\n",
    "        return np.array(np.sum(lg, axis=0)/n + 2*l*beta)\n",
    "    \n",
    "    def fgrad(self, k, y, l, eta=1., max_iter=100):\n",
    "        \"\"\"fast gradient descent\n",
    "        \n",
    "        Args:\n",
    "            k (nXn ndarray): computer kernel matrix\n",
    "            y (1Xn ndarray): true labels\n",
    "            l (float): regularization coefficient\n",
    "            eta (float): [optional, 0 < eta < 1] learning rate\n",
    "            max_iter (int): [optional, max_iter > 1]: maximum learning iterations\n",
    "            \n",
    "        Returns:\n",
    "            1Xn ndarray of optimized weight coefficients\n",
    "        \"\"\"\n",
    "        n, d  = k.shape\n",
    "        b0    = np.zeros(d)\n",
    "        theta = np.copy(b0)\n",
    "        grad  = self.gradient(k, y, b0, l)\n",
    "        i = 0\n",
    "        \n",
    "        # continue optimizing until either the max iterations is reached\n",
    "        # or gradient stops moving\n",
    "        while i < max_iter and not np.isclose(0, eta):\n",
    "            eta = backtracking(k, y, b0, l, eta, self.gradient, self.objective)\n",
    "\n",
    "            b1 = theta - eta*grad\n",
    "            theta = b1 + (i/(i+3))*(b1-b0)\n",
    "            grad  = self.gradient(k, y, theta, l)\n",
    "            b0 = b1\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return b0\n",
    "    \n",
    "    @staticmethod\n",
    "    def objective(k, y, l, beta, h=0.5):\n",
    "        \"\"\"objective function\n",
    "        \n",
    "        Args:\n",
    "            k (nXn ndarray): computed kernel matrix\n",
    "            y (1Xn ndarray): true labels\n",
    "            beta (1Xn ndarray): weight coefficients\n",
    "            l (float): regularization coefficient\n",
    "            h (float): [optional, 0 < h < 1] hyperparameter\n",
    "            \n",
    "        Returns:\n",
    "            float, loss\n",
    "        \"\"\"\n",
    "        n, d = k.shape\n",
    "        loss = np.zeros(n)\n",
    "        yk = y * (k @ beta)\n",
    "        mask = np.abs(1 - yk)\n",
    "\n",
    "        loss[mask <= h] = ((1 + h-yk)**2 / (4*h))[mask <= h]\n",
    "        loss[yk < 1-h] = (1 - yk)[yk < 1-h]\n",
    "\n",
    "        return np.sum(loss)/n + l*norm(beta)**2\n",
    "\n",
    "    def predict(self, kp, beta):\n",
    "        \"\"\"predict labels\n",
    "        \n",
    "        Args:\n",
    "            kp (nXn ndarray): computed kernel matrix\n",
    "            beta (1Xn ndarray): weight coefficients\n",
    "        \n",
    "        Returns:\n",
    "            1Xn ndarray of predicted labels\n",
    "        \"\"\"\n",
    "        return [1 if ki @ beta.T > 0 else -1 for ki in kp]\n",
    "    \n",
    "    def predict_proba(self, kp, beta):\n",
    "        \"\"\"predict probabilities\n",
    "        \n",
    "        Args:\n",
    "            kp (nXn ndarray): computed kernel matrix\n",
    "            beta (1Xn ndarray): weight coefficients\n",
    "        \n",
    "        Returns:\n",
    "            1Xn ndarray of predicted probabilities\n",
    "        \"\"\"\n",
    "        return [ki @ beta.T for ki in kp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e05ad3",
   "metadata": {},
   "source": [
    "### Training the SVM with the huberized hinge loss and an order 7 polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4218096",
   "metadata": {},
   "source": [
    "running one vs rest with a polynomial kernel of degree 7 with $\\lambda$=1 gives a horrible validation error: $\\approx$ 0.518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69bf31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernl = k_polynomial(7)\n",
    "OVR(MySVM(kernl), n_jobs=-1).fit(x_train, y_train, x_test, y_test, 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02f6b1",
   "metadata": {},
   "source": [
    "Using cross-validation we see a performance improvement of around 9%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr = OVR(MySVM(kernl), n_jobs=-1)\n",
    "cv(x_train, y_train, ovr, eargs=np.linspace(.001, 1., 5), nfolds=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c60e19",
   "metadata": {},
   "source": [
    "### Comparing the performance of kernel SVMs\n",
    "It quickly becomes clear the 7-degree polynomial kernel is a horrible choice. Below, I run a series of OVR polynomial and radial kernels that all show much better performance. "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
