{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78897e2",
   "metadata": {},
   "source": [
    "## Portfolio Optimalization - Asset Management\n",
    "#### Arno Goedhuys /  r0636556 / 29 juni 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ad529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as wb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "from scipy.stats import norm\n",
    "from scipy import interpolate\n",
    "from cvxopt import matrix\n",
    "from cvxopt.blas import dot\n",
    "from cvxopt.solvers import qp, options\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0bcd85",
   "metadata": {},
   "source": [
    "#### Downloading the data\n",
    "The data is downloaded from google finance. \n",
    "<p> The time interval runs from the first of january 2009 to the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842e1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = ['FOX','HAS','DISCA','MAR','NVDA','VOD','SBUX','GOOGL','ADBE','TRV','UNH','VZ','WMT','GS','DD','AXP','AAPL','MSFT','AMZN','YHOO','KO','CSCO','JPM','UTX','V','MCD','DIS', 'NKE','INTC','GE', 'PG', 'JNJ','HD','IBM','MMM']\n",
    "pf_data = pd.DataFrame()\n",
    "\n",
    "for asset in assets:\n",
    "    pf_data[asset] = wb.DataReader(asset, data_source = 'google', start='2009-1-1')['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22902d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stock(list_of_new_stocks):\n",
    "    for asset in list_of_new_stocks:\n",
    "        assets.append(asset)\n",
    "        pf_data[asset] = wb.DataReader(asset, data_source = 'google', start='2009-1-1')['Close']\n",
    "    return pf_data\n",
    "pf_data = add_stock(['CHKP','CA','COST','BIDU'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_data = add_stock(['DLTR','EA','EBAY','EXPE','FAST','HOLX','GILD','TSCO','TXN','STX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_data = add_stock(['ULTA','XRAY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33784923",
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_assets = len(pf_data.columns)\n",
    "print 'amount of assets: ', amount_of_assets\n",
    "amount_of_trading_days = len(pf_data.index)\n",
    "pf_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c656cf",
   "metadata": {},
   "source": [
    "#### Normalize data\n",
    "Devide each value of each asset by its initial value and multipy by 100 so each asset starts with a value of 100. <p> Plot the evolution of each asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814eec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_risk_weights = convert_weights(convex_optimizer_weights(Covariance_matrix, daily_returns, 0))\n",
    "return_minimal_risk = portfolio_return(daily_returns, minimal_risk_weights)\n",
    "print return_minimal_risk, #minimal_risk_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d9ebd",
   "metadata": {},
   "source": [
    "#### The optimizations\n",
    "The optimization for the minimal risk portfolio. The extra constraints that each weight lies between 0 and 0.5 are set by the \n",
    "bounds attribute. The optimization used is the sequantial least squares programming. The site PyOpt explains it as:\n",
    "> SLSQP optimizer is a sequential least squares programming algorithm which uses the Han–Powell quasi–Newton method with a BFGS update of the B–matrix and an L1–test function in the step–length algorithm. The optimizer uses a slightly modified version of Lawson and Hanson’s NNLS nonlinear least-squares solver. \n",
    ">\n",
    "> [PyOpt](http://www.pyopt.org/reference/optimizers.slsqp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0877ff4d",
   "metadata": {},
   "source": [
    "The optimization for the maximal sharp portfolio. The same algorithm is used as with the minimal risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97db620",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximal_sharp_weights = optimize.fmin_slsqp(sharp, amount_of_assets*[1./float(amount_of_assets),],eqcons=[constraint1],\n",
    "                     bounds=gen_bounds(amount_of_assets))\n",
    "return_maximal_sharp = portfolio_return(daily_returns, maximal_sharp_weights)\n",
    "risk_maximal_sharp = portfolio_volatility(Covariance_matrix, np.array(maximal_sharp_weights))\n",
    "print return_maximal_sharp, risk_maximal_sharp,# maximal_sharp_weights, \n",
    "plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(111)\n",
    "indeces = [x for x in range(len(assets))]\n",
    "indeces2 = [x+0.2 for x in range(len(assets))]\n",
    "ax.bar(indeces, maximal_sharp_weights,0.2)\n",
    "ax.bar(indeces2, minimal_risk_weights,0.2)\n",
    "ax.set_xticks(indeces)\n",
    "ax.set_xticklabels(assets)\n",
    "ax.set_ylabel(\"Weight\")\n",
    "ax.set_xlabel(\"Asset\")\n",
    "ax.set_title('Weight of each asset in optimal portfolios')\n",
    "plt.setp(plt.xticks()[1], rotation=90)\n",
    "plt.legend(['Maximal Sharp Weights', 'Minimal Risk Weights'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528294a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Efficient_Frontier(C_matrix, D_returns, amount_of_points, plot=True):\n",
    "    opt_returns = []\n",
    "    opt_risks = []\n",
    "    optimum_weights = []\n",
    "    minimal_risk_return = portfolio_return(D_returns, np.array(convert_weights(convex_optimizer_weights(C_matrix, D_returns, 0))))\n",
    "    for i in range(amount_of_points):\n",
    "        optimum_weights = []\n",
    "        opt_weights = convex_optimizer_weights(C_matrix, D_returns, minimal_risk_return+i*(max_return(D_returns)-minimal_risk_return-0.03)/amount_of_points)\n",
    "        #opt_weights = np.array(opt_weights)\n",
    "        optimum_weights = convert_weights(opt_weights)\n",
    "        opt_returns.append(portfolio_return(D_returns, optimum_weights))\n",
    "        optimum_weights = np.array(optimum_weights)\n",
    "        #print optimum_weights\n",
    "        #opt_risks.append(np.sqrt(opt_weights[1]))\n",
    "        opt_risks.append(portfolio_volatility(C_matrix, optimum_weights))\n",
    "    f = interpolate.interp1d(opt_risks, opt_returns, kind='cubic') \n",
    "    xnew = np.arange(opt_risks[0],opt_risks[-2],(opt_risks[-2]-opt_risks[0])/amount_of_points)\n",
    "    ynew = f(xnew)\n",
    "    plt.title(\"Efficient Frontier\")\n",
    "    plt.xlabel('Expected Volatility')\n",
    "    plt.ylabel('Expected Return')\n",
    "    if plot:\n",
    "        plt.plot(xnew, ynew, 'k', linewidth=2.0)\n",
    "    return f\n",
    "Efficient_Frontier(Covariance_matrix, daily_returns, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b96d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Efficient_Frontier(Rf_Covariance_matrix, new_daily_returns, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b41e69",
   "metadata": {},
   "source": [
    "#### The results\n",
    "The two optimized portfolios are plotted on a graph with the x axis the expected risk and the y axis the expected return. To illustrate that these are valid values 25000 other portfolio's with random weights are also plotted on the same graph. Out of the 25000 random portfolio's the one with the minimal risk is selected and returned to verify with the calculated weights, the same is done for the maximal sharp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edfbee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_plot(D_returns):\n",
    "    \n",
    "    plt.figure(figsize=(12,5))\n",
    "    Efficient_Frontier(Rf_Covariance_matrix, new_daily_returns, 400)\n",
    "    Efficient_Frontier(Covariance_matrix, D_returns, 400)\n",
    "    plt.xlabel('Expected Volatility')\n",
    "    plt.ylabel('Expected Return')\n",
    "    \n",
    "    port_minrisk = portfolio_volatility(Covariance_matrix, np.array(minimal_risk_weights))\n",
    "    port_return = portfolio_return(D_returns, minimal_risk_weights)\n",
    "    plt.scatter(port_minrisk,port_return,marker=(5,1,0),color='g',s=200)\n",
    "    \n",
    "    #port_maxsharp = portfolio_volatility(assets, maximal_sharp_weights)\n",
    "    #port_return = portfolio_return(assets, maximal_sharp_weights)\n",
    "    plt.scatter(risk_maximal_sharp,return_maximal_sharp,marker=(5,1,0),color='r',s=200)\n",
    "    \n",
    "    plt.title(\"Minimal Risk and Maximal Sharp Ratio\")\n",
    "    #plt.title(\"Minimal Risk Portfolios\")\n",
    "    p1 = plt.Rectangle((0, 0), 0.1, 0.1, fc='g')\n",
    "    p2 = plt.Rectangle((0, 0), 0.1, 0.1, fc='r')\n",
    "\n",
    "    plt.legend((p1, p2), ('Minimal Risk Portfolio', 'Maximal Sharp Ratio'), loc='best')\n",
    "    #plt.legend((p1,), ('Minimal Risk Portfolio',), loc='best')\n",
    "    plt.show()\n",
    "    return #[min_risk_weights, max_sharp_weights, minimal_risk_weights, maximal_sharp_weights]\n",
    "example_plot(daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_performance_min_risk = normalized_data.dot(minimal_risk_weights)\n",
    "portfolio_performance_max_sharp = normalized_data.dot(maximal_sharp_weights)\n",
    "(portfolio_performance_min_risk).plot(figsize=(15,8))\n",
    "(portfolio_performance_max_sharp).plot(figsize=(15,8))\n",
    "plt.legend(('Minimal Risk Portfolio', 'Maximal Sharp Ratio'), loc='best')\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Portfolio value over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0732d",
   "metadata": {},
   "source": [
    "### Different risk parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644e053",
   "metadata": {},
   "source": [
    "#### 1. VaR (Value at Risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cec4d",
   "metadata": {},
   "source": [
    "Value at risk describes the worst losses at a given confidence level $ \\alpha $ and over a given time period t. VaR can be calculated in two ways by taking the largest return in the $\\alpha$ % interval of worst returns or by assuming the returns are normal and deriving the VaR from the mean and standard deviation.\n",
    "\n",
    "First some functions that will be useful later are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d565a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_daily_returns(assets, weights):\n",
    "    returns = (assets.shift(1) - assets)/ assets\n",
    "    portfolio_returns = returns.dot(weights)\n",
    "    return portfolio_returns.drop(portfolio_returns.index[0])\n",
    "portfolio_daily_returns(normalized_data, gen_random_weights(amount_of_assets)).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc573a7",
   "metadata": {},
   "source": [
    "First the VaR will be calculated by selecting the largest return in the lowest interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c3fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = pf_data / pf_data.iloc[0] * 100\n",
    "(normalized_data).plot(figsize=(15,8))\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=12, mode=\"expand\", borderaxespad=1.5)\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Asset prices over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f11b0b",
   "metadata": {},
   "source": [
    "#### Convert prices to returns and sample the returns\n",
    "The returns will be modelled as simple returns: $$ r_i = \\frac{P_{i+1}-P_i}{P_i} = \\frac{P_{i+1}}{P_i} -1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [x for x in range(len(normalized_data))]\n",
    "daily_returns1 = normalized_data / normalized_data.shift(1) -1\n",
    "daily_returns2 = daily_returns1.sample(frac=0.25,weights = probabilities)\n",
    "amount_of_trading_days = len(daily_returns2.index)\n",
    "daily_returns = daily_returns2.sort_index(ascending = True)\n",
    "print len(daily_returns) #daily_returns.tail(), len(daily_returns), daily_returns1.tail(20)\n",
    "(daily_returns).plot(figsize=(15,5))\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=12, mode=\"expand\", borderaxespad=1.5)\n",
    "plt.ylabel(\"Daily return\")\n",
    "plt.title(\"Daily returns over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22e9ac",
   "metadata": {},
   "source": [
    "### Risk free rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b24ee8",
   "metadata": {},
   "source": [
    "The risk free rate is taken as the 10 year yield of the US treasury bond. Current yield : [Bloomberg Markets](https://www.bloomberg.com/markets/rates-bonds/government-bonds/us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a49c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_rate = 0.0232 # current 10 year yield on US treasury bond\n",
    "#risk_free_return = [100*(1+0.0232/252.0)**x for x in range(amount_of_trading_days)]\n",
    "risk_free_return = [0.0232/252.0 for x in range(len(daily_returns))]\n",
    "risk_free_return = np.array(risk_free_return)\n",
    "new_daily_returns = daily_returns.assign(risk_free=pd.Series(risk_free_return).values)\n",
    "new_daily_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2b129",
   "metadata": {},
   "source": [
    "#### Calculating the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Covariance_matrix = daily_returns.cov() * 252\n",
    "Rf_Covariance_matrix = new_daily_returns.cov() * 252\n",
    "#print Covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a89a59",
   "metadata": {},
   "source": [
    "#### Calculate the expected annual return and expected annual risk of a portfolio\n",
    "A function to generate a given amount of random weights while making sure the sum of the weights equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_weights(amount):\n",
    "    weights = np.random.random(amount)\n",
    "    weights /= np.sum(weights)\n",
    "    return weights\n",
    "#print gen_random_weights(amount_of_assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc135a8",
   "metadata": {},
   "source": [
    "The expected annual return is calculated as the amount of yearly trading days times the average daily return: $ E(r_{yearly}) = 252 * E(r_{daily}) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a4e56",
   "metadata": {},
   "source": [
    "The portfolios expected annual return is given by: $ p = w^T * R_{yearly} $. Where w is the weight vector and $ R_{yearly} $ the expected annual return vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_return(D_returns, weights):\n",
    "    annual_returns = D_returns.mean() * 252\n",
    "    total_return = np.sum(annual_returns * weights)\n",
    "    return total_return\n",
    "\n",
    "portfolio_return(daily_returns, gen_random_weights(amount_of_assets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9e15b",
   "metadata": {},
   "source": [
    "A function to calculate the expected volatility of a portfolio. The volatility is modelled as the standard deviation of portfolios returns. This be calculated as: $$ \\sigma^2 = w^T*C*w $$\n",
    "Where w is again the weights of the assets and C is the covariance matrix of the returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c59408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_volatility(C_matrix, weights):\n",
    "    total_volatility = np.sqrt(np.dot(weights.T,np.dot(C_matrix, weights)))\n",
    "    return total_volatility\n",
    "\n",
    "portfolio_volatility(Covariance_matrix, gen_random_weights(amount_of_assets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_covariance_matrix(C_matrix):\n",
    "    cov_matrix = C_matrix\n",
    "    cov_matrix = cov_matrix.values\n",
    "    cov_matrix = matrix(cov_matrix)\n",
    "    return cov_matrix\n",
    "#print convert_covariance_matrix(Covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_return(D_returns):\n",
    "    annual_returns = D_returns.mean() * 252\n",
    "    #print annual_returns\n",
    "    return max(annual_returns)\n",
    "max_return(daily_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccabe22",
   "metadata": {},
   "source": [
    "#### Functions to return the parameters that need to be optimized with their weights as variables \n",
    "They are all calculated the same way as their numeric functions previously defined only the weights are variables instead of numeric values. Parameters that have to be maximized are taken negatively because the optimizations look for minimal values and the weights that give the maximal value are the same weights that given the minimal negative value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398920e",
   "metadata": {},
   "source": [
    "The return in function of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns(x):\n",
    "    #amount_of_assets = len(normalized_data.columns)\n",
    "    annual_returns = daily_returns.mean() * 252\n",
    "    variable = []\n",
    "    for i in range(amount_of_assets):\n",
    "        variable.append(x[i])\n",
    "    variable = np.array(variable)\n",
    "    return -np.sum(annual_returns * variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f5489",
   "metadata": {},
   "source": [
    "The risk in function of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk(x):\n",
    "    #amount_of_assets = len(normalized_data.columns)\n",
    "    cov_matrix = Covariance_matrix\n",
    "    variable = []\n",
    "    for i in range(amount_of_assets):\n",
    "        variable.append(x[i])\n",
    "    variable = np.array(variable)\n",
    "    return np.sqrt(np.dot(variable.T,np.dot(cov_matrix, variable)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3dda3c",
   "metadata": {},
   "source": [
    "The sharp ratio in function of the weights. The sharp ratio is the return devided by the risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90c74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HistoricVaR(assets, weights, alpha):\n",
    "    return np.percentile(portfolio_daily_returns(assets,weights), alpha)\n",
    "HistoricVaR(normalized_data, gen_random_weights(amount_of_assets), 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543769d3",
   "metadata": {},
   "source": [
    "This method is more accurate than assuming a normal distribution because returns are not perfectly normal, but this method can not be used for optimization because every return is in function of the weights so it is unknown where the $ \\alpha $ interval ends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96072440",
   "metadata": {},
   "source": [
    "By assuming the returns follow a normal distribution, the VaR can be calculated as: $ VaR = \\mu - F^{-1}(\\alpha)* \\sigma $. First a function to do this nurmerically then in function of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharp(x):\n",
    "    #amount_of_assets = len(normalized_data.columns)\n",
    "    #log_returns = np.log(normalized_data / normalized_data.shift(1))\n",
    "    annual_returns = daily_returns.mean() * 252\n",
    "    cov_matrix = Covariance_matrix\n",
    "    variable = []\n",
    "    for i in range(amount_of_assets):\n",
    "        variable.append(x[i])\n",
    "    variable = np.array(variable)\n",
    "    return -(np.sum(annual_returns * variable)-risk_free_rate) / (np.sqrt(np.dot(variable.T,np.dot(cov_matrix, variable)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f75e65",
   "metadata": {},
   "source": [
    "#### Extra funtions usefull for the optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fdbbb7",
   "metadata": {},
   "source": [
    "The contraint for the optimization, that the sum of the weights must be equal to one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint1(x):\n",
    "    variable = []\n",
    "    for i in range(amount_of_assets):\n",
    "        variable.append(x[i])\n",
    "    variable = np.array(variable)\n",
    "    return np.sum(variable) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0ecf53",
   "metadata": {},
   "source": [
    "A function to generate the valid interval for each weight."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
