{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b41dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path_image):\n",
    "    '''\n",
    "    Reads in image and used matplotlib.pyplot to display it\n",
    "    \n",
    "    path_image: use full path for image. example: /home/andrej/owncloud/images/1_1.jpeg\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    a = plt.imread(path_image)\n",
    "    #print('Image dimensions:', a.shape)\n",
    "    plt.imshow(a)    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3aa549",
   "metadata": {},
   "source": [
    "The first 5 downloaded images are show below.\n",
    "\n",
    "* Image with label: 9,No passing. Was selected because is not in he common position of the images from training set. So even if that image class was very common on the training set given the fact that data augmentation was not done is expected to misclasify this image.\n",
    "* Image with label: 14,Stop. Was taken because semantically is one of the most important traffic signs on the road. The image was selected with high brigtness to test the importance of including contrast enhancement during preprocessing images. This one is expected to be correctly classified.\n",
    "* Image with label: 20,Dangerous curve to the right. Was chosen for being one with less common class type in the training set. This image is expected to be misclassifed.\n",
    "* Image with label: 19,Dangerous curve to the left. Was chosen also for being one of the commonless in the training set. Is the flipped verion of the downloaded image with label 20,Dangerous curve to the right. This image is expected to be misclasified too.\n",
    "* Image with label: 27,Pedestrians. Was selected because in the read is mandatory to detect any information related with pedestrians. In this case, kids from kindergarten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68912b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "read_image('./new_images_original/9_1.jpeg')\n",
    "plt.title('Label: 9,No passing.')\n",
    "\n",
    "plt.figure()\n",
    "read_image('./new_images_original/14_1.jpeg')\n",
    "plt.title('Label: 14,Stop.')\n",
    "\n",
    "plt.figure()\n",
    "read_image('./new_images_original/20_1.jpeg')\n",
    "plt.title('Label: 20,Dangerous curve to the right.')\n",
    "\n",
    "plt.figure()\n",
    "read_image('./new_images_original/20_1_flipped.jpeg')\n",
    "plt.title('Label: 19,Dangerous curve to the left. Flipped version of previous image.')\n",
    "\n",
    "plt.figure()\n",
    "read_image('./new_images_original/27_1.jpg')\n",
    "plt.title('Label: 27,Pedestrians')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeacaba",
   "metadata": {},
   "source": [
    "The other 5 images were downloaded from the [German Traffic Sign Recognition Benchmark](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news) and are shown below. They were chosen to be very difficult for me, as a human, to clasify them.\n",
    "\n",
    "* Image with label: 2,Speed limit (50km/h). Was selected because the image was taken while on the road and some pixels seem to be shifted. It is one of the most common images on the data set so it is expected that it should be correctly classified.\n",
    "* Image with label: 14,Stop. Was selected because exhibits a very variable level of brigthness among the different pixels. A stop signal is one of the most important signals to be detected.\n",
    "* Image with label: 25,Road work. Was selected because is one of the classes with the low level number of examples in the training set.\n",
    "* Image with label:  29,Bicycles crossing. Was chosen becuase as previously mentioned, everything related to pedestrians or human people involved is a priority. Also the image is not easy to identify by a human eye given the fact it has a high brightness level.\n",
    "* Image with label: 35,Ahead only. Was selected as an attempt to push the network to misclasify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b1c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "read_image('./new_images_original/2_1.ppm')\n",
    "plt.title('Label: 2,Speed limit (50km/h).')\n",
    "\n",
    "plt.figure()\n",
    "read_image('./new_images_original/14_2.ppm')\n",
    "plt.title('Label: 14,Stop.')\n",
    "\n",
    "plt.figure()\n",
    "read_image('./new_images_original/25_1.ppm')\n",
    "plt.title('Label: 25,Road work.')\n",
    "\n",
    "plt.figure()\n",
    "read_image('./new_images_original/29_1.ppm')\n",
    "plt.title('Label: 29,Bicycles crossing')\n",
    "\n",
    "plt.figure()\n",
    "read_image('./new_images_original/35_1.ppm')\n",
    "plt.title('Label: 35,Ahead only')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e91afc",
   "metadata": {},
   "source": [
    "A function to crop, resize and save downloaded images was defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d39e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(path_in_image, x1,x2,x3,x4, path_out_image, flag):\n",
    "    '''\n",
    "    crops images\n",
    "    path_in_image: use full path for incoming image. example: /home/andrej/owncloud/images/1_1.jpeg\n",
    "    path_out_image: use full path for outgoing image. example: /home/andrej/owncloud/out_images/1_1_resized.jpeg\n",
    "    x1,x2,x3,x4: are boundaries to crop image\n",
    "    '''\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    a = read_image(path_in_image)\n",
    "    \n",
    "    a_cropped = a[x1:x2, x3:x4]\n",
    "        \n",
    "    a_resized = cv2.resize(a_cropped, (32,32))\n",
    "    print('resized image size:',a_resized.shape)\n",
    "        \n",
    "    if flag == 0:\n",
    "        plt.imshow(a)\n",
    "    elif flag == 1:\n",
    "        plt.imshow(a_cropped)\n",
    "    elif flag == 2:\n",
    "        plt.imshow(a_resized)\n",
    "    else:\n",
    "        None\n",
    "    \n",
    "    plt.imsave(path_out_image, a_resized)\n",
    "    \n",
    "    return print('resized image and saved in path:', path_out_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f008c6",
   "metadata": {},
   "source": [
    "Downloaded images were cropped, resized to 33x33x3 and then saved in the folder './new_images_resized'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41acb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_image('./new_images_original/2_1.ppm', 0,45,0,44, './new_images_resized/2_1_resized.png',2)\n",
    "crop_image('./new_images_original/9_1.jpeg', 25,220,35,275, './new_images_resized/9_1_resized.png',2)\n",
    "crop_image('./new_images_original/14_1.jpeg', 15,180,45,215, './new_images_resized/14_1_resized.png',2)\n",
    "crop_image('./new_images_original/14_2.ppm', 0,43,0,42, './new_images_resized/14_2_resized.png',2)\n",
    "crop_image('./new_images_original/20_1_flipped.jpeg', 45,190,15,165, './new_images_resized/19_1_resized.png',2)\n",
    "crop_image('./new_images_original/20_1.jpeg', 45,190,15,165, './new_images_resized/20_1_resized.png',2)\n",
    "crop_image('./new_images_original/25_1.ppm', 0,49,0,51, './new_images_resized/25_1_resized.png',2)\n",
    "crop_image('./new_images_original/27_1.jpg', 35,345,15,340, './new_images_resized/27_1_resized.png',2)\n",
    "crop_image('./new_images_original/29_1.ppm', 0,31,0,33, './new_images_resized/29_1_resized.png',2)\n",
    "crop_image('./new_images_original/35_1.ppm', 0,51,0,53, './new_images_resized/35_1_resized.png',2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f48e5",
   "metadata": {},
   "source": [
    "A type list variable with cropped and resized images was created. Also a vector with expected labels was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32676cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the new incoming images.\n",
    "in_images_names=['2_1_resized.png', '9_1_resized.png', '14_1_resized.png', '14_2_resized.png', '19_1_resized.png', '20_1_resized.png','25_1_resized.png', '27_1_resized.png', '29_1_resized.png', '35_1_resized.png' ]\n",
    "# Expected labels for the new incoming images.\n",
    "expected_labels_new=[2,9,14,14,19,20,25,27,29,35]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ce46d",
   "metadata": {},
   "source": [
    "All new images are uploaded in a single mini data set variable with name *X_new*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16193693",
   "metadata": {},
   "source": [
    "With the following hyper parameters values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd4b142",
   "metadata": {},
   "source": [
    "* Batch size = 128 \n",
    "* Training Epochs = 50 (Please check last plot where the model has performace above 93% in validation set after epoch number 25 aproximately ). Was preferred to stop model at 50 epochs to avoid over-training.\n",
    "* Learning rate : 0.001 (I did not used decaying learning rate because [Adam optimizer computes individual adaptive learning rate](https://arxiv.org/pdf/1412.6980.pdf).\n",
    "* Dropout probability = 0.1 ( Keep probability 0.9).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1dc56a",
   "metadata": {},
   "source": [
    "**Note**: The selected optimization algoritm was [Adam](https://arxiv.org/pdf/1412.6980.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282e4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc58adf",
   "metadata": {},
   "source": [
    "Model architecture was implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4059a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "tf.reset_default_graph()\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    \n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x6.\n",
    "    conv1_w = tf.Variable(tf.truncated_normal((5,5,3,6), mean= mu, stddev= sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    conv1   = tf.nn.conv2d(x, conv1_w, strides=[1,1,1,1], padding='VALID') + conv1_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    drop1a = tf.nn.dropout(conv1, keep_prob=0.8)\n",
    "    # TODO: Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    pool1 = tf.nn.max_pool(drop1a, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "\n",
    "    drop1b = tf.nn.dropout(pool1, keep_prob=0.9)\n",
    "\n",
    "\n",
    "    # TODO: Layer 2: Convolutional. Input= 14x14x16. Output = 10x10x16.\n",
    "    conv2_w = tf.Variable(tf.truncated_normal((5,5,6,16), mean= mu, stddev= sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    conv2   = tf.nn.conv2d(drop1b, conv2_w, strides=[1,1,1,1], padding='VALID') + conv2_b\n",
    "    \n",
    "    \n",
    "    # TODO: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    drop2a = tf.nn.dropout(conv2, keep_prob=0.9)\n",
    "    \n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    pool2 = tf.nn.max_pool(drop2a, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    \n",
    "    drop2b = tf.nn.dropout(pool2, keep_prob=0.9)\n",
    "    \n",
    "    \n",
    "    # TODO: Flatten. Input = 5x5x16. Output = 400.(because 5x5x16 = 400)\n",
    "    fc0 = flatten(drop2b)\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fc1_w = tf.Variable(tf.truncated_normal(shape=(400,120), mean=mu, stddev=sigma) )\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    fc1 = tf.matmul(fc0, fc1_w) + fc1_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    dropfc1 = tf.nn.dropout(fc1, keep_prob=0.9)\n",
    "    \n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fc2_w = tf.Variable(tf.truncated_normal(shape=(120,84), mean=mu, stddev=sigma))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    fc2 = tf.matmul(dropfc1, fc2_w) + fc2_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    dropfc2 = tf.nn.dropout(fc2, keep_prob=0.9)\n",
    "    \n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    fc3_w = tf.Variable(tf.truncated_normal(shape=(84,43), mean=mu, stddev=sigma))\n",
    "    fc3_b = tf.Variable(tf.zeros(43))\n",
    "    fc3 = tf.matmul(dropfc2, fc3_w) + fc3_b\n",
    "    \n",
    "    logits = fc3\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69601a0b",
   "metadata": {},
   "source": [
    "Place holders where included to feed_in input data to CNN with labels one-hot enconded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f1bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c46ca",
   "metadata": {},
   "source": [
    "Model predictions are followed by softmax probability. Then cross-entropy between predicted labels and expected label values is computed. And the objective function selected was the mean error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093aeeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63f80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78739ae",
   "metadata": {},
   "source": [
    "To report training, valudaton and test model accuracy a custom funtion called *evaluate* was defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f1d25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data):\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "    \n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90889dbf",
   "metadata": {},
   "source": [
    "After training, model parameters where saved for further model performance evalaution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "training_acc= []\n",
    "validation_acc= []\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        training_accuracy = evaluate(X_train, y_train)\n",
    "        training_acc.append(training_accuracy)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Training Accuracy = {:.3f}\".format(training_accuracy))\n",
    "        print()\n",
    "        \n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        validation_acc.append(validation_accuracy)\n",
    "        #print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './checkpoints/lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5b68a",
   "metadata": {},
   "source": [
    "Model output performance for training and validation is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_newr = []\n",
    "for i in in_images_names:\n",
    "    #print(i)\n",
    "    X_newr.append (cv2.imread('./new_images_resized/'+ i)[:,:,:3] )\n",
    "\n",
    "X_new = np.asarray( X_newr )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb647d17",
   "metadata": {},
   "source": [
    "As for image preprocessing, Contrast enhacement  and image normalization was done to the new data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_new.shape[0]):\n",
    "    X_new[i][:,:,0] = clahe.apply(X_new[i][:,:,0])\n",
    "    X_new[i][:,:,1] = clahe.apply(X_new[i][:,:,1])\n",
    "    X_new[i][:,:,2] = clahe.apply(X_new[i][:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_new - tmean\n",
    "X_new = X_new/tstd\n",
    "\n",
    "print('New mini batch dataset mean:', np.mean(X_new))\n",
    "print('New mini batch dataset standard deviation:', np.std(X_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572078a",
   "metadata": {},
   "source": [
    "Notice that the mean for the new images is far away from being zero, so it is expected some data be outside the expected manifold. However, is near to be unit variance data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f5ad2",
   "metadata": {},
   "source": [
    "### Step 3.2: Performance on new images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa81e3c",
   "metadata": {},
   "source": [
    "After prerpoced the new input data, then the pretrained model is loaded and tested with new mini dataset of 10 images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560ea25e",
   "metadata": {},
   "source": [
    "### Exploratory visualization of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8981501",
   "metadata": {},
   "source": [
    "Here we visualize the German Traffic Signs Dataset using the pickled file(s). Especifically we are displaying:\n",
    "* In the first figure: A sample of 20 images from the training set.\n",
    "* In the seccond figure: A histogram(with distribution of classes) for the training and the validation set.\n",
    "* In the third image: A histogram(with distribution of classes) for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7910b",
   "metadata": {},
   "source": [
    "From the first figure (with 20 samples of training images) we can subjectively notice that there is a brightness difference among the sampled images. Also, in some of these, a  high rate brightness variation is present (e.g. image located in third row, third column). This suggest brightness and contrast transfomation would be a good candidate for data preprocessing.\n",
    "\n",
    "From data set histograms (training,validation and test) unbalanced data is present. So data augmentation could be a solution to increase model generalization. More specifically to increase the number of feature examples for the less common classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('Sample of 20 image from the Training set')\n",
    "for i in range (20):\n",
    "    plt.subplot(4,5, i+1)\n",
    "    plt.imshow(X_trainr[i*1000+1])\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(1,2, 1)\n",
    "plt.hist(y_trainr, bins= n_classes)\n",
    "plt.title('Histrogram training-set')\n",
    "plt.xlabel('Image labels')\n",
    "plt.ylabel('Number of images')\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1,2, 2)\n",
    "plt.hist(y_validr, bins= n_classes)\n",
    "plt.title('Histrogram validation-set')\n",
    "plt.xlabel('Image labels')\n",
    "plt.ylabel('Number of images')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(3) \n",
    "plt.subplot(1,1, 1)\n",
    "plt.hist(y_testr, bins= n_classes)\n",
    "plt.title('Histrogram test-set')\n",
    "plt.xlabel('Image labels')\n",
    "plt.ylabel('Number of images')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567c343",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0f051",
   "metadata": {},
   "source": [
    "### Step 2.1: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54cad47",
   "metadata": {},
   "source": [
    "After data exploration the follwing observations were taken into account to preprocess the data:\n",
    "    \n",
    "* Original input pixel image values range as usually was (0,255).\n",
    "* The brighness among images use to highly variable.\n",
    "* Some images have shown abrupt changes in brightness locally (very variable brightness in a single image).\n",
    "* Data set was unbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3f3404",
   "metadata": {},
   "source": [
    "This is the selected preprocessing approach:\n",
    "    \n",
    "Initially, it was considered to use gray scale images to have a more homogeneous contrast variation among the whole images dataset.\n",
    "However, the three chanel colors representation was kept and all data preprocessing was done channel by channel. The previous idea was just an attempt to keep as much amount of possible usefull information on input data that the network might use during the weights tunning process.\n",
    "In addition, to reduce abrupt changes present in some images it was decided to used histogram equalization by image sub-sections instead of using a global histogram equalization ([Click here for further information](https://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html)).\n",
    "By the other hand, even if the input data set was unbalanced it was wanted to [challenge the model generalization ability with unbalanced information](http://ieeexplore.ieee.org/document/6677603/) and penalized overfitting by using a regularization technique. More specifically, weights dropout during the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccdcbf8",
   "metadata": {},
   "source": [
    "#### Step 2.1.1: Contrast Limited Adaptive Histogram Equalization (CLAHE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e6370f",
   "metadata": {},
   "source": [
    "[Contrast Limited Adaptive Histogram Equalization](https://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html) was used to get a better contrast enhancement than the one gotten with global histogram equalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa012c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Working with copies/clones of original data\n",
    "X_train = np.copy(X_trainr)\n",
    "y_train = np.copy(y_trainr)\n",
    "\n",
    "X_valid = np.copy(X_validr)\n",
    "y_valid = np.copy(y_validr)\n",
    "\n",
    "X_test = np.copy(X_testr)\n",
    "y_test = np.copy(y_testr)\n",
    "\n",
    "\n",
    "\n",
    "#  Contrast Limited Adaptive Histogram Equalization (CLAHE)\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\n",
    "X_train[0][:,:,0] = clahe.apply(X_train[0][:,:,0])\n",
    "X_train[0][:,:,1] = clahe.apply(X_train[0][:,:,1])\n",
    "X_train[0][:,:,2] = clahe.apply(X_train[0][:,:,2])\n",
    "\n",
    "# Applying CLAHE to training set.\n",
    "for i in range(n_train):\n",
    "    X_train[i][:,:,0] = clahe.apply(X_train[i][:,:,0])\n",
    "    X_train[i][:,:,1] = clahe.apply(X_train[i][:,:,1])\n",
    "    X_train[i][:,:,2] = clahe.apply(X_train[i][:,:,2])\n",
    "\n",
    "# Applying CLAHE to validation set.\n",
    "for i in range(n_validation):\n",
    "    X_valid[i][:,:,0] = clahe.apply(X_valid[i][:,:,0])\n",
    "    X_valid[i][:,:,1] = clahe.apply(X_valid[i][:,:,1])\n",
    "    X_valid[i][:,:,2] = clahe.apply(X_valid[i][:,:,2])\n",
    "\n",
    "# Applying CLAHE to test set.\n",
    "for i in range(n_test):\n",
    "    X_test[i][:,:,0] = clahe.apply(X_test[i][:,:,0])\n",
    "    X_test[i][:,:,1] = clahe.apply(X_test[i][:,:,1])\n",
    "    X_test[i][:,:,2] = clahe.apply(X_test[i][:,:,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2551b1",
   "metadata": {},
   "source": [
    "A sample of 5 images is shown. It contains the original images and the corresponding images after contrast enhacement is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79f8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting some training images after applying CLAHE\n",
    "plt.figure()\n",
    "plt.title('Sample of images before and after contrast enhacement')\n",
    "images_to_check = 6\n",
    "pointer = 0\n",
    "for i in range(images_to_check):\n",
    "        \n",
    "    plt.subplot(images_to_check,2, pointer+1)\n",
    "    plt.imshow(X_trainr[i*500+1])\n",
    "    \n",
    "    plt.subplot(images_to_check,2, pointer+2)\n",
    "    plt.imshow(X_train[i*500+1])\n",
    "    \n",
    "    pointer +=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4b27cb",
   "metadata": {},
   "source": [
    "#### Step 2.1.2: Shuffled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9ccc2f",
   "metadata": {},
   "source": [
    "Training data is shuffled before starting preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5856ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t= np.linspace(1, EPOCHS, EPOCHS)\n",
    "plt.title('Model performance. Final setup. Run for '+str(EPOCHS)+' epochs.')\n",
    "plt.plot( np.asarray(training_acc), 'b', label='Training Set')\n",
    "plt.plot( np.asarray(validation_acc), 'r', label='Validation Set')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Performance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be089a",
   "metadata": {},
   "source": [
    "In addtion, model performance with test-set was measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f4095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c1f08",
   "metadata": {},
   "source": [
    "#### Step 2.1.3: Data manifold normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbf400",
   "metadata": {},
   "source": [
    "Training set **mean** and **standard deviation** are computed to get zero-mean unit variace input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e07c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tmean = np.mean(X_trainr)\n",
    "tstd = np.std(np.copy(X_trainr) - tmean)\n",
    "\n",
    "X_train= X_train - np.mean(X_train)\n",
    "X_train = X_train / np.std(X_train)\n",
    "\n",
    "X_valid = X_valid - np.mean(X_valid)\n",
    "X_valid = X_valid /np.std(X_valid)\n",
    "\n",
    "X_test= X_test - np.mean(X_test)\n",
    "X_test = X_test / np.std(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1042d",
   "metadata": {},
   "source": [
    "So we had closer zero-mean unit-variance input data. Also, a sample of the images after normalization is displayed below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7205e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Now we have aproximately data centered with unit-variance.')\n",
    "print('Training-set mean: ', np.mean(X_train))\n",
    "print('Training-set standard deviation: ', np.std(X_train))\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Sample of training images after normalization')\n",
    "images_to_check = 6\n",
    "\n",
    "for i in range(images_to_check):\n",
    "       \n",
    "    plt.subplot(images_to_check,3, i+1)\n",
    "    plt.imshow(X_train[i*500+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01abe903",
   "metadata": {},
   "source": [
    "#### Step 2.2: Solution Aproach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1684716",
   "metadata": {},
   "source": [
    "Used a modified version of [LeNet-5](http://yann.lecun.com/exdb/lenet/).\n",
    "\n",
    "As starting reference [LeNet-5 from laboratory]((https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/601ae704-1035-4287-8b11-e2c2716217ad/concepts/d4aca031-508f-4e0b-b493-e7b706120f81) and a [convolutional network proposed by Aymericdamien](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/convolutional_network.py) were used.\n",
    "Then was modified the architecture changing and/or adding network parameters across the whole model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95727edf",
   "metadata": {},
   "source": [
    "Very general model tunning process followed is described as follows:\n",
    "    \n",
    "1. Tested LeNet-5 from lab as starting point.\n",
    "2. Incresed batch size to 256 and run 10 Epochs.\n",
    "3. Added dropout layers (after each ReLU) with keep_probability = 0.5 and run 50 Epochs.\n",
    "4. Increased Number of filters in convolutional layers as going deeper (starting with 16 and ending with 64 filters)\n",
    "5. Decreased Number of filters in convolutional layers as going deeper (starting with 128 and ending with 64 filters).\n",
    "6. Changed probability to keep parameters in convolutional layers.\n",
    "7. Reduced number of filters and added extra dropout layers after pooling_layers and increased probability to keep weights on convolutional, pooling and fully connedted layers.\n",
    "\n",
    "So, basically I started with a simple networks that overfits quickly the training set, increased the number of free paramenters in the network and run for many epochs, but then I included regularization in a simpler model to increase generalization. I also had noticed that the harder you regularize the more epochs in takes more epochs to train the network but improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b2f9f",
   "metadata": {},
   "source": [
    "Below is shown the accuracy performance for each model ***previously described from the second to the seventh one*** during the tunning process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(plt.imread('./other_images/2.png'))\n",
    "plt.figure()\n",
    "plt.imshow(plt.imread('./other_images/3.png'))\n",
    "plt.figure()\n",
    "plt.imshow(plt.imread('./other_images/3.png'))\n",
    "plt.figure()\n",
    "plt.imshow(plt.imread('./other_images/4.png'))\n",
    "plt.figure()\n",
    "plt.imshow(plt.imread('./other_images/5.png'))\n",
    "plt.figure()\n",
    "plt.imshow(plt.imread('./other_images/6.png'))\n",
    "plt.figure()\n",
    "plt.imshow(plt.imread('./other_images/7.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62d2eb",
   "metadata": {},
   "source": [
    "#### Step 2.3: Model Architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404d39f",
   "metadata": {},
   "source": [
    "Finally, the main goal was to get a model as simple as possible. For that reason, the following architecture was chosen:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09881bd3",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;border-color:#bbb;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#bbb;color:#594F4F;background-color:#E0FFEB;border-top-width:1px;border-bottom-width:1px;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-color:#bbb;color:#493F3F;background-color:#9DE0AD;border-top-width:1px;border-bottom-width:1px;}\n",
    ".tg .tg-yw4l{vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-031e\">Layer name<br></th>\n",
    "    <th class=\"tg-031e\">Layer type<br></th>\n",
    "    <th class=\"tg-031e\">Parameters/Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">conv1</td>\n",
    "    <td class=\"tg-031e\">Convolution</td>\n",
    "    <td class=\"tg-031e\">Filters: 6 (5x5)<br>Pad: 0<br>At stride: 1<br>Activation: ReLU<br>Input: (32x32x3)<br>Output:(28x28x6)<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">drop1a</td>\n",
    "    <td class=\"tg-031e\">Dropout</td>\n",
    "    <td class=\"tg-031e\">Keep Prob: 0.8<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-031e\">pool1</td>\n",
    "    <td class=\"tg-031e\">Pooling</td>\n",
    "    <td class=\"tg-031e\">Kernels size: (2x2)<br>Pad: 0<br>At stride: 2<br>Input: (28x28x6)<br>Output: (14x14x6)<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">drop1b</td>\n",
    "    <td class=\"tg-yw4l\">Dropout</td>\n",
    "    <td class=\"tg-yw4l\">Keep Prob: 0.9</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">conv2</td>\n",
    "    <td class=\"tg-yw4l\">Convolution</td>\n",
    "    <td class=\"tg-yw4l\">Filters: 16 (5x5)<br>Pad: 0<br>At stride: 1<br>Activation: ReLU<br>Input: (14x14x16)<br>Output:(10x10x16)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">drop2a</td>\n",
    "    <td class=\"tg-yw4l\">Dropout</td>\n",
    "    <td class=\"tg-yw4l\">Keep Prob: 0.9</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">pool2</td>\n",
    "    <td class=\"tg-yw4l\">Pooling</td>\n",
    "    <td class=\"tg-yw4l\">Kernels size: (2x2)<br>Pad: 0<br>At stride: 2<br>Input: (10x10x16)<br>Output: (5x5x16)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">drop2b</td>\n",
    "    <td class=\"tg-yw4l\">Dropout</td>\n",
    "    <td class=\"tg-yw4l\">Keep Prob: 0.9</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">fc0</td>\n",
    "    <td class=\"tg-yw4l\">Flatten</td>\n",
    "    <td class=\"tg-yw4l\">Input: (5x5x16)<br>Output: 400 Neurons<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">fc1</td>\n",
    "    <td class=\"tg-yw4l\">Fully Connected<br></td>\n",
    "    <td class=\"tg-yw4l\">Input: 400 Neurons<br>Output: 120 Neurons</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">dropfc1</td>\n",
    "    <td class=\"tg-yw4l\">Dropout</td>\n",
    "    <td class=\"tg-yw4l\">Keep Prob: 0.9</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">fc2</td>\n",
    "    <td class=\"tg-yw4l\">Fully Connected</td>\n",
    "    <td class=\"tg-yw4l\">Input: 120 Neurons<br>Output: 84 Neurons</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">dropfc2</td>\n",
    "    <td class=\"tg-yw4l\">Dropout</td>\n",
    "    <td class=\"tg-yw4l\">Keep Prob: 0.9</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-yw4l\">fc3</td>\n",
    "    <td class=\"tg-yw4l\">Fully Connected</td>\n",
    "    <td class=\"tg-yw4l\">Input: 84 Neurons<br>Output: 43 Neurons (Class Scores)<br></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645651fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./checkpoints/'))\n",
    "\n",
    "    new_predicted = sess.run( tf.argmax(logits, 1), feed_dict={x:X_new })\n",
    "    print('new_predicted values:', new_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ade6fc",
   "metadata": {},
   "source": [
    "Predicted labels (network output scores) are in the variable with name **new_predicted** and they are going to be compared with the expected labels saved on the variable **expected_labels_new** for the incoming new images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cf30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Please compare predicted output with expected output:')\n",
    "print('new_predicted values:', new_predicted)\n",
    "print('expected_labels_new: ', np.asarray(expected_labels_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c06cb9",
   "metadata": {},
   "source": [
    "Predicted values changed with every run. So in the first run we had 3/10 predicted classes, that is 30% of accuracy in the new images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec3e33",
   "metadata": {},
   "source": [
    "**Note**: Please notice that the previous way to measure performance is not statistically significant. We should perform previous test many times and average results to get a better insight about the real model performance in the new images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8723416",
   "metadata": {},
   "source": [
    "#### Test set Accuracy vs New downloaded images accuracy discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3e315",
   "metadata": {},
   "source": [
    "From the test set the minimum accuracy performance obtained was 93% in a total  of 12630 examples. By the other hand, the accuracy on the mini data sample set from the downloaded images after running just one time was 3/10 = 30% in a total of 10 images.\n",
    "\n",
    "Notice that the amount of image examples from test set is bigger that the amount of examples in the mini data set of new downloaded images in a ratio of 12360/10 = 1236.\n",
    "\n",
    "Reader migth be tempted to make a performance comparison between *test* and *new images* data sets. In fact, you can be tempted to concluded that *the probability that **the accuracy of 93% obtained in the test set** to characterize the network performance is higher than the probability that **the accuracy of 30% obtained on the new mini set***.\n",
    "However, that asumption might not be true and is preferred not to generate affirmations before studying better the test data set and the new images mini data set.\n",
    "\n",
    "In addition, author does not compromise to any conclusion about this attempt to compare *test_set* with *new_imags_mini_dataset*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6430f5",
   "metadata": {},
   "source": [
    "### Step 3.3: Output Top 5 Softmax Probabilities For Each Image Found on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c27b722",
   "metadata": {},
   "source": [
    "In this subsection top softmax probabilities for each new image found on the web are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4513412e",
   "metadata": {},
   "source": [
    "A new function to get top 5 probabilities was defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1090e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_five(in_image):\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('./checkpoints/'))\n",
    "        \n",
    "        softmax = tf.nn.softmax(logits)\n",
    "        top_prob = tf.nn.top_k(softmax, k =5 )\n",
    "        top_five = sess.run( top_prob, feed_dict={x: in_image })\n",
    "        #print(top_five)\n",
    "    return top_five"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c61c35",
   "metadata": {},
   "source": [
    "A function to plot top 5 predicted probabilities was defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fd0e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_5_prob(top_k, path_in_image, image_label):\n",
    "    '''\n",
    "    Plots top 5 logits probabilities in new images\n",
    "    \n",
    "    top_k: Vector with top predictions\n",
    "    path_in_image: (str). Full path for image used for testing the model. Example: './new_images_resized/2_1_resized.png'\n",
    "    image_label: (str). Label number for the class the image belongs to. Example: '2'\n",
    "    '''\n",
    "    certainty = np.asarray(top_k[0][0])\n",
    "    #predValues= list(map(unicode,list(top_k[1][0])))\n",
    "    predValues= list(top_k[1][0])\n",
    "    y_pos = np.arange(len(certainty))\n",
    "    \n",
    "    inImg= plt.imread(path_in_image)\n",
    "    \n",
    "    im1 = plt.subplot(1,2,1)\n",
    "    plt.imshow(inImg)\n",
    "    plt.title('Expected label: '+str(image_label))\n",
    "\n",
    "\n",
    "    im2 = plt.subplot(1,2,2)\n",
    "    im2.set_yticks(y_pos+0.1)\n",
    "    im2.set_yticklabels(predValues)\n",
    "    im2.barh(y_pos, certainty)\n",
    "    im2.invert_yaxis()    \n",
    "\n",
    "    plt.title('Top 5 probabilities predicted.')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37425c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(in_images_names)):\n",
    "    print(i)\n",
    "    aux = np.reshape(X_new[i], (1,32,32,3))\n",
    "    aux2 = get_top_five(aux)\n",
    "    plot_5_prob(aux2, './new_images_resized/'+str(in_images_names[i]), str(expected_labels_new[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f0fe9",
   "metadata": {},
   "source": [
    "#### Model Certainty- Softmax Probabilities discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a24ea4",
   "metadata": {},
   "source": [
    "1. Notice that the fact that the model predicts a class label with probability 1 does not necessarily implies that the clasification is correct, it might be wrong. \n",
    "\n",
    "    For example, in the image with *expected label:25* the softmax probability of being label as 'predicted_label:25' was  1 = 100%. However for the image with *expected label: 27* the softmax probability of being labeled as 'predicted_label:28' was 1, which is totally wrong.\n",
    "    \n",
    "    Also please notice that the amount of examples for class '25' is one of the most common in training set, but, the amount of examples for class '27' is one of the commonless.\n",
    "    \n",
    "2. Notice that sometimes the expected label number was not listed in the top 5 softmax probabilities, showing that the trained network has real trouble clasifiying these images.\n",
    "\n",
    "3. Notice also that the network was tested just one time with the *new mini data set of images downloaded from the web*. So you will have different performance every time you run the test in this new mini data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507fb07",
   "metadata": {},
   "source": [
    "## Step 4: Visualize the Neural Network's State with Test Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2577922",
   "metadata": {},
   "source": [
    "On this section a visualization of the filters activation is done but not the one with the activation maps.\n",
    "\n",
    "Withal, the author is working in extracting the feature maps from hidden layers just to check hidden layers networks response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73888c87",
   "metadata": {},
   "source": [
    "A new function to plot filters activation and feature maps was defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1fbb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize your network's feature maps here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# image_input: the test image being fed into the network to produce the feature maps\n",
    "# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer\n",
    "# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and max values of the output\n",
    "# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry\n",
    "\n",
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    # Here make sure to preprocess your image_input in a way your network expects\n",
    "    # with size, normalization, ect if needed\n",
    "    # image_input =\n",
    "    # Note: x should be the same name as your network's tensorflow data placeholder variable\n",
    "    # If you get an error tf_activation is not defined it may be having trouble accessing the variable from inside a function\n",
    "    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})\n",
    "    featuremaps = activation.shape[3]\n",
    "    plt.figure(plt_num, figsize=(15,15))\n",
    "    for featuremap in range(featuremaps):\n",
    "        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column\n",
    "        plt.title('Filter ' + str(featuremap)) # displays the feature map number\n",
    "        if activation_min != -1 & activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_max != -1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "        elif activation_min !=-1:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb5cc8",
   "metadata": {},
   "source": [
    "An stimulus image is used to get the filters response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbf6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('./checkpoints/'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca2e5a1",
   "metadata": {},
   "source": [
    "## Step 3: Test the Model on New Images\n",
    "\n",
    "Five (5) pictures of German traffic signs were downloaded from the web using google browser and other five (5) where downloaded from the [German Traffic Sign Recognition Benchmark](http://benchmark.ini.rub.de/?section=gtsrb&subsection=news). Then the pretrained model was used to predict the traffic sign type.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb44afa",
   "metadata": {},
   "source": [
    "### Step3.1: Load and Output the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51322305",
   "metadata": {},
   "source": [
    "A total of 10 new images were used for further testing purposes. Some of them where selected to be the less common classes images in the dataset to test model robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e835f9",
   "metadata": {},
   "source": [
    "A function to read in images was defined."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
