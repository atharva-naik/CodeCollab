{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e25771d",
   "metadata": {},
   "source": [
    "## 9. The Simplest Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c06bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8812ccd9",
   "metadata": {},
   "source": [
    "Now that the data is ready, we see that there are six input features: gre, gpa, and the four rank dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8987ea",
   "metadata": {},
   "source": [
    "---\n",
    "### Mean Square Error\n",
    "\n",
    "We're going to make a small change to how we calculate the error here. Instead of the SSE, we're going to use the __mean__ of the square errors (__MSE__). Now that we're using a lot of data, summing up all the weight steps can lead to really large updates that make the gradient descent diverge. To compensate for this, you'd need to use a quite small learning rate. Instead, we can just divide by the number of records in our data, m to take the average. This way, no matter how much data we use, our learning rates will typically be in the range of 0.01 to 0.001. Then, we can use the MSE (shown below) to calculate the gradient and the result is the same as before, just averaged instead of summed:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff29dee",
   "metadata": {},
   "source": [
    "<img src=\"./screenshots/gd1.png\" width=\"200\">\n",
    "<img src=\"./screenshots/gd2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9d3ce",
   "metadata": {},
   "source": [
    "### Implementing with NumPy\n",
    "For the most part, this is pretty straightforward with Numpy.\n",
    "\n",
    "First, you'll need to initialize the weights. We want these to be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends. It's also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry. So, we'll initialize the weights from a normal distribution centered at 0. A good value for the scale is $1 / \\sqrt{n} $ where $n$ is the number of input units. This keeps the input to the sigmoid low for increasing numbers of input units:\n",
    "```\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "```\n",
    "Numpy provides a function that calculates the dot product of two arrays, which conveniently calculates h for us. The dot product multiplies two arrays element-wise, the first element in array 1 is multiplied by the first element in array 2, and so on. Then, each product is summed.\n",
    "\n",
    "```\n",
    "# input to the output layer\n",
    "output_in = np.dot(weights, inputs)\n",
    "```\n",
    "And finally, we can update $\\Delta w_i$ and $w_i$ by incrementing them with    `weights += ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2906af",
   "metadata": {},
   "source": [
    "### Efficiency tip!\n",
    "\n",
    "You can save some calculations since we're using a sigmoid here. For the sigmoid function, $f'(h) = f(h)(1âˆ’f(h))$. That means that once you calculate $f(h)$, the activation of the output unit, you can use it to calculate the gradient for the error gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d90058",
   "metadata": {},
   "source": [
    "### Programming exercise\n",
    "Below, you'll implement gradient descent and train the network on the admissions data. Your goal here is to train the network until you reach a minimum in the mean square error (MSE) on the training set. You need to implement:\n",
    "\n",
    "The network output: output.\n",
    "The output error: error.\n",
    "The error term: error_term.\n",
    "Update the weight step: del_w +=.\n",
    "Update the weights: weights +=.\n",
    "After you've written these parts, run the training by pressing \"Test Run\". The MSE will print out, as well as the accuracy on a test set, the fraction of correctly predicted admissions.\n",
    "\n",
    "Feel free to play with the hyperparameters and see how it changes the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ad5b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "#   --> we can use error_term = error * nn_output * (1 - nn_output) instead\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    delta_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Note: We haven't included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output\n",
    "\n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(np.dot(x,weights))\n",
    "\n",
    "        # TODO: Calculate the error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate the error term\n",
    "        error_term = error * output * (1 - output)\n",
    "\n",
    "        # TODO: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        delta_w += error_term*x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += learnrate * delta_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb3eec",
   "metadata": {},
   "source": [
    "## 14. Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the weights:\n",
    "\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c1254",
   "metadata": {},
   "source": [
    "This creates a 2D array (i.e. a matrix) named weights_input_to_hidden with dimensions n_inputs by n_hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_input_to_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c917adb",
   "metadata": {},
   "source": [
    "Remember how the input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights. To do that, we now need to use matrix multiplication. In this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, j=1, you'd take the dot product of the inputs with the first column of the weights matri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286b817e",
   "metadata": {},
   "source": [
    "In Numpy, you can do this for all the inputs and all the outputs at once using np.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_inputs = np.dot(features, weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6cfa77",
   "metadata": {},
   "source": [
    "The important thing with matrix multiplication is that the dimensions match. For matrix multiplication to work, there has to be the same number of elements in the dot products. In the first example, there are three columns in the input vector, and three rows in the weights matrix. In the second example, there are three columns in the weights matrix and three rows in the input vector. If the dimensions don't match, you'll get this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b37ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_inputs = np.dot(weights_input_to_hidden, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2044319",
   "metadata": {},
   "source": [
    "### Making a column vector\n",
    "You see above that sometimes you'll want a column vector, even though by default Numpy arrays work like row vectors. It's possible to get the transpose of an array like so `arr.T`, but for a 1D array, the transpose will return a row vector. Instead, use `arr[:,None]` to create a column vector:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
