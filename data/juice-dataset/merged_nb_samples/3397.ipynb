{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f08ae6",
   "metadata": {},
   "source": [
    "While studying machine learning theory over the past year or so, I came across an interesting theorem that really sparked my interest. The [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that any feed forward neural network with a *single* hidden layer containing a finite number of units/neurons can fit any function. More specifically, we can attain any desired non-zero amount of error. In order words, we will always be able to output a prediction $ g(x) $ where $ |g(x) - f(x)| < \\epsilon $ for all values of x. $ f(x) $ is the label and $ \\epsilon $ is a very small positive non-zero number. When I read that for the first time, I almost did a double take because I was so surprised that something like that could be true. \n",
    "\n",
    "Let's think about what it really means. It's basically saying that if, for example, you're given a set of $ n $ training examples $ X $ where each $ x_i \\in X $ is some $ k $-dimensional vector and you have a set of $ n $ training labels $ Y $ where each $ y_i \\in Y $ is a label from the set {0,1} (binary classification), then you will be able to generate a neural network (with one hidden layer) that is able be trained to classify every single one of those $ n $ examples correctly. \n",
    "\n",
    "That was ridiculously interesting to me when I first saw it. \n",
    "\n",
    "And it made me think \"Well, if this theorem is true, shouldn't neural networks be perfect at pretty much any task?\". The key here, though, is that we encounter a **generalization** and **overfitting** problem when we try to fit a neural net too tightly to a particular training set, and expect the same results on the testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e2625",
   "metadata": {},
   "source": [
    "So, let's say that I want to create a neural network, with one hidden layer, that correctly classifies every example in the MNIST dataset. As you may recall, MNIST has about 55,000 images and is a 10 class classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classic imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2231ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the MNIST data and visualize the dimensions\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# oneImage is a tuple where oneImage[0] is the image and oneImage[1] is the one-hot label\n",
    "oneImage = mnist.train.next_batch(1)\n",
    "print ('Shape of the input: ' + str(oneImage[0].shape))\n",
    "print ('Shape of the label: ' + str(oneImage[1].shape))\n",
    "print ('Number of total images: ' + str(mnist.train.images.shape[0]))\n",
    "numInputDimensions = oneImage[0].shape[1]\n",
    "totalNumTrainImages = mnist.train.images.shape[0]\n",
    "totalNumTestImages = mnist.test.images.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b7c66",
   "metadata": {},
   "source": [
    "So now, let's just create a simple neural network with one hidden layer. This is what the architecture will look like. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1918f",
   "metadata": {},
   "source": [
    "![alt text](Data/FunctionApproximation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e587d22",
   "metadata": {},
   "source": [
    "One of the big caveats with this theorem is that although it states that a one hidden layer neural network can approximate any function, it doesn't specify how many hidden units will be necessary to attain that 100% classification accuracy. When we think about the task of MNIST digit classification, the inputs will have 784 input features. Let's see what happens if we have a neural net with just 25 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numHiddenUnits = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35554605",
   "metadata": {},
   "source": [
    "Also have to define some other useful parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3914c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "numClasses = 10\n",
    "trainingIterations = 500000\n",
    "batchSize = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d61c8a",
   "metadata": {},
   "source": [
    "Now, let's just create our simple neural network. If you'd like to see a more in depth tutorial on that, go ahead and take a look at my other tutorial [here](https://github.com/adeshpande3/Tensorflow-Programs-and-Tutorials/blob/master/Simple%20Neural%20Networks.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e312ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Just to make sure that we start with a new graph\n",
    "X = tf.placeholder(tf.float32, shape = [None, numInputDimensions])\n",
    "y = tf.placeholder(tf.float32, shape = [None, numClasses])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([numInputDimensions, numHiddenUnits], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.1), [numHiddenUnits])\n",
    "W2 = tf.Variable(tf.truncated_normal([numHiddenUnits, numClasses], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1), [numClasses])\n",
    "\n",
    "hiddenLayerOutput = tf.matmul(X, W1) + B1\n",
    "hiddenLayerOutput = tf.nn.relu(hiddenLayerOutput)\n",
    "finalOutput = tf.matmul(hiddenLayerOutput, W2) + B2\n",
    "finalOutput = tf.nn.relu(finalOutput)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = finalOutput))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .01).minimize(loss)\n",
    "\n",
    "correctPrediction = tf.equal(tf.argmax(finalOutput,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPrediction, \"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd548714",
   "metadata": {},
   "source": [
    "This is just a quick helper function to test how our network is doing throughout the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cceb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testOnDataset(imageDataset, labelDataset, numImages, session):\n",
    "    numCorrectClassifications = 0\n",
    "    for i in range(numImages):\n",
    "        image = imageDataset[i]\n",
    "        image = np.reshape(image, (1, numInputDimensions))\n",
    "        label = labelDataset[i]\n",
    "        label = np.reshape(label, (1, numClasses))\n",
    "        pred = session.run(correctPrediction, feed_dict={X: image, y: label})\n",
    "        if pred[0] == True:\n",
    "            numCorrectClassifications += 1\n",
    "    return numCorrectClassifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209c1363",
   "metadata": {},
   "source": [
    "Now that we've defined the graph, let's start to train it the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c058b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Training the network\n",
    "halfParamCorrect = []\n",
    "for i in range(trainingIterations):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    batchInput = batch[0]\n",
    "    batchLabels = batch[1]\n",
    "    _, trainingLoss = sess.run([opt, loss], feed_dict={X: batchInput, y: batchLabels})\n",
    "    if i%20000 == 0:\n",
    "        trainAccuracy = accuracy.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n",
    "        print (\"step %d, training accuracy %g\"%(i, trainAccuracy))\n",
    "        numCorrect = testOnDataset(mnist.train.images, mnist.train.labels, totalNumTrainImages, sess)\n",
    "        halfParamCorrect.append(numCorrect)\n",
    "\n",
    "# Testing network on training set\n",
    "numCorrectClassifications = testOnDataset(mnist.train.images, mnist.train.labels, totalNumTrainImages, sess)\n",
    "print (\"Number of digits (in training set) classified correctly: %d/%d\"%(numCorrectClassifications, totalNumTrainImages))\n",
    "\n",
    "# Testing network on test set\n",
    "numCorrectClassifications = testOnDataset(mnist.test.images, mnist.test.labels, totalNumTestImages, sess)\n",
    "print (\"Number of digits (in test set) classified correctly: %d/%d\"%(numCorrectClassifications, totalNumTestImages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c004a",
   "metadata": {},
   "source": [
    "So, as you can see, the network was able to reach a decent accuracy, but it still isn't classifying every example in our training set correctly. Now, let's see what happens when we bump up the number of hidden units by a **lot** and create the graph again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numHiddenUnits = 2000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eed036",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Just to make sure that we start with a new graph\n",
    "X = tf.placeholder(tf.float32, shape = [None, numInputDimensions])\n",
    "y = tf.placeholder(tf.float32, shape = [None, numClasses])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([numInputDimensions, numHiddenUnits], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.1), [numHiddenUnits])\n",
    "W2 = tf.Variable(tf.truncated_normal([numHiddenUnits, numClasses], stddev=0.1))\n",
    "B2 = tf.Variable(tf.constant(0.1), [numClasses])\n",
    "\n",
    "hiddenLayerOutput = tf.matmul(X, W1) + B1\n",
    "hiddenLayerOutput = tf.nn.relu(hiddenLayerOutput)\n",
    "finalOutput = tf.matmul(hiddenLayerOutput, W2) + B2\n",
    "finalOutput = tf.nn.relu(finalOutput)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = finalOutput))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .1).minimize(loss)\n",
    "\n",
    "correctPrediction = tf.equal(tf.argmax(finalOutput,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPrediction, \"float\"))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "doubleParamCorrect = []\n",
    "for i in range(trainingIterations):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    batchInput = batch[0]\n",
    "    batchLabels = batch[1]\n",
    "    _, trainingLoss = sess.run([opt, loss], feed_dict={X: batchInput, y: batchLabels})\n",
    "    if i%10000 == 0:\n",
    "        trainAccuracy = accuracy.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n",
    "        print (\"step %d, training accuracy %g\"%(i, trainAccuracy))\n",
    "        numCorrect = testOnDataset(mnist.train.images, mnist.train.labels, totalNumTrainImages, sess)\n",
    "        doubleParamCorrect.append(numCorrect)\n",
    "        \n",
    "# Testing network on training set\n",
    "numCorrectClassifications = testOnDataset(mnist.train.images, mnist.train.labels, totalNumTrainImages, sess)\n",
    "print (\"Number of digits (in training set) classified correctly: %d/%d\"%(numCorrectClassifications, totalNumTrainImages))\n",
    "\n",
    "# Testing network on test set\n",
    "numCorrectClassifications = testOnDataset(mnist.test.images, mnist.test.labels, totalNumTestImages, sess)\n",
    "print (\"Number of digits (in test set) classified correctly: %d/%d\"%(numCorrectClassifications, totalNumTestImages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440cf59d",
   "metadata": {},
   "source": [
    "Ah, we're reached 100% classification accuracy! This does back up the theorem that any dataset (like MNIST) can be fully fit with a single hidden layer neural network, **provided that we have a sufficiently large number of hidden units**. As you saw with the two runs of the neural networks, the first run showed that the number of hidden units wasn't large enough as we weren't able to fully fit the function. When we increased the number of hidden units, the training set accuracy got to 100%. However, the crux of this topic lies in the problem that the test accuracy of the smaller network actually turned out to be greater than the accuracy of the larger network that was able to fully fit to the dataset. This is the classic machine learning problem of **overfitting**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704f74eb",
   "metadata": {},
   "source": [
    "Here's a final look at the classification accuracies as the number of training iterations increases. "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
