{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a9a167d",
   "metadata": {},
   "source": [
    "# STUDENT INTERVENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc572112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"intervention.jpg\", width=1000, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a13e4",
   "metadata": {},
   "source": [
    "# 1. Business Problem\n",
    "## 1.1 Context\n",
    "These dataset have been collected from two portugal schools which consists of student achievement in secondary education. The data was collected by using school reports and questionnaires.\n",
    "\n",
    "## 1.2 Problems with current approach\n",
    "Nowadays,teachers are not much interacting with students due to which they don't know the prons and cons of students.Many of the students cannot recognize their weaknesses and this stops them from developing their skills.Proper student intervention procedure does not take place in many of the schools.They lag behind in academics and even their parents would not be able to help them if the time crosses.\n",
    "\n",
    "## 1.3 Problem Statement\n",
    "Many of the schools have hired us as data science consultants.If the students are made to understand their weaknesses,then,they can surely work on it and could improve themselves.We need to identify students who might need early intervention before they fail to graduate.\n",
    "\n",
    "## 1.4 Business Objectives and Constraints\n",
    "* Deliverable: Trained model file\n",
    "* Model interprtability is very important\n",
    "* Ouput Probabilities along with the prediction\n",
    "* No latency constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d789c3",
   "metadata": {},
   "source": [
    "# 2. Machine Learning Problem\n",
    "## 2.1 Data Overview\n",
    "\n",
    "For this project:\n",
    "1. The dataset has 395 observations.\n",
    "2. Each observation includes the student's status.\n",
    "\n",
    "**Target variable**<br>\n",
    "'passed' – Current student status (Passed/Failed)\n",
    "\n",
    "**Features**\n",
    "\n",
    "Student information\n",
    "* school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira) \n",
    "* gender - student's gender (binary: 'F' - female or 'M' - male) \n",
    "* age - student's age (numeric: from 15 to 22) \n",
    "* address - student's home address type (binary: 'U' - urban or 'R' - rural) \n",
    "* studytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours) \n",
    "* failures - number of past class failures (numeric: n if 1<=n<3, else 4) \n",
    "* schoolsup - extra educational support (binary: yes or no) \n",
    "* paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no) \n",
    "* activities - extra-curricular activities (binary: yes or no) \n",
    "* nursery - attended nursery school (binary: yes or no) \n",
    "* higher - wants to take higher education (binary: yes or no) \n",
    "* romantic - with a romantic relationship (binary: yes or no)\n",
    "* freetime - free time after school (numeric: from 1 - very low to 5 - very high) \n",
    "* goout - going out with friends (numeric: from 1 - very low to 5 - very high) \n",
    "* Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high) \n",
    "* Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high) \n",
    "* health - current health status (numeric: from 1 - very bad to 5 - very good) \n",
    "* absences - number of school absences (numeric: from 0 to 93) \n",
    "\n",
    "Family information\n",
    "\n",
    "* famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3) \n",
    "* Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart) \n",
    "* Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education) \n",
    "* Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education) \n",
    "* Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') \n",
    "* Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other') \n",
    "* reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other') \n",
    "* guardian - student's guardian (nominal: 'mother', 'father' or 'other') \n",
    "* famsup - family educational support (binary: yes or no) \n",
    "* internet - Internet access at home (binary: yes or no) \n",
    "* famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent) \n",
    "\n",
    "Distance information\n",
    "*  traveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "\n",
    "## 2.2 Mapping Business problem to ML problem\n",
    "### 2.2.1 Type of Machine Learning Problem\n",
    "This should be a classification problem.\n",
    "This is because there possibly two discrete outcomes, typical of a classification problem:\n",
    "* Students who need early intervention.\n",
    "* Students who do not need early intervention.\n",
    "\n",
    "We can classify accordingly with a binary outcome such as:\n",
    "* Yes, 1, for students who need early intervention.\n",
    "* No, 0, for students who do not need early intervention.\n",
    "Evidently, we are not trying to predict a continuous outcome, hence this is not a regression problem.\n",
    "\n",
    "### 2.2.2 Evaluation Metric (KPI)\n",
    "Since this is binary classification problem, we use the following metrics:\n",
    "* **Confusion matrix** - For getting a better clarity of the no of correct/incorrect predictions by the model\n",
    "* **ROC-AUC** - It considers the rank of the output probabilities and intuitively measures the likelihood that model can distinguish between a positive point and a negative point. (**Note:** ROC-AUC is typically used for binary classification only). We will use AUC to select the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9eee64",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis\n",
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "# import color maps\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to perform data standardization \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Libraries to perform hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# import xgboost\n",
    "import os\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-7.2.0-posix-seh-rt_v5-rev0\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance  ## to plot feature importance\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "\n",
    "# To save the final model on disk\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae37332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set printing options.\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "# These options determine the way floating point numbers, arrays and other NumPy objects are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57184894",
   "metadata": {},
   "source": [
    "## 3.1 Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f768674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reason'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also use bar plots instead\n",
    "plt.figure(figsize=(9,7))\n",
    "sns.countplot(y='Mjob', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c42bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also use bar plots instead\n",
    "plt.figure(figsize=(9,7))\n",
    "sns.countplot(y='Fjob', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3dc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can also use bar plots instead\n",
    "plt.figure(figsize=(9,7))\n",
    "sns.countplot(y='reason', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['passed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26f90c",
   "metadata": {},
   "source": [
    "Approx 67.08% of students have passed and 32.91% of students have failed.\n",
    "\n",
    "This means the dataset is **not balanced**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa454a1",
   "metadata": {},
   "source": [
    "## 3.4 Segmentations\n",
    "Segment the target variable (status) with key features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43290edd",
   "metadata": {},
   "source": [
    "### Univariate segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "## passed vs absences\n",
    "sns.boxplot(y='passed', x='absences', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194a9bd",
   "metadata": {},
   "source": [
    "The students who remained less absent passed in the exam .Here,we can see that also the students who failed remained present in the class.This don't make much sense.So,lets check the next feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## passed vs failures\n",
    "sns.boxplot(y='passed', x='failures', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e68a62",
   "metadata": {},
   "source": [
    "* This makes intuitive sense as the students who never failed in the exam passed the final exam.\n",
    "* The students who failed previously also failed in the final exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe2ca9",
   "metadata": {},
   "source": [
    "### 6.1.1 Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf419c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate object for target variable\n",
    "y = df.passed\n",
    "\n",
    "# Create separate object for input features\n",
    "X = df.drop('passed', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c55577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1234,\n",
    "                                                    stratify=df.passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e696d9",
   "metadata": {},
   "source": [
    "### 6.1.2 Data standardization\n",
    "* In Data Standardization we perform zero mean centring and unit scaling; i.e. we make the mean of all the features as zero and the standard deviation as 1.\n",
    "* Thus we use **mean** and **standard deviation** of each feature.\n",
    "* It is very important to save the **mean** and **standard deviation** for each of the feature from the **training set**, because we use the same mean and standard deviation in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20044b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = X_train.mean()\n",
    "train_std = X_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afefb3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save these mean and std dev values\n",
    "train_mean.to_pickle(\"train_mean.pkl\")\n",
    "train_std.to_pickle(\"train_std.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690ba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardize the train data set\n",
    "X_train = (X_train - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f435717",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for mean and std dev.\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1999726",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: We use train_mean and train_std_dev to standardize test data set\n",
    "X_test = (X_test - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for mean and std dev. - not exactly 0 and 1\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b93de",
   "metadata": {},
   "source": [
    "## 6.2 Model-1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018e9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802007a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns of the dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dafebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f77807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes[df.dtypes=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100) ## display max 100 columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a300ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 5 rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c68a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of students\n",
    "n_students = df.shape[0]\n",
    "\n",
    "# Calculate number of features\n",
    "n_features = df.shape[1] - 1\n",
    "\n",
    "# Calculate passing students\n",
    "# Data filtering using .loc[rows, columns]\n",
    "passed = df.loc[df.passed == 'yes', 'passed']\n",
    "n_passed = passed.shape[0]\n",
    "\n",
    "# Calculate failing students\n",
    "failed = df.loc[df.passed == 'no', 'passed']\n",
    "n_failed = failed.shape[0]\n",
    "\n",
    "# Calculate graduation rate\n",
    "total = float(n_passed + n_failed)\n",
    "grad_rate = float(n_passed * 100 / total)\n",
    "\n",
    "print(\"Total no.of students =\",n_students)\n",
    "print(\"No.of features =\",n_features)\n",
    "print(\"No.of students who passed =\",n_passed)\n",
    "print(\"No.of students who failed =\",n_failed)\n",
    "print(\"Graduation rate of the class =\",grad_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e28d8",
   "metadata": {},
   "source": [
    "## 3.2.Distribution of Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram grid\n",
    "df.hist(figsize=(10,10), xrot=-45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73388c",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "We can make out quite a few observations:\n",
    "\n",
    "Let us look at all the plotted numeric features.\n",
    "\n",
    "Consider the histogram for alcohol consumption(DALC and WALC):\n",
    "\n",
    "Workdays:\n",
    "* Above 200 students consumes less amount of alcohol during workdays and very less number of students consumes very high amount of alcohol.\n",
    "\n",
    "Weekend:\n",
    "* As compared to DALC,most of the students consumes alcohol at the weekend.\n",
    "\n",
    "Consider the histogram for Father's and Mother's education(Fedu and Medu):\n",
    "\n",
    "* We can notice that most of the parents are somewhat educated.\n",
    "\n",
    "Absences:\n",
    "\n",
    "* Almost above 290 out of 300 students were present everyday.Very few students never attended school.This can be selected as an important feature considering the fact that those students who were absent almost everyday have a very less chance of passing the final exam.\n",
    "\n",
    "Failures:\n",
    "\n",
    "* Few students have failed in the past exams.This can also be a very good feature which we can consider while doing predictions.\n",
    "\n",
    "Famrel:\n",
    "\n",
    "* Some students have very bad family relationships which can affect them mentally which could ultimately lead to their failures.\n",
    "\n",
    "Free time and Go out:\n",
    "\n",
    "* Above 150 students have normal free time after school and the same range of students go out with their friends.\n",
    "\n",
    "Health:\n",
    "\n",
    "* Almost 45-50 students have very bad health which can affect them physically and may lead to failures.\n",
    "\n",
    "Study time:\n",
    "\n",
    "* 200 students studies only 2 hours per week which is a very less time.\n",
    "\n",
    "Travel time:\n",
    "\n",
    "* Most of the students take a very less time to go from home to school and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e7fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize numerical features\n",
    "# Generates descriptive statistics.\n",
    "# Summarizes the central tendency(the tendency for the values of a random variable to cluster round its mean, mode, or median.)\n",
    "# Summarizes dispersion and shape of a dataset’s distribution, excluding NaN values.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e813252",
   "metadata": {},
   "source": [
    "* Just scan over the min, max and mean rows and make sure the values make sense.\n",
    "* There are no indicator variables since no features have std deviation as 0.\n",
    "* The minimum and maximum values of all the features looks reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee176d0",
   "metadata": {},
   "source": [
    "## 3.3 Distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed4fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize categorical features\n",
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58457c",
   "metadata": {},
   "source": [
    "* There are many categorical features as compared to numeric features.\n",
    "* Most of the features are binary.\n",
    "* Only 3 features are nominal.\n",
    "\n",
    "Let's check the frequency of features having nominal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a59bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mjob'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fjob'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef12b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to get the column name \"passed\" which is the last \n",
    "df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would get everything except for the last element that is \"passed\"\n",
    "df.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13afad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature columns\n",
    "# As seen above, we're getting all the columns except \"passed\" here but we're converting it to a list\n",
    "feature_cols = list(df.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target column 'passed'\n",
    "# As seen above, since \"passed\" is last in the list, we're extracting using [-1]\n",
    "target_col = df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the list of columns\n",
    "print(\"Feature columns =\",feature_cols)\n",
    "print(\"\\nTarget column =\",target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f927410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data into feature data and target data (X_all and y_all, respectively)\n",
    "X_all = df[feature_cols]\n",
    "Y_all = df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the feature information by printing the first five rows\n",
    "X_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the target information by printing the first five rows\n",
    "Y_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99dc7b",
   "metadata": {},
   "source": [
    "** Finally convert 'passed' (target variable) into a binary indicator variable.**\n",
    "* 'Failed' should be 1\n",
    "* 'Passed' should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert passed to an indicator variable\n",
    "df['passed'] = pd.get_dummies( df.passed ).no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b4268",
   "metadata": {},
   "source": [
    "To confirm we did that correctly, display the proportion of students in our dataset who failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f299ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proportion of observations who 'failed'\n",
    "df.passed.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4fd22c",
   "metadata": {},
   "source": [
    "Matches with the earlier count. Seems good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce89df",
   "metadata": {},
   "source": [
    "## 5.2 One-Hot Encoding for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f466902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with dummy features\n",
    "df = pd.get_dummies(df, columns=['school', 'gender', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic'])\n",
    "\n",
    "# Display first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e9133",
   "metadata": {},
   "source": [
    "**Save this dataframe as your analytical base table to use for future use.**\n",
    "* Remember to set the argument index=None to save only the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78db579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save analytical base table\n",
    "df.to_csv('Student_new_DB.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55500e53",
   "metadata": {},
   "source": [
    "# 6. Machine Learning Models\n",
    "## 6.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3215d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c932907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c0a0e",
   "metadata": {},
   "source": [
    "**What to look for?**\n",
    "* The colorbar on the right explains the meaning of the heatmap - Dark colors indicate **strong negative correlations** and light colors indicate **strong positive correlations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d8f15",
   "metadata": {},
   "source": [
    "# 4. Data Cleaning\n",
    "## 4.1 De-duplication and dropping unwanted observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170ab6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede42fa",
   "metadata": {},
   "source": [
    "There are no duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62747a88",
   "metadata": {},
   "source": [
    "## 4.2 Outliers\n",
    "An eye test for all the previous analysis tells us that it doesn't look like outliers will be a huge problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e2fba1",
   "metadata": {},
   "source": [
    "## 4.3 Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d21cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of missing values by feature\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2688970",
   "metadata": {},
   "source": [
    "There are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94c349",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering\n",
    "* Our datset is small and contains sparse data.\n",
    "* A common problem in machine learning is sparse data, which alters the performance of machine learning algorithms and their ability to calculate accurate predictions. \n",
    "* Data is considered sparse when certain expected values in a dataset are missing, which is a common phenomenon in general large scaled data analysis.But,we don't have missing value.Hence,it will not be a huge problem.\n",
    "* We won't always have a lot of domain knowledge for the problem. In these situations, we should rely on exploratory analysis to provide us hints better feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b3ee7",
   "metadata": {},
   "source": [
    "## 5.1 Identify feature and target columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f77c3b",
   "metadata": {},
   "source": [
    "* It is often the case that the data you obtain contains non-numeric features. \n",
    "* This can be a problem, as most machine learning algorithms expect numeric data to perform computations with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## passed vs studytime\n",
    "sns.boxplot(y='passed', x='studytime', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7926a6",
   "metadata": {},
   "source": [
    "* This,too makes sense because the students who studied only 2 hours per week failed in the exam.\n",
    "* The students who studied ranging from 1 our to 3 hours passed the exam. Let us assume that the students who studied only 1-2 hours has more grasping power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0b3d6",
   "metadata": {},
   "source": [
    "### Bivariate segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2b286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of studytime vs. failures\n",
    "sns.lmplot(x='studytime', y='failures', hue='passed', data=df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9919e6",
   "metadata": {},
   "source": [
    "* This is a **bivariate segmentation** because we are plotting the relationship between two variables while segmenting classes using color.\n",
    "* It's a quick way to see if there are potential interactions between different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd815db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the prediction for the positive class (1)\n",
    "y_pred_proba = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 10 predictions\n",
    "y_pred_proba[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604501d5",
   "metadata": {},
   "source": [
    "**Note:** Just as above, we can use these probabilities for model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a82d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve from y_test and pred\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2d88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='l1')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Diagonal 45 degree line\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "\n",
    "# Axes limits and labels\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5001d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC for Train set\n",
    "roc_auc_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ea111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC for Test set\n",
    "print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc22c6",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the model again with the best hyperparameters\n",
    "model = RandomForestClassifier(n_estimators=200, min_samples_split=5, min_samples_leaf=2)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ff1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(-model.feature_importances_)\n",
    "print(\"The features in order of importance are:\")\n",
    "print(50*'-')\n",
    "for feature in X.columns[indices]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7c4fde",
   "metadata": {},
   "source": [
    "## 6.4 Model-3 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed290adc",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* The probality values represent the probability of a data point belonging to class 1 ('Failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bf1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC for Train set\n",
    "print(roc_auc_score(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2039c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the model again with the best hyperparameters\n",
    "model = LogisticRegression(C=10, penalty = 'l2')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(-abs(model.coef_[0,:]))\n",
    "print(\"The features in order of importance are:\")\n",
    "print(50*'-')\n",
    "for feature in X.columns[indices]:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a814d",
   "metadata": {},
   "source": [
    "## 6.3 Model-2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48994a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"student-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5088b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03527d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470acdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd2ae8",
   "metadata": {},
   "source": [
    "**Note:** Just as above we can use these probabilities to get model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC for Train\n",
    "roc_auc_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eb8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate AUC for Test\n",
    "print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994fb169",
   "metadata": {},
   "source": [
    "# 7. Save the winning model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50db514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of absences vs. failures\n",
    "sns.lmplot(x='absences', y='failures', hue='passed', data=df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e595ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot of studytime vs. absences\n",
    "sns.lmplot(x='studytime', y='absences', hue='passed', data=df, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b701c24",
   "metadata": {},
   "source": [
    "## 3.6 Correlations\n",
    "* Finally, let's take a look at the relationships between numeric features and other numeric features.\n",
    "* ***Correlation*** is a value between -1 and 1 that represents how closely values for two separate features move in unison.\n",
    "* Positive correlation means that as one feature increases, the other increases; eg. a child's age and her height.\n",
    "* Negative correlation means that as one feature increases, the other decreases; eg. hours spent studying and number of parties attended.\n",
    "* Correlations near -1 or 1 indicate a strong relationship.\n",
    "* Those closer to 0 indicate a weak relationship.\n",
    "* 0 indicates no relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e068d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7e7b7a",
   "metadata": {},
   "source": [
    "#### A lot of numbers make things difficult to read. So let's visualize this.\n",
    "But first, it's important to notice that the correlations for 'basement' all show as NaN. This is expected because right now that feature doesn't vary at all (its standard deviation is 0), as we saw all the way back in step 2. We'll fix this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000], 'penalty': ['l1', 'l2']}\n",
    "model = GridSearchCV(LogisticRegression(), tuned_params, scoring = 'roc_auc', n_jobs=-1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a810e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict Train set results\n",
    "y_train_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e511c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict Test set results\n",
    "y_pred = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
