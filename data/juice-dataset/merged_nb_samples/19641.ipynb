{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda74880",
   "metadata": {},
   "source": [
    "#### Cross Validation is an important part of machine learning. There are different ways for cross validation. \n",
    "We can use it to evaluate:\n",
    "- what models are more effective\n",
    "- what parameters to use for a specific model\n",
    "- selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397425be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# note: sklearn.cross_validation import train_test_split will be deprecated\n",
    "# note: sklearn.cross_validation import cross_val_score will be deprecated\n",
    "# note: sklearn.cross_validation import KFold will be deprecated\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import math, time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5234d321",
   "metadata": {},
   "source": [
    "#### Model Evaluation Metrics\n",
    "In order to evaluate each model system, we need to have metrics systems to help us. \n",
    "- for classification: the target(s) are category data, so we use ***metrics.accuracy_score*** for measuring\n",
    "  * **error** - binary classification error rate. It is calculated as # (wrong cases) / #(all casees). Treat predicted values with probability p > 0.5 as positive\n",
    "  * **merror** - multiclass classification error rate. It is calculated as # (wrong cases) / #(all casees).\n",
    "- for regression: the target(s) are continuous data. The goal is to ___minimize___ them in the loss functions:\n",
    "  * **Mean Absolute Error (MAE): metrics.mean_absolute_error** \n",
    "  $$mae = \\frac{1}{n}\\sum_{i=0}^n|y_{i} - \\bar{y}_{i}|$$\n",
    "  * **Mean Square Error (MSE): metics.mean_squared_error **\n",
    "  $$mse = \\frac{1}{n}\\sum_{i=0}^n(y_{i} - \\bar{y}_{i})^2$$\n",
    "  * **Root Mean Square Error (RMSE) **\n",
    "  $$rmse = \\sqrt{\\frac{1}{n}\\sum_{i=0}^n(y_{i} - \\bar{y}_{i})^2}$$\n",
    "  * **Logloss** - negaive log-likelihood \n",
    "  * **AUC**  - area under curve          (***Maximize this***)\n",
    "  * **NDCG** - normalized discounted cumulative gain   (***Maximize this***)\n",
    "  * **MAP**  - mean average precision                  (***Maximize this***)\n",
    "- by default, an error metric will be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc25134",
   "metadata": {},
   "source": [
    "### without cross_validation\n",
    "- run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ab6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# fetch data first\n",
    "X = load_iris().data\n",
    "y = load_iris().target\n",
    "\n",
    "# preprocessing data and split it into train and test sets\n",
    "X = preprocessing.scale(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# model\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"The model score is:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0013561",
   "metadata": {},
   "source": [
    "When we do train_test_split, part of the samples are used for testing. However, it provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy!\n",
    "\n",
    "How to make use of those test data for training ===> K-folds cross_validation would solve this problem:\n",
    "- A model is trained using k-1 of the folds as training data;\n",
    "- the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "- The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop (using different test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e081a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the step by step cross validation (cv)\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# fetch data first\n",
    "X = load_iris().data\n",
    "X = preprocessing.scale(X)\n",
    "y = load_iris().target\n",
    "\n",
    "# cv fold\n",
    "nfolds = 10\n",
    "kf = KFold(n_splits=nfolds, shuffle=True, random_state=int(time.time()))\n",
    "\n",
    "clf = SVC(kernel='linear', C=1)\n",
    "rmse = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    #print(\"%s, %s\" % (train_index, test_index))\n",
    "    #print(\"%d, %d\" % (len(train_index), len(test_index)))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    rmse.append(math.sqrt(metrics.mean_squared_error(y_pred, y_test)))\n",
    "\n",
    "print(rmse)\n",
    "print(np.sqrt(np.mean(rmse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770f39f",
   "metadata": {},
   "source": [
    "The **Good** thing is that you usually don't need to inplement the details about cross validation. The sklearn package provides a high level function ***cross_val_score()*** to do all the above.\n",
    "- In addition, for classification problems, ***stratified sampling*** is recommended for creating the folds; that is\n",
    "  * each response (or target) should be represented with equal proportions in each of the K folds.\n",
    "  * **sklearn.cross_val_score()** function does this by default!\n",
    "- Validation options are:\n",
    "    - ['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the simplified version of cross validation (cv)\n",
    "\n",
    "X = load_iris().data\n",
    "X = preprocessing.scale(X)\n",
    "y = load_iris().target\n",
    "\n",
    "#scores = -cross_val_score(svm.SVC(), X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "scores = cross_val_score(SVC(), X, y, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1923edef",
   "metadata": {},
   "source": [
    "#### Now let's use this to tune model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f214abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Booster best train score: {}\".format(bst.best_score))\n",
    "print(\"Booster best iteration: {}\".format(bst.best_iteration))\n",
    "print(\"Booster best number of trees limit: {}\".format(bst.best_ntree_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4fb27",
   "metadata": {},
   "source": [
    "### Cross validating results\n",
    "Native XGBoost package provides an option for cross-validating results (but not as sophisticated as sklearn package). \n",
    "\n",
    "The next input shows a basic execution. \n",
    "\n",
    "***Notice that we are passing only single DMatrix, so it would be good to merge train and test into one object to have more training samples***\n",
    "- by default, we get a pandas data frame object (can be changed with as_pandas param)\n",
    "- metrics are passed as an argument (multiple values are allowed)\n",
    "- we can use own evaluation metrics (param feval and maximize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981be9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 10   # how many estimators\n",
    "\n",
    "hist = xgb.cv(params, dtrain, num_rounds, nfold=10, metrics={'error'}, seed=seed)\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d3823",
   "metadata": {},
   "source": [
    "## Hyper-parameter tuning more\n",
    "- many parameters are tunable. Each one results in different output. The question is which conbination produces best results.\n",
    "- scikit-learn provides a lot of such modules for us to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a5b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Note: from sklearn.cross_validation import StratifiedKFold has been deprecated\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "seed = 342  # fixed seed makes results reproducible\n",
    "np.random.seed(seed)\n",
    "\n",
    "# generate artificial dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=8, n_redundant=3, n_repeated=2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92b042",
   "metadata": {},
   "source": [
    "Define cross-validation strategy for testing. Let's use ***StratifiedKFold*** which guarantees that target labels are equally distributed across each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cv.get_n_splits(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a18c58",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "In grid-search, we start by defining a dictionary holding possible parameter values we want to test. \n",
    "- All combinations will be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640f837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = { 'max_depth' : [1,2,3], \n",
    "                'n_estimators' : [5, 10, 25, 50],\n",
    "                'learning_rate' : np.linspace(1e-16, 1, 3)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ca236a",
   "metadata": {},
   "source": [
    "#### add a dictionary for fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231533d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_fixed = { 'objective' : 'binary:logistic',\n",
    "                 'silent' : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7838c540",
   "metadata": {},
   "source": [
    "Create a GridSearchCV estimator, We will be looking for combination giving the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee166be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# fetch data first\n",
    "X = load_iris().data\n",
    "#X = preprocessing.scale(X)\n",
    "y = load_iris().target\n",
    "\n",
    "k_range = range(1,31)\n",
    "k_scores = []\n",
    "\n",
    "for i in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    k_scores.append(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())\n",
    "    \n",
    "print(k_scores)\n",
    "\n",
    "# plotting\n",
    "plt.plot(k_range, k_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e586651f",
   "metadata": {},
   "source": [
    "#### The following will be done in XGBoost\n",
    "- the dataset will be taken from Mushroom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890812df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from pprint import pprint\n",
    "\n",
    "# for reproducibility, if you don't want this, you could use time.time() to get different value every time\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "dtrain = xgb.DMatrix('../../data/agaricus.txt.train')\n",
    "dtest  = xgb.DMatrix('../../data/agaricus.txt.test')\n",
    "\n",
    "# train parameters - we are going to use 5 decision tree stumps with average learning rate.\n",
    "# the defaul error metric is 'error'\n",
    "params = {'objective' : 'binary:logistic',\n",
    "          'max_depth' : 2,\n",
    "          'silent' : 1,\n",
    "          'eta' : 0.5}\n",
    "num_rounds = 5\n",
    "watch_list = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "\n",
    "# training\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51f141",
   "metadata": {},
   "source": [
    "### let's change the error metric to logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eval_metric'] = 'logloss'\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a4064",
   "metadata": {},
   "source": [
    "### we could use multiple error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37da1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eval_metric'] = ['auc', 'map']\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7b662d",
   "metadata": {},
   "source": [
    "### Creating custom evaluation metric\n",
    "In order to create our own evaluation metric, the only thing needed to do is to create a method taking two arguments - ***predicted probabilities*** and ***DMatrix*** objet holding training data\n",
    "\n",
    "In this example, our classification metric will simply count the number of mis-classified examples assuming that classes with p > 0.5 are positive. You can change this threshold if you want more certainty\n",
    "\n",
    "The algorithm is getting better when the number of mis-classified examples is getting lower. Remember to also set the argument ***maximize=False*** while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassified(pred_probs, dtrain):\n",
    "    labels = dtrain.get_label()   # obtain true labels\n",
    "    preds  = pred_probs > 0.5     # obtain predicted values\n",
    "    return 'misclassified', np.sum(labels != preds)\n",
    "\n",
    "params['eval_metric'] = []\n",
    "# the argument order is important! if you switch them, you will get error messages\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list, feval=misclassified, maximize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f0827",
   "metadata": {},
   "source": [
    "### Extracting the evaluation results\n",
    "We can get evaluation scores by declaring a dictionary for holding values and passing it as a parameter for ***evals_result*** argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8641cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_results = {}\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list, feval=misclassified, maximize=False, evals_result=evals_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now reuse these scores for other purposes (such as plotting)\n",
    "pprint(evals_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4854e9",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "There is a nice optimization trick when fitting multiple trees.\n",
    "\n",
    "You can train the model until the validation score stops improving. Validation error needs to decrease at least every early_stopping_rounds to continue training. This approach results in simpler model, because the lowest number of trees will be found (simplicity).\n",
    "\n",
    "In the following example a total number of 1500 trees is to be created, but we are telling it to stop if the validation score does not improve for last ten iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520331a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 1500\n",
    "params['eval_metric'] = 'error'\n",
    "\n",
    "bst = xgb.train(params, dtrain, num_rounds, watch_list, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300db37d",
   "metadata": {},
   "source": [
    "When using early_stopping_rounds parameter, the resulting model will have 3 additional fields - ***bst.best_score***, ***bst.best_iteration*** and ***bst.best_ntree_limit***\n",
    "\n",
    "- Note: train() will return a model from the last iteration, not the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ad9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_grid = GridSearchCV( estimator=XGBClassifier(**params_fixed, seed=seed),\n",
    "                         param_grid=params_grid,\n",
    "                         cv=cv,\n",
    "                         scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd9201",
   "metadata": {},
   "source": [
    "Before running the calculations, notice that we will have 3 \\* 4 \\* 3 \\* 10 = 360 models created to test all combinations.\n",
    "- you should always have rough estimations about what is going to happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd14257",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949286e",
   "metadata": {},
   "source": [
    "Now, we can look at all obtained scores, and try to manually see what matters and what not, A quick glance looks that the larger n_estimators then the accuracy is higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c38995",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca356dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst_grid.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0250ff",
   "metadata": {},
   "source": [
    "If there are too many results, we can filter them manually to get the best combination\n",
    "- Note: looking for best parameters is an iterative process. You should start with coarsed-granularity and move to more detailed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best accuracy obtained: {0}\".format(bst_grid.best_score_))\n",
    "print(\"Parameters\")\n",
    "for key, value in bst_grid.best_params_.items():\n",
    "    print(\"\\t{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c90aba",
   "metadata": {},
   "source": [
    "### Randomized Grid-Search\n",
    "when the number of parameters and their values is getting big, the traditional grid-search approach quickly becomes ineffective.\n",
    "- A possible solution might be to randomly pick certain parameters from their distribution. While it's not an exhaustive solution, it's worth giving a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333982e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parameters distribution dictionary:\n",
    "params_dist_grid = { 'max_depth' : [1, 2, 3, 4],\n",
    "                     'gamma' : [0, 0.5, 1],\n",
    "                     'n_estimators' : randint(1, 1001),   # uniform discrete random distribution\n",
    "                     'learning_rate' : uniform,           # gaussain distribution\n",
    "                     'subsample' : uniform(),             # gaussain distribution\n",
    "                     #'colsample_bytree' : uniform()       # gaussain distribution\n",
    "                   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679fc1c8",
   "metadata": {},
   "source": [
    "Initialize ***RandomizedSearchCV*** to randomly pick 10 combinations of parameters. \n",
    "- with this approach you can easily control the number of tested models"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
