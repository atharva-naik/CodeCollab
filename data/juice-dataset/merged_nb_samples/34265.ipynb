{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe6084d",
   "metadata": {},
   "source": [
    "# Post classification Experiment using Scikit learn\n",
    "\n",
    "* Date 20/02/18\n",
    "* Dylan Butler\n",
    "\n",
    "## Task\n",
    "The overall task of this experiment is to create a trained classifier to correctly classify whether or not a post is useful for quizes and knowledge testing of Java core concepts.\n",
    "\n",
    "## Data\n",
    "The data for this experiment consists of a manually labelled dataset of 1500 stackoverflow posts. These posts have been filtered according to the following characteristics:\n",
    "\n",
    "* They posses the structure of either a \"how-to\"(procedural intent) or a \"why\"(casual intent) type of question\n",
    "* They have a minimum score of 7 (post score)\n",
    "* They have not been deleted\n",
    "* They have not been closed\n",
    "* They have an accepted answer\n",
    "\n",
    "After extracting this data I conducted an analysis on the resulting dataset to gain a deeper understanding of the data:\n",
    "\n",
    "### Extracted Data insights\n",
    "* Group 1 (useful for quizzes):\n",
    "    * How to split a string in Java?\n",
    "    * Read and convert an input stream to a string?\n",
    "    * How to read all files in a folder in Java?\n",
    "    * How to round a number to n decimal places in Java?\n",
    "    * How to parse JSON in Java?\n",
    "    * How do I declare and initialize an array in Java?\n",
    "    * Why is it faster to process an unsorted array vs a sorted array\n",
    "    * How do I compare strings in Java?\n",
    "* Group 2 (not useful fr quizzes):\n",
    "    * How do I fix android.os.NetworkOnMainThreadException?\n",
    "    * How do you assert that a certain exception is thrown in JUnit 4 tests?\n",
    "    * How to fix java.lang.UnsupportedClassVersionError: Unsupported major.minor version\n",
    "    * How to add local jar files to a Maven project?\n",
    "    * How do I set up IntelliJ IDEA for Android applications?\n",
    "    * How does autowiring work in Spring?\n",
    "    * How do I tell Maven to use the latest version of a dependency?\n",
    "    * Unfortunately MyApp has stopped. How can I solve this?\n",
    "    * Why is subtracting these two times (in 1927) giving a strange result?\n",
    "\n",
    "### Key Findings\n",
    "* Useless Q's\n",
    "    * A key difference I can spot is that most of the questions that pose no use are environment, framework, related and focus on a technology that uses Java.\n",
    "    * Verbs like; set-up, fix, stopped ... i.e. less java specific and more generic - used in everyday language. \n",
    "* Useful Q's\n",
    "    * The useful questions seem to be following a pattern in which the main words in the questions (split, string, read, java, JSON, declare, initialize) are all words closely related to Java and programming concepts in general.  \n",
    "    * The verbs/action words used in the useful q's are closely associated with java itself.\n",
    "    \n",
    "    \n",
    "# Experiment Process\n",
    "\n",
    "1. Chunk titles and bodies into a single body\n",
    "    * eliminate code snippets \n",
    "    * remove stop words\n",
    "    * lemmatise each body\n",
    "2. Extract the core features from the text that the algorithm can learn from\n",
    "3. Train a classifier\n",
    "4. Evaluate\n",
    "5. Improve results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22d33dc",
   "metadata": {},
   "source": [
    "# 1) Generating the data\n",
    "The format I will converting the data into for this first experiment will be flattened chunks of (tags, title and body) of each post. \n",
    "\n",
    "1. Remove all the code snippets from the bodys and titles of the  text --> using BeautifulSoup\n",
    "2. Merge the title, bodies into a single chunk\n",
    "3. remove all stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e04eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "df = pandas.read_csv('./data/procedural_casual_Q_1500_SO_Java.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd5617",
   "metadata": {},
   "source": [
    "Merge each posts body and title into a single chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd27bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(df_new['text'].values, df_new['OK'].values)\n",
    "pipeline.predict(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a9beaf",
   "metadata": {},
   "source": [
    "# 4) Cross-validating the model - K-fold\n",
    "\n",
    "At this stage in the process it is required to cross validate the model i.e. check its accuracy to ensure that it can give accurate predictions when faced with new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ef6482",
   "metadata": {},
   "source": [
    "Shuffling the data to ensure that our training and test sets are balanced when we perform the 80:20 split, training:test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97305295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frac keyword - specifies the number of rows to return in the rand\n",
    "# sample -> 1 returns all rows\n",
    "df_new = df_new.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c38064",
   "metadata": {},
   "source": [
    "## create an instance of K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d73337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "kf = KFold(n=len(df_new), n_folds = 10)\n",
    "scores = [] #holds the score for each\n",
    "confusion = np.array([[0,0], [0,0]]) #initialize the confusion matrix\n",
    "\n",
    "for train_ind, test_ind in kf:\n",
    "    \n",
    "    #training data(x) and classification(y)\n",
    "    train_x = df_new.iloc[train_ind]['text'].values\n",
    "    train_y = df_new.iloc[train_ind]['OK'].values\n",
    "    \n",
    "    #testing training data\n",
    "    test_x = df_new.iloc[test_ind]['text'].values\n",
    "    test_y = df_new.iloc[test_ind]['OK'].values\n",
    "    \n",
    "    #train and predict each of the values\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predictions = pipeline.predict(test_x)\n",
    "    \n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total posts classified:', len(df_new))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c4c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c0453",
   "metadata": {},
   "source": [
    "### save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c75a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pipeline, open('./models/multinomialnb_post_classifier.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b436a2",
   "metadata": {},
   "source": [
    "# Generating more features with N-grams\n",
    "\n",
    "The counts where generated using the \"bag of words\" approach which counts single instances of words. Using n-grams we can count phrases for example \"this is a phrase\" --> \"this is\" \"is a\" \"a phrase\"\n",
    "\n",
    "CountVectorizer can be instructed to use this approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e01628",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n=len(df_new), n_folds = 10)\n",
    "scores = [] #holds the score for each\n",
    "confusion = np.array([[0,0], [0,0]]) #initialize the confusion matrix\n",
    "\n",
    "for train_ind, test_ind in kf:\n",
    "    \n",
    "    #training data(x) and classification(y)\n",
    "    train_x = df_new.iloc[train_ind]['text'].values\n",
    "    train_y = df_new.iloc[train_ind]['OK'].values\n",
    "    \n",
    "    #testing training data\n",
    "    test_x = df_new.iloc[test_ind]['text'].values\n",
    "    test_y = df_new.iloc[test_ind]['OK'].values\n",
    "    \n",
    "    #train and predict each of the values\n",
    "    pipeline.fit(train_x, train_y)\n",
    "    predictions = pipeline.predict(test_x)\n",
    "    \n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=1)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pipeline, open('./models/ngrams_multinomialnb_post_classifier.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a353bbd",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc6756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('count_vectorizer', CountVectorizer('''ngram_range=(1, 2)''')),\n",
    "    ('tfidf_transformer', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1599f",
   "metadata": {},
   "source": [
    "This model performs exceptionally bad compared to the other two previous. An overall accuracy of 54% is recorded. We can disregard this model for the moment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb20e8e",
   "metadata": {},
   "source": [
    "# Bernoulli Naive Bayes Model\n",
    "\n",
    "This algorithm focuses on the n-grams occurences rather than the counts. A vector of booleans representing the presence of absence of an n-gram. \n",
    "\n",
    "After some research I found that this model is said to perform better on shorter documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise a new column\n",
    "df_new['text'] = \"\"\n",
    "\n",
    "# loop thorugh the data frame\n",
    "for index, row in df_new.iterrows():\n",
    "    \n",
    "    #target chunk of data\n",
    "    words = row['cleaned_body_title']\n",
    "    tmp =[]\n",
    "    for word in words.split():\n",
    "        #stopword removal\n",
    "        if word not in stopWords:\n",
    "            #lemmatise\n",
    "            word = wordnet_lemmatizer.lemmatize(word)\n",
    "            tmp.append(word)\n",
    "    df_new.loc[index, 'text'] = ' '.join(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bda5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(['cleaned_body_title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976345b",
   "metadata": {},
   "source": [
    "# 2) Extracting Features from the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "counts = cv.fit_transform(df_new['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed270975",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d8c2b1",
   "metadata": {},
   "source": [
    "### list all of the elements in the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fe60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269e4e6",
   "metadata": {},
   "source": [
    "# 3) Classifying the Posts\n",
    "\n",
    "The first classifier I will be implementing is a naive bayes classifier. Bayes theorom - each feature (in this case word counts) is independent from every other one and each one contributes to the probability that an example belongs to a particular class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5a029a",
   "metadata": {},
   "source": [
    "## Create, Initialize and train a new MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "#targets are the OK column in the df_new dataframe above\n",
    "targets = df_new['OK'].values\n",
    "#train the NB classifier\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebca8a2",
   "metadata": {},
   "source": [
    "### test out the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99683079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merges title and body into a single chunk\n",
    "df['Title_Body_Chunk'] = df[df.columns[2:4]].apply(lambda x: ','.join(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Title_Body_Chunk = df.Title_Body_Chunk.apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ecadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_attrs(soup):\n",
    "    for tag in soup.findAll(True): \n",
    "        tag.attrs = None\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248e99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise a new column\n",
    "df['cleaned_body_title'] = \"\"\n",
    "\n",
    "# loop thorugh the data frame\n",
    "for index, row in df.iterrows():\n",
    "        \n",
    "        #print(row.Title_Body_Chunk)\n",
    "        \n",
    "        soup = BeautifulSoup(row['Title_Body_Chunk'], 'html5lib')\n",
    "        \n",
    "        for code in soup.find_all(\"code\"):\n",
    "            code.decompose()\n",
    "        cleaned = soup.get_text()\n",
    "        \n",
    "        #create a new column to hold the cleaned data\n",
    "        df.loc[index, \"cleaned_body_title\"] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10996046",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Title', 'Body', 'Title_Body_Chunk'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d9e83",
   "metadata": {},
   "source": [
    "Generate a Dataframe with only the classification and the chunk of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b856b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['cleaned_body_title', 'OK']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d8e6b1",
   "metadata": {},
   "source": [
    "## remove all stopwords and lemmatise remaining values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da4d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\"How do I explicitly pass the type argument to a generic Java method? I do not understand how to achieve this\", \"How do I generate a new eclipse project? I am trying to create a new eclipse project and I need help setting it up\"]\n",
    "example_counts = cv.transform(examples)\n",
    "predictions = classifier.predict(example_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c733e",
   "metadata": {},
   "source": [
    "#### Notes on the above:\n",
    "\n",
    "The predictor can correctly classify between the two examples that were generated using the chunk of text provided for each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160f54d1",
   "metadata": {},
   "source": [
    "## Pipelining - connecting the process\n",
    "\n",
    "a pipeline can be introduced to merge both the feature extraction and classification into one operation"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
