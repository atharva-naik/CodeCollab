{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c003f6",
   "metadata": {},
   "source": [
    "<img src='img/logo.png'>\n",
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b690785",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13909cbb",
   "metadata": {},
   "source": [
    "This notebook covers \"pipelines\" or machine learning workflows that are expressed as a sequence of named steps with parameters that may be varied in a grid or randomized search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b673e4b",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Pipelines](#Pipelines)\n",
    "\t* [Algorithm chains and pipelines](#Algorithm-chains-and-pipelines)\n",
    "\t\t* [Building pipelines](#Building-pipelines)\n",
    "\t\t* [Using pipelines in grid searches](#Using-pipelines-in-grid-searches)\n",
    "\t\t\t* [Another `Pipeline` example](#Another-Pipeline-example)\n",
    "\t\t\t* [And here it is with `Pipeline`](#And-here-it-is-with-Pipeline)\n",
    "\t\t* [The General Pipeline Interface](#The-General-Pipeline-Interface)\n",
    "\t\t* [`Pipeline` creation with ``make_pipeline``](#Pipeline-creation-with-make_pipeline)\n",
    "\t\t\t* [Accessing step attributes](#Accessing-step-attributes)\n",
    "\t\t\t* [Accessing attributes in grid-searched pipeline.](#Accessing-attributes-in-grid-searched-pipeline.)\n",
    "\t\t* [Grid-searching preprocessing steps and model parameters](#Grid-searching-preprocessing-steps-and-model-parameters)\n",
    "* [Summary](#Summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['image.interpolation'] = \"none\"\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "import src.mglearn as mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368b93d",
   "metadata": {},
   "source": [
    "## Algorithm chains and pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba65b8",
   "metadata": {},
   "source": [
    "The next few cells show a pipeline workflow written out in declarative form with the tools we have covered so far, including `MinMaxScaler` for scaling, and `GridSearchCV` for model selection with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450017f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load and split the data\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "# compute minimum and maximum on the training data\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "# rescale training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "svm = SVC()\n",
    "# learn an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "# scale test data and score the scaled data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "svm.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f1532",
   "metadata": {},
   "source": [
    "Grid search with cross validation on the scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc480f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "scores = np.empty(0)\n",
    "for train, test in kfold5.split(X):\n",
    "    X_train = X[train]\n",
    "    y_train = y[train]\n",
    "    X_test  = X[test]\n",
    "    y_test  = y[test]\n",
    "    \n",
    "    selector = SelectPercentile(score_func=f_regression, percentile=5).fit(X_train, y_train)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    score = Ridge().fit(X_train_selected, y_train).score(X_test_selected, y_test)\n",
    "    scores = np.append(scores, score)\n",
    "    \n",
    "    \n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff543b43",
   "metadata": {},
   "source": [
    "#### And here it is with `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "pipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression, percentile=5)), \n",
    "                 (\"ridge\", Ridge())])\n",
    "np.mean(cross_val_score(pipe, X, y, cv=kfold5.split(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe879f",
   "metadata": {},
   "source": [
    "### The General Pipeline Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9868cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "    X_transformed = X\n",
    "    for step in self.steps[:-1]:\n",
    "        # iterate over all but the final step\n",
    "        # fit and transform the data\n",
    "        X_transformed = step[1].fit_transform(X_transformed, y)\n",
    "    # fit the last step\n",
    "    self.steps[-1][1].fit(X_transformed, y)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4db76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "    X_transformed = X\n",
    "    for step in self.steps[:-1]:\n",
    "        # iterate over all but the final step\n",
    "        # transform the data\n",
    "        X_transformed = step[1].transform(X_transformed)\n",
    "    # fit the last step\n",
    "    return self.steps[-1][1].predict(X_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a8d73",
   "metadata": {},
   "source": [
    "**Pipeline Illustration**\n",
    "<img src=\"img/pipeline-diagram.png\" alt=\"Pipeline Illustration\" width=\"50%\"/>\n",
    "\n",
    "Image: CC-BY-NA, [Karl Rosaen](http://karlrosaen.com/ml/learning-log/2016-06-20/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad1641",
   "metadata": {},
   "source": [
    "### `Pipeline` creation with ``make_pipeline``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edd1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "# standard syntax\n",
    "pipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n",
    "# abbreviated syntax\n",
    "pipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_short.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e77291",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# auto-naming the steps\n",
    "pipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b07ccab",
   "metadata": {},
   "source": [
    "#### Accessing step attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97477325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline defined above to the cancer dataset\n",
    "pipe.fit(cancer.data)\n",
    "# extract the first two principal components from the \"pca\" step\n",
    "components = pipe.named_steps[\"pca\"].components_\n",
    "print(components.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c11e8",
   "metadata": {},
   "source": [
    "#### Accessing attributes in grid-searched pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8f2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab2d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=4)\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa26ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_.named_steps[\"logisticregression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_.named_steps[\"logisticregression\"].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8626bf4d",
   "metadata": {},
   "source": [
    "### Grid-searching preprocessing steps and model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54772a38",
   "metadata": {},
   "source": [
    "The following shows a pipeline that\n",
    " * Normalizes to 0 mean and unit variance (`StandardScaler`)\n",
    " * Adds polynomial features (`PolynomialFeatures`)\n",
    " * Runs `Ridge` regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f83ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PolynomialFeatures(),\n",
    "    Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14eb9a3",
   "metadata": {},
   "source": [
    "The steps were auto-named using lower case versions of the classes we used in the `Pipeline`.  Double underscores are used to control the `degree` parameter for polynomial features and `alpha` parameter for `Ridge` regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2bde7",
   "metadata": {},
   "source": [
    "`pipe = Pipeline(` in the next cell is a shorter way of expressing the logic in the cells above.  It creates a `Pipeline` with two named steps, `scaler` and `svm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3372d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f292bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe has an interface like SVC() but the \"fit\" method is inclusive of pipeline steps\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab572d",
   "metadata": {},
   "source": [
    "### Using pipelines in grid searches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d8d427",
   "metadata": {},
   "source": [
    "This is the same kind of `GridSearch` we have done, but we are passing `pipe`, our `Pipeline` with two steps, as the `estimator` argument to `GridSearchCV`.\n",
    "\n",
    "Where the `param_grid` needs to refer to parameters of named steps, it can use double underscores, as in `svm__C` to specify a list of parameters to try for support vector machine's error `C` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28483d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798903b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best cross-validation accuracy:\", grid.best_score_)\n",
    "print(\"test set score: \", grid.score(X_test, y_test))\n",
    "print(\"best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b522a5",
   "metadata": {},
   "source": [
    "#### Another `Pipeline` example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39d069",
   "metadata": {},
   "source": [
    "Let's start with 100 rows and 10,000 columns of random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7666d18",
   "metadata": {},
   "source": [
    "`SelectPercentile` feature selctor as first step, the `Ridge` regression (regression with `L2` norm penalty).\n",
    "\n",
    "Select the top 5% of the features with the lowest f_regression sore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec82b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "\n",
    "select = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\n",
    "X_selected = select.transform(X)\n",
    "print(X_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02c4da",
   "metadata": {},
   "source": [
    "To perform cross validation we need to fit and transform each fold separately.\n",
    "\n",
    "First, here's how it would be done delcaratively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold5 = KFold(5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee58f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# illustration purposes only, don't use this code\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "print(\"best cross-validation accuracy:\", grid.best_score_)\n",
    "print(\"test set score: \", grid.score(X_test_scaled, y_test))\n",
    "print(\"best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c8339",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_improper_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1035c86",
   "metadata": {},
   "source": [
    "### Building pipelines"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
