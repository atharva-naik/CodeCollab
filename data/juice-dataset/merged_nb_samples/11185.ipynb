{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c29ea46",
   "metadata": {},
   "source": [
    "**The pyspark operation about RDD**\n",
    "\n",
    "The information is from the link below.  \n",
    "\n",
    "* [https://spark.apache.org/docs/latest/rdd-programming-guide.html](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a407ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec34582",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"appName\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed518d",
   "metadata": {},
   "source": [
    "**1. Read the iris data(../data/iris.csv) and show first ten lines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf87dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sc.textFile(\"../data/iris.csv\")\n",
    "iris.take(10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7333c9",
   "metadata": {},
   "source": [
    "**2. From iris data, select the lines with condition that the last column is 'setosa' and show the first ten lines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.filter(lambda line: 'setosa' == line.split(',')[-1]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82980fe4",
   "metadata": {},
   "source": [
    "**3. From iris data, sample 10 lines with replacement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f081b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# without transformation\n",
    "iris.takeSample(True, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8366850a",
   "metadata": {},
   "source": [
    "**4. From iris data, sample lines each with 1/10 probability with condition that the species are 'setosa' and 'versicolor'. And union those.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a26f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa = iris.filter(lambda line: 'setosa' in line).sample(True, 1/10)\n",
    "versicolor = iris.filter(lambda line: 'versicolor' in line).sample(True, 1/10)\n",
    "\n",
    "setosa.union(versicolor).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be6487",
   "metadata": {},
   "source": [
    "**5. From iris data, make key-data with condition that the key is the last column and the value is the first column. And count the data per key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_iris = iris.map(lambda line: (line.split(',')[-1], float(line.split(',')[0])))\n",
    "key_value_iris.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4def0b6",
   "metadata": {},
   "source": [
    "**6. To the key-value data, sum-up based on the key(species).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ad603",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_iris.reduceByKey(lambda a,b:a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bdd7b1",
   "metadata": {},
   "source": [
    "**7. To the key-value data, sort with descending by key and show the first 10 lines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac733f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_iris.sortByKey(False).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ad4979",
   "metadata": {},
   "source": [
    "**8. Show the row size of iris data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c963237",
   "metadata": {},
   "source": [
    "**9. By map() and reduce(), calculate the sum of all values of iris data except for species column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d3c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.map(lambda line: line.split(',')[:-1]).map(lambda line: sum([float(fac) for fac in line])).reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same thing with smaller steps\n",
    "# omit species column\n",
    "value_columns = iris.map(lambda line: line.split(',')[:-1])\n",
    "\n",
    "# sum per row\n",
    "sum_per_row = value_columns.map(lambda line: sum([float(fac) for fac in line]))\n",
    "\n",
    "# total\n",
    "sum_per_row.reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c6cde",
   "metadata": {},
   "source": [
    "**The pyspark operation about DataFrames**\n",
    "\n",
    "The information is from the link below.\n",
    "\n",
    "* [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40b0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456b974",
   "metadata": {},
   "source": [
    "**10. Read the cat data(../data/cat.json).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fbd2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json\n",
    "cat = spark.read.json(\"../data/cat.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a7e66",
   "metadata": {},
   "source": [
    "**11. Check the type of iris and cat.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"iris: {}\".format(type(iris)))\n",
    "print(\"cat: {}\".format(type(cat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844e8c98",
   "metadata": {},
   "source": [
    "**12. Show the cat data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca75fca",
   "metadata": {},
   "source": [
    "**13. Show the schema of cat data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c3ba6",
   "metadata": {},
   "source": [
    "**14. Select the 'name' column of cat data and show it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc8ff0",
   "metadata": {},
   "source": [
    "**15. From cat data, select the line with condition that the value of 'name' column is 'Deborah' and show it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ad3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.filter(cat[\"name\"] == \"Deborah\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac9674",
   "metadata": {},
   "source": [
    "**16. By spark.sql(), show all the cat data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc09cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.createGlobalTempView(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7ed87",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM global_temp.cat\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c96a6",
   "metadata": {},
   "source": [
    "**17. Convert iris data(whose type is `<class 'pyspark.rdd.RDD'>`) to DataFrame and show it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_tuple = iris.map(lambda line: line.split(',')).map(lambda line: list(map(float, line[:-1])) + [str(line[-1])])\n",
    "\n",
    "iris_dataframe = spark.createDataFrame(iris_tuple, ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "iris_dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5a91e",
   "metadata": {},
   "source": [
    "**18. Check the type of each columns of iris DataFrame.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataframe.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09117f2e",
   "metadata": {},
   "source": [
    "**19. By spark.sql(), select lines from iris with condition that sepal_width > 3.0 and show it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803fc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sepal_width > 3.0\n",
    "iris_dataframe.createOrReplaceTempView(\"iris\")\n",
    "spark.sql(\"SELECT * FROM iris WHERE sepal_width > 3.0\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed54d53",
   "metadata": {},
   "source": [
    "**20. Change the iris_dataframe's column names to col_1 ~ col_5.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name = ['col_' + str(i+1) for i in range(5)]\n",
    "iris_dataframe = iris_dataframe.toDF(*new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31f557",
   "metadata": {},
   "source": [
    "**21. Convert the type of col_1 ~ col_4 of iris_dataframe to float.**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
