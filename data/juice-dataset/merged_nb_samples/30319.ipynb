{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e966ad",
   "metadata": {},
   "source": [
    "# Goal: Create a word vector from a game of thrones dataset and analyze them to see semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# for word encoding\n",
    "import codecs\n",
    "\n",
    "# regex\n",
    "import glob\n",
    "\n",
    "# concurrency\n",
    "import multiprocessing\n",
    "\n",
    "# operating system\n",
    "import os\n",
    "\n",
    "# pretty printing\n",
    "import pprint\n",
    "\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "# natural language toolkit\n",
    "import nltk\n",
    "\n",
    "# word2vec\n",
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "# dimensionality reduction\n",
    "import sklearn.manifold\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69880a77",
   "metadata": {},
   "source": [
    "# Step 1: process our data\n",
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca5087",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt') #pre-train tokenizer (we take a piece of text and we split it in sentences (in this case))\n",
    "nltk.download('stopwords') # words like and, the, an, of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ee975",
   "metadata": {},
   "source": [
    "## get the book names, matching txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_filenames = sorted(glob.glob(\"./data/*.txt\"))\n",
    "print(book_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fdb853",
   "metadata": {},
   "source": [
    "## Combine the books into one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = u\"\"\n",
    "for book_filename in book_filenames:\n",
    "    print(\"Reading '{}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, 'r', 'utf-8') as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "    print(\"Corpus raw is now {} characters long\".format(len(corpus_raw)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c74335",
   "metadata": {},
   "source": [
    "## split the corpus into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8425cae",
   "metadata": {},
   "source": [
    "## contert the sentences into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary , split into words, no hyphens\n",
    "# list of words\n",
    "def sentence_to_wordlist(raw):\n",
    "    clean = re.sub('[^a-zA-Z]', ' ', raw)\n",
    "    words = clean.split()\n",
    "    return words\n",
    "\n",
    "# sentences where each word is tokenized\n",
    "sentences = []\n",
    "\n",
    "for raw_sentence in raw_sentences:\n",
    "    if len(raw_sentence) > 0:\n",
    "        sentences.append(sentence_to_wordlist(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6035b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_sentences[60])\n",
    "print(sentences[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cbb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count = sum([len(sentence) for sentence in sentences])\n",
    "print(\"The book corpus contains {0:,} tokens\".format(token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c37c3",
   "metadata": {},
   "source": [
    "# Step 2: Build Word2Vec\n",
    "\n",
    "## Word Embedding\n",
    "\n",
    "![Linear relationships](./images/linear-relationships.png)\n",
    "\n",
    "### Once we have vectors, 3 main tasks that vectors help with:\n",
    "- DISTANCE\n",
    "- SIMILARITY\n",
    "- RANKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d9b5b",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c38ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the resulting word vectors\n",
    "# the more dimensions, more computationally expensive to train, \n",
    "# but also more accurate\n",
    "# more dimensions = more generalized\n",
    "num_features = 300\n",
    "\n",
    "# minimum word count threshold\n",
    "min_word_count = 3\n",
    "\n",
    "# number of threads to run in parallel.\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# contexxt window length\n",
    "context_size = 7\n",
    "\n",
    "# Downsample setting for frequent words\n",
    "# 0 - 1e-5 is good for this\n",
    "downsampling = 1e-3\n",
    "\n",
    "# Seed for random number generator, to make the result reproducible.\n",
    "# deterministic, good for debugging\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778e685",
   "metadata": {},
   "source": [
    "## Actual model using gensim\n",
    "Docstring:     \n",
    "Class for training, using and evaluating neural networks described in https://code.google.com/p/word2vec/\n",
    "\n",
    "- `sg` defines the training algorithm. By default (`sg=0`), CBOW is used.\n",
    "Otherwise (`sg=1`), skip-gram is employed.\n",
    "\n",
    "- `size` is the dimensionality of the feature vectors.\n",
    "\n",
    "- `window` is the maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "- `seed` = for the random number generator. Initial vectors for each\n",
    "word are seeded with a hash of the concatenation of word + str(seed).\n",
    "Note that for a fully deterministically-reproducible run, you must also limit the model to\n",
    "a single worker thread, to eliminate ordering jitter from OS thread scheduling. (In Python\n",
    "3, reproducibility between interpreter launches also requires use of the PYTHONHASHSEED\n",
    "environment variable to control hash randomization.)\n",
    "\n",
    "- `min_count` = ignore all words with total frequency lower than this.\n",
    "\n",
    "- `sample` = threshold for configuring which higher-frequency words are randomly downsampled;\n",
    "    default is 1e-3, useful range is (0, 1e-5).\n",
    "\n",
    "- `workers` = use this many worker threads to train the model (=faster training with multicore machines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec = w2v.Word2Vec(\n",
    "    sg=1,\n",
    "    seed=seed,\n",
    "    workers=num_workers,\n",
    "    size=num_features,\n",
    "    min_count=min_word_count,\n",
    "    window=context_size,\n",
    "    sample=downsampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd371065",
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8578977",
   "metadata": {},
   "source": [
    "# Step 3: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cf89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec.train(sentences, total_examples=thrones2vec.corpus_count, epochs=thrones2vec.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af48f75",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6fef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"trained\"):\n",
    "    os.makedirs(\"trained\")\n",
    "thrones2vec.save(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5beb4",
   "metadata": {},
   "source": [
    "# Step 4: Explore the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "thrones2vec = w2v.Word2Vec.load(os.path.join(\"trained\", \"thrones2vec.w2v\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d5b48",
   "metadata": {},
   "source": [
    "## Compress the word vectors into 2D space and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ae8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ba4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = tokenizer.tokenize(corpus_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf33dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = sklearn.manifold.TSNE(n_components=2, random_state=0)\n",
    "all_word_vectors_matrix = thrones2vec.wv.syn0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92583d59",
   "metadata": {},
   "source": [
    "## train t-SNE"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
