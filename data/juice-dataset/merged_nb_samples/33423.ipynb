{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4df322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true;\n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d31769",
   "metadata": {},
   "source": [
    "- [0 Introduction](#0-Introduction)\n",
    "- [1 Data](#1-Data)\n",
    "  - [1.1 Lending Club Data](#1.1-Lending-Club-Data)\n",
    "    - [1.1.1 Loan Status](#1.1.1-Loan-Status)\n",
    "    - [1.1.2 Missing Data](#1.1.2-Missing-Data)\n",
    "    - [1.1.3 Transform String Variables](#1.1.3-Transform-String-Variables)\n",
    "    - [1.1.4 Variables with Constant Values](#1.1.4-Variables-with-Constant-Values)\n",
    "    - [1.1.5 Meaningless Non-predictors](#1.1.5-Meaningless-Non-predictors)\n",
    "    - [1.1.6 \"Post-hoc\" Variables](#1.1.6-\"Post-hoc\"-Variables)\n",
    "    - [1.1.7 Third party Credit Score](#1.1.7-Third-party-Credit-Score)\n",
    "    - [1.1.8 Drop Highly-correlated Variables](#1.1.8-Drop-Highly-correlated-Variables)\n",
    "    - [1.1.9 Transform \"annual_inc\" to Log Measure](#1.1.9-Transform-\"annual_inc\"-to-Log-Measure)\n",
    "    - [1.1.10 Trivial Changes](#1.1.10-Trivial-changes)\n",
    "  - [1.2 Census Data](#1.2-Census-Data)\n",
    "  - [1.3 Preprocessing Categorical Variables](#1.3-Preprocessing-Categorical-Variables)\n",
    "\n",
    "- [2 Analysis](#2-Analysis)\n",
    "  - [2.1 Visualization and Plots](#2.1-Visualization-and-plots)\n",
    "    - [2.1.1 Annual Income and Loan Amount](#2.1.1-Annual-Income-and-Loan-Amount)\n",
    "    - [2.1.2 Debt to Income Ratio and Loan Amount](#2.1.2-Debt-to-Income-Ratio-and-Loan-Amount)\n",
    "    - [2.1.3 Home Ownership](#2.1.3-Home-Ownership)\n",
    "  - [2.2 Feature Selection](#2.2-Feature-Selection)\n",
    "    - [2.2.1 Lasso Regularization](#2.2.1-Lasso-Regularization)\n",
    "    - [2.2.2 Random Forest Feature Importance](#2.2.2-Random-Forest-Feature-Importance)\n",
    "    - [2.2.3 Step-wise Backward Feature Selection](#2.2.3-Step-wise-Backward-Feature-Selection)\n",
    "    - [2.2.4 Features Selection Conclusion](#2.2.4-Features-Selection-Conclusion)\n",
    "- [3 Classification Models](#3-Classification)\n",
    "  - [3.1 Performance Evaluation Metrics](#3.1-Performance-Evaluation-Metrics)\n",
    "  - [3.2 Classification Models](#3.2-Classification-Models)\n",
    "    - [3.2.1 Logistic Regression Model](#3.2.1-Logistic-Regression-Model)\n",
    "    - [3.2.2 LDA and QDA](#3.2.2-LDA-and-QDA)\n",
    "    - [3.2.3 Random Forest](#3.2.3-Random-Forest)\n",
    "    - [3.2.4 Gradient Boost](#3.2.4-Gradient-Boost)\n",
    "  - [3.3 Comparison of Different Models](#3.3-Comparison-of-Different-Models)\n",
    "  - [3.4 Performance Improvements](3.4-Performance-Improvements)\n",
    "    - [3.4.1 Re-weight Classes](#3.4.1-Re-weight-Classes)\n",
    "    - [3.4.2 Adjust Threshold](#3.4.2-Adjust-Threshold)\n",
    "  - [3.5 Replicate Lending Club Credit Grades](#3.5-Replicate-Lending-Club-Credit-Grades)\n",
    "- [4 Future Work](#4-Future-Work)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression as Lin_Reg\n",
    "from sklearn.linear_model import Ridge as Ridge_Reg\n",
    "from sklearn.linear_model import Lasso as Lasso_Reg\n",
    "from sklearn.linear_model import LogisticRegression as Logit\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder as LabelEncoder\n",
    "import sklearn.preprocessing as Preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler as Standardize\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "import itertools as it\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7466d",
   "metadata": {},
   "source": [
    "# 0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d8021",
   "metadata": {},
   "source": [
    "## Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa65cd9f",
   "metadata": {},
   "source": [
    "This project explores the features of Lending Club loans and builds a variety of classification models to predict the loan outcomes based on available information of borrowers and loans. As an improvement of the prediction results, I also tuned the hyper-paremeters of the models to pursue a higher prediction precision at the cost of lower \"pass rate\", simulating a more strict preselection system. The results of this project could be used to assist loan investors to create customized pool of loans based on their risk tolerance and desire for risk diversification. Another application of this project is, the \"pass rate\" could be treated as credit grade similar to Lending Club grade (\"A\" to \"G\"), and the loans predicted as \"non-default\" under the \"pass rate\" are analogical to the loans with corresponding grades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233fb2fd",
   "metadata": {},
   "source": [
    "## Background and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11046fe8",
   "metadata": {},
   "source": [
    "Lending Club is the world's largest peer-to-peer online lending platform today. It connects people who need money for various purposes with people who seek investment opportunities in loan markets, and increases the mobility and efficiency of the financial market. Lending Club offers investors the access to a large pool of loans with different levels of default risks. Lending Club assigns a credit grade and subgrade to each loan based on the features of loans and borrowers. The interest rate of the loan is based on the credit grade and subgrade. Once decided, the interest rate is fixed throught the term of the loan. Investors could select which loans to invest in based on their risk and return preference. Borrowers can prepay the loans at any time to eliminate future interest payments and there is no prepayment penalty or fee.\n",
    "\n",
    "This project predicts the loan outcomes based soly on raw information available to investors. Third-party credit scores, Lending Club credit grade, or interest rate are not used as predictors in my classification models (although from the perspective of investors, these are powerful predictors of loan outcome), since they are artificially created by human beings and already incorporated useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e203c6e",
   "metadata": {},
   "source": [
    "# 1 Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdee0e39",
   "metadata": {},
   "source": [
    "## 1.1 Lending Club Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f99f8d",
   "metadata": {},
   "source": [
    "Lending Club data is available at https://www.lendingclub.com/info/download-data.action. In this project I used the data of loans issued in the first three quarters in 2016. I compressed and uploaded this data to the project github page under 'data/LoanStats_securev1_2016Q1.csv.tar.gz', 'LoanStats_securev1_2016Q2.csv.tar.gz', and 'data/LoanStats_2016Q3.csv.tar.gz'. For each loan, 115 features are provided and the description of the features is under 'data/LCDataDictionary.xlsx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "####k-folds cross validation\n",
    "\n",
    "X_train_sel = X_train\n",
    "X_test_sel = X_test\n",
    "\n",
    "n_obs = X_train.shape[1]\n",
    "#Parameters for tuning\n",
    "n_trees = np.arange(200, 800, 100)  # Trees and depth are explored on an exponentially growing space,\n",
    "depths = np.arange(4, 10, 2)   # since it is assumed that trees and depth will add accuracy in a decaying fashion.\n",
    "\n",
    "# To keep track of the best model\n",
    "best_score = 0\n",
    "best_recall = 0\n",
    "best_auc = 0\n",
    "\n",
    "# Run grid search for model with 5-fold cross validation\n",
    "# print '5-fold cross validation:'\n",
    "\n",
    "for trees in n_trees:\n",
    "    for depth in depths:\n",
    "        \n",
    "        # Cross validation for every experiment\n",
    "        kf = KFold(n_obs, 5, shuffle = True)\n",
    "        scores = []\n",
    "        recalls = []\n",
    "        auc = []\n",
    "        for train_indices, validation_indices in kf:\n",
    "            # Generate training data\n",
    "            x_train_cv = X_train_sel.iloc[train_indices, :]\n",
    "            y_train_cv = y_train[train_indices]\n",
    "            \n",
    "#             print x_train_cv.shape, y_train_cv.shape\n",
    "            # Generate validation data\n",
    "            x_validate = X_train_sel.iloc[validation_indices, :]\n",
    "            y_validate = y_train[validation_indices]\n",
    "            \n",
    "            # Fit random forest on training data\n",
    "            rf = RandomForest(n_estimators=trees, max_depth=depth)\n",
    "            rf.fit(x_train_cv, y_train_cv)\n",
    "            # Score on validation data\n",
    "            scores += [rf.score(x_validate, y_validate)]\n",
    "            y_hat_validate = rf.predict(x_validate)\n",
    "            recalls += [recall(y_validate, y_hat_validate)]\n",
    "            \n",
    "            y_pred_logit = rf.predict_proba(x_validate)[:, 1]\n",
    "            auc += [roc_auc_score(y_validate, y_pred_logit)]\n",
    "        # Record and report accuracy\n",
    "        average_score = np.mean(scores)\n",
    "        recall_rate = np.mean(recalls)\n",
    "        avg_auc = np.mean(auc)\n",
    "#         print \"Trees:\", trees, \"Depth:\", depth, \"Score:\", average_score, \"AUC\", avg_auc\n",
    "        \n",
    "        # Update our record of the best parameters see so far\n",
    "        if average_score > best_score:\n",
    "            best_score = average_score\n",
    "            best_trees = trees\n",
    "            best_depth = depth\n",
    "        if avg_auc > best_auc:\n",
    "            best_auc = avg_auc\n",
    "            best_auc_trees = trees\n",
    "            best_auc_depth = depth\n",
    "# print 'number of trees, depth Chosen by Accuracy:', best_trees, ',', best_depth, ',', best_score\n",
    "# print 'number of trees, depth Chosen by AUC:', best_auc_trees, ',', best_auc_depth, ',', best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on entire train set using chosen number of trees and depth\n",
    "rf = RandomForest(n_estimators=300, max_depth=8, min_samples_split = 10)\n",
    "rf.fit(X_train_sel,y_train)\n",
    "\n",
    "importance_list = rf.feature_importances_\n",
    "name_list = X_train_sel.columns\n",
    "importance_list, name_list = zip(*sorted(zip(importance_list[:25], name_list[:25])))\n",
    "plt.figure(figsize=(5, 6))\n",
    "plt.barh(range(len(name_list)),importance_list,align='center', color = 'red', alpha = 0.5)\n",
    "plt.yticks(range(len(name_list)),name_list)\n",
    "plt.xlabel('Relative Importance in the Random Forest')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Relative importance of Each Feature')\n",
    "plt.show()\n",
    "# print name_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf028d9",
   "metadata": {},
   "source": [
    "### 2.2.3 Step-wise Backward Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ee7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcwd_list = ['acc_open_past_24mths', 'addr_state', 'all_util', 'avg_cur_bal', 'bc_open_to_buy', \n",
    "             'bc_util', 'delinq_amnt', 'dti', 'loan_amnt', 'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_rcnt_tl',\n",
    "             'mths_since_rcnt_il', 'mths_since_recent_inq', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'revol_bal', \n",
    "             'tot_coll_amt', 'tot_cur_bal', 'tot_hi_cred_lim', 'total_acc', 'total_bal_ex_mort', 'total_bal_il',\n",
    "             'total_bc_limit', 'total_cu_tl', 'total_il_high_credit_limit', 'total_rev_hi_lim', 'Average',\n",
    "             'acc_now_delinq',  'chargeoff_within_12_mths',  'initial_list_status',  'collections_12_mths_ex_med',\n",
    "             'application_type',  'inq_fi',  'mo_sin_rcnt_rev_tl_op',  'emp_length',  'inq_last_12m', \n",
    "             'inq_last_6mths',  'annual_inc',  'delinq_2yrs',  'home_ownership',]\n",
    "\n",
    "X_sel_train = X_train[bcwd_list]\n",
    "X_sel_test = X_test[bcwd_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bad686",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Step-wise Backward Selection\n",
    "d = X_sel_train.shape[1] # total no. of predictors\n",
    "\n",
    "# Keep track of current set of chosen predictors\n",
    "current_predictors = range(d)\n",
    "\n",
    "# Keep track of the best subset of predictors\n",
    "best_subset = [] \n",
    "\n",
    "# Iterate over all possible subset sizes, d predictors to 1 predictor\n",
    "for size in range(d - 1, 0, -1): # stop before 0 to avoid choosing an empty set of predictors\n",
    "    max_auc = -1e10 # set some initial small value for max R^2\n",
    "    worst_predictor = -1 # set some throwaway initial number for the worst predictor to remove\n",
    "    \n",
    "#     print size\n",
    "    \n",
    "    # Iterate over current set of predictors (for potential elimination)\n",
    "    for i in current_predictors:\n",
    "        # Create copy of current predictors, and remove predictor 'i'\n",
    "        temp = current_predictors[:]\n",
    "        temp.remove(i)\n",
    "                                    \n",
    "        # Use only a subset of predictors in the training data\n",
    "        x_subset = X_sel_train.values[:, temp]\n",
    "        \n",
    "         # Fit and evaluate AUC\n",
    "        logit.fit(x_subset, y_train)\n",
    "\n",
    "        y_pred_logit = logit.predict_proba(x_subset)[:, 1]\n",
    "        auc = roc_auc_score(y_train, y_pred_logit)\n",
    "        \n",
    "        # Check if we get a higher AUC than current max AUC, if so, update\n",
    "        if(auc > max_auc):\n",
    "            max_auc = auc\n",
    "            worst_predictor = i\n",
    "    # Remove worst predictor from current set of predictors\n",
    "    current_predictors.remove(worst_predictor)\n",
    "#     print [features[i] for i in current_predictors], max_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31dbf74",
   "metadata": {},
   "source": [
    "### 2.2.4 Features Selection Conclusion\n",
    "\n",
    "Based on the three feature selection methods and analysis of each features, the 25 predictors I choose to continue with include:\n",
    "'loan_amnt','term', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'purpose',\n",
    "  'title', 'addr_state','delinq_2yrs', 'open_acc', 'revol_util','open_acc_6m', 'open_il_12m', 'total_cu_tl',  'mo_sin_old_il_acct', 'mort_acc',   'mths_since_recent_bc', 'num_actv_rev_tl', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'avg_cur_bal','bc_open_to_buy','total_rev_hi_lim', 'revol_bal'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90efc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = ['loan_amnt','term', 'emp_length', 'home_ownership', 'annual_inc', 'verification_status', 'purpose',\n",
    "          'title', 'addr_state','delinq_2yrs', 'open_acc', 'revol_util','open_acc_6m', 'open_il_12m', \n",
    "           'total_cu_tl', 'mo_sin_old_rev_tl_op',  'mort_acc',   'mths_since_recent_bc',\n",
    "          'num_actv_rev_tl', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'avg_cur_bal','bc_open_to_buy','total_rev_hi_lim',\n",
    "           'revol_bal']\n",
    "# len(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef752e24",
   "metadata": {},
   "source": [
    "# 3. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5892a25",
   "metadata": {},
   "source": [
    "## 3.1 Performance Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8850c0e6",
   "metadata": {},
   "source": [
    "I defined a function called \"predict_table\", which returns a comprehensive table of the classification results including True(False) Positive(Nagative) predictions as well as recall and precisions on default (class 1) and non-default (class 0) groups. Also I calculated the Accuracy and AUC of the classification results and \"Pass rate\" defined as the proportion of predicted non-default loans among all of the loans. All of the evaluation metrics are calculated on test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e401e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_table(y, y_hat):\n",
    "    tp = 0.0\n",
    "    tn = 0.0\n",
    "    fp = 0.0\n",
    "    fn = 0.0\n",
    "    for i in range(len(y)):\n",
    "        if(y[i] == 1 and y_hat[i] == 1):\n",
    "            tp += 1.0\n",
    "        elif(y[i] == 0 and y_hat[i] == 0):\n",
    "            tn += 1.0\n",
    "        elif(y[i] == 1 and y_hat[i] == 0):\n",
    "            fn += 1.0\n",
    "        elif(y[i] == 0 and y_hat[i] == 1):\n",
    "            fp += 1.0\n",
    "        else:\n",
    "            print \"Something is wrong with labels!\"\n",
    "    print \"Total number of test data: \" + str(len(y))\n",
    "    print \"True  Positive:            \" + str(tp)\n",
    "    print \"True  Negative:            \" + str(tn)\n",
    "    print \"False Positive:            \" + str(fp)\n",
    "    print \"False Negative:            \" + str(fn)\n",
    "    print \"Recall    on class 1:      \" + str(tp/(tp + fn))\n",
    "    print \"Precision on class 1:      \" + str(tp/(tp + fp))\n",
    "    print \"Recall    on class 0:      \" + str(tn/(tn + fp))\n",
    "    print \"Precision on class 0:      \" + str(tn/(tn + fn))\n",
    "    print \"Accuracy:                  \" + str((tp + tn)/len(y))\n",
    "    print \"Pass rate:            \" + str((tn + fn)/len(y))\n",
    "#     return (tn + fn)/len(y), (tp + tn)/len(y), tp/(tp + fn), tn/(tn + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d090c",
   "metadata": {},
   "source": [
    "With the predictors I selected in the last step, I tried to classify the loans to \"default\" and \"non-default\" with different classification models: Logistic Regression, LDA and QDA, Random Forest, and Gradient Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9ec79",
   "metadata": {},
   "source": [
    "### 1.1.2 Missing Data\n",
    "\n",
    "I dropped variables with more than 10% missing values in the data set. For the rest missing data, I used scikit-learn impute function to fill the missing data with the mean of the variable in the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511350bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "too_many_missing = []\n",
    "for column in x.columns:\n",
    "    if (x[column].isnull().sum() >0.1*x.shape[0]):\n",
    "        too_many_missing.append(column)\n",
    "print \"The number of variables deleted due to too many massing values is\", len(too_many_missing)\n",
    "x = x.drop(too_many_missing, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e55d7",
   "metadata": {},
   "source": [
    "### 1.1.3 Transform String Variables\n",
    "\n",
    "The variable \"issue_d\" is a string containing the issue year and month of the loans. I created two variables of 'issue_y' and 'issue_m' and deleted the variable \"issue_d\". Same for string variables 'earliest_cr_line' and 'revol_util'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate year variable\n",
    "x['issue_y'] = x['issue_d'].str[4:]\n",
    "\n",
    "## generate month variable\n",
    "x['issue_m'] = x['issue_d'].str[:3]\n",
    "\n",
    "x = x.drop(['issue_d'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3f9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x['cr_y'] = x['issue_y'].apply(lambda k: float(k)) - x['earliest_cr_line'].str[4:].apply(lambda k: float(k))\n",
    "x['revol_util'] = x['revol_util'].apply(lambda k: float(str(k).strip('%'))/100)\n",
    "\n",
    "x = x.drop(['earliest_cr_line'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9e500",
   "metadata": {},
   "source": [
    "### 1.1.4 Variables with Constant Values\n",
    "\n",
    "1. \n",
    "There might be variables with constant values in the data set, which have no use in predicting the loan outcomes. Therefore I checked the number of unique values for each variable in the data set and deleted the constant variables.\n",
    "2. \n",
    "There might be variables that have valid meaning for default loans only and have no meaning for non-default loans (for example, \"collection_recovery_fee\"), or vice versa. These variables are strongly associated with the loan outcomes. Therefore, we cannot use them as predictors of loan status, but should not be used as predictors of loan outcomes since they are essentially variable of loan outcomes. Therefore I also deleted variables with constant values for all default loans or with constant values for all non-default loans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd36c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at the unique values of each variable in the entire dataset\n",
    "unique_values = [] \n",
    "for column in x.columns:\n",
    "    \n",
    "    ## get unique values of variables\n",
    "    unique = len(x[column].unique())\n",
    "    if unique == 1:\n",
    "        print column, \"has a constant value for all observations.\"\n",
    "    ## append the unique value to unique_values list\n",
    "    unique_values.append(unique)\n",
    "\n",
    "#print \"The numbers of unique values of corresponding variables are\", unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcf43d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(['policy_code', 'pymnt_plan', 'issue_y'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e580f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at the unique values of each variable by loan status\n",
    "unique_values_by_status = pd.DataFrame({}) \n",
    "for column in x.columns:\n",
    "    \n",
    "    ## get unique values of variables grouped by loan status\n",
    "    unique = pd.Series((len(x[column][y == 0].unique()), len(x[column][y == 1].unique())))\n",
    "    \n",
    "    if len(x[column][y == 0].unique()) == 1:\n",
    "        print column, \"only valid for default loans\"\n",
    "        \n",
    "    if len(x[column][y == 1].unique()) == 1:\n",
    "        print column, \"only valid for non-default loans\"\n",
    "    ## append the unique value series to unique_values_by_status dataframe\n",
    "    unique_values_by_status = pd.concat([unique_values_by_status, unique], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(['collection_recovery_fee', 'out_prncp', 'out_prncp_inv', 'recoveries'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ec0d4",
   "metadata": {},
   "source": [
    "### 1.1.5 Meaningless Non-predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3901993",
   "metadata": {},
   "source": [
    "Delete variables that are not meaningful and thus do not contribute to the prediction of loan outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop non-predictors and keep valid predictors only\n",
    "x = x.drop(['member_id', 'id', 'url', 'emp_title'], axis = 1, inplace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330403fa",
   "metadata": {},
   "source": [
    "###  1.1.6 \"Post-hoc\" Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16018724",
   "metadata": {},
   "source": [
    "\"Post-hoc\" variables are variables that are not available until the issuance of the loans, such as principle received up to today. Since we are predicting the loan outcomes to decide whether or not to lend the money, we need to make the decision with avaialble information before the issuance of loans. Therefore I went through the documentation and discovered all the \"post-hoc\" variables. Including the \"post-hoc\" variables as predictors gave me perfect predicting accuracy (around 98%), but in practical it makes no sense to include the post-hoc information in prediction model, so I excluded all the post-hoc variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3abd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(['total_pymnt', 'total_pymnt_inv', 'last_pymnt_amnt','last_pymnt_d', 'total_rec_int', 'total_rec_late_fee', 'last_credit_pull_d', 'total_rec_prncp'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98296c0",
   "metadata": {},
   "source": [
    "### 1.1.7 Third-party Credit Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b3941",
   "metadata": {},
   "source": [
    "The Lending Club data set also include FICO scores of the borrowers and Lending Club credit grade of the loans. Since the credit score and grade are artificially generated by human beings trying to predict the default probability of the loans, and the goal of this project is to predict the outcomes of loans from raw information, it is appropriate to exclude the credit grade as predictors. Similarly, the interest rate of loans should be excluded because it is also a reflection of the default probability of the loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b818ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(['grade', 'sub_grade', 'int_rate'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210db20",
   "metadata": {},
   "source": [
    "### 1.1.8 Drop Highly-correlated variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac606a7",
   "metadata": {},
   "source": [
    "Among the remaining 80 features, some of them are highly-correlated. I plotted the correlation between each pair of the features and identify the features of high correlation. Then I deleted the highly-correlated features and keep only one representing to represent its peer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03742633",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_m = x.corr()\n",
    "# corr_m\n",
    "high_corr = ['funded_amnt', 'funded_amnt_inv', 'installment']\n",
    "x = x.drop(high_corr, axis = 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4a36a",
   "metadata": {},
   "source": [
    "### 1.1.9 Transform \"annual_inc\" to Log Measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1018087",
   "metadata": {},
   "source": [
    "### 2.1.1 Annual Income and Loan Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8227c2",
   "metadata": {},
   "source": [
    "First I explored the relationship between loan amount and annual income, and plotted the relationship curves separately for different loan outcomes (default and non-default). For each range of loan amount and for two different loan outcomes, I calculate the average annual income of borrowers. I found that the two relationship curves far away from each other when the loan amount is relatively low, but converges together as loan amount exceeds 20000. Note that annual income has been transformed to log measure due to right skewness of distribution.\n",
    "\n",
    "This result gives a very important hint: for small loans with amount less than 20000, borrower's annual income has a very important influence on loan outcome. However, for large loans with amount greater than 20000, borrower's annual income does not make a difference in loan outcome.\n",
    "\n",
    "Generally, the red line lies below the blue line, indicating whatever the loan amount, the average annual income of non-defualt loan borrowers is higher than the default loan borrowers. This is consistent with our intuition.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c6768",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_means_0, bin_edges_0, binnumber_0 = stats.binned_statistic(x['loan_amnt'][y == 0], x['annual_inc'][y == 0], statistic='mean', bins=25)\n",
    "bin_width_0 = (bin_edges_0[1] - bin_edges_0[0])\n",
    "bin_centers_0 = bin_edges_0[1:] - bin_width_0/2\n",
    "\n",
    "bin_means_1, bin_edges_1, binnumber_1 = stats.binned_statistic(x['loan_amnt'][(y == 1) & (x['annual_inc'].isnull().values == False)], x['annual_inc'][(y == 1)& (x['annual_inc'].isnull().values == False)], statistic='mean', bins=25)\n",
    "bin_width_1 = (bin_edges_1[1] - bin_edges_1[0])\n",
    "bin_centers_1 = bin_edges_1[1:] - bin_width_1/2\n",
    "\n",
    "plt.scatter(bin_centers_0, bin_means_0, c= 'blue', label = 'non-default')\n",
    "plt.plot(bin_centers_0, bin_means_0, c= 'blue')\n",
    "plt.scatter(bin_centers_1, bin_means_1, c='red', label = 'default')\n",
    "plt.plot(bin_centers_1, bin_means_1, c='red')\n",
    "\n",
    "plt.title(\"average annual_income for each range of loan amount\")\n",
    "plt.xlabel('loan amount')\n",
    "plt.ylabel('annual income')\n",
    "plt.legend(loc = 2)\n",
    "plt.xlim(0, 40000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0204e65",
   "metadata": {},
   "source": [
    "### 2.1.2 Debt to Income Ratio and Loan Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa71c7a",
   "metadata": {},
   "source": [
    "Then I plotted the relationship curves of debt to income ratio and loan amount by loan outcomes in the same way as 2.1.1. Debt to income ratio is also different for default and non-default loans for each range of loan amount. Generally the borrowers of loans that default have higher debt to income ratio than the borrowers whose loans did not default.\n",
    "\n",
    "The two curves are clearly separated and parallel with each other when loan amount is small. However, as loan amount grows, the two curves converges to each other and even crosses over. This indicates that debt to income ratio might work well in separating the default and non-default loans for small amount loans, but for large amount loans, the influence of debt to income ratio on the loan outcome becomes trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfce20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_means_0, bin_edges_0, binnumber_0 = stats.binned_statistic(x['loan_amnt'][y == 0], x['dti'][y == 0], statistic='mean', bins=25)\n",
    "bin_width_0 = (bin_edges_0[1] - bin_edges_0[0])\n",
    "bin_centers_0 = bin_edges_0[1:] - bin_width_0/2\n",
    "\n",
    "bin_means_1, bin_edges_1, binnumber_1 = stats.binned_statistic(x['loan_amnt'][y == 1], x['dti'][y == 1], statistic='mean', bins=25)\n",
    "bin_width_1 = (bin_edges_1[1] - bin_edges_1[0])\n",
    "bin_centers_1 = bin_edges_1[1:] - bin_width_1/2\n",
    "\n",
    "plt.scatter(bin_centers_0, bin_means_0, c= 'blue', label = 'non-default')\n",
    "plt.plot(bin_centers_0, bin_means_0, c= 'blue')\n",
    "plt.scatter(bin_centers_1, bin_means_1, c='red', label = 'default')\n",
    "plt.plot(bin_centers_1, bin_means_1, c='red')\n",
    "\n",
    "plt.title(\"debt to income ratio for each range of loan amount\")\n",
    "plt.xlabel('loan amount')\n",
    "plt.ylabel('debt to income')\n",
    "plt.legend(loc = 2)\n",
    "plt.xlim(0, 40000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9842f2",
   "metadata": {},
   "source": [
    "### 2.1.3 Home Ownership"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d73e41",
   "metadata": {},
   "source": [
    "The type of home ownership might also be related with loan outcomes, since different home ownership might be an indication of the borrower's financial situation and influence the cash flow capability of the borrowers.\n",
    "\n",
    "From the results we can find that the \"RENT\" type of ownership has the highest proportion of default loans, while \"MORTGAGE\" type of ownership has the lowest. This makes sense because people who choose to apply for mortgage and got approved are those with good credit history and stable income cash flows compared to people who rent to live. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540144a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## one observation with home_ownership == any, drop it\n",
    "# y = y[x['home_ownership'].values != 'ANY']\n",
    "# x = x[x['home_ownership']!= 'ANY']\n",
    "\n",
    "print \"The proportion of default by different types of home ownership is:\"\n",
    "print x[y == 1].home_ownership.value_counts()/(x[y == 1].home_ownership.value_counts() + x[y == 0].home_ownership.value_counts())\n",
    "ax1 = plt.subplot()\n",
    "x[y == 1].home_ownership.value_counts().plot(kind='bar', color = 'red', position=0,width=0.25,alpha = 0.4, axes = ax1, label = 'default')\n",
    "x[y == 0].home_ownership.value_counts().plot(kind='bar', color = 'blue',position=1,width=0.25, alpha = 0.4, axes = ax1, label = 'non-default')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Bar plot of Home Ownership')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663f5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(['loan_status'], axis = 1)\n",
    "x_copy = x[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb125e",
   "metadata": {},
   "source": [
    "## 2.2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e3fdd",
   "metadata": {},
   "source": [
    "Before building any classification model, I analyzed the features in the data set and tries to select the most predictive features. I used three methods to select features: Lasso, Random Forest Importance Rank, and Step-wise Backward Feature Selection. The features selected by different methods are analyzed in combination trying to figure out the \"best\" set of predictors.\n",
    "\n",
    "Before conducting feature selection, I split the whold data set into training and testing data sets to avoid fitting to the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714c9757",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_encod, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y, y_hat):\n",
    "    score = np.mean(y_hat[y == 1])\n",
    "    return score\n",
    "\n",
    "def precision(y, y_hat):\n",
    "    score = np.mean(y[y_hat == 1])\n",
    "    return score\n",
    "\n",
    "def F_score(y, y_hat):\n",
    "    prec = precision(y,y_hat)\n",
    "    rec = recall(y, y_hat)\n",
    "    score = 2*(prec*rec)/(prec+rec)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d25e0b",
   "metadata": {},
   "source": [
    "### 2.2.1 Lasso Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47915ccf",
   "metadata": {},
   "source": [
    "First I use logistic regression with Lasso regularization with different penaly hyper-parameters C. Smaller C means stricter regularization and fewer non-zero parameter features. There is a trade-off between bias and variance: models with fewer features tend to be robust to changes of data, but with more classification error; models with more features are more flexible and good at fitting the training data, but tend to be volatile and sensitivity to changes in data set. There is a \"sweet point\" in-between such that a model gives good performance results with appropriate number of predictors.\n",
    "\n",
    "I drew the ROC curves of all the logistic models with different C-parameters, and based on the analysis of Logistic regression results and ROC curves, I found the \"sweet point\" with C = 0.001 and 27 non-zero parameter predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677015d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "fpr = [0]*len(Cs)\n",
    "tpr = [0]*len(Cs)\n",
    "\n",
    "for i in range(len(Cs)):\n",
    "    features = []\n",
    "    C = Cs[i]\n",
    "    print \"penalty C is\", C\n",
    "    logit = Logit(penalty='l1', C=C)\n",
    "    \n",
    "    logit.fit(X_train, y_train)\n",
    "    coefs = logit.coef_\n",
    "\n",
    "    \n",
    "    #### predict y_hat\n",
    "    y_hat = logit.predict(X_train)\n",
    "    \n",
    "    #### draw ROC \n",
    "    y_pred_logit = logit.predict_proba(X_train)[:, 1]\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_train, y_pred_logit)\n",
    "    \n",
    "    #### display the predictors with non-zero parameters\n",
    "    count = 0\n",
    "    for k in range(len(coefs[0])):\n",
    "        if coefs[0][k] != 0:\n",
    "            features.append(X_train.columns.values[k])\n",
    "            count+=1\n",
    "    print \"Number of non-zero parameters is\", count\n",
    "#     print features\n",
    "    print \"accuracy rate is\", logit.score(X_train, y_train)\n",
    "    print \"recall rate is\", recall(y_train, y_hat)\n",
    "    print \"precision rate is\", precision(y_train, y_hat)\n",
    "    print \"F score is\", F_score(y_train, y_hat)\n",
    "    print \"AUC score is\", roc_auc_score(y_train, y_pred_logit)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afc88bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Cs)):\n",
    "    plt.plot(fpr[i], tpr[i], label= 'C =' + str(Cs[i]))\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.legend(loc = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4bae39",
   "metadata": {},
   "source": [
    "### 2.2.2 Random Forest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724d955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(x['annual_inc'])\n",
    "# plt.clf()\n",
    "x['annual_inc'] = np.log(x['annual_inc']+1)\n",
    "# plt.hist(x['annual_inc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b219b",
   "metadata": {},
   "source": [
    "### 1.1.10 Trivial changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6909d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel_train = X_train[feature]\n",
    "X_sel_test = X_test[feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d078cff",
   "metadata": {},
   "source": [
    "## 3.2 Classification Models\n",
    "\n",
    "### 3.2.1 Logistic Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34ff02",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_slc = Logit()\n",
    "clf_slc.fit(X_train_sel, y_train)\n",
    "y_predict_slc = clf_slc.predict(X_test_sel)\n",
    "y_predict_prob_slc = clf_slc.predict_proba(X_test_sel)[:,1]\n",
    "\n",
    "fpr_slc, tpr_slc, _ = roc_curve(y_test, y_predict_prob_slc)\n",
    "\n",
    "print \"**************************************************\"\n",
    "print \" Sklearn logistic regression Model performance report\"\n",
    "print \"**************************************************\"\n",
    "predict_table(y_test, y_predict_slc)\n",
    "print \"auc score = \", roc_auc_score(y_test, y_predict_prob_slc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808ecdbc",
   "metadata": {},
   "source": [
    "### 3.2.2 LDA and QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21692bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lda = LDA()\n",
    "clf_lda.fit(X_train_sel, y_train)\n",
    "y_predict_lda = clf_lda.predict(X_test_sel)\n",
    "y_predict_prob_lda = clf_lda.predict_proba(X_test_sel)[:,1]\n",
    "\n",
    "fpr_lda, tpr_lda, _ = roc_curve(y_test, y_predict_prob_lda)\n",
    "\n",
    "clf_qda = QDA()\n",
    "clf_qda.fit(X_train_sel, y_train)\n",
    "y_predict_qda = clf_qda.predict(X_test_sel)\n",
    "y_predict_prob_qda = clf_qda.predict_proba(X_test_sel)[:,1]\n",
    "\n",
    "fpr_qda, tpr_qda, _ = roc_curve(y_test, y_predict_prob_qda)\n",
    "\n",
    "\n",
    "print \"**************************************************\"\n",
    "print \" LDA Model performance report\"\n",
    "print \"**************************************************\"\n",
    "predict_table(y_test, y_predict_lda)\n",
    "print \"auc score = \", roc_auc_score(y_test, y_predict_prob_lda)\n",
    "\n",
    "print \"**************************************************\"\n",
    "print \" QDA performance report\"\n",
    "print \"**************************************************\"\n",
    "predict_table(y_test, y_predict_qda)\n",
    "print \"auc score = \", roc_auc_score(y_test, y_predict_prob_qda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab6b385",
   "metadata": {},
   "source": [
    "### 3.2.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afde187",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sel = X_train[feature]\n",
    "X_test_sel = X_test[feature]\n",
    "\n",
    "n_obs = X_train.shape[1]\n",
    "#Parameters for tuning\n",
    "n_trees = np.arange(200, 800, 100)  # Trees and depth are explored on an exponentially growing space,\n",
    "depths = np.arange(4, 10, 2)   # since it is assumed that trees and depth will add accuracy in a decaying fashion.\n",
    "\n",
    "# To keep track of the best model\n",
    "best_score = 0\n",
    "best_recall = 0\n",
    "best_auc = 0\n",
    "\n",
    "# Run grid search for model with 5-fold cross validation\n",
    "print '5-fold cross validation:'\n",
    "\n",
    "for trees in n_trees:\n",
    "    for depth in depths:\n",
    "        \n",
    "        # Cross validation for every experiment\n",
    "        kf = KFold(n_obs, 5, shuffle = True)\n",
    "        scores = []\n",
    "        recalls = []\n",
    "        auc = []\n",
    "        for train_indices, validation_indices in kf:\n",
    "            # Generate training data\n",
    "            x_train_cv = X_train_sel.iloc[train_indices, :]\n",
    "            y_train_cv = y_train[train_indices]\n",
    "            \n",
    "#             print x_train_cv.shape, y_train_cv.shape\n",
    "            # Generate validation data\n",
    "            x_validate = X_train_sel.iloc[validation_indices, :]\n",
    "            y_validate = y_train[validation_indices]\n",
    "            \n",
    "            # Fit random forest on training data\n",
    "            rf = RandomForest(n_estimators=trees, max_depth=depth)\n",
    "            rf.fit(x_train_cv, y_train_cv)\n",
    "            # Score on validation data\n",
    "            scores += [rf.score(x_validate, y_validate)]\n",
    "            y_hat_validate = rf.predict(x_validate)\n",
    "            recalls += [recall(y_validate, y_hat_validate)]\n",
    "            \n",
    "            y_pred_logit = rf.predict_proba(x_validate)[:, 1]\n",
    "            auc += [roc_auc_score(y_validate, y_pred_logit)]\n",
    "        # Record and report accuracy\n",
    "        average_score = np.mean(scores)\n",
    "        recall_rate = np.mean(recalls)\n",
    "        avg_auc = np.mean(auc)\n",
    "        print \"Trees:\", trees, \"Depth:\", depth, \"Score:\", average_score, \"AUC\", avg_auc\n",
    "        \n",
    "        # Update our record of the best parameters see so far\n",
    "        if average_score > best_score:\n",
    "            best_score = average_score\n",
    "            best_trees = trees\n",
    "            best_depth = depth\n",
    "        if avg_auc > best_auc:\n",
    "            best_auc = avg_auc\n",
    "            best_auc_trees = trees\n",
    "            best_auc_depth = depth\n",
    "print \"Best number of trees and best tree depth according to accuracy is\", best_trees, best_depth\n",
    "print \"Best number of trees and best tree depth according to AUC is\", best_auc_trees, best_auc_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForest(n_estimators = 300, max_depth = 8, random_state = 8)\n",
    "rf.fit(X_train_sel, y_train)\n",
    "\n",
    "y_predict_rf = rf.predict(X_test_sel)\n",
    "y_predict_prob_rf = rf.predict_proba(X_test_sel)[:,1]\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_predict_prob_rf)\n",
    "\n",
    "\n",
    "print \"**************************************************\"\n",
    "print \" Random Forest Model performance report\"\n",
    "print \"**************************************************\"\n",
    "predict_table(y_test, y_predict_rf)\n",
    "print \"auc score = \", roc_auc_score(y_test, y_predict_prob_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d24743",
   "metadata": {},
   "source": [
    "### 3.2.4 Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c7886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_estimators': [50, 100, 200], 'learning_rate': [1.0, 5.0, 10.0], 'max_depth': [1, 3, 5]}]\n",
    "\n",
    "scores = ['recall']\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(GradientBoostingClassifier(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clf.fit(X_train_sel, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "   \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79316e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_boost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf_boost.fit(X_train_sel, y_train)\n",
    "y_predict_boost = clf_boost.predict(X_test_sel)\n",
    "y_predict_prob_boost = clf_boost.predict_proba(X_test_sel)[:,1]\n",
    "\n",
    "\n",
    "fpr_boost, tpr_boost, _ = roc_curve(y_test, y_predict_prob_boost)\n",
    "\n",
    "print \"**************************************************\"\n",
    "print \"Boosting Model performance report\"\n",
    "print \"**************************************************\"\n",
    "predict_table(y_test, y_predict_boost)\n",
    "print \"auc score = \", roc_auc_score(y_test, y_predict_prob_boost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17371d",
   "metadata": {},
   "source": [
    "## 3.3 Comparison of Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3455fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(\"./data/LoanStats_securev1_2016Q1.csv\", skiprows = 1)\n",
    "data2 = pd.read_csv(\"./data/LoanStats_securev1_2016Q2.csv\", skiprows = 1)\n",
    "data3 = pd.read_csv(\"./data/LoanStats_2016Q3.csv\", skiprows = 1)\n",
    "## concatenate datasets into one dataset\n",
    "data_all = pd.concat((data1, data2, data3), axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c8075",
   "metadata": {},
   "source": [
    "### 1.1.1 Loan Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e236acc8",
   "metadata": {},
   "source": [
    "A total of 330867 loans were issued by lending club in the first three quarters of year 2016. The loans might have one of several status such as \"Current\", \"Fully Paid\", \"Charged Off\", etc. I treated the \"Current\" status as unknown status, and focus on the loans with known status that are either \"Fully Paid\" (meaning non-default) or \"Default\" (including \"Charged Off\", \"Late\", \"In Grace Period\", \"Does not meet the credit policy\", \"Default\"). I assigned the value of 1 to the default loans and the value of 0 to non-default loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a1f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"The distribution of loan status:\"\n",
    "print\n",
    "print data_all['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1555710",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove \"current status\"\n",
    "data = data_all[data_all['loan_status']!= 'Current']\n",
    "\n",
    "## remove observations with unknown (NaN) loan status\n",
    "data = data[data['loan_status'] == data['loan_status']]\n",
    "\n",
    "# y == 1 if default, y == 0 if fully paid\n",
    "y = np.ones(data.shape[0])\n",
    "y[data['loan_status'].values =='Fully Paid'] = 0\n",
    "\n",
    "total_default = len(y[y==1])\n",
    "total_non_default = len(y[y==0])\n",
    "print \"The number of newly-defined defaulted loans is\", total_default, \", and the number of non-default loans is\", total_non_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ace47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## x get a copy of data\n",
    "x = data[:]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
