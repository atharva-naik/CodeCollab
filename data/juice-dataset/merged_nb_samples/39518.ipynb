{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5540a30",
   "metadata": {},
   "source": [
    "# Saving and Loading Gluon Models\n",
    "\n",
    "In reality you almost always save the models you train into files. There is a number of reasons to do this. For example,\n",
    "1. You might want to do inference on a machine that is different from the one where the model was trained.\n",
    "2. It is possible the model's performance on validation set decreased towards the end of the training for reasons like overfitting. If you saved your model parameters after every epoch, at the end you can decide to use the model that performs best on the vaidation set.\n",
    "\n",
    "So, we need ways to save models to file and restore them back from file. In this tutorials we will learn ways to save and load Gluon models.\n",
    "\n",
    "Let's start by importing the modules we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import mxnet.ndarray as nd\n",
    "from mxnet import nd, autograd, gluon\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4082647",
   "metadata": {},
   "source": [
    "## Build and train a simple model\n",
    "\n",
    "We need a trained model before we can save it to a file. So let's go ahead and build a very simple convolutional network and train it on MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e96f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if one exists, else use CPU\n",
    "ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()\n",
    "\n",
    "# MNIST images are 28x28. Total pixels in input layer is 28x28 = 784\n",
    "num_inputs = 784\n",
    "# Clasify the images into one of the 10 digits\n",
    "num_outputs = 10\n",
    "# 64 images in a batch\n",
    "batch_size = 64\n",
    "\n",
    "# Helper to preprocess data for training\n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "\n",
    "# Load the training data\n",
    "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform), \n",
    "                                   batch_size, shuffle=True)\n",
    "\n",
    "# A helper method to train a given model using MNIST data\n",
    "def train_model(model):\n",
    "\n",
    "    # Initialize the parameters with Xavier initializer\n",
    "    net.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "    # Use cross entropy loss\n",
    "    softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    # Use Adam optimizer\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': .001})\n",
    "\n",
    "    # Train for one epoch\n",
    "    for epoch in range(1):\n",
    "        # Iterate through the images and labels in the training data\n",
    "        for batch_num, (data, label) in enumerate(train_data):\n",
    "            # get the images and labels\n",
    "            data = data.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            # Ask autograd to record the forward pass\n",
    "            with autograd.record():\n",
    "                # Run the forward pass\n",
    "                output = model(data)\n",
    "                # Compute the loss\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "            # Update parameters\n",
    "            trainer.step(data.shape[0])\n",
    "\n",
    "            # Print loss once in a while\n",
    "            if batch_num % 50 == 0:\n",
    "                curr_loss = nd.mean(loss).asscalar()\n",
    "                print(\"Epoch: %d; Batch %d; Loss %f\" % (epoch, batch_num, curr_loss))\n",
    "\n",
    "# Build a simple convolutional network\n",
    "def build_lenet(net):    \n",
    "    with net.name_scope():\n",
    "        # First convolution\n",
    "        net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        # Second convolution\n",
    "        net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "        net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "        # Flatten the output before the fully connected layers\n",
    "        net.add(gluon.nn.Flatten())\n",
    "        # First fully connected layers with 512 neurons\n",
    "        net.add(gluon.nn.Dense(512, activation=\"relu\"))\n",
    "        # Second fully connected layer with as many neurons as the number of classes\n",
    "        net.add(gluon.nn.Dense(num_outputs))\n",
    "        \n",
    "        return net\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "net = build_lenet(net)\n",
    "train_model(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb90986",
   "metadata": {},
   "source": [
    "## Saving model parameters to file\n",
    "\n",
    "Okay, we now have a model (`net`) that we can save to a file. Let's save the parameters of this model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c88585",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"net.params\"\n",
    "net.save_params(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511c707",
   "metadata": {},
   "source": [
    "That's it! We have successfully saved the parameters of the model into a file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb586d",
   "metadata": {},
   "source": [
    "## Loading model parameters from file\n",
    "\n",
    "Let's now create a network with the parameters we saved into the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4b001",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_net = build_lenet()\n",
    "new_net.load_params(file_name, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f116fb",
   "metadata": {},
   "source": [
    "Let's test the model we just loaded from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bac70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "\n",
    "# Load ten random images from the test dataset\n",
    "sample_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                              10, shuffle=True)\n",
    "\n",
    "for data, label in sample_data:\n",
    "\n",
    "    # Display the images\n",
    "    img = nd.transpose(data, (1,0,2,3))\n",
    "    img = nd.reshape(img, (28,10*28,1))\n",
    "    imtiles = nd.tile(img, (1,1,3))\n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "\n",
    "    # Display the predictions\n",
    "    data = nd.transpose(data, (0, 3, 1, 2))\n",
    "    out = new_net(data.as_in_context(ctx))\n",
    "    predictions = nd.argmax(out, axis=1)\n",
    "    print('Model predictions: ', predictions.asnumpy())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb7606",
   "metadata": {},
   "source": [
    "## Saving model architecture and weights to file\n",
    "\n",
    "Hybrid models can be serialized as JSON files using the `export` function. Once serialized, these models can be loaded from other language bindings like C++ or Scala for faster inference or inferences in different environments.\n",
    "\n",
    "Note that the network we created above is not a Hybrid network and therefore cannot be serializes into a JSON file. So, let's create a Hybrid version of the same network and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.HybridSequential()\n",
    "net = build_lenet(net)\n",
    "net.hybridize()\n",
    "train_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cabca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.export(\"lenet\", epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15215bdc",
   "metadata": {},
   "source": [
    "That's it! `export` in this case creates `lenet-symbol.json` and `lenet-0001.params` in the current directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9622c49",
   "metadata": {},
   "source": [
    "## Loading model architecture and weights from a different frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3ada8",
   "metadata": {},
   "source": [
    "Primary reason to serialize model architecture into a JSON file is to load it from a different frontend like C, C++ or Scala. Here is a couple of examples:\n",
    "1. [Loading serialized Hybrid networks from C](https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/predict-cpp/image-classification-predict.cc)\n",
    "2. [Loading serialized Hybrid networks from Scala](https://github.com/apache/incubator-mxnet/blob/master/scala-package/infer/src/main/scala/org/apache/mxnet/infer/ImageClassifier.scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f749fdb",
   "metadata": {},
   "source": [
    "## Loading model architecture from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e5b5e",
   "metadata": {},
   "source": [
    "Serialized Hybrid networks (saved as .JSON and .params file) can be loaded and used inside Python frontend using `mx.model.load_checkpoint` and `gluon.nn.SymbolBlock`. To demonstrate that, let's load the network we serialized above."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
