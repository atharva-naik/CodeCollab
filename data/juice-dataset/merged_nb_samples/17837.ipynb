{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aa7c2c6",
   "metadata": {},
   "source": [
    "# Generate auxiliary features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42d5ef4",
   "metadata": {},
   "source": [
    "# TOC\n",
    "\n",
    "* [1 Loading the data](#1-Loading-the-data)\n",
    "* [2 Adding date features](#2-Adding-date-features)\n",
    "* [3 Adding text features](#3-Adding-text-features)\n",
    "* [4 Adding leakage features](#4-Adding-leakage-features)\n",
    "* [5 The big merge](#5-The-big-merge)\n",
    "* [6 Type casting](#6-Type-casting)\n",
    "* [7 DataFrame trimming](#7-DataFrame-trimming)\n",
    "* [8 Normalization](#8-Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17589af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import sklearn\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from pathlib import Path\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89723369",
   "metadata": {},
   "source": [
    "# 1 Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('.').absolute().joinpath('data')\n",
    "\n",
    "sales_train = pd.read_csv(data_dir.joinpath('sales_train.csv.gz'))\n",
    "sales_test = pd.read_csv(data_dir.joinpath('test.csv.gz'))\n",
    "items = pd.read_csv(data_dir.joinpath('items.csv'))\n",
    "item_categories = pd.read_csv(data_dir.joinpath('item_categories.csv'))\n",
    "shops = pd.read_csv(data_dir.joinpath('shops.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f91a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = Path('.').absolute().joinpath('generated_data')\n",
    "data_aggregate = pd.read_hdf(generated_data.joinpath('data_aggregate.hdf'),\n",
    "                             key='data_aggregate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4fbcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = data_aggregate.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794968e",
   "metadata": {},
   "source": [
    "Cast the dates to actual dates for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5743a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.loc[:, 'date'] = pd.to_datetime(sales_train.loc[:, 'date'], format='%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6513847c",
   "metadata": {},
   "source": [
    "# 2 Adding date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba27bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "shop_id_both.loc[:, 'shop_id_count'].hist(ax=ax, bins=200)\n",
    "ax.set_xlabel('shop_id_count')\n",
    "ax.set_ylabel('count')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6edebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id_both.loc[:, 'shop_id_count'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eba151",
   "metadata": {},
   "source": [
    "It appears that the number of rows for each `shop_id` is well spread, and not clustering around a specific number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e345b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "item_id_both.loc[:, 'item_id_count'].hist(ax=ax, bins=200)\n",
    "ax.set_xlabel('item_id_count')\n",
    "ax.set_ylabel('count')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f277b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_both.loc[:, 'item_id_count'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a4cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_both.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a081a9",
   "metadata": {},
   "source": [
    "### The ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32295e70",
   "metadata": {},
   "source": [
    "As we saw from the EDA, we saw that the `ID` was highly correlated to the `shop_id`, so we include it here. Item and shops without an ID will be given `-1` (although we could probably construct a more appropriate `ID` feature if we checked the feature more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af41959",
   "metadata": {},
   "source": [
    "**NOTE**: We do an outer join here as some combinations of `shop_id` and `item_id` is only present in the test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a685977",
   "metadata": {},
   "outputs": [],
   "source": [
    "on = ['shop_id', 'item_id']\n",
    "id_df = pd.merge(sales_train.loc[:, on], sales_test, how='outer', on=['shop_id', 'item_id'])\n",
    "id_df.loc[:,'ID'].fillna(-1, inplace=True)\n",
    "id_df.loc[:,'ID'] = id_df.loc[:,'ID'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437b254",
   "metadata": {},
   "source": [
    "### Additional leakage parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c6b480",
   "metadata": {},
   "source": [
    "As the test set contains data after the train data, these will have a higher row number. Therefore, we could have added the row number as another leakage feature. However, as we has expanded the training set as we did when we added the aggregated features, we choose not to add this feature. In addition, we could be unlucky and have a test set which is shuffled with respect to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff0bead",
   "metadata": {},
   "source": [
    "### Holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0c5969",
   "metadata": {},
   "source": [
    "We will here generate the number of holidays in the previous month, the current month and the next month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02248be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_russian_holidays(year):\n",
    "    \"\"\"\n",
    "    Returns a Series of Russian holidays in a given year\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        The year to investigate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    holidays : Series\n",
    "        Series of the holidays on datetime64 format\n",
    "    \"\"\"\n",
    "    \n",
    "    url = f'https://www.timeanddate.com/holidays/russia/{year}'\n",
    "    html = requests.get(url).content\n",
    "    # A list is returned\n",
    "    table_df = pd.read_html(html)[0]\n",
    "    # Rename\n",
    "    table_df = table_df.rename(columns={'Date': 'date'})\n",
    "    holidays = pd.to_datetime(table_df['date'], format='%b %d')\n",
    "    \n",
    "    # Replace the year and cast to datetime\n",
    "    holidays = holidays.apply(lambda x: x.replace(year=year))\n",
    "\n",
    "    return holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f30c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_months_len(df):\n",
    "    \"\"\"\n",
    "    Returns the number of entries grouped by year and month of the input data frame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame with a column named 'date'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        The input DataFrame where the number of entries grouped by year and month\n",
    "        is appended to the column named 'year_month_count' \n",
    "    \"\"\"\n",
    "    \n",
    "    new_df = df.copy()\n",
    "    \n",
    "    new_df.loc[:, 'year'] = new_df.loc[:, 'date'].dt.year\n",
    "    new_df.loc[:, 'month'] = new_df.loc[:, 'date'].dt.month\n",
    "    \n",
    "    df.loc[:, 'year_month_count'] = new_df.groupby(['year', 'month'])['date'].transform(len)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bdd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We include 2012 to get the first prev_holiday_count later\n",
    "holiday_2012 = get_russian_holidays(2012).to_frame()\n",
    "holiday_2013 = get_russian_holidays(2013).to_frame()\n",
    "holiday_2014 = get_russian_holidays(2014).to_frame()\n",
    "holiday_2015 = get_russian_holidays(2015).to_frame()\n",
    "holidays = pd.concat([holiday_2012, holiday_2013, holiday_2014, holiday_2015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a97918",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_count = get_year_months_len(holidays).rename(columns={'year_month_count': 'holiday_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0be538",
   "metadata": {},
   "source": [
    "Let's now generate the previous month holidays count.\n",
    "We can get that by increasing the month by one (if the holiday count of February was 1 and the holiday count of March was 2, the holiday count of March will be 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0da6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_holiday_count = holiday_count.copy()\n",
    "prev_holiday_count.loc[:, 'date'] = prev_holiday_count.loc[:, 'date'] + pd.DateOffset(months=1)\n",
    "prev_holiday_count = prev_holiday_count.rename(columns={'holiday_count': 'prev_holiday_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f853696",
   "metadata": {},
   "source": [
    "Likewise, we can find the next month holiday count by subtracting the months by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4701de",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_holiday_count = holiday_count.copy()\n",
    "next_holiday_count.loc[:, 'date'] = next_holiday_count.loc[:, 'date'] + pd.DateOffset(months=-1)\n",
    "next_holiday_count = next_holiday_count.rename(columns={'holiday_count': 'next_holiday_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e6d8e",
   "metadata": {},
   "source": [
    "We drop the `date` and create `year` and `month` features we can merge on. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784f33c1",
   "metadata": {},
   "source": [
    "**NOTE**: In order to merge the date data smoothly afterwards, we should drop the resulting duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a08b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(df, col, return_all=False):\n",
    "    \"\"\"\n",
    "    Returns a new DataFrame with added text features\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        The data frame to add the text features to\n",
    "    col : str\n",
    "        The column to obtain the text features from\n",
    "    return_all : bool\n",
    "        If True, intermediate columns will be returned\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_nlp : DataFrame\n",
    "        The data frame with the added text features\n",
    "        * {col}_clean - col column cleaned so that only alphabetical and numerical characters are present \n",
    "                        (only returned if return_all is True)\n",
    "        * cyrillic_latin - column where cyrillic and latin letters has been separated \n",
    "                           (only returned if return_all is True)\n",
    "        * cyrillic - column with only stemmed cyrillic words present (only returned if return_all is True)\n",
    "        * latin - column with only stemmed latin words present (only returned if return_all is True)\n",
    "        * {col}_nlp - combination of the cyrillic and latin column described above\n",
    "        * {col}_cyrillic_words - cyrillic word count\n",
    "        * {col}_latin_words - latin word count\n",
    "        * {col}_total_words - total word count\n",
    "    \"\"\"\n",
    "    \n",
    "    df_nlp = df.copy()\n",
    "    \n",
    "    # First we clean the text by removing non-alphabetical characters and non-numeric characters\n",
    "    \n",
    "    df_nlp.loc[:, f'{col}_clean'] = \\\n",
    "    df_nlp.loc[:, f'{col}'].apply(lambda s: re.sub('[^а-яА-Яa-zA-Z0-9 ]', ' ', s))\n",
    "\n",
    "    # Remove duplicated whitespaces\n",
    "    df_nlp.loc[:, f'{col}_clean'] = \\\n",
    "        df_nlp.loc[:, f'{col}_clean'].apply(lambda s: re.sub(' +',' ', s))\n",
    "    \n",
    "    df_nlp.loc[:, 'cyrillic_latin'] = df_nlp.loc[:, f'{col}_clean'].apply(separate_cyrillic_latin)\n",
    "    df_nlp.loc[:, 'cyrillic'] = df_nlp.loc[:, 'cyrillic_latin'].apply(lambda s: s.split('_SEP_')[0])\n",
    "    df_nlp.loc[:, 'latin'] = df_nlp.loc[:, 'cyrillic_latin'].apply(lambda s: s.split('_SEP_')[1])\n",
    "    \n",
    "    df_nlp.loc[:, 'cyrillic'] = df_nlp.loc[:, 'cyrillic'].apply(russian_stemmer.stem)\n",
    "    df_nlp.loc[:, 'latin'] = df_nlp.loc[:, 'latin'].apply(english_stemmer.stem)\n",
    "    \n",
    "    # Recombine words\n",
    "    df_nlp.loc[:, f'{col}_nlp'] = df_nlp.loc[:, 'cyrillic'].str[:] + ' ' + df_nlp.loc[:, 'latin'].str[:]\n",
    "    \n",
    "    # We add the word count of each type together with the total.\n",
    "    # The rationale for doing is\n",
    "    # 1. It's possible that product with complex names are not sold as much\n",
    "    # 2. In case there is a lot of English words in the product, it could be that it's less sellable in Russia\n",
    "    # 3. Possible other reasons not mentioned here\n",
    "    \n",
    "    df_nlp.loc[:, f'{col}_cyrillic_words'] = \\\n",
    "        df_nlp.loc[:, 'cyrillic'].apply(lambda s: len(s.split(' ')) if s != '' else 0)\n",
    "    df_nlp.loc[:, f'{col}_latin_words'] = \\\n",
    "        df_nlp.loc[:, 'latin'].apply(lambda s: len(s.split(' ')) if s != '' else 0)\n",
    "    \n",
    "    # NOTE: This is in fact an interaction feature\n",
    "    df_nlp.loc[:, f'{col}_total_words'] = \\\n",
    "        df_nlp.loc[:, f'{col}_cyrillic_words'] + df_nlp.loc[:, f'{col}_latin_words']\n",
    "    \n",
    "    if not return_all:\n",
    "        remove = [f'{col}_clean', 'cyrillic_latin', 'cyrillic', 'latin']\n",
    "        df_nlp.drop(remove, axis=1, inplace=True)\n",
    "    \n",
    "    return df_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_nlp = get_text_features(items, 'item_name')\n",
    "item_category_nlp = get_text_features(item_categories, 'item_category_name')\n",
    "shop_nlp = get_text_features(shops, 'shop_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7333d3",
   "metadata": {},
   "source": [
    "Check how many tokens we are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_corpus = ' '.join(item_nlp.loc[:, 'item_name_nlp'].values)\n",
    "item_corpus_tokens = nltk.word_tokenize(item_corpus)\n",
    "print(f'Unique item_name_tokens {len(set(item_corpus_tokens))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_category_corpus = ' '.join(item_category_nlp.loc[:, 'item_category_name_nlp'].values)\n",
    "item_category_corpus_tokens = nltk.word_tokenize(item_category_corpus)\n",
    "print(f'Unique item_category_name_tokens {len(set(item_category_corpus_tokens))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_corpus = ' '.join(shop_nlp.loc[:, 'shop_name_nlp'].values)\n",
    "shop_corpus_tokens = nltk.word_tokenize(shop_corpus)\n",
    "print(f'Unique shop_name_tokens {len(set(shop_corpus_tokens))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25df866",
   "metadata": {},
   "source": [
    "We should take care not to use all tokens as this may result in a [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). \n",
    "Let's see how the words are distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e258c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f4753",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fd_item = nltk.FreqDist(item_corpus_tokens)\n",
    "fd_item.plot(samples, cumulative=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6013d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fd_item_category = nltk.FreqDist(item_category_corpus_tokens)\n",
    "fd_item_category.plot(samples, cumulative=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80065acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "fd_shop = nltk.FreqDist(shop_corpus_tokens)\n",
    "fd_shop.plot(samples, cumulative=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a2ad70",
   "metadata": {},
   "source": [
    "We can see that a couple of words constitutes the most of the corpuses. In other words we can expect a high information gain from the first couple of features and diminishing returns as we add more words. We will from graphical inspection try with max features $35$ for TF-IDF for the item corpus, $25$ for the item category corpus and $10$ for the shop corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1493c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = sales_train.loc[:, ['date_block_num', 'date']]\n",
    "\n",
    "# Add one date in place of the dates for prediction\n",
    "# NOTE: The relativedelta module takes care of problems with dates ending with 28, 30, 31\n",
    "next_month = dates.loc[:, 'date'].max() + relativedelta(months=1)\n",
    "next_date_block_num = dates.loc[:, 'date_block_num'].max() + 1\n",
    "test_month = pd.DataFrame({'date_block_num':[next_date_block_num], 'date':[next_month]})\n",
    "dates = pd.concat([dates, test_month], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19cdfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f6db2",
   "metadata": {},
   "source": [
    "Recall from the EDA that we found out that the last date in dataset was `2015-10-31`, this means we are going to predict for `2015-11`. \n",
    "\n",
    "Further, we note that only the year and month data is present in the test dataset, meaning that using information on the day level does not make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5827077",
   "metadata": {},
   "source": [
    "### Standard date features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab3950",
   "metadata": {},
   "source": [
    "We here add date features as seasonal trends are present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates.loc[:, 'year'] = dates.loc[:, 'date'].dt.year\n",
    "dates.loc[:, 'month'] = dates.loc[:, 'date'].dt.month\n",
    "dates.loc[:, 'days_in_month'] = dates.loc[:, 'date'].dt.days_in_month\n",
    "dates.loc[:, 'quarter'] = dates.loc[:, 'date'].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_aggregate = pd.merge(corpus_aggregate, item_nlp.drop_duplicates(), how='left', on='item_id')\n",
    "corpus_aggregate = pd.merge(corpus_aggregate, item_category_nlp.drop_duplicates(), how='left', on='item_category_id')\n",
    "corpus_aggregate = pd.merge(corpus_aggregate, shop_nlp.drop_duplicates(), how='left', on='shop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8227d2",
   "metadata": {},
   "source": [
    "We reduce to $20 %$ of original dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_new_dimensions = int((n_item_features + n_item_category_features + n_shop_features)*.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d53df5",
   "metadata": {},
   "source": [
    "**NOTE:** We normally fit on the training set and transform on the test set. However, since the whole set is available for the training set, such split does not make sense here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54505cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_cols = [col for col in corpus_aggregate if '_tf_idf_' in col]\n",
    "\n",
    "nmf = decomposition.NMF(n_components=n_new_dimensions)\n",
    "reduced_corpus = nmf.fit_transform(corpus_aggregate[tf_idf_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_cols = [f'nlp_nmf_{i}' for i in range(reduced_corpus.shape[1])]\n",
    "reduced_corpus = pd.DataFrame(reduced_corpus, columns=nmf_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c7ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_aggregate = pd.concat([corpus_aggregate, reduced_corpus], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8112fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if corpus_aggregate.isnull().any().any():\n",
    "    raise AssertionError('NaNs were introduced in the corpus')\n",
    "    \n",
    "if corpus_aggregate.shape[0] > n_corpus_aggregate:\n",
    "    raise AssertionError(f'The set was expanded: '\n",
    "                         f'n_corpus_aggregate={n_corpus_aggregate} and '\n",
    "                         f'corpus_aggregate.shape[0]={corpus_aggregate.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "del item_nlp\n",
    "del item_category_nlp\n",
    "del shop_nlp\n",
    "del reduced_corpus\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dee8942",
   "metadata": {},
   "source": [
    "# 4 Adding leakage features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a86703",
   "metadata": {},
   "source": [
    "The leakage features are features where we use information about the test set.\n",
    "\n",
    "As both shop id and item id are features of the test set, and since these are not related to time, these are leakages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b148871",
   "metadata": {},
   "source": [
    "### Number of ids in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del shop_id_train\n",
    "del shop_id_test\n",
    "del item_id_train\n",
    "del item_id_test\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3926bb9",
   "metadata": {},
   "source": [
    "# 5 The big merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4d041",
   "metadata": {},
   "source": [
    "We are now ready to merge the different features into one big data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847decdc",
   "metadata": {},
   "source": [
    "# 3 Adding text features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a3603",
   "metadata": {},
   "source": [
    "Taking into the possibility that the names are correlated to the target, we add some text features as well. We split `item_name`, `shop_name` and `item_category_name` into cyrillic and latin words. We will stem these, and then combine them again before fitting a TF-IDF model to them.\n",
    "\n",
    "**NOTE**: The TF-IDF model does not care about the relative position of the words, so it is ok if the order is scrambled when recombining the words to sentences again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa3ae2",
   "metadata": {},
   "source": [
    "We would now like to stem the words (ideally we would like to lemmatize the words, but it looks like the lemmatization for non-english languages are not as readily available at the moment).\n",
    "\n",
    "**NOTE**: The stemmer casts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aafe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_stemmer = nltk.stem.SnowballStemmer('russian')\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90f3973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_cyrillic_latin(words):\n",
    "    \"\"\"\n",
    "    Separates the cyrillic and latin words\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    This function does not conserve word order\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    words : str\n",
    "        The string of words to be split\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    separated_words : str\n",
    "        The words separated by _SEP_\n",
    "        Cyrillic words are to the left of the separator, the latin to the right\n",
    "    \"\"\"\n",
    "    \n",
    "    words_split = words.split(' ')\n",
    "    cyrillic_words = list()\n",
    "    latin_words = list()\n",
    "    \n",
    "    for word in words_split:\n",
    "        # https://stackoverflow.com/questions/48255244/python-check-if-a-string-contains-cyrillic-characters\n",
    "        if re.search('[а-яА-Я]', word) is not None:\n",
    "            cyrillic_words.append(word)\n",
    "        else:\n",
    "            latin_words.append(word)\n",
    "    \n",
    "    cyrillic_words = ' '.join(cyrillic_words)\n",
    "    latin_words = ' '.join(latin_words)\n",
    "    \n",
    "    separated_words = f'{cyrillic_words}_SEP_{latin_words}'\n",
    "    \n",
    "    return separated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29e2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_item_features = 35\n",
    "n_item_category_features = 25\n",
    "n_shop_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a94841",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_item_vec = sklearn.feature_extraction.text.TfidfVectorizer(max_features=n_item_features)\n",
    "tf_idf_item = tf_idf_item_vec.fit_transform(item_nlp['item_name_nlp']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15683015",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_item_category_vec = sklearn.feature_extraction.text.TfidfVectorizer(max_features=n_item_category_features)\n",
    "tf_idf_item_category = tf_idf_item_category_vec.fit_transform(item_category_nlp['item_category_name_nlp']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eebf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_shop_vec = sklearn.feature_extraction.text.TfidfVectorizer(max_features=n_shop_features)\n",
    "tf_idf_shop = tf_idf_shop_vec.fit_transform(shop_nlp['shop_name_nlp']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e29b8ae",
   "metadata": {},
   "source": [
    "Combine the TF-IDF results with the corresponding data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64716620",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [f'item_tf_idf_{i}' for i in range(tf_idf_item.shape[1])]\n",
    "tf_idf_item_df = pd.DataFrame(tf_idf_item, columns=col_names)\n",
    "item_nlp = pd.concat([item_nlp, tf_idf_item_df], axis=1)\n",
    "item_nlp.drop(['item_name', 'item_category_id', 'item_name_nlp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113bd6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [f'item_category_tf_idf_{i}' for i in range(tf_idf_item_category.shape[1])]\n",
    "tf_idf_item_category_df = pd.DataFrame(tf_idf_item_category, columns=col_names)\n",
    "item_category_nlp = pd.concat([item_category_nlp, tf_idf_item_category_df], axis=1)\n",
    "item_category_nlp.drop(['item_category_name', 'item_category_name_nlp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [f'shop_tf_idf_{i}' for i in range(tf_idf_shop.shape[1])]\n",
    "tf_idf_shop_df = pd.DataFrame(tf_idf_shop, columns=col_names)\n",
    "shop_nlp = pd.concat([shop_nlp, tf_idf_shop_df], axis=1)\n",
    "shop_nlp.drop(['shop_name', 'shop_name_nlp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f6d32",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb71da",
   "metadata": {},
   "source": [
    "In order to save computational time, we would like to decrease the dimensionality of the corpus (as the tf-idf output is a relatively sparse matrix). As we will do this across item name, item category and shop name, we will gain the advantage of interaction between the words in our corpus.\n",
    "\n",
    "Since the tf-idf outputs non-negative numbers, we can reduce the numbers using Non-negative Matrix Factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd7f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_id_train = sales_train.loc[:, 'shop_id']\n",
    "shop_id_test = sales_test.loc[:, 'shop_id']\n",
    "shop_id_both = pd.concat([shop_id_train, shop_id_test], axis=0).to_frame()\n",
    "shop_id_both.loc[:, 'shop_id_count'] = shop_id_both.groupby('shop_id')['shop_id'].transform(len)\n",
    "\n",
    "# NOTE: Drop duplicated as we want to merge\n",
    "shop_id_both.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa8eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_train = sales_train.loc[:, 'item_id']\n",
    "item_id_test = sales_test.loc[:, 'item_id']\n",
    "item_id_both = pd.concat([item_id_train, item_id_test], axis=0).to_frame()\n",
    "item_id_both.loc[:, 'item_id_count'] = item_id_both.groupby('item_id')['item_id'].transform(len)\n",
    "\n",
    "# NOTE: Drop duplicated as we want to merge\n",
    "item_id_both.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8738a363",
   "metadata": {},
   "source": [
    "Out of curiosity we check how these are distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb422e26",
   "metadata": {},
   "source": [
    "Item ID is a combination of `item_id` and `shop_id`. As before, if an ID doesn't exist for the combination, it will be assigned `-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7369abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data,\n",
    "                    id_df.loc[:, ['item_id', 'shop_id', 'ID']].drop_duplicates(),\n",
    "                    how='left',\n",
    "                    on=['item_id', 'shop_id'])\n",
    "all_data.loc[:, 'ID'].fillna(-1, inplace=True)\n",
    "all_data.loc[:, 'ID'] = all_data.loc[:, 'ID'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc279dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lag = max([int(col.split('_lag_')[-1]) for col in data_aggregate.columns if '_lag_' in col])\n",
    "cols_wo_nan = [col for col in data_aggregate.columns if not ('month' in col and '_lag_' not in col)]\n",
    "\n",
    "if all_data.loc[all_data.loc[:, 'date_block_num'] > max_lag, cols_wo_nan].isnull().any().any():\n",
    "    raise AssertionError('NaNs were introduced in the data containing valid lags')\n",
    "    \n",
    "if all_data.shape[0] > n_train_samples:\n",
    "    raise AssertionError(f'The set was expanded: '\n",
    "                         f'n_train_samples={n_train_samples} and '\n",
    "                         f'all_data.shape[0]={all_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c504514b",
   "metadata": {},
   "source": [
    "# 6 Type casting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53606f",
   "metadata": {},
   "source": [
    "In order to save resources, we downcast the types (as they by default are loaded as double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84560a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    \"\"\"\n",
    "    Downcasts float64 to float32 and int64 to int32\n",
    "    \n",
    "    Paramters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The data frame to downcast\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        The downcasted date frame\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df.columns if df.loc[:, c].dtype == 'float64']\n",
    "    int_cols = [c for c in df.columns if df.loc[:, c].dtype == 'int64']\n",
    "    \n",
    "    # Downcast\n",
    "    df.loc[:, float_cols] = df.loc[:, float_cols].astype(np.float32)\n",
    "    df.loc[:, int_cols] = df.loc[:, int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e29e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = downcast_dtypes(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b5f76",
   "metadata": {},
   "source": [
    "We now store the full dataset in case we would like to modify it later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ebb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(all_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6480c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_hdf(generated_data.joinpath('all_data.hdf'), key='all_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2435a22e",
   "metadata": {},
   "source": [
    "# 7 DataFrame trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd04f1",
   "metadata": {},
   "source": [
    "We can now start to trim the dataset for features and rows which are not needed.\n",
    "Firstly, since we are lagging the features up to `max_lag`, we will throw away the first `max_lag` values in our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb90dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506b8f25",
   "metadata": {},
   "source": [
    "This looks correct. Let's merge these to a common data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dda15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.merge(dates.drop('date', axis=1).drop_duplicates(),\n",
    "                 holidays.drop(['year', 'month'] ,axis=1),\n",
    "                 how='left', \n",
    "                 on='date_block_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8dea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "del holiday_count\n",
    "del next_holiday_count\n",
    "del prev_holiday_count\n",
    "del holiday_2012\n",
    "del holiday_2013\n",
    "del holiday_2014\n",
    "del holiday_2015\n",
    "del holidays\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1799d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_holiday_count.loc[:, 'year'] = prev_holiday_count.loc[:, 'date'].dt.year\n",
    "prev_holiday_count.loc[:, 'month'] = prev_holiday_count.loc[:, 'date'].dt.month\n",
    "prev_holiday_count.drop(['date'], axis=1, inplace=True)\n",
    "prev_holiday_count.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624140eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_holiday_count.loc[:, 'year'] = next_holiday_count.loc[:, 'date'].dt.year\n",
    "next_holiday_count.loc[:, 'month'] = next_holiday_count.loc[:, 'date'].dt.month\n",
    "next_holiday_count.drop(['date'], axis=1, inplace=True)\n",
    "next_holiday_count.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69c9af",
   "metadata": {},
   "source": [
    "We merge the previous, current and next holiday count into one frame.\n",
    "The resulting `NaN`s will be locations without vacations.\n",
    "We start by merging with `dates`, as this contains all relevant `year`-`month` combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9619ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = pd.merge(dates.loc[:, ['year', 'month']].drop_duplicates(),\n",
    "                    holiday_count, how='left', on=['year', 'month']).fillna(0)\n",
    "holidays = pd.merge(holidays, prev_holiday_count, how='left', on=['year', 'month']).fillna(0)\n",
    "holidays = pd.merge(holidays, next_holiday_count, how='left', on=['year', 'month']).fillna(0)\n",
    "\n",
    "# Re-shuffle the columns for better overview\n",
    "holidays = holidays.loc[:, ['year', 'month', 'prev_holiday_count', 'holiday_count', 'next_holiday_count']]\n",
    "\n",
    "# All columns can be integers\n",
    "holidays = holidays.astype(np.int32)\n",
    "\n",
    "# Sort by year and month for better overview\n",
    "holidays.sort_values(['year', 'month'], inplace=True)\n",
    "holidays.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Add the date block number\n",
    "holidays.loc[:, 'date_block_num'] = range(holidays.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7aacfc",
   "metadata": {},
   "source": [
    "Inspect that we did the correct thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884683d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(data_aggregate, dates, how='left', on='date_block_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60684c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, corpus_aggregate, how='left', on=['shop_id', 'item_id', 'item_category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, shop_id_both, how='left', on='shop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(all_data, item_id_both, how='left', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_data = all_data.loc[all_data.loc[:, 'date_block_num'] > max_lag].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c6f35",
   "metadata": {},
   "source": [
    "We rename the target, so that the name matches the competition target name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8dca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_target_name = 'month_shop_item_id_item_cnt_sum'\n",
    "new_target_name = 'item_cnt_month'\n",
    "trimmed_data.rename({old_target_name: new_target_name}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if trimmed_data.loc[trimmed_data.loc[:, 'date_block_num'] < \n",
    "                    trimmed_data.loc[:, 'date_block_num'].max(), new_target_name].isnull().any():\n",
    "    raise AssertionError('NaNs were introduced in the target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1d4d5",
   "metadata": {},
   "source": [
    "Next, we have some columns which has just been place-holders, and are now mean encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8923ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['item_category_id', \n",
    "             'item_id', \n",
    "             'shop_id']\n",
    "\n",
    "trimmed_data.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b569c",
   "metadata": {},
   "source": [
    "The columns which were used to create the lag is no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ecfce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [col for col in trimmed_data.columns if 'month_' in col and '_lag_' not in col]\n",
    "trimmed_data.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2f4dc1",
   "metadata": {},
   "source": [
    "We reduced the dimensionality on the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de1c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [col for col in trimmed_data.columns if '_tf_idf_' in col]\n",
    "trimmed_data.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d2f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(trimmed_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if trimmed_data.drop(new_target_name, axis=1).isnull().any().any():\n",
    "    raise AssertionError('NaNs were introduced')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c971f83",
   "metadata": {},
   "source": [
    "We recall that tree-based models does not depend on the normalization. This means that we can use this data-set as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdf9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_aggregate = data_aggregate.loc[:, ['shop_id', 'item_id', 'item_category_id']].copy()\n",
    "corpus_aggregate.drop_duplicates(inplace=True)\n",
    "n_corpus_aggregate = corpus_aggregate.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb885f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_count.loc[:, 'year'] = holiday_count.loc[:, 'date'].dt.year\n",
    "holiday_count.loc[:, 'month'] = holiday_count.loc[:, 'date'].dt.month\n",
    "holiday_count.drop(['date'], axis=1, inplace=True)\n",
    "holiday_count.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98896bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_data.to_hdf(generated_data.joinpath('dt_data.hdf'), key='dt_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e28c7f0",
   "metadata": {},
   "source": [
    "# 8 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2713cac",
   "metadata": {},
   "source": [
    "If we are not using tree based methods, we should use normalize the data. Here we will use the `MaxMinScaler` on the ordinal features, and the `StandardScaler` (which keeps the information of the distribution intact) on the rest of the numerical features.\n",
    "\n",
    "In a real world scenario we would be interested to fit a scale to a fold, and use the fitted scaling on the validation or the test fold as explained [here](https://stats.stackexchange.com/a/174865/132830). In this way we would not bias our scaling towards the test/validation-set, and we can assume that the generalization would perform better.\n",
    "\n",
    "Here however, as we are in a competition setting, our primary goal is to optimize the loss metric of the competition. Thus, such a fallacy could be advantageous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb37733",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ['ID',\n",
    "                    'date_block_num',\n",
    "                    'month',\n",
    "                    'quarter',\n",
    "                    'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3138c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_min_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "trimmed_data.loc[:, ordinal_features] = max_min_scaler.fit_transform(trimmed_data.loc[:, ordinal_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd4752",
   "metadata": {},
   "source": [
    "**NOTE**: We do not need to scale the target"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
