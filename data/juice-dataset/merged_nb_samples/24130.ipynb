{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24455f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeb6942",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39346ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(gs_knn.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa92c67",
   "metadata": {},
   "source": [
    "True positives: 68\n",
    "True negatives:  149\n",
    "False negatives:  35\n",
    "False positives:  16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2e7c4a",
   "metadata": {},
   "source": [
    "#### Print the best parameters and score for the gridsearched kNN model. How does it compare to the logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ba1889",
   "metadata": {},
   "source": [
    "GridSearch - LogisticRegression  \n",
    "Best_score = 0.79012345679012341  \n",
    "Best_params = {'C': 1.3894954943731359, 'penalty': 'l2', 'solver': 'liblinear'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae1a61",
   "metadata": {},
   "source": [
    "GridSearch - KNN  \n",
    "Best_score = 0.8058361391694725  \n",
    "Best_params = {'leaf_size': 10, 'n_neighbors': 20, 'p': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2414b95d",
   "metadata": {},
   "source": [
    "#### How does the number of neighbors affect the bias-variance tradeoff of your model?\n",
    "\n",
    "#### [BONUS] Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78988721",
   "metadata": {},
   "source": [
    "Bias is increasing while increasing number of neighbors.         \n",
    "Variance is decreasing while increasing number of neighbors\n",
    "\n",
    "The k-nearest neighbors algorithm has low bias and high variance, but the trade-off can be changed by increasing the value of k which increases the number of neighbors that contribute t the prediction and in turn increases the bias of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b014ec5",
   "metadata": {},
   "source": [
    "#### In what hypothetical scenario(s) might you prefer logistic regression over kNN, aside from model performance metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eeb305",
   "metadata": {},
   "source": [
    "Logistic regression doesn't need any parameter tuning.         \n",
    "Logistic regression predicts probabilities, which are a measure of the confidence of prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bcb232",
   "metadata": {},
   "source": [
    "#### Fit a new kNN model with the optimal parameters found in gridsearch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal LR model\n",
    "confusion_matrix(y_test,y_logreg_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38fbf5",
   "metadata": {},
   "source": [
    "### Optimal KNN model\n",
    "\n",
    "True positives: 68              \n",
    "True negatives:  149            \n",
    "False negatives:  35           \n",
    "False positives:  16       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3345a776",
   "metadata": {},
   "source": [
    "### Optimal LogisticRegression model\n",
    "\n",
    "True positives: 72  \n",
    "True negatives:  134  \n",
    "False negatives:  31  \n",
    "False positives:  31  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20942b0b",
   "metadata": {},
   "source": [
    "Optimal KNN model is way better than LR on True negatives prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c496ca",
   "metadata": {},
   "source": [
    "#### [BONUS] Plot the ROC curves for the optimized logistic regression model and the optimized kNN model on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74373cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def auc_plotting_function(rate1, rate2, rate1_name, rate2_name, curve_name):\n",
    "    AUC = auc(rate1, rate2)\n",
    "    plt.plot(rate1, rate2, label=curve_name + ' (area = %0.2f)' % AUC, linewidth=4)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(rate1_name, fontsize=18)\n",
    "    plt.ylabel(rate2_name, fontsize=18)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "def plot_roc(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc_plotting_function(fpr, tpr, 'False Positive Rate', 'True Positive Rate', 'ROC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b097796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_logreg_score = logreg_optimal.decision_function(X_test)\n",
    "plot_roc(y_test, y_logreg_score)\n",
    "y_knn_score = knn_optimal.predict(X_test)\n",
    "plot_roc(y_test, y_knn_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efdd816",
   "metadata": {},
   "source": [
    "## [BONUS] Precision-recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262bbecf",
   "metadata": {},
   "source": [
    "#### Gridsearch the same parameters for logistic regression but change the scoring function to 'average_precision'\n",
    "\n",
    "`'average_precision'` will optimize parameters for area under the precision-recall curve instead of for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score,KFold\n",
    "logreg = LogisticRegression(class_weight='balanced')\n",
    "cv = KFold(len(y), n_folds=5, shuffle=True)\n",
    "logreg_parameters = {\n",
    "    'penalty':['l1','l2'],\n",
    "    'C':np.logspace(-5,1,50),\n",
    "    'solver':['liblinear']\n",
    "}\n",
    "gs_logreg = GridSearchCV(logreg,logreg_parameters,cv=cv,n_jobs=-1,scoring='average_precision')\n",
    "gs_logreg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24b05f",
   "metadata": {},
   "source": [
    "#### Examine the best parameters and score. Are they different than the logistic regression gridsearch in part 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_optimal = gs_knn.best_estimator_\n",
    "knn_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f9f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_optimal.fit(X_train,y_train)\n",
    "y_knn_pred = knn_optimal.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a94357b",
   "metadata": {},
   "source": [
    "#### Construct the confusion matrix for the optimal kNN model. Is it different from the logistic regression model? If so, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a93a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal KNN model\n",
    "confusion_matrix(y_test,y_knn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7de17ca",
   "metadata": {},
   "source": [
    "#### Fit a new Logreg model with optimal params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb8221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare accuracy score\n",
    "\n",
    "# tree with grid: 0.76119402985074625\n",
    "# Knn with grid: 0.80970149253731338\n",
    "# LR-Grid_search with scoring = 'average_precision': 0.77611940298507465"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14af1fc2",
   "metadata": {},
   "source": [
    "#### Plot all three optimized models' ROC curves on the same plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23691bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_logreg_score = logreg_optimal_scoring.decision_function(X_test)\n",
    "plot_roc(y_test, y_logreg_score)\n",
    "\n",
    "y_knn_score = knn_optimal.predict(X_test)\n",
    "plot_roc(y_test, y_knn_score)\n",
    "\n",
    "y_tree_score = tree_optimal.predict(X_test)\n",
    "plot_roc(y_test, y_tree_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c93402f",
   "metadata": {},
   "source": [
    "#### Use sklearn's BaggingClassifier with the base estimator your optimized decision tree model. How does the performance compare to the single decision tree classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0692f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "baggingtree = BaggingClassifier(tree_optimal)\n",
    "evaluate_model(baggingtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to 0.76119402985074625 from optimized tree model. Bagging result is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e899ad",
   "metadata": {},
   "source": [
    "#### Gridsearch the optimal n_estimators, max_samples, and max_features for the bagging classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4466f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_params = {'n_estimators': [10, 20, 30, 40],\n",
    "                  'max_samples': [0.3,0.5,0.7,0.8,1.0],\n",
    "                  'max_features': [0.1,0.3,0.5,0.7,0.8,1.0]}\n",
    "cv = KFold(len(y),n_folds=5,shuffle=True)\n",
    "\n",
    "gsbaggingtree = GridSearchCV(baggingtree,\n",
    "                            bagging_params, n_jobs=-1,\n",
    "                            cv=cv)\n",
    "gsbaggingtree.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca18a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsbaggingtree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsbaggingtree.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024712b9",
   "metadata": {},
   "source": [
    "#### Create a bagging classifier model with the optimal parameters and compare it's performance to the other two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17cd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5 best_params\n",
    "#{'C': 1.3894954943731359, 'penalty': 'l2', 'solver': 'liblinear'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103463a",
   "metadata": {},
   "source": [
    "#### Create the confusion matrix. Is it different than when you optimized for the accuracy? If so, why would this be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d8a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(gs_logreg.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733029a8",
   "metadata": {},
   "source": [
    "It is better than LR and previous LR with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1bf37",
   "metadata": {},
   "source": [
    "#### Plot the precision-recall curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051fb774",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_optimal_scoring = gs_logreg.best_estimator_\n",
    "logreg_optimal_scoring.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f58fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg_optimal_scoring.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b3b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "def auc_plotting_function(rate1, rate2, rate1_name, rate2_name, curve_name):\n",
    "    plt.plot(rate1, rate2, linewidth=4)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(rate1_name, fontsize=18)\n",
    "    plt.ylabel(rate2_name, fontsize=18)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "def plot_roc(y_true, y_score):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    auc_plotting_function(recall,precision,  'Recall', 'Precision', 'RP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b77605",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = logreg_optimal_scoring.decision_function(X_test)\n",
    "plot_roc(y_test, y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52133653",
   "metadata": {},
   "source": [
    "## [VERY BONUS] Decision trees, ensembles, bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16379122",
   "metadata": {},
   "source": [
    "#### Gridsearch a decision tree classifier model on the data, searching for optimal depth. Create a new decision tree model with the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f75a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(gsbaggingtree.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c41d4",
   "metadata": {},
   "source": [
    "### Random Forest, AdaBoost Regressor, Gradient Boosting Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77fa1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_optimal = gs_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47004afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(tree_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19bc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_optimal.fit(X_train,y_train)\n",
    "y_pred = tree_optimal.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a566bb9",
   "metadata": {},
   "source": [
    "#### Compare the performace of the decision tree model to the logistic regression and kNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "cv = KFold(len(y), n_folds=5, shuffle=True)\n",
    "tree_params = {\n",
    "    'max_depth' : [1,2,3,4,5,6]\n",
    "}\n",
    "tree = DecisionTreeClassifier(class_weight='balanced')\n",
    "gs_tree = GridSearchCV(tree,tree_params,cv=cv,n_jobs=-1,)\n",
    "gs_tree.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dc8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547dab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_tree.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler 'age' and 'fare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a4811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#age\n",
    "from sklearn.preprocessing import normalize,StandardScaler\n",
    "#preprocessing_age = normalize(df.Age)\n",
    "#preprocessing_age\n",
    "scalar = StandardScaler().fit(df.Age)\n",
    "Age_transformed = scalar.transform(df.Age)\n",
    "Age_transformed = pd.Series(Age_transformed,name='Age_transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d498ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fare\n",
    "scalar = StandardScaler().fit(df.Fare)\n",
    "Fare_transformed = scalar.transform(df.Fare)\n",
    "Fare_transformed = pd.Series(Fare_transformed,name='Fare_transformed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada11243",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = pd.concat([df.PassengerId,df.Name,df.Survived,df.Sex,\n",
    "                         Embarked_dummies,Pclass_dummies,Age_transformed,Fare_transformed\n",
    "                         ,df.SibSp,df.Parch], axis=1)\n",
    "df_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c68292",
   "metadata": {},
   "source": [
    "## Logistic Regression and Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cbacdf",
   "metadata": {},
   "source": [
    "#### Define the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965f81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "X = df_complete.drop(df_complete[[0,1,2]],axis=1)\n",
    "y = df_complete.Survived\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e2fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c96786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "def evaluate_model(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "    print cm\n",
    "    print cr\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f121c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "evaluate_model(LogisticRegression(class_weight='balanced'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3accf0",
   "metadata": {},
   "source": [
    "True positives: 72\n",
    "True negatives:  134\n",
    "False negatives:  31\n",
    "False positives:  31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0380d5",
   "metadata": {},
   "source": [
    "## Gridsearch - LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d5e9b3",
   "metadata": {},
   "source": [
    "Same on accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526ebdfb",
   "metadata": {},
   "source": [
    "#### Explain the difference between the difference between the L1 (Lasso) and L2 (Ridge) penalties on the model coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48befd53",
   "metadata": {},
   "source": [
    "Ridge regression can't zero out coefficients; thus, you either end up including all the coefficients in the model, or none of them. In contrast, the LASSO does both parameter shrinkage and variable selection automatically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b6a63",
   "metadata": {},
   "source": [
    "#### What hypothetical situations are the Ridge and Lasso penalties useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394001af",
   "metadata": {},
   "source": [
    "Large number of variables or low ratio of no. observations to no. variables (including the n≪pn≪p case), high collinearity, seeking for a sparse solution (i.e., embed feature selection when estimating model parameters), or accounting for variables grouping in high-dimensional data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51610a39",
   "metadata": {},
   "source": [
    "## Gridsearch and kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af47ab56",
   "metadata": {},
   "source": [
    "#### Perform Gridsearch for the same classification problem as above, but use KNeighborsClassifier as your estimator\n",
    "\n",
    "At least have number of neighbors and weights in your parameters dictionary."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
