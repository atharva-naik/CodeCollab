{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data file\n",
    "rev = pd.read_csv(\"C:/Users/USER/Desktop/Course_Data_Scientist_Springboard/1Project_E-commerce/Womens Clothing E-Commerce Reviews.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf0e45",
   "metadata": {},
   "source": [
    "### Basic Feature Extraction for Review Text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85061ed9",
   "metadata": {},
   "source": [
    "First, I clean the data removing the NaN values and save as 'rev_new'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873000ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out data removing the NaN values\n",
    "rev_new = rev[rev['Review Text'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6bbfb6",
   "metadata": {},
   "source": [
    "Here, we calculate the number of characters in each review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef70d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common words for different \"star\" rating \n",
    "\n",
    "# 1 star \n",
    "\n",
    "word_freq_1star= pd.Series(' '.join(preprocessing(rev_new_1star['RT_new'])).split()).value_counts()\n",
    "top_freq_1star = word_freq_1star[:20]\n",
    "tot_num_word_1star = len(pd.Series(' '.join(preprocessing(rev_new_1star['RT_new'])).split()))\n",
    "print (pd.DataFrame({'Word Freq' :top_freq_1star , 'Percentage': (100*(top_freq_1star/tot_num_word_1star))}, columns = ['Word Freq','Percentage']))\n",
    "\n",
    "\n",
    "# 2 star \n",
    "\n",
    "word_freq_2star= pd.Series(' '.join(preprocessing(rev_new_2star['RT_new'])).split()).value_counts()\n",
    "top_freq_2star = word_freq_2star[:20]\n",
    "tot_num_word_2star = len(pd.Series(' '.join(preprocessing(rev_new_2star['RT_new'])).split()))\n",
    "print (pd.DataFrame({'Word Freq' :top_freq_2star , 'Percentage': (100*(top_freq_2star/tot_num_word_2star))}, columns = ['Word Freq','Percentage']))\n",
    "\n",
    "# 3 star\n",
    "\n",
    "word_freq_3star= pd.Series(' '.join(preprocessing(rev_new_3star['RT_new'])).split()).value_counts()\n",
    "top_freq_3star = word_freq_3star[:20]\n",
    "tot_num_word_3star = len(pd.Series(' '.join(preprocessing(rev_new_3star['RT_new'])).split()))\n",
    "print (pd.DataFrame({'Word Freq' :top_freq_3star , 'Percentage': (100*(top_freq_3star/tot_num_word_3star))}, columns = ['Word Freq','Percentage']))\n",
    "\n",
    "\n",
    "# 4 star \n",
    "\n",
    "word_freq_4star= pd.Series(' '.join(preprocessing(rev_new_4star['RT_new'])).split()).value_counts()\n",
    "top_freq_4star = word_freq_4star[:20]\n",
    "tot_num_word_4star = len(pd.Series(' '.join(preprocessing(rev_new_4star['RT_new'])).split()))\n",
    "print (pd.DataFrame({'Word Freq' :top_freq_4star , 'Percentage': (100*(top_freq_4star/tot_num_word_4star))}, columns = ['Word Freq','Percentage']))\n",
    "\n",
    "# 5 star\n",
    "\n",
    "word_freq_5star= pd.Series(' '.join(preprocessing(rev_new_5star['RT_new'])).split()).value_counts()\n",
    "top_freq_5star = word_freq_5star[:20]\n",
    "tot_num_word_5star = len(pd.Series(' '.join(preprocessing(rev_new_5star['RT_new'])).split()))\n",
    "print (pd.DataFrame({'Word Freq' :top_freq_5star , 'Percentage': (100*(top_freq_5star/tot_num_word_5star))}, columns = ['Word Freq','Percentage']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e27f1",
   "metadata": {},
   "source": [
    "### TF - IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0606a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer(stop_words='english')   \n",
    "text_counts = count_vect.fit_transform(rev_new['RT_new'])\n",
    "feat_name =  count_vect.get_feature_names()\n",
    "print (text_counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "text_tfidf = tfidf_transformer.fit_transform(text_counts)\n",
    "print (text_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check TF-IDF mean score for whole dataset\n",
    "\n",
    "\n",
    "# TF-IDF mean score excluding 0s\n",
    "\n",
    "mean_tfidf_score = np.true_divide(text_tfidf.sum(0),(text_tfidf != 0).sum(0))\n",
    "\n",
    "#print (mean_tfidf_score)\n",
    "#print (mean_tfidf_score.shape)\n",
    "#print (type(mean_tfidf_score))\n",
    "\n",
    "mean_tfidf_score_list = np.array(mean_tfidf_score)[0].tolist()\n",
    "\n",
    "# Function to get the maximum tf-idf scores \n",
    "\n",
    "def f(a,N):\n",
    "    return np.argsort(a)[::-1][:N]\n",
    "\n",
    "\n",
    "max_tfidf_indices_list = f(mean_tfidf_score_list,10)\n",
    "#print (max_tfidf_indices_list)\n",
    "\n",
    "\n",
    "#for i in range(len(max_tfidf_indices_list)):\n",
    "#    print (\"The tf_idf mean score is:\" , mean_tfidf_score_list[max_tfidf_indices_list[i]], \"and the word is:\" , feat_name[max_tfidf_indices_list[i]]  )\n",
    "    \n",
    "\n",
    "# TF-IDF mean score including 0s\n",
    "\n",
    "mean_tfidf_score_with0s = np.mean(text_tfidf , axis=0)\n",
    "\n",
    "mean_tfidf_score_with0s_list = np.array(mean_tfidf_score_with0s)[0].tolist()\n",
    "max_tfidf_with0s_indices_list = f(mean_tfidf_score_with0s_list,20)\n",
    "\n",
    "#print (max_tfidf_with0s_indices_list)\n",
    "\n",
    "#for i in range(len(max_tfidf_with0s_indices_list)):\n",
    "#    print (\"The tf_idf mean score is:\" , mean_tfidf_score_with0s_list[max_tfidf_with0s_indices_list[i]], \"and the word is:\" , feat_name[max_tfidf_with0s_indices_list[i]]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9bc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF for each rating group\n",
    "\n",
    "# TF-idf for rating \"1 star\" \n",
    "\n",
    "print (\"------ The words with highest TF-IDF scores in 1 star group ------- \")\n",
    "\n",
    "text_counts_1star = count_vect.fit_transform(rev_new_1star['RT_new'])\n",
    "feat_name_1star =  count_vect.get_feature_names()\n",
    "text_tfidf_1star = tfidf_transformer.fit_transform(text_counts_1star)\n",
    "\n",
    "mean_tfidf_score_1star = np.mean(text_tfidf_1star , axis=0)\n",
    "\n",
    "mean_tfidf_score_1star_list = np.array(mean_tfidf_score_1star)[0].tolist()\n",
    "max_tfidf_1star_indices_list = f(mean_tfidf_score_1star_list,20)\n",
    "\n",
    "for i in range(len(max_tfidf_1star_indices_list)):\n",
    "    print ( feat_name_1star[max_tfidf_1star_indices_list[i]], mean_tfidf_score_1star_list[max_tfidf_1star_indices_list[i]] )\n",
    "\n",
    "#print (max_tfidf_1star_indices_list)\n",
    "\n",
    "#for i in range(len(max_tfidf_1star_indices_list)):\n",
    "#    print (\"The index is:\" , max_tfidf_1star_indices_list[i], \" , the tf_idf mean score is:\" , mean_tfidf_score_1star_list[max_tfidf_1star_indices_list[i]], \"and the word is:\" , feat_name_1star[max_tfidf_1star_indices_list[i]]  )\n",
    "    \n",
    "# TF-idf for rating \"2 star\" \n",
    "\n",
    "print (\"------ The words with highest TF-IDF scores in 2 star group ------- \")\n",
    "\n",
    "text_counts_2star = count_vect.fit_transform(rev_new_2star['RT_new'])\n",
    "feat_name_2star =  count_vect.get_feature_names()\n",
    "text_tfidf_2star = tfidf_transformer.fit_transform(text_counts_2star)\n",
    "\n",
    "mean_tfidf_score_2star = np.mean(text_tfidf_2star , axis=0)\n",
    "\n",
    "mean_tfidf_score_2star_list = np.array(mean_tfidf_score_2star)[0].tolist()\n",
    "max_tfidf_2star_indices_list = f(mean_tfidf_score_2star_list,20)\n",
    "\n",
    "#print (max_tfidf_2star_indices_list)\n",
    "\n",
    "for i in range(len(max_tfidf_2star_indices_list)):\n",
    "    print ( feat_name_2star[max_tfidf_2star_indices_list[i]], mean_tfidf_score_2star_list[max_tfidf_2star_indices_list[i]] )\n",
    "\n",
    "#    print (\"The index is:\" , max_tfidf_2star_indices_list[i], \" , the tf_idf mean score is:\" , mean_tfidf_score_2star_list[max_tfidf_2star_indices_list[i]], \"and the word is:\" , feat_name_2star[max_tfidf_2star_indices_list[i]]  )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# TF-idf for rating \"3 star\" \n",
    "\n",
    "print (\"------ The words with highest TF-IDF scores in 3 star group ------- \")\n",
    "\n",
    "text_counts_3star = count_vect.fit_transform(rev_new_3star['RT_new'])\n",
    "feat_name_3star =  count_vect.get_feature_names()\n",
    "text_tfidf_3star = tfidf_transformer.fit_transform(text_counts_3star)\n",
    "\n",
    "mean_tfidf_score_3star = np.mean(text_tfidf_3star , axis=0)\n",
    "\n",
    "mean_tfidf_score_3star_list = np.array(mean_tfidf_score_3star)[0].tolist()\n",
    "max_tfidf_3star_indices_list = f(mean_tfidf_score_3star_list,20)\n",
    "\n",
    "#print (max_tfidf_3star_indices_list)\n",
    "\n",
    "for i in range(len(max_tfidf_3star_indices_list)):\n",
    "    print ( feat_name_3star[max_tfidf_3star_indices_list[i]], mean_tfidf_score_3star_list[max_tfidf_3star_indices_list[i]] )\n",
    "        \n",
    " #   print (\"The index is:\" , max_tfidf_3star_indices_list[i], \" , the tf_idf mean score is:\" , mean_tfidf_score_3star_list[max_tfidf_3star_indices_list[i]], \"and the word is:\" , feat_name_3star[max_tfidf_3star_indices_list[i]]  )\n",
    "    \n",
    "# TF-idf for rating \"4 star\" \n",
    "\n",
    "print (\"------ The words with highest TF-IDF scores in 4 star group ------- \")\n",
    "\n",
    "text_counts_4star = count_vect.fit_transform(rev_new_4star['RT_new'])\n",
    "feat_name_4star =  count_vect.get_feature_names()\n",
    "text_tfidf_4star = tfidf_transformer.fit_transform(text_counts_4star)\n",
    "\n",
    "mean_tfidf_score_4star = np.mean(text_tfidf_4star , axis=0)\n",
    "\n",
    "mean_tfidf_score_4star_list = np.array(mean_tfidf_score_4star)[0].tolist()\n",
    "max_tfidf_4star_indices_list = f(mean_tfidf_score_4star_list,20)\n",
    "\n",
    "#print (max_tfidf_4star_indices_list)\n",
    "\n",
    "for i in range(len(max_tfidf_4star_indices_list)):\n",
    "    print ( feat_name_4star[max_tfidf_4star_indices_list[i]], mean_tfidf_score_4star_list[max_tfidf_4star_indices_list[i]] )\n",
    "    \n",
    "  #  print (\"The index is:\" , max_tfidf_4star_indices_list[i], \" , the tf_idf mean score is:\" , mean_tfidf_score_4star_list[max_tfidf_4star_indices_list[i]], \"and the word is:\" , feat_name_4star[max_tfidf_4star_indices_list[i]]  )\n",
    "\n",
    "    \n",
    "# TF-idf for rating \"5 star\" \n",
    "\n",
    "print (\"------ The words with highest TF-IDF scores in 5 star group ------- \")\n",
    "\n",
    "text_counts_5star = count_vect.fit_transform(rev_new_5star['RT_new'])\n",
    "feat_name_5star =  count_vect.get_feature_names()\n",
    "text_tfidf_5star = tfidf_transformer.fit_transform(text_counts_5star)\n",
    "\n",
    "mean_tfidf_score_5star = np.mean(text_tfidf_5star , axis=0)\n",
    "\n",
    "mean_tfidf_score_5star_list = np.array(mean_tfidf_score_5star)[0].tolist()\n",
    "max_tfidf_5star_indices_list = f(mean_tfidf_score_5star_list,20)\n",
    "\n",
    "#print (max_tfidf_5star_indices_list)\n",
    "\n",
    "for i in range(len(max_tfidf_5star_indices_list)):\n",
    "    print ( feat_name_5star[max_tfidf_5star_indices_list[i]], mean_tfidf_score_5star_list[max_tfidf_5star_indices_list[i]] )\n",
    "    \n",
    " #   print (\"The index is:\" , max_tfidf_5star_indices_list[i], \" , the tf_idf mean score is:\" , mean_tfidf_score_5star_list[max_tfidf_5star_indices_list[i]], \"and the word is:\" , feat_name_5star[max_tfidf_5star_indices_list[i]]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62c091",
   "metadata": {},
   "source": [
    "## Study the most frequent words divided in 2 groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25327c",
   "metadata": {},
   "source": [
    "Instead, here we divide our dataset in only 2 groups: the highly (>= 3) rating and the low one (<3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Study of Most Frequent Words in Highly and Low Rated Comments\n",
    "\n",
    "#ps = PorterStemmer()\n",
    "\n",
    "#Tokenization refers to dividing the text into a sequence of words or sentences.\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def wordfreq(text, x):\n",
    "    word_dist = nltk.FreqDist(text)\n",
    "    top_N = x\n",
    "    rlst = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                        columns=['Word', 'Frequency']).set_index('Word')\n",
    "    return rlst\n",
    "\n",
    "def preprocessing(data):\n",
    "    txt = data.str.lower().str.cat(sep=' ') #1\n",
    "    words = tokenizer.tokenize(txt) #2\n",
    "    words = [w for w in words if not w in stop_words] #3\n",
    "    #words = [ps.stem(w) for w in words] #4\n",
    "    return words\n",
    "\n",
    "\n",
    "title ='Most Frequent Words in Highly Rated Comments'\n",
    "temp = rev_new['RT_new'][rev_new.Rating.astype(int) >= 3]\n",
    "\n",
    "# Bar Chart\n",
    "ax = wordfreq(preprocessing(temp), 20).plot.bar(rot=45, legend=False, figsize=(15, 5), color='g',\n",
    "                                          title=title)\n",
    "plt.ylabel('Occurrence Count')\n",
    "plt.xlabel('Most Frequent Words')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Save figure\n",
    "#fig = ax.get_figure()\n",
    "#fig.savefig('Figures/Freq_Rat_high.png')\n",
    "\n",
    "#Low Raited\n",
    "title ='Most Frequent Words in Low Rated Comments'\n",
    "temp_low = rev_new['RT_new'][rev_new.Rating.astype(int) < 3]\n",
    "\n",
    "# Bar Chart\n",
    "ax = wordfreq(preprocessing(temp_low), 20).plot.bar(rot=45, legend=False, figsize=(15, 5), color='g',\n",
    "                                          title=title)\n",
    "plt.ylabel('Occurrence Count')\n",
    "plt.xlabel('Most Frequent Words')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Save figure\n",
    "#fig = ax.get_figure()\n",
    "#fig.savefig('Figures/Freq_Rat_low.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83c7ec",
   "metadata": {},
   "source": [
    "## The most common words in general:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c34b3b1",
   "metadata": {},
   "source": [
    "We find the most 10 common words used not dividing our dataset, also using the Tf-Idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c1991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = rev_new['RT_new']\n",
    "\n",
    "# The most common words used\n",
    "freq_common = pd.Series(' '.join(preprocessing(temp)).split()).value_counts()[:10]\n",
    "print ('The most common words used:')\n",
    "print(freq_common)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13b9ac",
   "metadata": {},
   "source": [
    "### Tf-Idf: Text Review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Take the title text\n",
    "text = rev_new['RT_new']\n",
    "\n",
    "# Create the vectorizer method\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit(text)\n",
    "\n",
    "\n",
    "# summarize\n",
    "#print(text_tfidf.vocabulary_)\n",
    "#print(text_tfidf.idf_)\n",
    "\n",
    "# encode document\n",
    "vector = text_tfidf.transform(text) \n",
    "\n",
    "# summarize encoded vector\n",
    "#print(vector.shape)\n",
    "#print(vector.toarray())\n",
    "\n",
    "\n",
    "#Print the first 10 words based on tfidf score\n",
    "\n",
    "indices = np.argsort(tfidf_vec.idf_)[::1]\n",
    "features = tfidf_vec.get_feature_names()\n",
    "top_features = [features[i] for i in indices[:10]]\n",
    "print (top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d825e054",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "We try to understand the customer reviews also study the textual sentiment. \n",
    "For the sentiment analysis we extract the polarity: values nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. Also in this case, we divide the polarity in 5 groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the length of characters of review text. This also includes spaces\n",
    "rev_new['char_length_RT'] = rev_new['Review Text'].map(str).apply(len)\n",
    "print (rev_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6450154",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_new.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22c2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(rev_new, col=\"Rating\")\n",
    "ax =g.map(plt.hist, 'char_length_RT')\n",
    "\n",
    "#To save the figure\n",
    "#ax.savefig('Figures/Char_length_RT.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c6b3e1",
   "metadata": {},
   "source": [
    "#### From this analysis I can understand that the text has limit of 500 character length!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0106c3ad",
   "metadata": {},
   "source": [
    "We extract the number of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0a0e1f",
   "metadata": {},
   "source": [
    "From these plots, as expected, we note that a recommended item indicates clearly a positive sentiment in the review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, hue = \"Sentiment\", \"Rating\"\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(16, 9))\n",
    "sns.countplot(x=x, hue=hue, data=rev_new, ax=axes[0], order=[\"Strong Negative\", \"Negative\",\"Neutral\",\"Positive\", \"Strong Positive\"])\n",
    "axes[0].set_title(\"Occurence of {}\\nby {}\".format(x, hue))\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "prop_df = (rev_new[[x, hue]]\n",
    "     .reset_index(drop=True)\n",
    "     .groupby([x])[hue]\n",
    "     .value_counts(normalize=True)\n",
    "     .rename('Percentage').mul(100)\n",
    "     .reset_index()\n",
    "     .sort_values(hue))\n",
    "\n",
    "sns.barplot(x=x, y=\"Percentage\", hue=hue, data=prop_df, ax=axes[1], order=[\"Strong Negative\", \"Negative\",\"Neutral\",\"Positive\", \"Strong Positive\"])\n",
    "axes[1].set_title(\"Percentage Normalized Occurence of {}\\nby {}\".format(x, hue))\n",
    "axes[1].set_ylabel(\"% Percentage by {}\".format(hue))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3798c62",
   "metadata": {},
   "source": [
    "Like the distribution of rating, most reviews have a positive sentiment.\n",
    "From the second plot, we can notice that, from our division, the distribution for the neutral, positive and strong positive sentiment increasing when the rating number increases. Instead, the negative sentiment, as expected, has more low rating occurence (1 and 2). For the strong negative sentiment, the distribution is particular: has a peak for rating 1 and 2, as expected, but also more or less 15 % of occurence for rating 5.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_new_rec = rev_new[rev_new['Recommended IND'] == 1]\n",
    "rev_new_no_rec = rev_new[rev_new['Recommended IND'] == 0]\n",
    "\n",
    "x, hue = \"Sentiment\", \"Division Name\"\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(16, 9))\n",
    "sns.countplot(x=x, hue=hue, data=rev_new_rec, ax=axes[0],order=[\"Strong Negative\", \"Negative\",\"Neutral\",\"Positive\", \"Strong Positive\"])\n",
    "axes[0].set_title(\"Occurence of {}\\nby {} for recommended cloths\".format(x, hue))\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "prop_df = (rev_new_rec[[x, hue]]\n",
    "     .reset_index(drop=True)\n",
    "     .groupby([x])[hue]\n",
    "     .value_counts(normalize=True)\n",
    "     .rename('Percentage').mul(100)\n",
    "     .reset_index()\n",
    "     .sort_values(hue))\n",
    "\n",
    "sns.barplot(x=x, y=\"Percentage\", hue=hue, data=prop_df, ax=axes[1], order=[\"Strong Negative\", \"Negative\",\"Neutral\",\"Positive\", \"Strong Positive\"])\n",
    "axes[1].set_title(\"Percentage Normalized Occurence of {}\\nby {} for recommended cloths\".format(x, hue))\n",
    "axes[1].set_ylabel(\"% Percentage by {}\".format(hue))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(16, 9))\n",
    "sns.countplot(x=x, hue=hue, data=rev_new_no_rec, ax=axes[0],order=[\"Strong Negative\", \"Negative\",\"Neutral\",\"Positive\", \"Strong Positive\"])\n",
    "axes[0].set_title(\"Occurence of {}\\nby {} for not recommended cloths\".format(x, hue))\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "prop_df = (rev_new_no_rec[[x, hue]]\n",
    "     .reset_index(drop=True)\n",
    "     .groupby([x])[hue]\n",
    "     .value_counts(normalize=True)\n",
    "     .rename('Percentage').mul(100)\n",
    "     .reset_index()\n",
    "     .sort_values(hue))\n",
    "\n",
    "sns.barplot(x=x, y=\"Percentage\", hue=hue, data=prop_df, ax=axes[1], order=[\"Strong Negative\", \"Negative\",\"Neutral\",\"Positive\", \"Strong Positive\"])\n",
    "axes[1].set_title(\"Percentage Normalized Occurence of {}\\nby {} for not recommended cloths\".format(x, hue))\n",
    "axes[1].set_ylabel(\"% Percentage by {}\".format(hue))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5ec56",
   "metadata": {},
   "source": [
    "Here, we use the same variables, but the upper row is for recommended reviews, while the bottom row is for non-recommended reviews.\n",
    "From these plots, the 'strong negative' sentiment presents a particular feature: for the recommended reviews only the 'General Petite' is shown, while for the non-recommended only the 'General' Division Name is shown.\n",
    "Instead, the distribution of division names for the other sentiments does not seem to change depending on status of recommendation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14c117",
   "metadata": {},
   "source": [
    "## Title Analysis\n",
    "\n",
    "Here, we consider the 'Title' feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c8c9d",
   "metadata": {},
   "source": [
    "### Some Basic Feature Extraction for Title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out data removing the NaN values\n",
    "rev_new_T = rev[rev['Title'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_new_T['char_length_Title'] = rev_new_T['Title'].map(str).apply(len)\n",
    "\n",
    "\n",
    "g = sns.FacetGrid(rev_new_T, col=\"Rating\")\n",
    "g.map(plt.hist, \"char_length_Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b2dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of word of the Title\n",
    "rev_new_T['word_count_Title'] = rev_new_T['Title'].apply(lambda x: len(str(x).split(\" \")))\n",
    "print (rev_new_T[['Title','char_length_Title', 'word_count_Title']].head(10))\n",
    "\n",
    "g = sns.FacetGrid(rev_new_T, col=\"Rating\")\n",
    "g.map(plt.hist, \"word_count_Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038652b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of special characters '!'\n",
    "rev_new_T['spec_char_Title'] = rev_new_T['Title'].apply(lambda x: len([x for x in x.split() if x.endswith('!')]))\n",
    "print (rev_new_T[['Title','spec_char_Title']].head())\n",
    "\n",
    "g = sns.FacetGrid(rev_new_T, col=\"Rating\")\n",
    "g.map(plt.hist, \"spec_char_Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794841aa",
   "metadata": {},
   "source": [
    "Also, for the title, we find more exclamation points for high rating and in particular for rating 5; it means that the exclamation points express enthusiasm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1727ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_new_T[['char_length_Title', 'word_count_Title']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50480b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_new_T.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f91f46",
   "metadata": {},
   "source": [
    "### Study the most frequent words in highly and low rated title\n",
    "\n",
    "Here, we extract the most frequent words for the titles dividing the dataset in two groups: highly rated title (>= 3) and low rated title (< 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee16a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ps = PorterStemmer()\n",
    "\n",
    "#Tokenization refers to dividing the text into a sequence of words or sentences.\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def wordfreq(text, x):\n",
    "    word_dist = nltk.FreqDist(text)\n",
    "    top_N = x\n",
    "    rlst = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                        columns=['Word', 'Frequency']).set_index('Word')\n",
    "    return rlst\n",
    "\n",
    "def preprocessing(data):\n",
    "    txt = data.str.lower().str.cat(sep=' ') #1\n",
    "    words = tokenizer.tokenize(txt) #2\n",
    "    words = [w for w in words if not w in stop_words] #3\n",
    "    #words = [ps.stem(w) for w in words] #4\n",
    "    return words\n",
    "\n",
    "\n",
    "title ='Most Frequent Words in Highly Rated Titles'\n",
    "temp = rev_new_T['Title'][rev_new_T.Rating.astype(int) >= 3]\n",
    "\n",
    "# Bar Chart\n",
    "ax = wordfreq(preprocessing(temp), 20).plot.bar(rot=45, legend=False, figsize=(15, 5), color='g',\n",
    "                                          title=title)\n",
    "plt.ylabel('Occurrence Count')\n",
    "plt.xlabel('Most Frequent Words')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Save figure\n",
    "#fig = ax.get_figure()\n",
    "#fig.savefig('Figures/Freq_Rat_high.png')\n",
    "\n",
    "#Low Raited\n",
    "title ='Most Frequent Words in Low Rated Titles'\n",
    "temp_low = rev_new_T['Title'][rev_new_T.Rating.astype(int) < 3]\n",
    "\n",
    "# Bar Chart\n",
    "ax = wordfreq(preprocessing(temp_low), 20).plot.bar(rot=45, legend=False, figsize=(15, 5), color='g',\n",
    "                                          title=title)\n",
    "plt.ylabel('Occurrence Count')\n",
    "plt.xlabel('Most Frequent Words')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Save figure\n",
    "#fig = ax.get_figure()\n",
    "#fig.savefig('Figures/Freq_Rat_low.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615dec3",
   "metadata": {},
   "source": [
    "### Tf-Idf Title\n",
    "\n",
    "We find the most frequent words in general using the Tf-Idf method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748da26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of word of the Review Text\n",
    "rev_new['word_count_RT'] = rev_new['Review Text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "rev_new[['Review Text','word_count_RT']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7658735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(rev_new, col=\"Rating\")\n",
    "g.map(plt.hist, \"word_count_RT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624b61b",
   "metadata": {},
   "source": [
    "We will also extract another feature which will calculate the average word length of each review text.\n",
    "\n",
    "Here, we simply take the sum of the length of all the words and divide it by the total length of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19094422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#average word length\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "rev_new['avg_word'] = rev_new['Review Text'].apply(lambda x: avg_word(x))\n",
    "\n",
    "# Replace all the missing values in the avg_word column with the mean\n",
    "word_mean = rev_new.avg_word.mean()\n",
    "rev_new['avg_word'] = rev_new.avg_word.fillna(word_mean)\n",
    "\n",
    "rev_new[['Review Text','avg_word']].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c980b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_new[['char_length_RT', 'word_count_RT']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02829aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(rev_new, col=\"Rating\")\n",
    "g.map(plt.hist, \"avg_word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9bf2b2",
   "metadata": {},
   "source": [
    "We calculate the number of **stopwords** that can give us some extra information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41367bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "rev_new['stopwords_len'] = rev_new['Review Text'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "rev_new[['Review Text','stopwords_len']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bd24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of special characters '!'\n",
    "rev_new['spec_char'] = rev_new['Review Text'].apply(lambda x: len([x for x in x.split() if x.endswith('!')]))\n",
    "rev_new[['Review Text','spec_char']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe61388",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(rev_new, col=\"Rating\")\n",
    "g.map(plt.hist, \"spec_char\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d244ff5",
   "metadata": {},
   "source": [
    "As expected, we found more exclamation points ('!') for high Rating (4 and 5) than for low ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058af46",
   "metadata": {},
   "source": [
    "### Basic Pre-processing for Text Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make lowercase\n",
    "rev_new['RT_new'] = rev_new['Review Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "# Remove whitespaces\n",
    "rev_new['RT_new'] = rev_new['RT_new'].apply(lambda x: \" \".join(x.strip() for x in x.split()))\n",
    "    \n",
    "# Remove special characters\n",
    "rev_new['RT_new'] = rev_new['RT_new'].apply(lambda x: \"\".join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n",
    "   \n",
    "#Removing Punctuation\n",
    "rev_new['RT_new'] = rev_new['RT_new'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "rev_new['RT_new'] = rev_new['RT_new'].apply(lambda x: \" \".join(x for x in x.split() if x.isalpha()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4eacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove of Stop Words\n",
    "stop = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "rev_new['RT_new'] = rev_new['RT_new'].apply(lambda x: \" \".join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "rev_new[['Review Text','RT_new']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce839cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "rev_new['RT_new'] = rev_new['RT_new'].astype(str)\n",
    "\n",
    "rev_new[['Review Text','RT_new']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072b792",
   "metadata": {},
   "source": [
    "### Separate the dataset into 5 groups based on their rating and extract the common words for each groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d997b74",
   "metadata": {},
   "source": [
    "Here, we decide to divide our dataset in 5 groups based on the rating. After that, we calculate the 20 most frequent words for each group and also use the Tf-Idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f967860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "rev_new['sent'] = rev_new['Review Text'].apply(lambda x: pd.Series(TextBlob(x).sentiment.polarity))\n",
    "rev_new[['Review Text','sent', 'Recommended IND', 'Rating']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2135c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising polarity between recommending and non-recommending customers, then getting value counts\n",
    "g = sns.FacetGrid(rev_new, col=\"Recommended IND\", col_order=[1, 0])\n",
    "g = g.map(plt.hist, \"sent\", bins=20, color=\"g\")\n",
    "\n",
    "recommend = rev_new.groupby(['Recommended IND'])\n",
    "recommend['sent'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb7c2a",
   "metadata": {},
   "source": [
    "From the above graphs, it appears that the reviews left by customers who recommended their product gave more positive reviews (with a mean of 0.28), compared to those who did not recommend (with a mean of 0.13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0872944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 0 to 1 Decimal Score to a Categorical Variable: 5 groups\n",
    "rev_new['Sentiment'] = ''\n",
    "rev_new.loc[rev_new['sent'] > 0.6, 'Sentiment'] = 'Strong Positive'\n",
    "rev_new.loc[(rev_new['sent'] > 0.2) & (rev_new['sent'] < 0.6), 'Sentiment'] = 'Positive'\n",
    "rev_new.loc[(rev_new['sent'] < 0.2) & (rev_new['sent'] > -0.2), 'Sentiment'] = 'Neutral'\n",
    "rev_new.loc[(rev_new['sent'] < -0.2) & (rev_new['sent'] > -0.6), 'Sentiment'] = 'Negative'\n",
    "rev_new.loc[rev_new['sent'] < -0.6, 'Sentiment'] = 'Strong Negative'\n",
    "\n",
    "rev_new[['Review Text', 'sent', 'Sentiment', 'Recommended IND', 'Rating']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, hue = \"Sentiment\", \"Recommended IND\"\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(1, 2, figsize=(16, 9))\n",
    "sns.countplot(x=x, hue=hue, data=rev_new, ax=axes[0], order=[\"Strong Negative\", \"Negative\",\"Neutral\",\"Positive\", \"Strong Positive\"])\n",
    "axes[0].set_title(\"Occurence of {}\\nby {}\".format(x, hue))\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "prop_df = (rev_new[[x, hue]]\n",
    "     .reset_index(drop=True)\n",
    "     .groupby([x])[hue]\n",
    "     .value_counts(normalize=True)\n",
    "     .rename('Percentage').mul(100)\n",
    "     .reset_index()\n",
    "     .sort_values(hue))\n",
    "\n",
    "sns.barplot(x=x, y=\"Percentage\", hue=hue, data=prop_df, ax=axes[1], order=[\"Strong Negative\", \"Negative\",\"Neutral\",\"Positive\", \"Strong Positive\"])\n",
    "axes[1].set_title(\"Percentage Normalized Occurence of {}\\nby {}\".format(x, hue))\n",
    "axes[1].set_ylabel(\"% Percentage by {}\".format(hue))\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
