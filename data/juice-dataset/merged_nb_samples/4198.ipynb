{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a0b8e2",
   "metadata": {},
   "source": [
    "# Assignment Sheet 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b92e3",
   "metadata": {},
   "source": [
    "Bruce Schultz  \n",
    "bschultz@uni-bonn.de  \n",
    "  \n",
    "Miguel A. Ibarra-Arellano  \n",
    "ibarrarellano@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0518732",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_point_edges(p, sigma_distance, edge_radius):\n",
    "    \"\"\"\n",
    "    This function constructs a graph describing similarity of points in the given\n",
    "       array.\n",
    "\n",
    "    :param p: An array of shape (nPoint,2) where each row provides the\n",
    "             coordinates of one of nPoint points in the plane.\n",
    "    :param sigma_distance: The standard deviation of the Gaussian distribution used\n",
    "             to weigh down longer edges.\n",
    "    :param edge_radius: A positive float providing the maximal length of edges.\n",
    "    :return:  tuple (edge_weight,edge_indices) where edge_weight is an array of\n",
    "              length n_edge providing the weight of all produced edges and\n",
    "              EdgeIndices is an integer array of shape (n_edge,2) where each row\n",
    "              provides the indices of two pixels which are connected by an edge.\n",
    "    \"\"\"\n",
    "    # Initialize lists\n",
    "    weights = list()\n",
    "    indices = list()\n",
    "\n",
    "    # Iterate over points\n",
    "    for i in range(len(p)):\n",
    "        for j in range(i + 1, len(p)):\n",
    "\n",
    "            # If less than edge radius then store indices an weights\n",
    "            fx = ((p[i][0] - p[j][0]) ** 2) + ((p[i][1] - p[j][1]) ** 2)\n",
    "            if fx ** 0.5 < edge_radius:\n",
    "                c = np.exp(-(fx / (2 * (sigma_distance ** 2))))\n",
    "                weights.append(c)\n",
    "                indices.append((i, j))\n",
    "\n",
    "    return weights, indices\n",
    "\n",
    "\n",
    "def get_laplacian(n, weights, indices):\n",
    "    \"\"\"\n",
    "    Constructs a matrix providing the Laplacian for the given graph.\n",
    "\n",
    "    :param n: The number of vertices in the graph (resp. pixels in the image).\n",
    "    :param weights: A one-dimensional array of nEdge floats providing the weight\n",
    "             for each edge.\n",
    "    :param indices: An integer array of shape (nEdge,2) where each row provides\n",
    "             the vertex indices for one edge.\n",
    "    :return: A matrix providing the Laplacian for the given graph.\n",
    "    \"\"\"\n",
    "    # Empty matrix filled with zeros\n",
    "    adjacency = np.zeros((n, n))\n",
    "    degree = np.zeros((n, n))\n",
    "\n",
    "    # Iterate over weights\n",
    "    for k in range(len(weights)):\n",
    "        adjacency[indices[k][0], indices[k][1]] = adjacency[indices[k][1], indices[k][0]] = weights[k]\n",
    "        degree[indices[k][0], indices[k][0]] += weights[k]\n",
    "        degree[indices[k][1], indices[k][1]] += weights[k]\n",
    "\n",
    "    return degree - adjacency\n",
    "\n",
    "\n",
    "def get_fiedler_vector(laplacian):\n",
    "    \"\"\"\n",
    "    Given the Laplacian matrix of a graph this function computes the normalized\n",
    "    Eigenvector for its second-smallest Eigenvalue (the so-called Fiedler vector)\n",
    "    and returns it.\n",
    "\n",
    "    :param laplacian: Laplacian matrix\n",
    "    :return: laplacian matrix normalized by the Fiedler vector\n",
    "    \"\"\"\n",
    "\n",
    "    return np.linalg.eigh(laplacian)[1][:, 1]\n",
    "\n",
    "if (__name__ == \"__main__\"):\n",
    "    # This list of points is to be clustered\n",
    "    points = np.asarray(\n",
    "        [(-8.097, 10.680), (-3.902, 8.421), (-9.711, 7.372), (0.859, 12.859), (4.732, 11.084), (-0.594, 9.147),\n",
    "         (-4.224, 13.585), (-9.066, 11.891), (-13.181, 8.663), (-12.374, 3.983), (-11.406, -2.068), (-9.630, 2.854),\n",
    "         (-13.665, -6.667), (-15.521, -0.454), (-15.117, -6.587), (-11.970, -10.621), (-6.000, -12.799),\n",
    "         (-2.853, -14.978), (-8.501, -10.217), (2.311, -11.670), (3.441, -14.171), (5.861, -10.137), (10.138, -6.909),\n",
    "         (15.382, -5.215), (14.091, 0.675), (11.187, 3.903), (8.685, 8.502), (7.879, 11.649), (5.216, 10.680),\n",
    "         (11.025, 6.888), (13.446, 2.612), (12.962, -7.393), (8.363, -9.330), (-0.594, -0.212), (1.666, 1.401),\n",
    "         (1.424, -1.019), (-0.351, -2.552), (-2.127, 0.675), (-0.271, 2.128), (-4.743, -4.016)])\n",
    "\n",
    "    n_vertex = points.shape[0]\n",
    "\n",
    "    # Construct the graph for the points\n",
    "    edge_weight, edge_indices = get_point_edges(points, 1.0, 7.0)\n",
    "\n",
    "    # Construct the Laplacian matrix for the graph\n",
    "    laplacian = get_laplacian(n_vertex, edge_weight, edge_indices)\n",
    "\n",
    "    # Compute the Fiedler vector\n",
    "    fiedler_vector = get_fiedler_vector(laplacian)\n",
    "\n",
    "    # Show the results\n",
    "    plt.plot(list(range(0, len(fiedler_vector))), sorted(fiedler_vector), c=\"b\")\n",
    "    plt.show()\n",
    "\n",
    "    for i, point in enumerate(points):\n",
    "        if fiedler_vector[i] > -0.1:\n",
    "            plt.scatter(point[0], point[1], color=\"b\")\n",
    "        else:\n",
    "            plt.scatter(point[0], point[1], color=\"r\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df5dc7c",
   "metadata": {},
   "source": [
    "a. Plot the sorted coeffcients of the Fiedler vector. Do they make it easy to define a suitable threshold\n",
    "to obtain a clear clustering?  \n",
    "\n",
    "Yes, they help.  \n",
    "\n",
    "b.  What threshold would you choose and why? (3P)  \n",
    "\n",
    "-0.1 divides perfectly the 2 groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260720f",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from skimage import color\n",
    "from sklearn import mixture\n",
    "from scipy import ndimage, misc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gaussian_mixture_model import *\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd844a88",
   "metadata": {},
   "source": [
    "**a) Read the grayscale image brain.png, which is provided on the lecture homepage. Reduce the salt\n",
    "and pepper noise in the image using a median filter. (3P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3231a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read image into array data\n",
    "raw_img_read = plt.imread(\"brain-noisy.png\", True)\n",
    "\n",
    "# Denoising image\n",
    "mf_img = ndimage.median_filter(raw_img_read, size=5)\n",
    "\n",
    "plt.imshow(mf_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239d9b2",
   "metadata": {},
   "source": [
    "**b) Produce a binary mask that marks all pixels with an intensity greater than zero. In all further\n",
    "steps, only treat pixels within that mask. (1P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating binary mask\n",
    "binary_mask = mf_img > 0  # Any value greater than 0 (background)\n",
    "bin_masked_img = mf_img.copy()\n",
    "bin_masked_img[binary_mask] = 255  # 255 == white\n",
    "\n",
    "plt.imshow(bin_masked_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62dd209",
   "metadata": {},
   "source": [
    "**c) Plot a log-scaled histogram of the pixels within the mask. It should show how frequently different\n",
    "intensity values occur in the image. What do the peaks in this histogram represent? Hint: One\n",
    "way to and out is to create masks that highlight the pixels belonging to each peak. (4P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ff76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot values from non-background pixels on a log scaled histogram\n",
    "bins = 50\n",
    "plt.gca().set_xscale(\"log\")\n",
    "counts, pixels, bars = plt.hist(mf_img[binary_mask], np.logspace(np.log10(10), np.log10(300), bins))\n",
    "plt.xlabel(\"Pixel Value (log)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Brain image by pixel value\")\n",
    "# plt.show()  # Peaks refer to segmentation thresholds, gray/white matter and background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656b143a",
   "metadata": {},
   "source": [
    "The peaks in this plot represent the different classes within the image, specifically the different parts of the brain. Each peak shows the pixel intensity that is most associated with that brain anatomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "# Create masks for the pixel values surrounding each peak in histogram\n",
    "\n",
    "# Determine histogram peaks and the corresponding pixel value\n",
    "peak_values = []\n",
    "threshold = 75\n",
    "for i in range(len(counts)-1):\n",
    "    if counts[i] > threshold and counts[i] > counts[i-1] and counts[i] > counts[i+1]:\n",
    "        peak_values.append(pixels[i])\n",
    "\n",
    "#  Visualize image with peak pixel value locations after converting to RGB array\n",
    "masks = []\n",
    "pix_range = 40  # To give an acceptable range for pixel values\n",
    "for pix_value in peak_values:\n",
    "    masks.append(np.logical_and(pix_value+pix_range >= mf_img, mf_img >= pix_value-pix_range))\n",
    "\n",
    "peak_img = bin_masked_img.copy()\n",
    "peak_img = color.gray2rgb(peak_img)  # Convert to RGB array\n",
    "prime_colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255]]  # Define primary colors [R, G, B]\n",
    "for counter, mask in enumerate(masks):\n",
    "    if counter > 2:\n",
    "        break\n",
    "    peak_img[mask] = prime_colors[counter]\n",
    "    \n",
    "plt.imshow(peak_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0dd368",
   "metadata": {},
   "source": [
    "Note that since I only used pixel intensities that were within 40 units of each peak, some pixels in the image remain white as they were not included in the range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a3c6e4",
   "metadata": {},
   "source": [
    "**d) Now, we will use a three-compartment Gaussian Mixture Model for image segmentation: Based\n",
    "on their gray level, pixels that fall within the mask from c) should be assigned to one of three\n",
    "Gaussians, capturing corticospinal \n",
    "uid (dark), gray matter (medium), or white matter (bright).\n",
    "To start this process, initialize the parameters of a three-compartment GMM to some reasonable\n",
    "values and use them to compute the responsibilities pik of cluster k for pixel i. (4P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some GMM functions\n",
    "\n",
    "from random import randint\n",
    "from math import pi, exp, sqrt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def GMM_init(data_points, n_distributions, means_init=None):\n",
    "    \"\"\"\n",
    "    Initializes N gaussian distributions for use in GMM modeling\n",
    "    :param data_points: 2D array containing data of interesting for GMM\n",
    "    :param n_distributions: Number of clusters predicted to be found\n",
    "    :param means_init: Optional, means to start the clustering process\n",
    "    :return: Separate 1D arrays for mixing coefficients, variance values, and means\n",
    "    \"\"\"\n",
    "    mix_coeff = [1/n_distributions] * n_distributions  # Sum of pi across all clusters must = 1\n",
    "    if means_init is not None:\n",
    "        means = means_init[:, 1]  # Set initial means to user specified values\n",
    "    else:\n",
    "        # Random initialization using y values ([:,1])\n",
    "        means = [randint(min(data_points[:, 1]), max(data_points[:, 1])) for i in range(n_distributions)]\n",
    "\n",
    "    # Initialize variance to sig**2 = sum(X-mu)**2 / N\n",
    "    init_variance = sum([(data_points[i, 1]-min(means))**2 for i in range(len(data_points[:, 1]))])\n",
    "    sigma = [sqrt(init_variance/len(data_points[:, 1]))] * n_distributions\n",
    "    return mix_coeff, sigma, means\n",
    "\n",
    "# TODO implement k-means init\n",
    "\n",
    "# E-step of GMM algorithm\n",
    "def GMM_responsibilities(data_points, n_distributions, mix_coeff, sigma, means):\n",
    "\n",
    "    # Calculate gaussians\n",
    "    # GMM array has x values in first column and GMM sum value in the last column\n",
    "    GMM_array = np.empty((len(data_points[:, 1]), n_distributions + 2))\n",
    "    GMM_array[:, 0] = data_points[:, 1]  # First column is our values\n",
    "    for i in range(len(data_points[:, 1])):  # Iterate through values\n",
    "        for k in range(n_distributions):  # Iterate through clusters\n",
    "            gauss = 1/(sqrt(2*pi)*sigma[k])*exp(-((data_points[i, 1]-means[k])**2)/(2*(sigma[k]**2)))\n",
    "            GMM_array[i, k+1] = mix_coeff[k]*gauss\n",
    "        GMM_array[i][n_distributions + 1] = sum(GMM_array[i][1:n_distributions+1])  # Sum N(x|uk, sigk**2) (Gauss_dis) values\n",
    "\n",
    "    # Calculate responsibilities\n",
    "    responsibilities = np.empty((len(data_points[:, 1]), n_distributions))\n",
    "    for i in range(len(data_points[:, 1])):\n",
    "        for k in range(n_distributions):\n",
    "            responsibilities[i][k] = GMM_array[i, k+1]/GMM_array[i, n_distributions+1]\n",
    "\n",
    "    # Only responsibilities values! i (sample #) rows by k (cluster #) columns\n",
    "    return responsibilities\n",
    "\n",
    "\n",
    "# M-step of GMM algorithm\n",
    "def GMM_optimize(data_points, n_distributions, mix_coeff, sigma, means):\n",
    "\n",
    "    rho = GMM_responsibilities(data_points, n_distributions, mix_coeff, sigma, means)\n",
    "\n",
    "    # Create lists to fill with optimized values\n",
    "    opt_mix_coeff = [0] * len(mix_coeff)\n",
    "    opt_means = [0] * len(means)\n",
    "    opt_sigma = [0] * len(sigma)\n",
    "\n",
    "    # Optimize parameters\n",
    "    for k in range(n_distributions):\n",
    "        cluster_resp_sum = sum(rho[:, k])\n",
    "\n",
    "    # Mixing Coefficients\n",
    "        opt_mix_coeff[k] = cluster_resp_sum / len(data_points[:, 1])\n",
    "\n",
    "    # Means\n",
    "        mean_numerator = sum([rho[i][k]*data_points[i][1] for i in range(len(data_points[:,1]))])\n",
    "        opt_means[k] = mean_numerator/cluster_resp_sum\n",
    "\n",
    "    # Sigma\n",
    "        sig_numerator = sum([(rho[i][k]*((data_points[i][1]-means[k])**2)) for i in range(len(data_points[:, 1]))])\n",
    "        opt_sigma[k] = sqrt(sig_numerator/cluster_resp_sum)\n",
    "\n",
    "    return opt_mix_coeff, opt_sigma, opt_means, rho\n",
    "\n",
    "\n",
    "def GMM_convergence(data_points, n_distributions, iterations=25, means_init=None, only_init=False):\n",
    "    mix_coeff, sigma, means = GMM_init(data_points, n_distributions, means_init)\n",
    "\n",
    "    if only_init:\n",
    "        rho = GMM_responsibilities(data_points, n_distributions, mix_coeff, sigma, means)\n",
    "        return mix_coeff, sigma, means, rho\n",
    "\n",
    "    # Create list to track changes with every iteration\n",
    "    mix_coefficient_list = [list(mix_coeff)]\n",
    "    sigma_list = [list(sigma)]\n",
    "    means_list = [list(means)]\n",
    "\n",
    "    i = 0\n",
    "    while i < iterations:\n",
    "        mix_coeff, sigma, means, rho = GMM_optimize(data_points, n_distributions, mix_coeff, sigma, means)\n",
    "        mix_coefficient_list.append(mix_coeff)\n",
    "        sigma_list.append(sigma)\n",
    "        means_list.append(means)\n",
    "        i += 1\n",
    "    return mix_coefficient_list, sigma_list, means_list, rho\n",
    "\n",
    "\n",
    "def pixel_cluster_matcher(mask_template, cluster_assignment_list, cluster_number):\n",
    "    \"\"\"\n",
    "    Uses a mask template to determine pixel location and iterates over new mask, changing Boolean\\\n",
    "    values to false if they don't match cluster_number\n",
    "    :param mask_template: Mask_template to use to determine pixels of interest to change bool values\n",
    "    :param cluster_assignment_list: 1D array with cluster assignment for every pixel that is True in mask_template\n",
    "    :param cluster_number: Which cluster you are building this mask for\n",
    "    :return: Mask with True values for only pixels at specified cluster_number location\n",
    "    \"\"\"\n",
    "    new_mask = mask_template.copy()\n",
    "    k = 0\n",
    "    for pixel in np.nditer(new_mask, op_flags=['readwrite']):\n",
    "        if pixel[...]:\n",
    "            if cluster_assignment_list[k] != cluster_number:\n",
    "                pixel[...] = False\n",
    "            k += 1\n",
    "    return new_mask\n",
    "\n",
    "\n",
    "# TODO Make this iterate and fix functions to work together better -- use OOP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D array with pixel number (x value) and pixel intensity (y value)\n",
    "gmm_data = np.column_stack(enumerate(mf_img[binary_mask])).transpose()\n",
    "points_init = np.array([[1, 2, 3], peak_values]).transpose()\n",
    "\n",
    "# Use homemade GMM functions to predict pixel clustering using only 1 iteration and no optimization\n",
    "mix_coeff, sigma, means, responsibilities = GMM_convergence(gmm_data, 3,\n",
    "                                                            iterations=iter, means_init=points_init, only_init=True)\n",
    "\n",
    "cluster_predictions = [np.argmax(sample) for sample in responsibilities]\n",
    "cluster_probabilities = [np.amax(sample) for sample in responsibilities]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74eb73",
   "metadata": {},
   "source": [
    "**e) Visualize the responsibilities by mapping the probabilities of belonging to the CSF, gray matter,\n",
    "and white matter clusters to the red, blue, and green color channels, respectively. Please submit\n",
    "the resulting image. (3P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa276e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the binary mask image and convert to RGB\n",
    "gmm_img = bin_masked_img.copy()\n",
    "gmm_img = color.gray2rgb(gmm_img)\n",
    "\n",
    "\n",
    "# Create masks for CSF, gray/white matter then assign them color layers\n",
    "csf_mask = pixel_cluster_matcher(binary_mask, cluster_predictions, 0)\n",
    "gray_mask = pixel_cluster_matcher(binary_mask, cluster_predictions, 1)\n",
    "white_mask = pixel_cluster_matcher(binary_mask, cluster_predictions, 2)\n",
    "\n",
    "gmm_img[csf_mask] = [255, 0, 0]\n",
    "gmm_img[gray_mask] = [0, 255, 0]\n",
    "gmm_img[white_mask] = [0, 0, 255]\n",
    "\n",
    "# Multiply each value by the probability of that pixel belonging to that class (darker == less probable)\n",
    "# gmm_img[binary_mask] = [[value*cluster_probabilities[i] for value in pixel] for i, pixel in enumerate(gmm_img[binary_mask])]\n",
    "plt.imsave('GMM_notoptimization.png', gmm_img)\n",
    "plt.imshow(gmm_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a3ac6",
   "metadata": {},
   "source": [
    "**f) Use the update rules provided in the lecture to re-compute the parameters muk, sigmak, and pik. (4P)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b58827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using my homemade GMM algorithm until convergence\n",
    "iter = 30\n",
    "mix_coeff, sigma, means, responsibilities = GMM_convergence(gmm_data, 3, iterations=iter, means_init=points_init)\n",
    "\n",
    "cluster_predictions = [np.argmax(sample) for sample in responsibilities]\n",
    "cluster_probabilities = [np.amax(sample) for sample in responsibilities]\n",
    "\n",
    "# Create masks for CSF, gray/white matter then assign them color layers\n",
    "csf_mask = pixel_cluster_matcher(binary_mask, cluster_predictions, 0)\n",
    "gray_mask = pixel_cluster_matcher(binary_mask, cluster_predictions, 1)\n",
    "white_mask = pixel_cluster_matcher(binary_mask, cluster_predictions, 2)\n",
    "\n",
    "gmm_img[csf_mask] = [255, 0, 0]\n",
    "gmm_img[gray_mask] = [0, 255, 0]\n",
    "gmm_img[white_mask] = [0, 0, 255]\n",
    "\n",
    "# Multiply each value by the probability of that pixel belonging to that class (darker == less probable)\n",
    "gmm_img[binary_mask] = [[value*cluster_probabilities[i] for value in pixel] for i, pixel in enumerate(gmm_img[binary_mask])]\n",
    "plt.imsave('GMM_convergence.png', gmm_img)\n",
    "plt.imshow(gmm_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86722d",
   "metadata": {},
   "source": [
    "Here I increased the iterations to 30. When iterations > 0, then the optimizing function from above kicks in and update the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf5ad24",
   "metadata": {},
   "source": [
    "**g) Iterate the E and M steps of the algorithm until convergence. Please submit the final parameter\n",
    "values, a visualization of the final responsibilities, and your code. (3P)**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
